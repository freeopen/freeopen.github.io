<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Freeopen - 机器学习</title>
    <subtitle>7 labo.</subtitle>
    <link rel="self" type="application/atom+xml" href="/categories/ji-qi-xue-xi/atom.xml"/>
    <link rel="alternate" type="text/html" href="/"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2021-02-17T00:00:00+00:00</updated>
    <id>/categories/ji-qi-xue-xi/atom.xml</id>
    <entry xml:lang="en">
        <title>冠军方案之 宫颈癌风险智能诊断</title>
        <published>2021-02-14T00:00:00+00:00</published>
        <updated>2021-02-14T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/cervical/"/>
        <id>/blog/cervical/</id>
        
        <content type="html" xml:base="/blog/cervical/">&lt;blockquote&gt;
&lt;p&gt;赛道1冠军：deep-thinker 团队&lt;&#x2F;p&gt;
&lt;p&gt;赛道2冠军：LLLLC 队&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;通过提供大规模经过专业医师标注的宫颈癌液基薄层细胞检测数据，选手能够提出并综合运用目标检测、深度学习等方法对宫颈癌细胞学异常鳞状上皮细胞进行定位以及对宫颈癌细胞学图片分类，提高模型检测的速度和精度，辅助医生进行诊断。&lt;&#x2F;p&gt;
&lt;p&gt;宫颈癌细胞学图片采用kfb格式，每张数据在20倍数字扫描仪下获取，大小300～400M。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;chu-sai-huan-jie&quot;&gt;初赛环节&lt;&#x2F;h4&gt;
&lt;p&gt;宫颈癌细胞学图片800张，其中阳性图片500张，阴性图片300张。阳性图片会提供多个ROI区域，在ROI区域里面标注异常鳞状上皮细胞位置，阴性图片不包含异常鳞状上皮细胞，无标注。初赛讨论的异常鳞状上皮细胞主要包括四类：ASC-US(非典型鳞状细胞不能明确意义)，LSIL(上皮内低度病变)，ASC-H(非典型鳞状细胞倾向上皮细胞内高度)，HSIL(上皮内高度病变)。（特别注明：阳性图片ROI区域之外不保证没有异常鳞状上皮细胞）&lt;&#x2F;p&gt;
&lt;h4 id=&quot;fu-sai-huan-jie&quot;&gt;复赛环节&lt;&#x2F;h4&gt;
&lt;p&gt;通过线上赛的方式，不允许选手下载数据，在线完成模型训练。&lt;&#x2F;p&gt;
&lt;p&gt;复赛训练集共提供1690张数据，其中1440张包含标注，250张没有标注。1440张有标注数据在ROI区域内标注了6类异常细胞，分别是阳性类别“ASC-H”、“ASC-US”、“HSIL”、“LSIL”，和阴性类别“Candida”、“Trichomonas”。250张没有标注数据表示未见上皮内细胞病变（NILM，可以理解为整图中不含任何前述六类细胞）。复赛测试集提供350张数据，给出ROI区域内6类异常细胞的&lt;strong&gt;位置、类别和概率&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;biao-zhu-shu-ju&quot;&gt;标注数据&lt;&#x2F;h4&gt;
&lt;p&gt;一张宫颈癌细胞学图片kfb文件和对应一个标注json文件。标注json文件内容是一个list文件，里面记录了每个ROI区域的位置和异常鳞状上皮细胞的位置坐标（细胞所在矩形框的左上角坐标和矩形宽高）。类别roi表示感兴趣区域，pos表示异常鳞状上皮细胞。json标注文件示例如下：&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;[{&amp;quot;x&amp;quot;: 33842, &amp;quot;y&amp;quot;: 31905, &amp;quot;w&amp;quot;: 101, &amp;quot;h&amp;quot;: 106, &amp;quot;class&amp;quot;: &amp;quot;pos&amp;quot;},
{&amp;quot;x&amp;quot;: 31755, &amp;quot;y&amp;quot;: 31016, &amp;quot;w&amp;quot;: 4728, &amp;quot;h&amp;quot;: 3696, &amp;quot;class&amp;quot;: &amp;quot;roi&amp;quot;},
{&amp;quot;x&amp;quot;: 32770, &amp;quot;y&amp;quot;: 34121, &amp;quot;w&amp;quot;: 84, &amp;quot;h&amp;quot;: 71, &amp;quot;class&amp;quot;: &amp;quot;pos&amp;quot;},
{&amp;quot;x&amp;quot;: 13991, &amp;quot;y&amp;quot;: 38929, &amp;quot;w&amp;quot;: 131, &amp;quot;h&amp;quot;: 115, &amp;quot;class&amp;quot;: &amp;quot;pos&amp;quot;},
{&amp;quot;x&amp;quot;: 9598, &amp;quot;y&amp;quot;: 35063, &amp;quot;w&amp;quot;: 5247, &amp;quot;h&amp;quot;: 5407, &amp;quot;class&amp;quot;: &amp;quot;roi&amp;quot;},
{&amp;quot;x&amp;quot;: 25030, &amp;quot;y&amp;quot;: 40115, &amp;quot;w&amp;quot;: 250, &amp;quot;h&amp;quot;: 173, &amp;quot;class&amp;quot;: &amp;quot;pos&amp;quot;}]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;sai-dao-yi-suan-fa-sai-dao&quot;&gt;赛道一: 算法赛道&lt;&#x2F;h4&gt;
&lt;p&gt;用常规机器学习算法得出结果。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;sai-dao-er-vnnimo-xing-liang-hua&quot;&gt;赛道二: VNNI模型量化&lt;&#x2F;h4&gt;
&lt;p&gt;由于病理图像输入尺寸非常大，通常可以达到几G几十亿个像素，传统的NvidiaGPU无法容纳更多的全局图像信息，并且低效的推理过程。本次大赛将由intel支持，参赛者可以摆脱GPU显存限制，验证intel VNNI在超高分辨率病理图像上的工程效率。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ping-gu-zhi-biao&quot;&gt;评估指标&lt;&#x2F;h4&gt;
&lt;p&gt;采用目标检测任务常用的mAP（mean Average Precision）指标作为本次宫颈癌肿瘤细胞检测的评测指标。我们采用两个IoU阈值（0.3，0.5）分别来计算AP，再综合平均作为最终的评测结果。&lt;&#x2F;p&gt;
&lt;p&gt;赛道二的评价指标，mAP@0.5 和 QPS，即精度和速度&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-fen-xi&quot;&gt;数据分析&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;数据集目标尺度差异较大，最大最小可相差将近十万倍。比如，大目标面积可达5500x4000像素，小的只有10x10像素。因此，要求模型具有较好的多尺度检测能力。&lt;&#x2F;li&gt;
&lt;li&gt;目标宽高比主要集中在0.5～2的区间，但仍存在一定数量的极端目标。因此，将增加anchor设计难度，也较为依赖特定的先验知识。&lt;&#x2F;li&gt;
&lt;li&gt;图像尺寸非常大，背景复杂。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;sai-dao-yi-fang-an&quot;&gt;赛道一方案&lt;&#x2F;h2&gt;
&lt;p&gt;没有采用常用的anchor-based模型，而是选择了非常契合本次赛题特点的anchor-free模型RepPoints。&lt;&#x2F;p&gt;
&lt;p&gt;RepPoints(ResNeXt101 + FPN + SE + DCN)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;cervical&#x2F;.&#x2F;20210214113701.jpg&quot; alt=&quot;20210214113701&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zai-xian-sui-ji-cai-jian&quot;&gt;在线随机裁剪&lt;&#x2F;h3&gt;
&lt;p&gt;随机选择输入图片中的一个目标，围绕目标随机切出边长在768~2048范围内的子图，然后缩放至边长为1024后，再送进网络。若目标边长超过了范围，则将目标与少量背景直接切出，再进行缩放。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;reppoints-mo-xing&quot;&gt;RepPoints 模型&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;核心： 物体表示上采用&lt;strong&gt;点集&lt;&#x2F;strong&gt;来替代传统边界框&lt;&#x2F;li&gt;
&lt;li&gt;边界框只提供粗糙定位，而RepPoints可自适应地分布于物体重要的局部语义区域，可提供更加细致的几何描述，有利于目标特征提取&lt;&#x2F;li&gt;
&lt;li&gt;采用&lt;strong&gt;中心点代替anchor&lt;&#x2F;strong&gt;作为初始时目标表示方式，相比传统Anchor机制，中心点更易覆盖定位二维的假设空间，无须依赖尺度和宽高比设置&lt;&#x2F;li&gt;
&lt;li&gt;其他各点可由中心点加上预测的偏移量计算得到，并自适应地分布于目标重要语义区域&lt;&#x2F;li&gt;
&lt;li&gt;为了利用仍为边界框的标注，训练时将点集转为边界框，从而计算目标定位的损失&lt;&#x2F;li&gt;
&lt;li&gt;对比其他anchor-free模型，不需要额外监督，自顶向下&lt;&#x2F;li&gt;
&lt;li&gt;通过两次预测偏移量，对目标表示中各点位置进行优化，最终获得精确定位&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;shi-yan-jie-guo-ji-mo-xing-rong-he&quot;&gt;实验结果及模型融合&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;cervical&#x2F;.&#x2F;20210214120840.jpg&quot; alt=&quot;20210214120840&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;cervical&#x2F;.&#x2F;20210214120902.jpg&quot; alt=&quot;20210214120902&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sai-dao-er-fang-an&quot;&gt;赛道二方案&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;shu-ju-zeng-qiang&quot;&gt;数据增强&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;从ROI中“在线”随机裁剪 （Online crop）&lt;&#x2F;li&gt;
&lt;li&gt;随机翻转（Random flip）&lt;&#x2F;li&gt;
&lt;li&gt;移动标注框（Shift GT），采用cv2.inpaint进行填补
&lt;img src=&quot;.&#x2F;shift-gt.png&quot; alt=&quot;shift-gt&quot; style=&quot;zoom:50%;&quot; &#x2F;&gt;&lt;&#x2F;li&gt;
&lt;li&gt;背景替换（Replace BG），利用阴性图片作物背景，并染色剂归一化
&lt;img src=&quot;.&#x2F;replace-bg.png&quot; alt=&quot;replace-bg&quot; style=&quot;zoom:50%;&quot; &#x2F;&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;zheng-ti-jia-gou&quot;&gt;整体架构&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;ji-yu-openvinode-liang-hua-tui-li-jia-gou&quot;&gt;基于OpenVINO的量化推理架构&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;cervical&#x2F;.&#x2F;openvino.png&quot; alt=&quot;openvino&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;模型结构：&lt;&#x2F;p&gt;
&lt;p&gt;主模型为RetinaNet ， 用开源的 imageNet 预训练模型初始化。&lt;&#x2F;p&gt;
&lt;p&gt;比较两种类型的backbone:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Res50-FPN256-Head256&lt;&#x2F;li&gt;
&lt;li&gt;MbV2-FPN128-Head64, 相较于上面的Res50 约12倍加速&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;mo-xing-xun-lian&quot;&gt;模型训练&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;xun-lian-ce-lue&quot;&gt;训练策略&lt;&#x2F;h4&gt;
&lt;p&gt;超参数：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;学习率：余弦下降，初始学习率0.01， 终止学习率为0.00001&lt;&#x2F;li&gt;
&lt;li&gt;Batch size: 8&lt;&#x2F;li&gt;
&lt;li&gt;预训练模型： ImageNet，训练时不固定BN参数&lt;&#x2F;li&gt;
&lt;li&gt;Epoch: 100&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;裁剪策略：&lt;&#x2F;p&gt;
&lt;p&gt;从ROI中裁剪1600 x 1600，在缩小到 800 x 800，这样可以增加标注框的数量，提高训练效率，在推理时也可以减少滑窗数量。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;liang-hua-ce-lue&quot;&gt;量化策略&lt;&#x2F;h4&gt;
&lt;p&gt;OpenVINO的量化工具，该工具的后两个步骤是将一些INT8层切换回FP32，用于提升acc，实验中发现这两个步骤对我们的模型不起作用，精度损失仍然很大。通过经验化的方法，我们发现FPN部分对量化比较敏感，因此在量化时不对FPN部分进行量化。&lt;&#x2F;p&gt;
&lt;p&gt;校验选择300张训练图片，除FPN部分的卷积层外，其余卷积层全部量化。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mo-xing-tui-li&quot;&gt;模型推理&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;tu-pian-du-qu-ji-yu-chu-li&quot;&gt;图片读取及预处理&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;图片读取部分我们采用了GDAL库，发现跟OpenCV相比有接近2倍的性能提升。&lt;&#x2F;li&gt;
&lt;li&gt;多个子进程同时读取图片，存放到共享队列中。&lt;&#x2F;li&gt;
&lt;li&gt;丢弃边界像素&lt;&#x2F;li&gt;
&lt;li&gt;裁剪1600x1600缩小到800x800&lt;&#x2F;li&gt;
&lt;li&gt;无重叠滑窗&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;qian-xiang-zhi-xing&quot;&gt;前向执行&lt;&#x2F;h4&gt;
&lt;p&gt;采用OpenVINO的异步模式&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;发起执行请求好，控制权交还回主程序，分摊数据读取和后处理的时间。其中子进程负责图片的读取、裁剪、缩放、拼batch 等数据操作，处理完的数据存放到共享队列中；&lt;&#x2F;li&gt;
&lt;li&gt;执行完成后，通过回调函数通知主程序。主进程从共享队列读取数据，负责模型推理、后处理操作。（使用 “生产者-消费者”模式，采用共享队列实现数据的通信）&lt;&#x2F;li&gt;
&lt;li&gt;可并发多个 infer request
&lt;img src=&quot;&#x2F;blog&#x2F;cervical&#x2F;20210214165514.jpg&quot; alt=&quot;20210214165514&quot; &#x2F;&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;shi-yan-jie-guo&quot;&gt;实验结果&lt;&#x2F;h4&gt;
&lt;p&gt;map0.5: 33.54%, 推理总时长：24s。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 X光限制品监测</title>
        <published>2021-02-14T00:00:00+00:00</published>
        <updated>2021-02-14T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/x-cargo/"/>
        <id>/blog/x-cargo/</id>
        
        <content type="html" xml:base="/blog/x-cargo/">&lt;blockquote&gt;
&lt;p&gt;冠军：YuanXu&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;包裹X光限制品监测作为日常包裹物流行业及安防行业的重要环节，承担着防止易燃易爆等危险品进入货运渠道，管理刀具等特殊货运物品，监测毒品等国家重点违禁品偷运等工作。随着线上购物的普及和快速发展，线上物流包裹数量已经远超人工可以处理的范围，给物流包裹监管带来了巨大挑战。&lt;&#x2F;p&gt;
&lt;p&gt;针对给出的限制品种类，利用X光图像及标注数据，研究开发高效的计算机视觉算法，监测图像是否包含危险品及其大致位置。通过自动化监测包裹携带品算法，降低漏检风险及误报率，提升危险品管理效率。&lt;&#x2F;p&gt;
&lt;p&gt;限制品包括：铁壳打火机、黑钉打火机、刀具、电池电容以及剪刀五类（类别id依次从1到5）。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ping-gu-zhi-biao&quot;&gt;评估指标&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;评测方式采用计算 &lt;code&gt;box mAP&lt;&#x2F;code&gt;的方式，对 &lt;code&gt;IoU = 0.5:0.05:0.95&lt;&#x2F;code&gt;，分别计算&lt;code&gt;mAP&lt;&#x2F;code&gt;，再做平均得到最后的&lt;code&gt;mAP&lt;&#x2F;code&gt;。&lt;&#x2F;li&gt;
&lt;li&gt;单个模型整体大小需不超过&lt;code&gt;600MB&lt;&#x2F;code&gt;（即不超过&lt;code&gt;VGG19&lt;&#x2F;code&gt;大小），模型不得超过&lt;code&gt;2&lt;&#x2F;code&gt;个。&lt;&#x2F;li&gt;
&lt;li&gt;响应时间越快越好&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;wen-ti-fen-xi&quot;&gt;问题分析&lt;&#x2F;h2&gt;
&lt;p&gt;比赛任务是经典的图像语义分割（semantic-segmentation）的问题，简单说就是要在像素级别将前景类别标识出来。研究kaggle上的几个图像语义分割的比赛，发现Unet和Mask-RCNN的成绩最好。&lt;&#x2F;p&gt;
&lt;p&gt;因为时间原因，最后选择 Unet。因为Unet能直接输出与图像1: 1的mask，且超参数少，属于端到端网络。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h2&gt;
&lt;p&gt;采用经典Unet模型。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;resnet 做 encoder&lt;&#x2F;li&gt;
&lt;li&gt;将各个decoder的输出cat在一起，作为最终的输出特征&lt;&#x2F;li&gt;
&lt;li&gt;除了输出前景物体的mask， 还单独输出物体的边沿。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;zhang-fen-dian&quot;&gt;涨分点&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;shu-ju-zeng-qiang&quot;&gt;数据增强&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;常用的图像数据增强，包括旋转、翻转、颜色、噪声、形变等。&lt;&#x2F;li&gt;
&lt;li&gt;考虑到X光的穿透性，将没有危险品的图片和有危险品的图片进行合成（blend操作）。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;kuai-su-xun-lian&quot;&gt;快速训练&lt;&#x2F;h3&gt;
&lt;p&gt;训练速度直接决定了开发的迭代速度和实验的总次数。&lt;&#x2F;p&gt;
&lt;p&gt;在训练的初期使用比较小的图像作为输入，然后再使用较大的图像作为输入。这个过程就像人学习一样，先从简单的、粗略的开始学起，然后在学习复杂的、精细的，这样最后网络收敛会更快、更好。具体为先训练128p的图像，再在原模型上训练256p的图像。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;rong-he-ce-lue&quot;&gt;融合策略&lt;&#x2F;h3&gt;
&lt;p&gt;因为数据集比较小，即使使用了各种数据增强技术，训练使用以rsenet154作为encoder这样大网络，选取一个snapshot作为最终模型的参数还是会有过拟合的风险，选取多个snapshot使用参数均值的方法对模型参数进行融合可以提高模型的泛化能力。&lt;&#x2F;p&gt;
&lt;p&gt;但是传统的方法是对一个模型进行多次训练来取得多个snapshot，这会需要很多的计算时间。根据不同评价标准选择融合的候选参数的方法，也就是选择最小验证loss，最大mIoU，和最小训练loss的三个模型参数进行融合。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ban-jian-du-xue-xi&quot;&gt;半监督学习&lt;&#x2F;h3&gt;
&lt;p&gt;使用已训练的模型在测试数据上的结果作为训练数据，来达到增加数据集、进而提高模型精度的方法。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ce-shi-zeng-qiang&quot;&gt;测试增强&lt;&#x2F;h3&gt;
&lt;p&gt;在推理时，通过对图像的旋转和翻转，并对结果取平均也能提高精度。但是这样推理测试的速度慢了8倍，考虑到数据中有很多图片没有危险品，而测试增强对于这些图片没有改进，所以避免对这些图片多次测试可大幅提高测试速度。代码改动也很少。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;zong-jie&quot;&gt;总结&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;端到端的一体化网络设计,方便使用不同大小的预训练网络&lt;&#x2F;li&gt;
&lt;li&gt;使用逐步增加训练图像大小等方法,提高训练速度和精度&lt;&#x2F;li&gt;
&lt;li&gt;使用数据增强、模型参数均值、半监督学习的方法提高模型泛化能力&lt;&#x2F;li&gt;
&lt;li&gt;使用测试增强提高结果精度(并优化)&lt;&#x2F;li&gt;
&lt;li&gt;一些实验结果
&lt;img src=&quot;&#x2F;blog&#x2F;x-cargo&#x2F;.&#x2F;20210214174954.jpg&quot; alt=&quot;20210214174954&quot; &#x2F;&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 面料剪裁利用率优化</title>
        <published>2021-02-13T00:00:00+00:00</published>
        <updated>2021-02-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/cloth-cut/"/>
        <id>/blog/cloth-cut/</id>
        
        <content type="html" xml:base="/blog/cloth-cut/">&lt;blockquote&gt;
&lt;p&gt;冠军：行星防御理事会 团队（乔德平等）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;面料切割利用率的提升是纺织行业长期追求的目标。如何提升面料切割利用率，既是企业生产精益化的难点，也是痛点。在切割之前，需要确定多个零件在面料上的位置和角度，再充分利用零件在形状上的互补特征，对零件排布的方式进行优化。面料切割问题的特性，是零件存在多种尺寸、形状，比如用作衬衫制作的袖子、后背等零件，用来切割的布匹本身存在多类瑕疵，如破洞、折皱、漏纱等，在排版中需要避开。此外，某些订单，对零件存在个性化排版需求，因此在下料环节中，需要依照订单要求进行排版下料。当前纺织行业布匹原材料的成本占到40%左右，价值较高。&lt;&#x2F;p&gt;
&lt;p&gt;本赛场聚焦面料剪裁利用率优化，要求选手研究开发高效可靠的算法，在较短时间范围内计算获得高质量可执行的排版结果，减少切割中形成的边角废料，提升面料切割利用率，减少计划时间、提高工作效率和避免人工计算的失误，提升价值降低成本。&lt;&#x2F;p&gt;
&lt;p&gt;在规则面料的情况下，满足零件旋转角度、零件最小间距、最小边距的约束，解决以下两类问题：&lt;&#x2F;p&gt;
&lt;p&gt;初赛赛题：基于所给零件，进行面料排版加工，耗料长度最短，面料利用率最高；&lt;&#x2F;p&gt;
&lt;p&gt;复赛赛题：在问题一的基础上，避开瑕疵区域面料加工，耗料长度最短，面料利用率最高。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ling-jian-shu-ju&quot;&gt;零件数据&lt;&#x2F;h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;编号&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;列名&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;说明&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;示例&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;1&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;下料批次号&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;Primary key&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;L0001&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;2&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;零件号&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;Primary key&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;s000001&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;3&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;数量&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;1&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;4&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;外轮廓&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;[[1420.0, 5998.6], [1420.0, 6062.8], [2183.1, 6062.8],[2183.1, 5998.6], [1420.0, 5998.6]]&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;5&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;允许旋转角度&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;逆时针旋转角度&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;0,90,180,270&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;6&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;面料号&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;M0001&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;注：外轮廓曲线数据均离散化为点坐标序列；所有尺寸的单位为毫米(mm).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mian-liao-shu-ju-shuo-ming&quot;&gt;面料数据说明&lt;&#x2F;h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;编号&lt;&#x2F;th&gt;&lt;th&gt;列名&lt;&#x2F;th&gt;&lt;th&gt;说明&lt;&#x2F;th&gt;&lt;th&gt;示例&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;面料号&lt;&#x2F;td&gt;&lt;td&gt;Primary key&lt;&#x2F;td&gt;&lt;td&gt;M0001&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;面料规格&lt;&#x2F;td&gt;&lt;td&gt;规则（矩形）面料，长度x宽度（单位：mm）&lt;&#x2F;td&gt;&lt;td&gt;10000x100&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;瑕疵区域&lt;&#x2F;td&gt;&lt;td&gt;瑕疵均为圆形区域，标注方式为圆形中心、圆形半径。比如[[2000,400],80]，即圆形中心坐标点为[2000,400]，半径为80。坐标系的原点为面料的左下角（参考“约束说明“第（7）条说明）&lt;&#x2F;td&gt;&lt;td&gt;[[[2000,400],80], [[1000,1200],50], ⋯]&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;零件间最小间距&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;最小边距&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;注：瑕疵区域均为圆形；所有尺寸的单位为毫米(mm)。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;yue-shu-shuo-ming&quot;&gt;约束说明&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;排样规则&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;1）排版的零件不能超出面料的可行区域；&lt;&#x2F;p&gt;
&lt;p&gt;2）排版零件互不重叠；&lt;&#x2F;p&gt;
&lt;p&gt;3）零件按批次，在同一面料上排版；&lt;&#x2F;p&gt;
&lt;p&gt;4）面料可能存在多个长宽度规格，如宽度为900mm、1000mm等、长度为10000mm、12000mm等；&lt;&#x2F;p&gt;
&lt;p&gt;5）允许用户设置切边预留量，如面料四边各预留5mm（最小边距）；切割零件间预留量5mm（最小间距）；&lt;&#x2F;p&gt;
&lt;p&gt;6）某些零件存在旋转角度上的要求，比如零件纹理方向必须保持一致；旋转角度为0表示，零件不允许发生旋转，必须原样放在面料上，面料的放置方向为面料窄边（宽度）在垂直方向，面料宽边（长度）在水平方向；旋转角度为90表示允许零件逆时针旋转90度。&lt;&#x2F;p&gt;
&lt;p&gt;7）切割零件需要避开面料上的瑕疵，瑕疵均为圆形区域，标注方式为圆形中心、圆形半径，坐标系的原点为面料的左下角（参考“数据说明”第（2）条“面料数据说明”），面料的放置方向为面料窄边（宽度）在垂直方向，面料宽边（长度）在水平方向；瑕疵与零件间间距视同零件间间距，即，如果零件间间距（最小距离）为5mm，零件与瑕疵的间距（最小距离）也为5mm。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ping-gu-zhi-biao&quot;&gt;评估指标&lt;&#x2F;h3&gt;
&lt;p&gt;决赛总分 = 0.3∗&lt;em&gt;B&lt;&#x2F;em&gt;榜成绩 + 0.4∗&lt;em&gt;C&lt;&#x2F;em&gt;榜成绩 + 0.3∗现场答辩&lt;&#x2F;p&gt;
&lt;p&gt;其中：&lt;&#x2F;p&gt;
&lt;p&gt;B榜成绩 =（0.5∗批次1面料利用率+0.5∗批次2面料利用率）∗100&lt;&#x2F;p&gt;
&lt;p&gt;C榜成绩 = 权重参数1∗面料利用率 − 权重参数2∗计算时间分值&lt;&#x2F;p&gt;
&lt;p&gt;面料利用率 = 一个批次包含的零件总面积&#x2F;消耗的面料总面积&lt;&#x2F;p&gt;
&lt;p&gt;计算时间分值 = f(一个批次排版的平均计算时间)&lt;&#x2F;p&gt;
&lt;p&gt;权重参数1 = 100.0
权重系数2 = 1.0&lt;&#x2F;p&gt;
&lt;h2 id=&quot;fang-an-si-lu&quot;&gt;方案思路&lt;&#x2F;h2&gt;
&lt;p&gt;问题： 二维不规则多边形放置&lt;&#x2F;p&gt;
&lt;p&gt;最先考虑山寨开源的svgnest，发现初赛结束前可能搞不出来。&lt;&#x2F;p&gt;
&lt;p&gt;又考虑NFP，但计算太复杂，初赛结束前可能做不出来。&lt;&#x2F;p&gt;
&lt;p&gt;按像素暴力枚举实现左底法，原理和实现都很简单，但性能和内存可能惨不忍睹。&lt;&#x2F;p&gt;
&lt;p&gt;预期纯左底优化到1小时内出个解就行，后来优化到1秒内，就觉得贪心暴力枚举也可以试试了。&lt;&#x2F;p&gt;
&lt;p&gt;复赛开始前把暴力贪心实现了，然后主要精力花在像素法的性能提高上，这个地方足够快以后，后面的贪心算法就可以暴力按像素枚举最佳位置了。&lt;&#x2F;p&gt;
&lt;p&gt;贪心算法原理和实现都很简单，缺陷也很明显，很容易挂在小规模或者不太随机的数据集上，比赛期间也没解决这个问题，以后有时间慢慢优化。&lt;&#x2F;p&gt;
&lt;p&gt;算法中还包含了遗传算法调优的部分，这部分直接参考了svgnest，只有很小的调整，提升效果不大。&lt;&#x2F;p&gt;
&lt;p&gt;遗传调优这部分直接删掉也行，效果略微降一点，时间可以降到3分钟内，内存降到几十兆。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen：冠军选手的思考轨迹，个人认为有很高学习价值，感谢冠军选手这么细致的分享。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;suan-fa-she-ji&quot;&gt;算法设计&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;第一部分：基于高精度像素法实现多边形相交检测（性能接近NFP且初始化时间很小，纯左底法单核总时间在1秒内出结果）&lt;&#x2F;li&gt;
&lt;li&gt;第二部分：贴合度+贪心算法得到初始解（L0004约86秒到85.7，L0005约170秒到85.1）&lt;&#x2F;li&gt;
&lt;li&gt;第三部分：左底+遗传算法持续迭代优化（约60秒左右接近最终解，利用率提升约0.4%）&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;ji-yu-gao-jing-du-xiang-su-fa-shi-xian-duo-bian-xing-xiang-jiao-jian-ce&quot;&gt;基于高精度像素法实现多边形相交检测&lt;&#x2F;h3&gt;
&lt;p&gt;像素法的基本原理：将不规则多边形零件及面料像素化（初始化）；检测零件的像素与已放置的像素是否有重叠（相交检测）；将零件的像素放到面料对应的像素上（放置）。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;zuo-di-fa&quot;&gt;左底法&lt;&#x2F;h4&gt;
&lt;p&gt;对每个零件，从左下角像素开始，逐像素上移直到与已放置像素无重叠为止。如果失败，向右移动一个像素并重复该过程。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;xiang-su-fa-de-jing-du-xiang-guan-wen-ti&quot;&gt;像素法的精度相关问题&lt;&#x2F;h4&gt;
&lt;p&gt;像素法将浮点数转换成了整数，必然会有精度丢失。为了保证间距边距要求，在多边形扩张时通常要将像素向上取整，这就造成了一定的浪费，通常为0到1像素大小。通常精度越低，浪费越大。&lt;&#x2F;p&gt;
&lt;p&gt;无论多高的精度，即使到原子级，都是有误差的；&lt;&#x2F;p&gt;
&lt;p&gt;为了减小浪费，通常需要提高精度，高精度往往意味着低性能。&lt;&#x2F;p&gt;
&lt;p&gt;本文实现的像素精度为0.1mm，对于面料即16000*200000=32亿像素。如果采用位图，仅面料就需要32亿像素&#x2F;8=4亿字节，约380M。如果全局搜索采用遗传算法之类的算法，内存消耗会再增加数十倍，性能也比较低。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ji-yu-sao-miao-xian-de-xiang-su-fa&quot;&gt;基于扫描线的像素法&lt;&#x2F;h4&gt;
&lt;p&gt;基于扫描线实现像素法，即将零件和面料横向和纵向的连续像素使用线段来表示。零件横向或纵向扫描线一般在1～2个线段，零件扫描线根数一般在几百到12000。&lt;&#x2F;p&gt;
&lt;p&gt;面料初始状态横向和纵向都是1个线段，在零件全部放置后，则纵向10个线段左右，最多20万根扫描线，一般内存消耗在20M左右；横向扫描线数量要高一些，但横向的扫描线只有16000根，内存消耗和纵向差不多。&lt;&#x2F;p&gt;
&lt;p&gt;纯左底法的内存消耗估计在20M左右，性能估计也会有数量级的提升。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;sao-miao-xian-xiang-jiao-jian-ce-de-hui-su-wen-ti&quot;&gt;扫描线相交检测的回溯问题&lt;&#x2F;h4&gt;
&lt;p&gt;左底法扫描线的基础版本，是对于零件的每条扫描线可以直接上移若干像素保证该条扫描线可放置或者直接失败。该方案当下一条扫描线需要上移时，需要回过头来从零件的第一根扫描线重新检测是否相交，这样会有大量的回溯检测，性能较低。&lt;&#x2F;p&gt;
&lt;p&gt;改进方法一（记录最小移动距离）：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;在将每条扫描线放置后，计算这条扫描线还可以继续移动多少像素，并记录所有扫描线中最小的移动长度。&lt;&#x2F;li&gt;
&lt;li&gt;每次放置扫描线时，如果需要移动扫描线，那么移动后就从剩余最小移动长度中扣除本次移动的长度，如果不能扣除了，才需要从第一根扫描线重新检测。&lt;&#x2F;li&gt;
&lt;li&gt;采用该方法后，纯左底法单核需要约120秒。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;改进方法二（调整扫描顺序，尽早失败，提前回溯）：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;考虑零件的几何连续性，对于扫描线相交检测，如果某条扫描线与已放置扫描线不相交，那么与该扫描线相邻的扫描线有较大几率不相交，反之同理。即检测一条扫描线后再检测相邻的一条扫描线，结果会大概率相同。&lt;&#x2F;li&gt;
&lt;li&gt;连续放置成功多条扫描线后，如果放置失败，有可能需要上移或者右移并重新扫描。如果我们能尽早检测到放置失败，就能尽可能避免这种回溯扫描。&lt;&#x2F;li&gt;
&lt;li&gt;不连续扫描的方案，以零件坐标的二进制表示的尾数0的数量排序，如某零件有100根线，检测顺序为 64-32-96-16-48-80 ...&lt;&#x2F;li&gt;
&lt;li&gt;一般检测几根到几十根线就能确定当前X坐标上垂直方法是否可放置，以决定放置成功还是继续右移。这时，左底单核从120秒减少到0.8秒，加上初始化0.15秒，总时间在1秒内。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;ji-yu-sao-miao-xian-de-duo-bian-xing-bian-yuan-kuo-zhang&quot;&gt;基于扫描线的多边形边缘扩张&lt;&#x2F;h4&gt;
&lt;p&gt;零件间距可以通过边缘扩张来处理，对于像素法的边缘扩张，对每个顶点画圆，将预先计算的扫描线叠加到多边形上；对每个线段向平移构成平行四边形，画该平行四边形，将扫描线叠加到多边形上。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;xiang-su-fa-gai-jin-xiao-jie&quot;&gt;像素法改进小结&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;基于扫描线实现像素法，内存从1G降到20M&lt;&#x2F;li&gt;
&lt;li&gt;记录最小剩余移动距离，120秒出结果&lt;&#x2F;li&gt;
&lt;li&gt;调整扫描顺序，0.8秒出结果&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;tie-he-du-tan-xin-suan-fa&quot;&gt;贴合度+贪心算法&lt;&#x2F;h3&gt;
&lt;p&gt;贪心算法的主要原理：每次放置都从所有零件中跳出最贴合的来放置。&lt;&#x2F;p&gt;
&lt;p&gt;但由于零件数量较大，每次都从所有零件中来选择的话，性能较低，所以我们每次只从其中一部分来选择最贴合的零件。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;tan-xin-suan-fa-zheng-ti-bu-zou&quot;&gt;贪心算法整体步骤&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;将所有零件按包围盒面积倒排（按包围盒面积倒排主要是将那些面积不大但形状奇特的零件也排到前面）&lt;&#x2F;li&gt;
&lt;li&gt;对前N(N=64)个零件计算贴合度score=ShapeBestFitScore(shape)，选择最贴合的零件放置到面料上&lt;&#x2F;li&gt;
&lt;li&gt;反复执行上一步骤直到所有零件全部放置完&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;ji-yu-sao-miao-xian-tie-he-ju-chi-de-tie-he-du-gong-shi&quot;&gt;基于扫描线贴合距离的贴合度公式&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;总体上离得越近分数越高，离的越远分数越低，但分数最低的位置应当在距离刚好窄到较难放置其他的零件的地方，采用统计剩余零件的大小来计算这个最低分的距离值&lt;&#x2F;li&gt;
&lt;li&gt;其他相关积分加成
&lt;ul&gt;
&lt;li&gt;零件面积大的有加成，我们通过远距离分数不为0来实现&lt;&#x2F;li&gt;
&lt;li&gt;横向的长度较长的分数有加成，我们通过垂直方向的分数加成来实现&lt;&#x2F;li&gt;
&lt;li&gt;超出当前最右侧的有惩罚。超出越大，惩罚越大，最大不超过自身的长度（动态计算该惩罚时，可以让四份数据都上85.2，但最高的只能到85.5）&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;zuo-di-yi-chuan-suan-fa&quot;&gt;左底+遗传算法&lt;&#x2F;h3&gt;
&lt;p&gt;使用遗传算法对左底的零件放置顺序进行调整。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;遗传算法在初始解本身已经比较好的情况下，再全局提升会比较缓慢&lt;&#x2F;li&gt;
&lt;li&gt;使用贪心算法得到的初始解最后放置的一部分零件效果往往不够好，将这部分零件取出来继续优化&lt;&#x2F;li&gt;
&lt;li&gt;面积小的零件就像润滑剂，也拿出来一起参与迭代优化&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;方法&lt;&#x2F;th&gt;&lt;th&gt;效果&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;纯左底改为左底或左上&lt;&#x2F;td&gt;&lt;td&gt;+0.0%～0.2%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;旋转的选择通过直接比较2x+y 取小的&lt;&#x2F;td&gt;&lt;td&gt;搜索空间大幅下降&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;允许零件向右滑动一小段距离&lt;&#x2F;td&gt;&lt;td&gt;提升0.x%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;逐步放大参与优化的零件数&lt;&#x2F;td&gt;&lt;td&gt;略微提升&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;zong-jie&quot;&gt;总结&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;xiang-su-fa-de-you-dian&quot;&gt;像素法的优点&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;原理和实现很简单&lt;&#x2F;li&gt;
&lt;li&gt;计算也很简单，主要是加减法，少量的乘除法和三角函数&lt;&#x2F;li&gt;
&lt;li&gt;可以处理任意形状，包括曲线边缘&lt;&#x2F;li&gt;
&lt;li&gt;不需要简化顶点数&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;xiang-su-fa-de-que-dian&quot;&gt;像素法的缺点&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;处理超高精度的放置需要解决固有精度问题&lt;&#x2F;li&gt;
&lt;li&gt;一些几何手段不容易利用，比如法向，切线等概念&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;tan-xin-suan-fa-de-you-que-dian&quot;&gt;贪心算法的优缺点&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;原理和实现很简单，能较快的速度出个结果&lt;&#x2F;li&gt;
&lt;li&gt;容易陷入局部最优&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;yi-chuan-suan-fa-de-you-que-dian&quot;&gt;遗传算法的优缺点&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;原理和实现简单，能比较简单地处理复杂的组合问题&lt;&#x2F;li&gt;
&lt;li&gt;开始阶段提升很快，提升到一定程度就比较慢了&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;jin-yi-bu-de-gong-zuo&quot;&gt;进一步的工作&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;xiang-su-fa-shi-xian-fang-mian-dai-gai-jin-de-di-fang&quot;&gt;像素法实现方面待改进的地方&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;代码中包含大量动态内存分配，可进一步优化&lt;&#x2F;li&gt;
&lt;li&gt;需进一步支持任意旋转角度，目前只支持4个方向&lt;&#x2F;li&gt;
&lt;li&gt;精度的选择应当动态适应，目前是直接10倍精度&lt;&#x2F;li&gt;
&lt;li&gt;初始化部分的性能可以较大幅度的优化&lt;&#x2F;li&gt;
&lt;li&gt;目前没有实现从面料上取出零件，而是全部删了重新放&lt;&#x2F;li&gt;
&lt;li&gt;尝试完全消除固有精度误差&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;zhen-dui-bu-tong-gui-mo-bu-tong-xing-zhuang-fen-bu-de-shu-ju-ji-geng-hao-de-gua-ying&quot;&gt;针对不同规模不同形状分布的数据集更好的适应&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;搜集、构建更广泛的数据集用于测试分析改进&lt;&#x2F;li&gt;
&lt;li&gt;进一步结合基于几何的方法，比如NFP、三角化等，为搜索放置策略提供更多的手段&lt;&#x2F;li&gt;
&lt;li&gt;进一步考虑组合放置策略，目前的贴合度贪心和左底遗传都是一个一个单独放置的&lt;&#x2F;li&gt;
&lt;li&gt;进一步尝试结合其他优化迭代方法，比如重叠移除等&lt;&#x2F;li&gt;
&lt;li&gt;左底遗传算法可能仍有较大的上升空间，可以进一步探索&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之农作物范围识别</title>
        <published>2021-02-13T00:00:00+00:00</published>
        <updated>2021-02-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/crop/"/>
        <id>/blog/crop/</id>
        
        <content type="html" xml:base="/blog/crop/">&lt;blockquote&gt;
&lt;p&gt;冠军：华南理工黄钦建等（冲啊大黄 团队 ）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;通过无人机航拍的地面影像，探索像素级农作物分类的算法，具体的分类目标为薏仁米、玉米、烤烟、人造建筑（复赛新增），其余所有位置归为背景类。&lt;&#x2F;p&gt;
&lt;p&gt;初赛、复赛提供的数据是同一片区域的航拍影像。其中初赛提供数据为农作物生长的早期（大多没长出来），分割难度较大。复赛数据农作物长势良好，并在初赛赛题基础上增加了一类“建筑”。&lt;&#x2F;p&gt;
&lt;p&gt;提供的label为与原始图像1:1大小的单通道图像（mask），像素的大小对应不同的标注类别。其中“烤烟”像素值为1，“玉米”像素值为2，“薏仁米”像素值为3，“人造建筑”像素值为4，背景类像素值为0.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;评估指标&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;评估指标为mIoU，榜上排名分数为所有计算所有类别IoU后取平均的结果。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-fen-xi&quot;&gt;数据分析&lt;&#x2F;h2&gt;
&lt;p&gt;图像分辨率超大，背景类占比远高于其它类别，mask无效的区域面积也较大，类别不平衡问题突出。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-qie-ge&quot;&gt;数据切割&lt;&#x2F;h2&gt;
&lt;p&gt;使用gdal库分割遥感影像，采用两种切割策略：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;策略一：以1024x1024的窗口大小，步长900滑窗，当窗口中mask无效区域比例大于7&#x2F;8则跳过，当滑动窗口中背景类比例小于1&#x2F;3时，增加采样率，减小步长为512；&lt;&#x2F;li&gt;
&lt;li&gt;策略二：以1024x1024的窗口大小，步长512滑窗，当滑动窗口中无效mask比例大于1&#x2F;3则跳过。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;mo-xing-xuan-ze&quot;&gt;模型选择&lt;&#x2F;h2&gt;
&lt;p&gt;DeeplabV3+ （注：决赛5个队伍中3个用了它），backbone为Xception-65和ResNet-101以及DenseNet-121。从 A榜分数看，不加任何trick时，DenseNet分数略高于另外两个，但是显存占用太大以及训练时间太长，在后来的方案里就舍弃了。决赛复现时，使用了两个Xception-65和一个ResNet-101投票，投票的每个模型用不同的数据训练，增加模型差异。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;crop&#x2F;.&#x2F;157137269012195351571372690503.png&quot; alt=&quot;157137269012195351571372690503&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;crop&#x2F;.&#x2F;157137272713270821571372727612.png&quot; alt=&quot;157137272713270821571372727612&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gai-jin-cuo-shi&quot;&gt;改进措施&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;peng-zhang-yu-ce&quot;&gt;膨胀预测&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;方格效应&lt;&#x2F;strong&gt;：如果直接做不重叠滑窗预测拼接，得到的预测结果拼接痕迹明显。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;原因分析&lt;&#x2F;strong&gt;：网络卷积计算时，为了维持分辨率进行了大量zero-padding，导致网络对边缘预测不准。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;膨胀预测&lt;&#x2F;strong&gt;：预测时，只保留预测结果的中心区域，舍弃预测不准的边缘。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;具体实现&lt;&#x2F;strong&gt;：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;填充右下边界至滑窗预测窗口大小的整数倍（方便切割）；&lt;&#x2F;li&gt;
&lt;li&gt;填充1&#x2F;2滑窗步长大小的外边框（考虑边缘数据的膨胀预测）；&lt;&#x2F;li&gt;
&lt;li&gt;以1024x1024为滑窗，512为步长，每次预测只保留滑窗中心512x512的预测结果（可以调整更大的步长，或保留更大的中心区域，提高效率）。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;ce-shi-zeng-qiang&quot;&gt;测试增强&lt;&#x2F;h3&gt;
&lt;p&gt;测试时，通过对图像水平翻转，垂直翻转，水平垂直翻转等多次预测，再对预测结果取平均可以提高精度，但相对的，推理时间也会大幅度升高。&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;with torch.no_grad():
    for (image,pos_list) in tqdm(dataloader):
        # forward --&amp;gt; predict
        image = image.cuda(device) # 复制image到model所在device上
        predict_1 = model(image)

        # 水平翻转
        predict_2 = model(torch.flip(image,[-1]))
        predict_2 = torch.flip(predict_2,[-1])
        # 垂直翻转
        predict_3 = model(torch.flip(image,[-2]))
        predict_3 = torch.flip(predict_3,[-2])
        # 水平垂直翻转
        predict_4 = model(torch.flip(image,[-1,-2]))
        predict_4 = torch.flip(predict_4,[-1,-2])

        predict_list = predict_1 + predict_2 + predict_3 + predict_4
        predict_list = torch.argmax(predict_list.cpu(),1).byte().numpy() # n x h x w
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;snapshot-ensemble&quot;&gt;snapshot ensemble&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;snapshot ensemble&lt;&#x2F;strong&gt; 是一个简单通用的提分trick，通过余弦周期退火的学习率调整策略，保存多个收敛到局部最小值的模型，通过模型自融合提升模型效果。详细的实验和实现可以看黄高老师ICLR 2017的这篇&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1704.00109.pdf&quot;&gt;论文&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;snapshot ensemble&lt;&#x2F;strong&gt; 另一个作用是作新方案的验证。深度学习训练的结果具有一定的随机性，在做新改进方案验证时，有时难以确定线上分数的小幅度提升是来自于随机性，还是改进方案really work。在比赛提交次数有限的情况下，&lt;strong&gt;snapshot ensemble&lt;&#x2F;strong&gt; 不失为一个稳定新方案验证的方法。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;hou-chu-li&quot;&gt;后处理&lt;&#x2F;h3&gt;
&lt;p&gt;对输出结果做最简单的填充孔洞和去除小连通域。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;biao-qian-ping-hua&quot;&gt;标签平滑&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;标签平滑&lt;&#x2F;strong&gt;想法参考了Hinton大神关于的&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1503.02531&quot;&gt;知识蒸馏&lt;&#x2F;a&gt;和&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1906.02629&quot;&gt;When does label smoothing help?&lt;&#x2F;a&gt;的工作，标签平滑训练的模型更加稳定和泛化能力更强。&lt;&#x2F;p&gt;
&lt;p&gt;在知识蒸馏中，用teacher模型输出的soft target训练的student模型，比直接用硬标签（onehot-label）训练的模型具有更强的泛化能力。我对这部分提升理解是：软标签更加合理反映样本的真实分布情况，硬标签只有全概率和0概率，太过绝对。知识蒸馏时teacher模型实现了easy sample 和 hard sample 的“分拣”（标签平滑），对hard sample输出较低的置信度，对easy sample 输出较高的置信度，使得student模型学到了更加丰富的信息。&lt;&#x2F;p&gt;
&lt;p&gt;参考相关论文的实验数据可以看出，软标签训练的模型类内更加凝聚，更加可分。&lt;&#x2F;p&gt;
&lt;p&gt;在图像分割任务中，每个像素的分类结果很大程度依赖于周围像素，基于此，即使不通过teacher模型，我们也可以发掘部分样本中的hard sample。&lt;&#x2F;p&gt;
&lt;p&gt;图像边缘：卷积时零填充太多，信息缺少，难以正确分类&lt;&#x2F;p&gt;
&lt;p&gt;不同类间交界处：类间交界难以界定，存在许多标注错误，训练时梯度不稳定；类间交界的点，往往只相差几个像素偏移，对网络来说输入信息高度相似，但训练时label 却不同，也是训练过程的不稳定因素。&lt;&#x2F;p&gt;
&lt;p&gt;针对性的解决方案是在图像边缘和类间交界设置过渡带，过渡带内的像素视为 hard sample作标签平滑处理，平滑的程度取决于训练时每个batch中 hard sample 像素占总输入像素的比例。而过渡带width的大小为一个超参数，在本次比赛中我们取 &lt;strong&gt;width = 11 个像素点&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;不加hard sample 的损失函数：&lt;&#x2F;p&gt;
&lt;p&gt;$$H(y,p)=\sum^{K}_{k=1}-y_klog(p_k)$$&lt;&#x2F;p&gt;
&lt;p&gt;对hard sample 加入平滑后：&lt;&#x2F;p&gt;
&lt;p&gt;$$H(y,p)=\sum_{k=1}^{K}-y_k^{easy}log(p_k^{easy}) + \sum_{k=1}^{K}-y_k^{hard}log(p_k^{hard})$$&lt;&#x2F;p&gt;
&lt;p&gt;$$y_k^{hard} = y_k(1-\alpha)+ \alpha&#x2F;K$$&lt;&#x2F;p&gt;
&lt;p&gt;其中，$\alpha$用于控制标签的平滑程度，取值为每次输入数据中hard sample像素占输入数据的比例。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wei-biao-qian-ruan-biao-qian&quot;&gt;伪标签 + 软标签&lt;&#x2F;h3&gt;
&lt;p&gt;伪标签是分类比赛中常用的trick之一，在模型分数已经较高的情况下可以尝试。提分显著，但对A榜过拟合的风险极大。具体实施是：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;利用在测试集表现最好的融合模型结果作伪标签，用多组不同置信度阈值过滤数据，结合训练集训练模型；&lt;&#x2F;li&gt;
&lt;li&gt;对所有伪标签数据进行标签平滑，缓解伪标签中错误数据对网络训练影响；&lt;&#x2F;li&gt;
&lt;li&gt;选取多个snapshot的方法对模型进行自融合提高模型的泛化能力；&lt;&#x2F;li&gt;
&lt;li&gt;利用3的结果，更新伪标签，重复步骤1~3。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;zong-jie&quot;&gt;总结&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;膨胀预测消除边缘预测不准问题；&lt;&#x2F;li&gt;
&lt;li&gt;使用测试增强、消除空洞和小连通域等后处理提高精度；&lt;&#x2F;li&gt;
&lt;li&gt;使用snapshot模型自融合、标签平滑、伪标签等方法提高模型稳定性。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之铝型材表面瑕疵识别</title>
        <published>2021-02-13T00:00:00+00:00</published>
        <updated>2021-02-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/gd-defect/"/>
        <id>/blog/gd-defect/</id>
        
        <content type="html" xml:base="/blog/gd-defect/">&lt;blockquote&gt;
&lt;p&gt;冠军：Are you OK 战队（中山大学曾兆阳等）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;在铝型材的实际生产过程中，由于各方面因素的影响，铝型材表面会产生裂纹、起皮、划伤等瑕疵，这些瑕疵会严重影响铝型材的质量。铝型材的表面自身会含有纹路，与瑕疵的区分度不高。传统人工肉眼检查十分费力，不能及时准确的判断出表面瑕疵，质检的效率难以把控。铝型材制造商迫切希望采用最新的AI技术来革新现有质检流程，自动完成质检任务，减少漏检发生率，提高产品的质量，使铝型材产品的生产管理者彻底摆脱了无法全面掌握产品表面质量的状态。&lt;&#x2F;p&gt;
&lt;p&gt;初赛数据量3000张图片，复赛数据量5000张图片，包含单瑕疵图片，多瑕疵图片，无瑕疵图片，用于参赛者设计图像识别算法。图片所含瑕疵类型总计10种，分别为：不导电、擦花、角位漏底、桔皮、漏底、喷流、漆泡、起坑、杂色、脏点。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;评价指标&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;参照2010年之后PASCALVOC的评估标准，检测框和真实框的交并比(IOU)阈值设定为0.5，同时，采用Interpolating all points方法插值获得PR曲线，并在此基础上计算mAP的值，计算10类瑕疵的mAP值作为赛手的分数。&lt;&#x2F;p&gt;
&lt;p&gt;本次大赛计算mAP时，对同一个ground-truth框，重复预测n次，取置信度(confidence)最高的预测框作为TP（true positive）样本，其余的n-1个框都作为FP(False positive)样本进行处理。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-fen-xi&quot;&gt;数据分析&lt;&#x2F;h2&gt;
&lt;p&gt;从数据中可以看到，&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;脏点&lt;&#x2F;code&gt;的占比面积特别小，&lt;code&gt;喷流&lt;&#x2F;code&gt;与背景很相似，&lt;code&gt;擦花&lt;&#x2F;code&gt;很不规则。&lt;&#x2F;li&gt;
&lt;li&gt;大部分的类别是十分均衡的，&lt;code&gt;脏点&lt;&#x2F;code&gt;这个类的数量较多。&lt;code&gt;缺陷框&lt;&#x2F;code&gt;的大小两级分化 比较严重。在这其中，小样本的缺陷框基本上都是&lt;code&gt;脏点&lt;&#x2F;code&gt;的类别&lt;&#x2F;li&gt;
&lt;li&gt;原始图片的分辨率非常的大，是1920*2560&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h2&gt;
&lt;p&gt;基本架构采用Faster R-CNN, backbone选取Resnet-101。&lt;&#x2F;p&gt;
&lt;p&gt;原图输入=&amp;gt;下采样2倍=&amp;gt;Resnet-101(下采样16倍)，也就是说，从原图到最后一层的卷积特征，空间大小一共下降了32倍（$60\cdot80$）。 由于之后每一个候选框特征会被缩放到 $7\cdot7$ 的大小，如果说本身缩放前的特征就非常的小，那么缩放之后的特征是 不具有判别力的。统计了一下数据集中边长 &amp;lt;=64 的样本，发现这类小样本占了整个数据集的10%，这会严重地影响性能。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;gai-jin-fang-an&quot;&gt;改进方案&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;te-zheng-jin-zi-ta&quot;&gt;特征金字塔&lt;&#x2F;h4&gt;
&lt;p&gt;为了解决这个问题，我们采用了学术界非常常用的特征金字塔结构来对网络进行改进。我们总结了一下，特征金字 塔在这个任务中具有两个优点：第一，低层的特征经过卷积，上采样操作之后和高层的信息进行融合在卷积神经网络中，高层，也就是后面的特征具有强的语义信息，低层的特征具有结构信息，因此将高低层的信息进行结合，是可以增强特征的表达能力的。第二，我们将候选框产生和提取特征的位置分散到了特征金字塔的每一层，这样可以增加小目标的特征映射分辨率，对最后的预测也是有好处的。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ke-bian-xing-juan-ji&quot;&gt;可变形卷积&lt;&#x2F;h4&gt;
&lt;p&gt;我们采用的第二个改进方案是Deformable Convolutoin可变形卷积。我们发现在数据集中，铝材的瑕疵有很多是这种条状的，传统正规的正方形结构的卷积对这种形状的缺陷处理能力还不够强。因此我们采用了可变形的卷积， 在卷积计算的过程中能够自动地计算每个点的偏移，从而从最合适的地方取特征进行卷积。右边的示意图大致描述 了可变形卷积的过程，它能够让卷积的区域尽可能地集中在缺陷上。&lt;&#x2F;p&gt;
&lt;p&gt;具体实现上，将原本resent结构的最后一个block改成了可变卷积，原因是在可变卷积的实现中，需要基于前面 的特征来学习一个偏移，前面的特征得足够强才能保证这个偏移不会乱学，因此我们只改动了最后一个block。总体 的框架还是跟前面FPN的一样。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;contextual-roi-pooling&quot;&gt;Contextual ROI Pooling&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;gd-defect&#x2F;.&#x2F;20210213120121.jpg&quot; alt=&quot;20210213120121&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;我们的第三个改进方案，是在提取ROI特征的时候，引入了context上下文信息，我们把这个操作叫做contextual roi pooling。我们用上面两个例子来说明上下文信息的好处。Faster R-CNN是一个先生成候选框，然后精调候选框的过程，那么第一步生成的候选框势必会有偏大或者偏小的情况。之前的方法可以理解成用框内部的信息来推断框的 位置，左边这个例子是框偏大的情况，根据内部信息是可以知道框应该往里调的，但是右边这个例子框偏小了， 我们能知道该往外调整，但是该调多少呢这个是无从知晓的。因此一个显而易见的想法，就是把整张图片的信息也 送给这个候选框当特征，这样相当于让每个候选框以整张图片作为参考，这样呢每个框就知道该往哪调了。&lt;&#x2F;p&gt;
&lt;p&gt;具体的实现是这样，我们把整张图片也作为一个roi，用同样的ROI Pooling提取全局的特征，然后跟每一个候选框 的特征相加，再进行后面的分类和回归操作。这样的实现只多进行了一个roi的特征提取和一个特征相加的操作， 却能大大地提升准确率。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;gd-defect&#x2F;.&#x2F;20210213120731.jpg&quot; alt=&quot;20210213120731&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;xun-lian-ji-qiao&quot;&gt;训练技巧&lt;&#x2F;h2&gt;
&lt;p&gt;数据集里面是有提供无缺陷样本的，我们也对这些图片进行了使用。 在检测器的训练过程中，有一步是正负样本的选择。我们在训练的时候使用了一个策略，每次会随机选择一张缺陷样本和一张无缺陷样本，然后训练的正样本会在缺陷图片中选择，负样本会在两张图片中都选择，两张图片的所有 正负样本合起来做一个OHEM（Online hard example mining），再进行后面的训练操作。这样的好处是，充分利用了无缺陷样本，&lt;strong&gt;增大了模型判别背景信息的能力&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;铝材的缺陷是具有翻转不变性的，将一张图片水平和竖直翻转之后，他的瑕疵信 息是不会变的，也就是说，我们将图片进行翻转之后，再将框做一个变换到对应的位置，这样可以构建出一批新的数据来。通过这样的数据扩增方式，我们把训练数据扩增了四倍，也因此&lt;strong&gt;提升了模型的鲁棒性&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;shi-yan-jie-guo-yan-jiu&quot;&gt;实验结果研究&lt;&#x2F;h4&gt;
&lt;p&gt;通过分析实验和结果，我们发现擦花和喷流差的原因是基本都是召回率较低。在生成检测结果的时候，用了softnms来提高模型分数。softnms的作用是在框之间互 相抑制的时候使用了较温和的策略，让被抑制过的框还有机会重新被选上，从而提高召回率。从实验结果可以看到，softnms在每个类上都有提升。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之布匹疵点智能检测</title>
        <published>2021-02-13T00:00:00+00:00</published>
        <updated>2021-02-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/guangdong-cloth/"/>
        <id>/blog/guangdong-cloth/</id>
        
        <content type="html" xml:base="/blog/guangdong-cloth/">&lt;blockquote&gt;
&lt;p&gt;冠军：哪儿都是坑啊 团队（徐光福等）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;在布匹的实际生产过程中，由于各方面因素的影响，会产生污渍、破洞、毛粒等瑕疵，为保证产品质量，需要对布匹进行瑕疵检测。布匹疵点检验是纺织行业生产和质量管理的重要环节，目前人工检测易受主观因素影响，缺乏一致性；并且检测人员在强光下长时间工作对视力影响极大。由于布匹疵点种类繁多、形态变化多样、观察识别难道大，导致布匹疵点智能检测是困扰行业多年的技术瓶颈。&lt;&#x2F;p&gt;
&lt;p&gt;大赛数据涵盖了纺织业中布匹的各类重要瑕疵，每张图片含一个或多种瑕疵。数据包括包括素色布和花色布两类，其中，素色布数据约8000张，用于初赛；花色布数据约12000张，用于复赛。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ping-gu-zhi-biao&quot;&gt;评估指标&lt;&#x2F;h4&gt;
&lt;p&gt;赛题分数计算方式: &lt;strong&gt;0.2ACC+0.8mAP&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;ACC&lt;&#x2F;strong&gt;：是有瑕疵或无瑕疵的分类指标，考察瑕疵检出能力。
其中提交结果name字段中出现过的测试图片均认为有瑕疵，未出现的测试图片认为是无瑕疵。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;mAP&lt;&#x2F;strong&gt;：参照PASCALVOC的评估标准计算瑕疵的mAP值。&lt;&#x2F;p&gt;
&lt;p&gt;本次大赛评分计算过程中，分别在检测框和真实框的交并比(IOU)在阈值0.1，0.3，0.5下计算mAP，最终mAP取三个值的平均值。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-fen-xi-ji-yu-chu-li&quot;&gt;数据分析及预处理&lt;&#x2F;h2&gt;
&lt;p&gt;本次赛题主要难点是小目标占比较大，尺度差异性大（长宽比），类别不均衡很严重，布匹花色信息容易和瑕疵点混淆。&lt;&#x2F;p&gt;
&lt;p&gt;官方提供了布匹的模板图片，该图片包含布匹的花色信息，有效利用模版，能提高泛化性。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zu-he-san-tong-dao&quot;&gt;组合三通道&lt;&#x2F;h3&gt;
&lt;p&gt;第一层用原图，第三层用模版，中间层用原图和模版的差异值。另外，为减轻模板的不对齐情况，对模板图片进行上下左右随机10个像素点左右的抖动。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h2&gt;
&lt;p&gt;目标检测问题的框架： Cascade rcnn + big backbone，由于比赛的评估指标map为0.1，0.3，0.5，须对cascade rcnn的RPN和三个串联的RCNN结构的阈值进行调整。&lt;&#x2F;p&gt;
&lt;p&gt;相关参数：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;backbone: ResNeXt + FPN + DCN + SE&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;pos_iou_thr: 0.5&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;net_iou_thr: 0.3&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;三个串联rcnn阈值分别为 0.3 、0.4、0.5&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;anchorshe-ji&quot;&gt;anchor设计&lt;&#x2F;h4&gt;
&lt;p&gt;对瑕疵进行聚类分析，针对不同的长宽比，精细设计anchor参数。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;qi-ta&quot;&gt;其他&lt;&#x2F;h4&gt;
&lt;p&gt;探索CenterNet、FCOS、REPPOINTS等基于anchor free的新方法。尝试改进版的BiFPN, 该结构在coco表现上非常好。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 FashionAI 服饰关键点定位</title>
        <published>2021-02-11T00:00:00+00:00</published>
        <updated>2021-02-11T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/ai-keypoints/"/>
        <id>/blog/ai-keypoints/</id>
        
        <content type="html" xml:base="/blog/ai-keypoints/">&lt;blockquote&gt;
&lt;p&gt;冠军：李weite 及 bilibili 团队&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;服装的机器分析很容易受到衣服的尺寸和形状，相机拍摄的距离和角度甚至服装的显示方式或模型摆放方式的影响。对图像中服装关键点的检测可以帮助提高应用程序的性能，例如衣服的对齐，衣服局部属性的识别以及服装图像的自动编辑。&lt;&#x2F;p&gt;
&lt;p&gt;基于服装设计知识，定义了一套服饰的关键点，并梳理了在女装6大专业类别（上衣、外套、裤子、半身裙、连身裙、连身裤）下的具体定义，要求参赛者设计算法进行定位预测。官方提供的数据集含前五个类别（连身裤类别被省略，因为在现实世界中并不常见），包括41个子类别和24种关键点。此数据集中总共有100,000个带批注的图像。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
 &lt;img width=&quot;90%&quot; src=&quot;.&#x2F;TB16Z8fXQCWBuNjy0FaXXXUlXXa.png&quot; &#x2F;&gt;
 &lt;figcaption&gt;
 女装关键点图例
 &lt;&#x2F;figcaption&gt;
 &lt;&#x2F;p&gt;
&lt;h3 id=&quot;guan-jian-dian-ding-yi&quot;&gt;关键点定义&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Keypoints\Category&lt;&#x2F;th&gt;&lt;th&gt;Blouse&lt;&#x2F;th&gt;&lt;th&gt;Outwear&lt;&#x2F;th&gt;&lt;th&gt;Trousers&lt;&#x2F;th&gt;&lt;th&gt;Skirt&lt;&#x2F;th&gt;&lt;th&gt;Dress&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;neckline_left&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;neckline_right&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;center_front&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;shoulder_left&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;shoulder_right&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;armpit_left&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;armpit_right&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;waistline_left&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;waistline_right&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;cuff_left_in&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;cuff_left_out&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;cuff_right_in&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;cuff_right_out&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;top_hem_left&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;top_hem_right&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;waistband_left&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;waistband_right&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;hemline_left&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;hemline_right&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;crotch&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bottom_left_in&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bottom_left_out&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bottom_right_in&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bottom_right_out&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;TOTAL&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;zhu-jie-ge-shi&quot;&gt;注解格式&lt;&#x2F;h3&gt;
&lt;p&gt;注释文件保存在csv格式表中，共有26列：第一列（image_id）包含图像文件名，第二列（image_category）表示图像所属的类别，其余24列记录了 上述24个关键点的位置。 仅显示两个图像项的示例表如下所示：&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;image_id,image_category,neckline_left,neckline_right,center_front,shoulder_left,shoulder_right,armpit_left,armpit_right,waistline_left,waistline_right,cuff_left_in,cuff_left_out,cuff_right_in,cuff_right_out,top_hem_left,top_hem_right,waistband_left,waistband_right,hemline_left,hemline_right,crotch,bottom_left_in,bottom_left_out,bottom_right_in,bottom_right_out
Images&amp;#x2F;blouse&amp;#x2F;d21eab37ddc74ea5a5f1b4a5d3d9055a.jpg,blouse,241_135_1,301_135_1,259_136_1,216_142_1,319_144_1,212_186_1,307_202_1,-1_-1_-1,-1_-1_-1,203_236_1,195_256_1,278_241_1,283_261_1,206_243_0,292_252_0,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1
Images&amp;#x2F;blouse&amp;#x2F;02b54c183d2dbd2c056db14303064886.jpg,blouse,244_76_1,282_76_1,257_99_1,228_81_0,303_85_1,222_134_1,295_131_1,-1_-1_-1,-1_-1_-1,199_153_1,178_100_0,293_173_1,332_150_1,229_161_1,297_162_0,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;每个关键点由一个三元组表示，每个元素由下划线连接，表示为“ x_y_v”，其中x和y为坐标，v为可见性。 如果关键点可见，则可见性等于1；如果关键点被遮挡，则可见性等于0；如果类别中不存在或未定义，则可见性等于-1。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ping-jia-biao-zhun&quot;&gt;评价标准&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;ai-keypoints&#x2F;.&#x2F;TB1nI4_FY1YBuNjSszeXXablFXa.tfsprivate.png&quot; alt=&quot;TB1nI4_FY1YBuNjSszeXXablFXa.tfsprivate&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Normalized Error（NE）用于评估提交的结果。 NE是预测关键点位置和注释位置之间的平均归一化距离。 注意，NE计算仅涉及可见的关键点。&lt;&#x2F;p&gt;
&lt;p&gt;其中 $k$ 为关键点 ID，$d_k$为预测关键点位置与带注释的关键点位置之间的距离，$s_k$为距离归一化参数（对于上衣，外套和衣服，它等于两个腋窝点之间的欧几里得距离;对于裤子和 裙边等于两个腰带点之间的距离），$v_k$即关键点的可见性。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;jie-ti-si-lu&quot;&gt;解题思路&lt;&#x2F;h2&gt;
&lt;p&gt;第一阶段：使用检测器把女装在图片上的位置检测出来；&lt;&#x2F;p&gt;
&lt;p&gt;第二阶段：再针对女装位置，做准确关键点的定位。&lt;&#x2F;p&gt;
&lt;p&gt;通过第一阶段的处理可以给第二阶段输入更干净的数据。&lt;&#x2F;p&gt;
&lt;p&gt;第一阶段的模型采用 Faster-RCNN，backbone resnet101&lt;&#x2F;p&gt;
&lt;p&gt;第二阶段的模型为自定义，命名为 asymmetric and dilated stacked hourglass networks 。参考google CVPR 2017年论文拟合两种信息，第一种信息判断每一个像素点是否在关键点的领域范围内，如果是在领悟范围内，则标注为临近点；拟合的第二种信息是临近点和关键点之间的向量偏差，有了这个向量偏差，就可以把预测出的向量点投票到对应偏差的坐标点上，这时就能取得一个准确的关键点定位。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h3&gt;
&lt;p&gt;asymmetric and dilated stacked hourglass networks (SHN)，非对称并带孔的SHN模型。主要特征为：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;有一个非对称的 encoder-decoder 结构；&lt;&#x2F;li&gt;
&lt;li&gt;设计了一个重量级的encoder（保有更多的空间信息）和一个轻量级的decoder；&lt;&#x2F;li&gt;
&lt;li&gt;对 encoder-decoder 结构进行叠加，通过多个stage对keypoints进行定位。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;采用非对称结构的理由，设计者认为重量级的encoder可以用来迁移学习，须保有更多空间信息，而decoder如果参数太多，就享受不到encoder初始化参数带来的好处。&lt;&#x2F;p&gt;
&lt;p&gt;当空间信息的重要性高于语义信息时的模型设计：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;图片尺寸的减少比例，从传统的 64 =&amp;gt; 32 =&amp;gt; 16 =&amp;gt; 8 改为 64 =&amp;gt; 32 =&amp;gt; 32 =&amp;gt; 32&lt;&#x2F;li&gt;
&lt;li&gt;采用空洞卷积&lt;&#x2F;li&gt;
&lt;li&gt;因为增大了空间尺度，通过减少通道数量(channel numbers)使模型的计算量不增加，即512(128) =&amp;gt; 1024(256) =&amp;gt; 2048(512) 改为 256(128) =&amp;gt; 512(128) =&amp;gt; 512(128)&lt;&#x2F;li&gt;
&lt;li&gt;不增加计算量的前提下，堆叠网络，即 256(128) =&amp;gt; 512(128) =&amp;gt; 512(128)改为两个  256(64) =&amp;gt; 512(64) =&amp;gt; 512(64)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;mo-xing-yan-bian-tu&quot;&gt;模型演变图&lt;&#x2F;h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: left&quot;&gt;Model&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;Param Size&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;FLOPs&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;Scores&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;ResNet50 backbone + 1 stage&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;49M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;7.86G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;4.11&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ 2 stage + more encoder layers&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;91M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;18.52G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.4x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ pre-trained&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;91M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;18.52G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.1x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ more data&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;91M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;18.52G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.1x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ ResNet101 + large input(352) + more channles&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;402M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;100G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.1x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ increase the number of boxes(more-crops)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;402M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;100G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.0x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Final Submission&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;402M&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;strong&gt;100G&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;3.30&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;qi-ta-ji-qiao&quot;&gt;其他技巧&lt;&#x2F;h2&gt;
&lt;p&gt;观察到难样本分布不均，采取在线难样本挖掘，使得训练聚集到难样本。难样本搜寻不是按照传统的在一张图中查找，而是在一个batch中搜寻。&lt;&#x2F;p&gt;
&lt;p&gt;最终提交模型时给了两个模型版本，一个高速版本一个低速版本，两个版本的分数差异不大，以证明模型的实用性很强。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 FashionAI 服饰属性标签识别</title>
        <published>2021-02-11T00:00:00+00:00</published>
        <updated>2021-02-11T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/ai-reconition/"/>
        <id>/blog/ai-reconition/</id>
        
        <content type="html" xml:base="/blog/ai-reconition/">&lt;blockquote&gt;
&lt;p&gt;冠军：西安交大在读博士及硕士组成的“禾思众成”团队&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;服装属性是时装领域的基础知识，它既庞大又复杂。 我们构建了一个层次结构的属性树作为结构化的分类目标，以描述服装的认知过程。 邀请您设计算法来识别服装图像的属性。 此任务可能会广泛应用于服装图片搜索，导航标签，混搭推荐等。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;ai-reconition&#x2F;.&#x2F;cloth-attributes.png&quot; alt=&quot;女装属性树&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;要求检测上图中所有可区分的服装属性标签。 统计下来，训练集的图片约18万张，8个大类（4个设计属性，包括各种领口的款式，4个长度属性，包括衣长、裙长、裤长、袖长），54个小类。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;biao-qian&quot;&gt;标签&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;ai-reconition&#x2F;.&#x2F;cloth-model.png&quot; alt=&quot;模特及标签&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;a）上图的数据由训练有素的注释器标记。 然后，时尚专家会仔细检查这些标签，以确保较高的标签准确性。 带注释的数据中存在一定数量的缺失标签。 例如，图像中可能只有一个颈部设计标签，且颈部设计和袖子长度可见。 不再标记袖子长度以保持每个属性维度的数据均匀性。&lt;&#x2F;p&gt;
&lt;p&gt;b）为这条赛道选择了八个主要属性尺寸，即领口设计(Neckline)、领子设计(Collar)、高领设计(High Neck)、翻领设计(Lapel)、袖子长度(Sleeves length)、上衣长度(Length of top)、裙长(Length of skirt)和裤子长度(Length of trousers)。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shu-ju-te-zheng&quot;&gt;数据特征&lt;&#x2F;h3&gt;
&lt;p&gt;a）互斥：特定属性维度下的属性值是互斥的。例如，在高领设计尺寸中，高领和荷叶边半高领不能在同一图像中共存。必须注意一件事：考虑到挑战的严峻性，为了保证属性的相互排斥，我们放弃了一些特定的图像，在这些图像中，模型穿着多个重叠的服装，从而在一维中生成多个不同的属性。&lt;&#x2F;p&gt;
&lt;p&gt;b）独立性：不同维度下的属性值可以共存于单个图像中，并且彼此独立。例如，“脖子高领设计－龟颈”和“脖子领设计－衬衣领”可以共存于单个图像中。&lt;&#x2F;p&gt;
&lt;p&gt;c）在每个属性维度下，都有一个“不可见”值。这意味着在透视图中定义了特定的属性（顶视图，底视图或身体外观），但在特定图像中未出现或被遮挡。例如，给定一个穿着连衣裙的模特的图像，该图像包含两个透视图，即顶部外观和底部外观。裙子的下摆被遮挡，因此裙子的长度尺寸将被标记为“不可见”。该算法应考虑这种“否定”。但是，我们将不检查在相应透视图中未定义的属性的求反能力。例如，就像只有底部外观的裤子图像一样，我们将不检查其顶部外观的属性（例如“袖长”）。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zhu-shi-wen-jian-de-ge-shi&quot;&gt;注释文件的格式&lt;&#x2F;h3&gt;
&lt;p&gt;ImageName：与“ Images”文件夹中特定图像文件相对应的图像名称。&lt;&#x2F;p&gt;
&lt;p&gt;AttrKey：属性尺寸，例如袖长（sleeve_length_labels），裤子长度（pant_length_labels）等。&lt;&#x2F;p&gt;
&lt;p&gt;AttrValues：与AttrKey中的属性维相对应的属性值。 例如，袖长尺寸有9个值：不可见，无袖，杯形袖，短袖，中长，3&#x2F;4袖，腕长袖，长袖和超长袖子，分别对应于“ nnnnnnmyn” 上图中的注释。 批注总共包含九个数字，每个数字代表以下三个字母之一： y（表示“是”，“必须”），m（表示“可能”，“可能”）和n（表示“否”，“必须”） 对于给定图像中的每个属性维度，可以有一个且只有一个“ y”带注释的数字，其他数字可以为“ m”或“ n”。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;mo-leng-liang-ke-de-bian-jie-de-ding-yi&quot;&gt;模棱两可的边界的定义&lt;&#x2F;h4&gt;
&lt;img src=&quot;.&#x2F;TB1hH7vXmBYBeNjy0FeXXbnmFXa.tfsprivate.png&quot; alt=&quot;TB1hH7vXmBYBeNjy0FeXXbnmFXa.tfsprivate&quot; style=&quot;zoom:48%;&quot; &#x2F;&gt;
&lt;p&gt;上面的示例图中出现歧义。 特别地，套筒长度在“长袖”和“超长袖”之间，但是前者的重量略大于后者。 在这种情况下，“长袖”数字标注为“ y”，“超长袖”数字标注为“ m”，其余数字为“ n”。 这种歧义经常发生，在现实世界的服装属性注释中是不可避免的。&lt;&#x2F;p&gt;
&lt;p&gt;服装属性注释中的遮挡也是不可避免的。 对于裁剪了衣服的下摆的图像，很难准确预测衣服的长度。 在这种情况下，“不可见”数字应标记为“ y”，其他数字应标记为“ n”。 因此，“ skirt_length_labels” 被注释为“ ynnnnn”。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ping-jia-biao-zhun&quot;&gt;评价标准&lt;&#x2F;h3&gt;
&lt;p&gt;计算所有属性维度的AP的均值以获得mAP，mAP用作服装属性识别数据集的最终排名得分。&lt;&#x2F;p&gt;
&lt;p&gt;另外，官方提出了BasicPrecision标准。当评估测试集的所有预测结果（ProbThreshold = 0）时，它是所有属性维度上的平均准确性。BasicPrecision是对准确性的更直接的估计，因此是合理的参考。通常，当BasicPrecision = 0.7时，排名得分（mAP）约为0.93。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h2&gt;
&lt;p&gt;图片显示的服装分为两类，一类为模特图片，拥有两个或两个以上的属性，一类为服装平铺图，只有一个属性。属性也分为两类，一类长度属性，一类设计属性。对于长度属性，更关注整体风格和上下关联，对于设计属性，更关注细节区别。因此在模型设计上，须兼顾这些特点，思考如何挖掘无标签属性以及每个任务之间的关系。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;shu-ju-zeng-qiang&quot;&gt;&lt;strong&gt;数据增强&lt;&#x2F;strong&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;采用random crop， random flip，random erasing,  random border，都是标准方法，不再赘述。参考其他团队的方法，还有种更好的方法是用目标检测模型找出模特和衣服，然后做数据增强效果会更好。因为这时的目标图片像素分布会比较均匀，减少了背景的干扰。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;zheng-ti-kuang-jia&quot;&gt;整体框架&lt;&#x2F;h4&gt;
&lt;img src=&quot;.&#x2F;20210212171535.jpg&quot; alt=&quot;20210212171535&quot; style=&quot;zoom:50%;&quot; &#x2F;&gt;
&lt;p&gt;整体框架采用半监督联合训练，先训练好的 teacher model 对一张图片的未标记属性进行预测，将预测结果作为soft label， 联合已知label对图片进行联合训练。这样， 一个模型被多个任务共同监督，这时任务之间的关联性能够被更好的发掘。&lt;&#x2F;p&gt;
&lt;p&gt;针对属性的差异性，和大多数参数团队一样，采用分任务训练，设计属性训练一个模型，长度属性训练一个模型，可参考第二名的方案，对长度属性的损失函数做专门设计，对预测与label相差距离更大的给更大权重，输出更大损失。第三名方案改进了长度属性的label, 一个1代表第一类，三个1代表第三类，六个1代表第六类，效果也有百分点的提升。&lt;&#x2F;p&gt;
&lt;p&gt;soft label 的技术，使用多个模型预测同一张图片，这些模型有Resnet、Inception、NAS 、DPN，即已经训练好的教师模型，每个教师模型专注于各自的任务类型，然后把预测出来的 label 加进学生网络，联合监督学生网络的训练。&lt;&#x2F;p&gt;
&lt;p&gt;Net2Net模型（将一个神经网络中的知识快速转移到另一个神经网络中），把原模型直接转成更高分辨率，兼顾感受野保同性和函数保同性，计算量远小于把图片分辨率提高所增加的计算量。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;gan-shou-ye-bao-tong-xing&quot;&gt;感受野保同性&lt;&#x2F;h4&gt;
&lt;p&gt;降采样层增加后续卷积层的感受野，极大减少模型计算量，但是免不了会丢失一些信息。而dilated卷积不会丢失信息，因此用空洞卷积替换掉降采样层。 下面是换算公式，替换stage3和stage4的降采样层，在保持感受野增加的前提下，最后输出的分辨率提高了4倍。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;ai-reconition&#x2F;.&#x2F;20210211175358.jpg&quot; alt=&quot;20210211175358&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;优点：Net2Net 在不同计算能力的设备上转换模型，不需要重新训练。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;han-shu-bao-tong-xing&quot;&gt;函数保同性&lt;&#x2F;h4&gt;
&lt;p&gt;基本原理为多出来的参数维度，随机拷贝原来参数的数据，再将多出来的数值求平均，来保证原始模型的知识保有。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;stochasitc-weight-average&quot;&gt;Stochasitc Weight Average&lt;&#x2F;h4&gt;
&lt;p&gt;将训练后的参数在参数空间之间平均，从而得到更鲁棒的模型，即不需要付出太多代价，就能得到一个融合的模型。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;qi-ta-si-lu&quot;&gt;其他思路&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;把&lt;code&gt;模特图&lt;&#x2F;code&gt; 和 &lt;code&gt;平铺图&lt;&#x2F;code&gt; 分别训练&lt;&#x2F;li&gt;
&lt;li&gt;把&lt;code&gt;可见属性&lt;&#x2F;code&gt;和&lt;code&gt;不可见属性&lt;&#x2F;code&gt;分别训练&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 高德算法大赛</title>
        <published>2021-02-10T00:00:00+00:00</published>
        <updated>2021-02-10T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/amap/"/>
        <id>/blog/amap/</id>
        
        <content type="html" xml:base="/blog/amap/">&lt;blockquote&gt;
&lt;p&gt;冠军：北邮在读博士 朱奕达及团队&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;bi-sai-ren-wu&quot;&gt;比赛任务&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;chu-sai-shu-ju&quot;&gt;&lt;strong&gt;初赛数据：&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;&#x2F;strong&gt;：给定一组含有GPS时间的图像序列（包含3-5帧图像），其中一幅图像作为参考帧。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;&#x2F;strong&gt;：以参考帧为准，输出该图像序列对应的路况状态（畅通、缓行和拥堵）。&lt;&#x2F;p&gt;
&lt;p&gt;图像序列由行车记录仪拍摄，路况真值（ground truth）是对应道路当前时刻真实的路况状态。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;数据信息：&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;图像序列的参考帧图像名。&lt;&#x2F;li&gt;
&lt;li&gt;图像序列的路况状态。&lt;&#x2F;li&gt;
&lt;li&gt;0：畅通，1：缓行，2：拥堵，-1：测试集真值未给出。&lt;&#x2F;li&gt;
&lt;li&gt;每帧图像采集时刻的GPS时间。&lt;&#x2F;li&gt;
&lt;li&gt;单位为秒。如GPS时间 1552806926 比 1552806921 滞后5秒钟。&lt;&#x2F;li&gt;
&lt;li&gt;A榜测试数据集换成高速行车记录仪采集的数据，GPS时间间隔缩短为1-2秒。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;fu-sai-shu-ju&quot;&gt;&lt;strong&gt;复赛数据：&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;新增&lt;code&gt;封闭&lt;&#x2F;code&gt;的道路路况&lt;&#x2F;li&gt;
&lt;li&gt;移除了GPS时间信息&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;shu-ju-fen-xi&quot;&gt;数据分析&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;前车遮挡视野内道路情况，从而影响对道路的判断；&lt;&#x2F;li&gt;
&lt;li&gt;对向车道和路边停靠车辆对行驶车道路况判断的影响；&lt;&#x2F;li&gt;
&lt;li&gt;数据众包导致相机安装存在角度偏差，从而导致图像角度不一致；&lt;&#x2F;li&gt;
&lt;li&gt;大雾天气和夜晚导致的图像不清晰。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;复赛阶段数据集：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;存在重复采样的图像序列（多为封闭类型的路况）&lt;&#x2F;li&gt;
&lt;li&gt;数据不均衡，缓行只有100多个序列，而封闭路段有2000多个序列&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;te-zheng-gong-cheng&quot;&gt;特征工程&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;fen-che-dao-mu-biao-jian-ce&quot;&gt;分车道目标检测&lt;&#x2F;h3&gt;
&lt;p&gt;目标检测采用 Faster RCNN 模型，将道路车辆分成5个类别进行标注，分别是：&lt;strong&gt;当前车辆前方行驶车道车辆，同向行驶车道左侧车辆，同向行驶车道右侧车辆，对向车道行驶车辆，街边侧向停车车辆&lt;&#x2F;strong&gt;。通过这样的标注对当前道路环境的车辆进行细致区分，后续将通过目标检测结果提取不同车道的车辆信息特征。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;te-zheng-ti-qu&quot;&gt;特征提取&lt;&#x2F;h3&gt;
&lt;p&gt;基于目标检测的结果，提取60维特征，包括：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;GPS时间特征&lt;&#x2F;li&gt;
&lt;li&gt;关键帧中不同车道的车辆数量、面积、距离等&lt;&#x2F;li&gt;
&lt;li&gt;不同帧间检测框的动态特征，如车辆相似度、数目和面积变化等&lt;&#x2F;li&gt;
&lt;li&gt;Focal loss 降低样本不平衡&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;其中：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;距离通过检测框的中心点，计算两点间的直线距离&lt;&#x2F;li&gt;
&lt;li&gt;不同分车道设计了两个权重，一为box数量与时间间隔的关系，数量增加分数降低，数量减少分数增加；二为box大小与时间间隔的关系，大小增加分数降低，大小减少分数增加。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;基于车辆相关特征训练 LGB 模型，B榜分数为0.6108。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shu-ju-kuo-chong-yu-zeng-qiang&quot;&gt;数据扩充与增强&lt;&#x2F;h3&gt;
&lt;p&gt;复赛阶段，针对类别数量的差异性，对缓行和拥堵类别做了数据扩充。对同一个序列中的图片，对每一帧图像都做了相同的数据变化，保证序列图片所处的相对环境是一致的。具体方法有：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;模型输入按照采样顺序选择3张图片；&lt;&#x2F;li&gt;
&lt;li&gt;针对相机安装存在角度偏差的问题，使用了平移，尺度加旋转变换的数据增强方法；&lt;&#x2F;li&gt;
&lt;li&gt;考虑到视频在采集时包含了一天中的不同时间节点，光照强度和天气条件也对图片造成了较大的影响， 加入了对比度，亮度和颜色增强的方法尽可能贴合实际数据。&lt;&#x2F;li&gt;
&lt;li&gt;从赛方给的数据中我们发现图片的清晰度和质量在不同序列之间存在差异，为此引入了运动模糊，中值滤波，高斯滤波，高斯模糊等方法进行数据增强。&lt;&#x2F;li&gt;
&lt;li&gt;此外图像中还存在一些脱敏信息，比如马赛克或者黑色条，我们也引入了一些cutof操作，减少模型对图像特定部分的依赖。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;mo-xing-zeng-qiang&quot;&gt;模型增强&lt;&#x2F;h2&gt;
&lt;p&gt;考虑到&lt;strong&gt;道路情况不止和车辆信息有关，还和道路及场景相关&lt;&#x2F;strong&gt;，基于分车道目标检测模型只对图像中的车道信息进行提取，而忽略了图片中的其他场景细节。并且目标检测模型本身存在一些误差，将目标检测结果特征再训练一个 LGB 模型可能会对误差进行传递。&lt;&#x2F;p&gt;
&lt;p&gt;端到端模型方案：即训练一个考虑图片全局信息的序列检测模型。&lt;&#x2F;p&gt;
 &lt;img src=&quot;densnet-amap.png&quot; alt=&quot;端到端模型&quot; width=&quot;1080&quot; height=&quot;344&quot; loading=&quot;lazy&quot; &#x2F;&gt;
&lt;p&gt;图片输入窗口大小是3，采用基于DenseNet121先对每张图片提取全局特征后，将图片特征按照时间顺序输入到GRU模型中得到最后的分类结果，在初赛中B榜上我们并没有使用数据增强的方法，最后取得了第一名的成绩0.6614。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shi-kong-te-zheng-ti-qu&quot;&gt;时空特征提取&lt;&#x2F;h3&gt;
&lt;p&gt;在复赛中，赛题发生了变化，新增了&lt;code&gt;封闭&lt;&#x2F;code&gt;这个类别后，基于特征的方案中需要根据障碍物标注进一步提高目标检测模型的鲁棒性，而实际障碍物标注数据质量并不高，尝试建模后发现效果并不如意。&lt;&#x2F;p&gt;
&lt;p&gt;为了减少误差的传递并考虑图片中更多的信息，基于端到端序列检测模型，提取了不同frame的特征后，再使用特征融合模块挖掘图片序列之间的时序特征关系。&lt;&#x2F;p&gt;
&lt;p&gt;特征融合模块中仍然使用双向GRU模型挖掘时间序列特征。将backbone替换成resnest101和SE_ResNext，在空间特征挖掘中，使用了&lt;strong&gt;位置注意力模块&lt;&#x2F;strong&gt;和&lt;strong&gt;通道注意力模块&lt;&#x2F;strong&gt;的双注意力网络。通道注意力模块校准序列图片中的特征图通道之间的关系，位置注意力模块对特征图中不同position进行关系强度的计算，从而挖掘序列图片中的局部关系变化和全局关系变化。&lt;&#x2F;p&gt;
&lt;p&gt;数据增强和样本扩充后，进行5折交叉验证的训练，选择了 5 个模型进行基于概率的融合，最后在B榜上得到了第二名的成绩0.7237，比第一名低了万分之9个点。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-luo-di-ji-zhan-wang&quot;&gt;模型落地及展望&lt;&#x2F;h2&gt;
&lt;p&gt;在实际场景下，不仅仅有图片序列信息，还有道路等级数据，GPS时间，POI点等信息，可以将这些路网和时间信息通过 embedding 等方式处理成相应的特征融合到我们的模型中，更丰富的感知源丰富了我们的时空信息特征空间。&lt;&#x2F;p&gt;
&lt;p&gt;除此之外，还可以不断优化障碍物和车辆目标检测模型识别精度，从而制定更多的约束规则进行前处理和后处理的规则约束。&lt;&#x2F;p&gt;
&lt;p&gt;在模型落地时，可以通过模型蒸馏、优化加速等方式将模型下放到边缘节点进行分布式计算。交通路况状态在局部区域是互相影响的，所以我们还可以根据路网信息对城市进行栅格化处理，将众包得到的数据分栅格进行管理，这样我们可以得到该路段信息的临近路况辅助修正模型结果。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 Apache Flink 极客挑战赛</title>
        <published>2021-02-10T00:00:00+00:00</published>
        <updated>2021-02-10T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/flink/"/>
        <id>/blog/flink/</id>
        
        <content type="html" xml:base="/blog/flink/">&lt;blockquote&gt;
&lt;p&gt;冠军：合肥工大 SkyPeaceLL&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;bi-sai-ren-wu&quot;&gt;比赛任务&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;shu-ju-ji&quot;&gt;数据集&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;新冠病例行动数据集&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;病例历史行动数据集（训练集1） 1M+&lt;&#x2F;li&gt;
&lt;li&gt;确诊病例数据 （测试集1） 500+&lt;&#x2F;li&gt;
&lt;li&gt;实时病例行动数据集（测试集2） 1000+&lt;&#x2F;li&gt;
&lt;li&gt;人脸特征512维&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;天猫精灵行为数据集&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;天猫精灵历史行为数据集（训练集2） 1M+&lt;&#x2F;li&gt;
&lt;li&gt;用户行为数据集（测试集3） 500+&lt;&#x2F;li&gt;
&lt;li&gt;实时用户行为数据集（测试集4） 1000+&lt;&#x2F;li&gt;
&lt;li&gt;行为特征700维&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;si-ge-ren-wu&quot;&gt;&lt;strong&gt;四个任务&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;根据测试集1每条数据的特征向量，在训练集1中找出该病例（人）对应的所有记录。&lt;&#x2F;li&gt;
&lt;li&gt;对测试集2的每条数据，根据其特征向量进行实时分类（人）。&lt;&#x2F;li&gt;
&lt;li&gt;根据测试集3每条数据的特征向量，在训练集2中找出该用户行为（领域+意图）对应的所有记录。&lt;&#x2F;li&gt;
&lt;li&gt;对测试集4的每条数据，根据其特征向量进行实时分类（领域+意图）。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;xing-neng-yao-qiu&quot;&gt;&lt;strong&gt;性能要求&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;四个任务总运行时间不能超过3小时。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;对每条实时数据完成实时分类的响应时间不能超过500ms。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;平台和组件&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Flink，PyFlink，Flink ai_flow，Proxima，Intel Zoo cluster serving&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;gong-zuo-liu&quot;&gt;工作流&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;blog&#x2F;flink&#x2F;flink-workflow.png&quot; alt=&quot;flink-workflow&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;官方提供一套docker环境及baseline代码，由于新冠病例和天猫精灵在算法任务上的相似性，编写两个 workflow 配置文件(yaml)，使得一套python代码运行两个算法。&lt;&#x2F;p&gt;
&lt;p&gt;基本思路：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;并发读入100万条训练集（并发100），训练AutoEncoder模型，为高维特征降维；&lt;&#x2F;li&gt;
&lt;li&gt;用训练好的模型对特征向量降维（并发16）；&lt;&#x2F;li&gt;
&lt;li&gt;用Proxima HnswBuilder 对降维后的特征向量创建索引；&lt;&#x2F;li&gt;
&lt;li&gt;对测试集1中的样本选出Top1024+1（样本）个候选者；&lt;&#x2F;li&gt;
&lt;li&gt;对候选者进行聚类（算法：Chinese Whisper），得出任务1的结果&lt;&#x2F;li&gt;
&lt;li&gt;从kafka读取测试集2，选出Top1，并以对应UUID作为分类label， 得出任务2的结果。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;shu-ju-yu-chu-li&quot;&gt;数据预处理&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;病例行动数据集&lt;&#x2F;strong&gt;
不含异常数据，且特征向量已经L2 Normalization，不需要特别的预处理。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;天猫精灵行为数据集&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;存在一些异常数据，需要做以下预处理：&lt;&#x2F;p&gt;
&lt;p&gt;移除某些特征向量数据末尾多出的空格（注：如果不做相应处理，score3通常得0分）&lt;&#x2F;p&gt;
&lt;p&gt;Re-generate UUID for duplicated UUID&lt;&#x2F;p&gt;
&lt;p&gt;Processing zero vector&lt;&#x2F;p&gt;
&lt;p&gt;Processing duplicated vector&lt;&#x2F;p&gt;
&lt;p&gt;L2 Normalization&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-xun-lian&quot;&gt;模型训练&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;目标：对特征向量降维&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;算法比较：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Simple AutoEncoder （实测效果好，稳定，性能好，采用）&lt;&#x2F;li&gt;
&lt;li&gt;Deep AutoEncoder（实测效果好，性能一般，最终未采用）&lt;&#x2F;li&gt;
&lt;li&gt;VAE (Variational AutoEncoder) （实测效果相对较差，未采用）&lt;&#x2F;li&gt;
&lt;li&gt;PCA (Principal Component Analysis) （实测效果相对较差，未采用）&lt;&#x2F;li&gt;
&lt;li&gt;NMF (Non-negative matrix factorization) （实测效果相对较差，未采用）&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;模型参数&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Loss Function: MSE&lt;&#x2F;li&gt;
&lt;li&gt;Active Function: Linear&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;维度选择&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;新冠病例： 512 =&amp;gt; 128&lt;&#x2F;li&gt;
&lt;li&gt;天猫精灵： 700 =&amp;gt; 128&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;ji-shu-zhan-xing-neng-zhi-biao&quot;&gt;技术栈性能指标&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Intel Zoo Cluster Serving&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;支持Tensorflow Saved Model 以及PyTorch Model for Inference&lt;&#x2F;li&gt;
&lt;li&gt;支持并发Inference（本赛题设置为16个并发），在多并发下运行稳定&lt;&#x2F;li&gt;
&lt;li&gt;模型针对CPU做了优化，无需GPU环境&lt;&#x2F;li&gt;
&lt;li&gt;自动生成配置，方便部署&lt;&#x2F;li&gt;
&lt;li&gt;响应时间短。平均每个请求响应时间实测小于35ms，充分满足本方案中的性能需求。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;达摩院proxima&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;使用Proxima HnswBuilder 创建索引，使用HnswSearch search vector&lt;&#x2F;li&gt;
&lt;li&gt;支持海量数据向量检索&lt;&#x2F;li&gt;
&lt;li&gt;召回率高，Top100 召回率超过98.5%&lt;&#x2F;li&gt;
&lt;li&gt;检索性能高，在本赛题中，平均每个请求(TopK=1024)的响应时间小于3ms，完全满足TopK筛选+再聚类这样类型的应用需求，对于实时的向量检索也毫无压力。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;online-data-kafka-chao-shi-wen-ti&quot;&gt;Online Data (kafka) 超时问题&lt;&#x2F;h2&gt;
&lt;p&gt;按参考代码标准流程，可能是初始化较慢的原因，在开始时有8秒延迟，将导致16条数据被超时。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;方法一、使用ai_flow 内建的算子&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用&lt;code&gt;ai_flow.read_example&lt;&#x2F;code&gt;、&lt;code&gt;ai_flow.predict&lt;&#x2F;code&gt;、&lt;code&gt;ai_flow.transform&lt;&#x2F;code&gt;和&lt;code&gt;ai_flow.write_example&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;在其中的&lt;code&gt;SourceExecutor&#x2F;SinkExecutor&lt;&#x2F;code&gt;实现类中使用PyFlink TABLE API(For Kafka) 读&#x2F;写Kafka Topic&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;为相应Flink job的&lt;code&gt;StreamExecutionEnvironment&lt;&#x2F;code&gt;设置参数：&lt;code&gt;stream_env.enable_checkpointing(250)&lt;&#x2F;code&gt;
该参数默认为&lt;code&gt;3000ms&lt;&#x2F;code&gt;，&lt;code&gt;3000ms&lt;&#x2F;code&gt;会导致每3秒才集中从Kafka Topic中读出6条数据。所以，如果不设置这个参数，必定会导致每6条数据中平均有5条会超时500ms，使得实时数据(score2和score4)得分很难超过100分（满分500分），因此必须改变这个参数设置。针对本赛题，可以设置为&lt;code&gt;250ms&lt;&#x2F;code&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;方法一在产线上应用没什么问题，但是在本比赛中它有一个小问题，那就是初始会有8秒延迟，这个延迟会使得赛题程序开始发送的约16条数据被TABLE API(For Kafka)读到时都会超时500ms，从而对最终评分有所影响（实测大概影响6分左右）。
使用方法一可确保只会有少量的初始数据（实测16条左右）产生超时。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;方法二、使用ai_flow的用户自定义算子&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;ai_flow支持更为灵活的用户自定义算子&lt;code&gt;af.user_define_operation&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;在用户自定义算子的Executor实现类中，直接使用Kafka Consumer&#x2F;Producer 读写Kafka Topic&lt;&#x2F;li&gt;
&lt;li&gt;直接通过 Kafka consumer从Kafka Topic读取数据，然后call Inference API (by Zoo cluster serving) 降维，然后使用Proxima search API search Top1 UUID，然后得出分类label，最后直接通过Kafka Producer 将结果数据写入Kafka Topic。
使用方法二可避免初始16条数据的超时问题，设置好关键参数 (如&lt;code&gt;fetch_max_wait_ms=200&lt;&#x2F;code&gt;)，可确保所有数据都不会超时。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>基于深度学习的图像分割算法思路演进</title>
        <published>2019-03-05T00:00:00+00:00</published>
        <updated>2019-03-22T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/segment/"/>
        <id>/blog/segment/</id>
        
        <summary type="html">&lt;p&gt;图像分割问题主要流行四类任务，它们分别是目标检测( Object Detection )、语义分割( Semantic Segmentation )、实例分割( Instance Segmentation ) 和全景分割( Panoptic Segmentation )。这四类任务的意义分别为：&lt;&#x2F;p&gt;
</summary>
        
    </entry>
    <entry xml:lang="en">
        <title>卷积备忘录</title>
        <published>2019-01-05T00:00:00+00:00</published>
        <updated>2021-02-17T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/convolution/"/>
        <id>/blog/convolution/</id>
        
        <summary type="html">&lt;p&gt;从卷积的数学原理开始，跟进卷积技术的发展和演变，持续进行增补的长文。&lt;&#x2F;p&gt;
</summary>
        
    </entry>
    <entry xml:lang="en">
        <title>概率图模型 HMM、MEMM、CRF</title>
        <published>2018-05-08T00:00:00+00:00</published>
        <updated>2018-05-08T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/crf/"/>
        <id>/blog/crf/</id>
        
        <content type="html" xml:base="/blog/crf/">&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;35866596&#x2F;answer&#x2F;236886066&quot;&gt;转自知乎&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;此文不错，在原译文基础上轻微修订&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;yi-preface&quot;&gt;&lt;strong&gt;一、Preface&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;之前刚接触NLP时做相关的任务，也必然地涉及到了序列处理任务，然后自然要接触到概率图模型。当时在全网搜中文资料，陆续失望地发现竟然真的没有讲得清楚的博文，发现基本是把李航老师书里或CRF tutorial等资料的文字论述和公式抄来抄去的。当然，没有说别人讲的是错的，只是觉得，要是没有把东西说的让读者看得懂，那也是没意义啊。或者有些吧，就是讲了一大堆的东西，貌似也明白了啥，但还是不能让我很好的理解CRF这些模型究竟是个啥，完了还是有一头雾水散不开的感觉。试想，一堆公式扔过来，没有个感性理解的过渡，怎么可能理解的了。我甚至觉得，如果博客让人看不懂，那说明要么自己没理解透要么就是思维不清晰讲不清楚。所以默想，深水区攻坚还是要靠自己，然后去做调研做research，所以就写了个这个学习记录。&lt;&#x2F;p&gt;
&lt;p&gt;所以概率图的研究学习思考列入了我的任务清单。不过平时的时间又非常的紧，只能陆陆续续的思考着，所以时间拖得也真是长啊。&lt;&#x2F;p&gt;
&lt;p&gt;这是个学习笔记。相比其他的学习模型，概率图貌似确实是比较难以理解的。这里我基本全部用自己的理解加上自己的语言习惯表达出来，off the official form，表达尽量接地气。我会尽量将我所有理解过程中的每个关键小细节都详细描述出来，以使对零基础的初学者友好。包括理论的来龙去脉，抽象具象化，模型的构成，模型的训练过程，会注重类比的学习。&lt;&#x2F;p&gt;
&lt;p&gt;根据现有资料，我是按照概率图模型将HMM，MEMM，CRF放在这里一起对比学习。之所以把他们拿在一起，是因为他们都用于标注问题。并且之所以放在概率图框架下，是完全因为自己top-down思维模式使然。另外，概率图下还有很多的模型，这儿只学习标注模型。&lt;&#x2F;p&gt;
&lt;p&gt;正儿八经的，我对这些个概率图模型有了彻悟，是从我明白了生成式模型与判别式模型的那一刻。一直在思考从概率图模型角度讲他们的区别到底在哪。&lt;&#x2F;p&gt;
&lt;p&gt;另外，篇幅略显长，但咱们不要急躁，好好看完这篇具有良好的上下文的笔记，那肯定是能理解的，或者就多看几遍。&lt;&#x2F;p&gt;
&lt;p&gt;个人学习习惯就是，&lt;strong&gt;要尽可能地将一群没有结构的知识点融会贯通，再用一条树状结构的绳将之串起来，结构化，就是说要成体系，这样把绳子头一拎所有的东西都能拿起来&lt;&#x2F;strong&gt;。学习嘛，应该要是一个熵减的过程，卓有成效的学习应该是混乱度越来越小！这个思维方式对我影响还是蛮大的。&lt;&#x2F;p&gt;
&lt;p&gt;在正式内容之前，还是先要明确下面这一点，最好脑子里形成一个定势：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;统计机器学习所有的模型（个别instant model和优化算法以及其他的特种工程知识点除外）的工作流程都是如此：
a.训练模型参数，得到模型（由参数唯一确定），
b.预测给定的测试数据。
拿这个流程去挨个学习模型，思路上会非常顺畅。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;除此之外，对初学者的关于机器学习的入门学习方式也顺带表达一下(empirical speaking)：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;a.完整特征工程竞赛
b.野博客理论入门理解
c.再回到代码深入理解模型内部
d.再跨理论，查阅经典理论巨作。这时感性理性都有一定高度，会遇到很多很大的理解上的疑惑，这时3大经典可能就可以发挥到最大作用了。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;很多beginer，就比如说学CRF模型，然后一上来就摆一套复杂的公式，什么我就问，这能理解的了吗？这是正确的开启姿势吗？当然了，也要怪那些博主，直接整一大堆核心公式，实际上读者的理解门槛可能就是一个过渡性的细枝末节而已。没有上下文的教育肯定是失败的（这一点我又想吐槽国内绝大部分本科的院校教育模式）。所以说带有完整上下文信息以及过程来龙去脉交代清楚才算到位吧。&lt;&#x2F;p&gt;
&lt;p&gt;而不是一上来就死啃被人推荐的“经典资料”，这一点相信部分同学会理解。好比以前本科零基础学c++ JAVA，上来就看primr TIJ，结果浪费了时间精力一直在门外兜圈。总结方法吸取教训，应该快速上手代码，才是最高效的。经典最好是用来查阅的工具书，我目前是李航周志华和经典的那3本迭代轮询看了好多轮，经常会反复查询某些model或理论的来龙去脉；有时候要查很多相关的东西，看这些书还是难以贯通，然后发现有些人的博客写的会更容易去理解。所以另外，学习资料渠道也要充分才行。&lt;&#x2F;p&gt;
&lt;p&gt;最后提示一下，&lt;strong&gt;请务必按照标题层级结构和目录一级一级阅读，防止跟丢。&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;er-prerequisite&quot;&gt;&lt;strong&gt;二、Prerequisite&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;2-1-gai-lu-tu&quot;&gt;&lt;strong&gt;2.1 概率图&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;之前刚接触CRF时，一上来试图越过一堆繁琐的概率图相关概念，不过sad to say, 这是后面的前驱知识，后面还得反过来补这个点。所以若想整体把握，系统地拿下这一块，应该还是要越过这块门槛的。&lt;&#x2F;p&gt;
&lt;p&gt;当然了，一开始只需略略快速看一篇，后面可再返过来补查。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-1-1-gai-lan&quot;&gt;&lt;strong&gt;2.1.1 概览&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构（个人非常喜欢这种类型的图）：&lt;&#x2F;p&gt;
&lt;img src=&quot;.&#x2F;v2.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;631&quot; data-rawheight=&quot;336&quot;  width=&quot;631&quot; &gt;
&lt;p&gt;在概率图模型中，数据(样本)由公式 $G=(V,E)$ 建模表示：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$V$ 表示节点，即随机变量（放在此处的，可以是一个token或者一个label），具体地，用 $Y = (y_1, \cdots, y_n)$ 为随机变量建模，注意 $Y$ 现在是代表了一批随机变量（想象对应一条sequence，包含了很多的token）， $P(Y)$ 为这些随机变量的分布；&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$E$ 表示边，即概率依赖关系。具体咋理解，还是要在后面结合HMM或CRF的graph具体解释。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;2-1-2-you-xiang-tu-vs-wu-xiang-tu&quot;&gt;&lt;strong&gt;2.1.2 有向图 vs. 无向图&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;上图可以看到，贝叶斯网络（信念网络）都是有向的，马尔科夫网络无向。所以，贝叶斯网络适合为有单向依赖的数据建模，马尔科夫网络适合实体之间互相依赖的建模。具体地，他们的核心差异表现在如何求 $P=(Y)$ ，即怎么表示 $Y=(y_1,\cdots,y_n)$ 这个的联合概率。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 有向图&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;对于有向图模型，这么求联合概率： $P(x_1, \cdots, x_n )=\prod_{i=0}P(x_i | \pi(x_{i}))$&lt;&#x2F;p&gt;
&lt;p&gt;举个例子，对于下面的这个有向图的随机变量(注意，这个图我画的还是比较广义的)：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;453&quot; src=&quot;.&#x2F;2.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;应该这样表示他们的联合概率:&lt;&#x2F;p&gt;
&lt;p&gt;$P(x_1, \cdots, x_n )=P(x_1)·P(x_2|x_1 )·P(x_3|x_2 )·P(x_4|x_2 )·P(x_5|x_3,x_4 )$&lt;&#x2F;p&gt;
&lt;p&gt;应该很好理解吧。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. 无向图&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;对于无向图，我看资料一般就指马尔科夫网络(注意，这个图我画的也是比较广义的)。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;260&quot; src=&quot;.&#x2F;3.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;如果一个graph太大，可以用因子分解将 &lt;img src=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;equation?tex=P%3D%28Y%29&quot; alt=&quot;P=(Y)&quot; &#x2F;&gt; 写为若干个联合概率的乘积。咋分解呢，将一个图分为若干个“小团”，注意每个团必须是“最大团”（就是里面任何两个点连在了一块，具体……算了不解释，有点“最大连通子图”的感觉），则有：&lt;&#x2F;p&gt;
&lt;p&gt;$P(Y )=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c} )$&lt;&#x2F;p&gt;
&lt;p&gt;其中, $Z(x) = \sum_{Y} \prod_{c}\psi_{c}(Y_{c} )$，公式应该不难理解吧，归一化是为了让结果算作概率。&lt;&#x2F;p&gt;
&lt;p&gt;所以像上面的无向图：&lt;&#x2F;p&gt;
&lt;p&gt;$P(Y )=\frac{1}{Z(x)} ( \psi_{1}(X_{1}, X_{3}, X_{4} ) · \psi_{2}(X_{2}, X_{3}, X_{4} ) )$&lt;&#x2F;p&gt;
&lt;p&gt;其中， $\psi_{c}(Y_{c} )$ 是一个最大团 &lt;img src=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;equation?tex=C&quot; alt=&quot;C&quot; &#x2F;&gt; 上随机变量们的联合概率，一般取指数函数的：&lt;&#x2F;p&gt;
&lt;p&gt;$\psi_{c}(Y_{c} ) = e^{-E(Y_{c})} =e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)}$
好了，管这个东西叫做&lt;code&gt;势函数&lt;&#x2F;code&gt;。注意 $e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)}$ 是否有看到CRF的影子。&lt;&#x2F;p&gt;
&lt;p&gt;那么概率无向图的联合概率分布可以在因子分解下表示为：&lt;&#x2F;p&gt;
&lt;p&gt;$P(Y )=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c} ) = \frac{1}{Z(x)} \prod_{c} e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)} = \frac{1}{Z(x)} e^{\sum_{c}\sum_{k}\lambda_{k}f_{k}(y_{i},y_{i-1},x,i)}$&lt;&#x2F;p&gt;
&lt;p&gt;注意，这里的理解还蛮重要的，注意递推过程，敲黑板，这是CRF的开端！
这个由&lt;code&gt;Hammersly-Clifford law&lt;&#x2F;code&gt;保证，具体不展开。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-1-3-ma-er-ke-fu-jia-she-ma-er-ke-fu-xing&quot;&gt;&lt;strong&gt;2.1.3 马尔科夫假设&amp;amp;马尔科夫性&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;这个也属于前馈知识。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 马尔科夫假设&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;额应该是齐次马尔科夫假设，这样假设：马尔科夫链 $(x_{1},\cdots,x_{n})$ 里的 $x_{i}$ 总是只受 $x_{i-1}$ 一个人的影响。
马尔科夫假设这里相当于就是个1-gram。&lt;&#x2F;p&gt;
&lt;p&gt;马尔科夫过程呢？即，在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。最简单的马尔科夫过程是一阶的，即只依赖于前一个状态。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. 马尔科夫性&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;马尔科夫性是是保证或者判断概率图是否为概率无向图的条件。&lt;&#x2F;p&gt;
&lt;p&gt;三点内容：a. 成对，b. 局部，c. 全局。&lt;&#x2F;p&gt;
&lt;p&gt;我觉得这个不用展开。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-2-pan-bie-shi-discriminative-mo-xing-vs-sheng-cheng-shi-generative-mo-xing&quot;&gt;&lt;strong&gt;2.2 判别式(discriminative)模型 vs. 生成式(generative)模型&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;在监督学习下，模型可以分为判别式模型与生成式模型。&lt;&#x2F;p&gt;
&lt;p&gt;重点来了。上面有提到，我理解了HMM、CRF模型的区别是从理解了判别式模型与生成式模型的那刻，并且瞬间对其他的模型有一个恍然大悟。我记得是一年前就开始纠结这两者的区别，但我只能说，栽在了一些烂博客上，大部分都没有自己的insightful理解，也就是一顿官话，也真是难以理解。后来在知乎上一直琢磨别人的答案，然后某日早晨终于豁然开朗，就是这种感觉。&lt;&#x2F;p&gt;
&lt;p&gt;好了，我要用自己的理解来转述两者的区别了below。&lt;&#x2F;p&gt;
&lt;p&gt;先问个问题，根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT……）与B批模型（NB、LDA……），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。&lt;&#x2F;p&gt;
&lt;p&gt;2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布 $P(X,Y)$ (注意 $X$ 包含所有的特征 $x_{i}$ ， $Y$ 包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布 $P(X,Y)$ ，再结合新样本给的 $X$ ，通过条件概率就能出来 $Y$：
$P(Y|X) = \frac{P(X,Y)}{P(X)}$
好了，应该说清楚了。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;1. 判别式模型&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：&lt;strong&gt;判别模型是直接对&lt;&#x2F;strong&gt; $P(Y|X)$ &lt;strong&gt;建模&lt;&#x2F;strong&gt;，就是说，直接根据X特征来对Y建模训练。&lt;&#x2F;p&gt;
&lt;p&gt;具体地，我的训练过程是确定构件 $P(Y|X)$ 模型里面“复杂映射关系”中的参数，完了再去inference一批新的sample。&lt;&#x2F;p&gt;
&lt;p&gt;所以判别式模型的特征总结如下：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;对 $P(Y|X)$ 建模&lt;&#x2F;li&gt;
&lt;li&gt;对所有的样本只构建一个模型，确认总体判别边界&lt;&#x2F;li&gt;
&lt;li&gt;观测到输入什么特征，就预测最可能的label&lt;&#x2F;li&gt;
&lt;li&gt;另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;2. 生成式模型&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型 $P(X,Y)$ ，也就是说，&lt;strong&gt;我在训练阶段是只对&lt;&#x2F;strong&gt; $P(X,Y)$&lt;strong&gt;建模&lt;&#x2F;strong&gt;，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算 $P(Y|X)$，导出 $Y$ ,但这已经不属于建模阶段了。&lt;&#x2F;p&gt;
&lt;p&gt;结合NB过一遍生成式模型的工作流程。学习阶段，建模： $P(X,Y)=P(X|Y)P(Y)$ （当然，NB具体流程去隔壁参考）,然后 $P(Y|X) = \frac{P(X,Y)}{P(X)}$ 。
另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。&lt;&#x2F;p&gt;
&lt;p&gt;所以生成式总结下有如下特点：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;对 $P(X,Y)$ 建模&lt;&#x2F;li&gt;
&lt;li&gt;这里我们主要讲分类问题，所以是要对每个label($y_{i}$) 都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）&lt;&#x2F;li&gt;
&lt;li&gt;中间生成联合分布，并可生成采样数据。&lt;&#x2F;li&gt;
&lt;li&gt;生成式模型的优点在于，所包含的信息非常齐全，我称之为“上帝信息”，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。
最后identity the picture below:&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;80%&quot; src=&quot;.&#x2F;4.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-3-xu-lie-jian-mo&quot;&gt;&lt;strong&gt;2.3 序列建模&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;为了号召零门槛理解，现在解释如何为序列问题建模。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;80%&quot; src=&quot;.&#x2F;5.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;序列包括时间序列以及general sequence，但两者无异。连续的序列在分析时也会先离散化处理。常见的序列有如：时序数据、本文句子、语音数据、等等。&lt;&#x2F;p&gt;
&lt;p&gt;广义下的序列有这些特点：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;节点之间有关联依赖性&#x2F;无关联依赖性&lt;&#x2F;li&gt;
&lt;li&gt;序列的节点是随机的&#x2F;确定的&lt;&#x2F;li&gt;
&lt;li&gt;序列是线性变化&#x2F;非线性的&lt;&#x2F;li&gt;
&lt;li&gt;……&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;对不同的序列有不同的问题需求，常见的序列建模方法总结有如下：&lt;&#x2F;p&gt;
&lt;p&gt;1. 拟合，预测未来节点（或走势分析）：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a. 常规序列建模方法：AR、MA、ARMA、ARIMA&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;b. 回归拟合&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;c. Neural Networks&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;2. 判断不同序列类别，即分类问题：HMM、CRF、General Classifier（ML models、NN models）&lt;&#x2F;p&gt;
&lt;p&gt;3. 不同时序对应的状态的分析，即序列标注问题：HMM、CRF、RecurrentNNs&lt;&#x2F;p&gt;
&lt;p&gt;在本篇文字中，我们只关注在2. &amp;amp; 3.类问题下的建模过程和方法。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;san-hmm&quot;&gt;&lt;strong&gt;三、HMM&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;最早接触的是HMM。较早做过一个项目，关于声波手势识别，跟声音识别的机制一样，使用的正是HMM的一套方法。后来又用到了 &lt;em&gt;kalman filter&lt;&#x2F;em&gt;，之后做序列标注任务接触到了CRF，所以整个概率图模型还是接触的方面还蛮多。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-1-li-jie-hmm&quot;&gt;&lt;strong&gt;3.1 理解HMM&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;在2.2、2.3中提序列的建模问题时，我们只是讨论了常规的序列数据，e.g., $(X_{1},\cdots,X_{n})$ ,像2.3的图片那样。像这种序列一般用马尔科夫模型就可以胜任。实际上我们碰到的更多的使用HMM的场景是每个节点 $X_{i}$ 下还附带着另一个节点 $Y_{i}$ ，正所谓&lt;strong&gt;隐含&lt;&#x2F;strong&gt;马尔科夫模型，那么除了正常的节点，还要将&lt;strong&gt;隐含状态节点&lt;&#x2F;strong&gt;也得建模进去。正儿八经地，将 $X_{i} 、 Y_{i}$ 换成 $i_{i} 、o_{i}$ ,并且他们的名称变为状态节点、观测节点。状态节点正是我的隐状态。&lt;&#x2F;p&gt;
&lt;p&gt;HMM属于典型的生成式模型。对照2.1的讲解，应该是要从训练数据中学到数据的各种分布，那么有哪些分布呢以及是什么呢？直接正面回答的话，正是&lt;strong&gt;HMM的5要素&lt;&#x2F;strong&gt;，其中有3个就是整个数据的不同角度的概率分布：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$N$ ，隐藏状态集 $N = \lbrace q_{1}, \cdots, q_{N} \rbrace$ , 我的隐藏节点不能随意取，只能限定取包含在隐藏状态集中的符号。&lt;&#x2F;li&gt;
&lt;li&gt;$M$，观测集 $M = \lbrace v_{1}, \cdots, v_{M} \rbrace$ , 同样我的观测节点不能随意取，只能限定取包含在观测状态集中的符号。&lt;&#x2F;li&gt;
&lt;li&gt;$A$ ，状态转移概率矩阵，这个就是其中一个概率分布。他是个矩阵， $ A = [a_{ij}\rbrack_{N \times N} $ （N为隐藏状态集元素个数），其中 $a_{ij} = P(i_{t+1}|i_{t})， i_{t}$ 即第i个隐状态节点,即所谓的状态转移嘛。&lt;&#x2F;li&gt;
&lt;li&gt;$B$ ，观测概率矩阵，这个就是另一个概率分布。他是个矩阵， $B = [b_{ij}\rbrack_{N \times M}$ （$N$为隐藏状态集元素个数，$M$为观测集元素个数），其中 $b_{ij} = P(o_{t}|i_{t})， o_{t}$ 即第$i$个观测节点，$i_{t}$ 即第$i$个隐状态节点，即所谓的观测概率（发射概率）嘛。&lt;&#x2F;li&gt;
&lt;li&gt;$π$ ，指模型在初始时刻各状态(来自状态集$N$)出现的概率。通常，第一个隐状态节点 $i_{t}$的隐状态可由EM方法学得,故$π$在初始化时可随机给定。(&lt;em&gt;这里原句读不通，由freeopenn修订&lt;&#x2F;em&gt;)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;所以图看起来是这样的：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;415&quot; src=&quot;.&#x2F;6.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;看的很清楚，我的模型先去学习要确定以上5要素，之后在inference阶段的工作流程是：首先，隐状态节点 $i_{t}$ 是不能直接观测到的数据节点， $o_{t}$ 才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系， $i_{t}$ 在A的指导下生成下一个隐状态节点 $i_{t+1}$ ，并且 $i_{t}$ 在 $B$ 的指导下生成依赖于该 $i_{t}$ 的观测节点 $o_{t}$ , 并且我只能观测到序列 $(o_{1}, \cdots, o_{i})$ 。&lt;&#x2F;p&gt;
&lt;p&gt;好，举例子说明（序列标注问题，POS，标注集BES）：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;input: &quot;学习出一个模型，然后再预测出一条指定&quot;&lt;&#x2F;p&gt;
&lt;p&gt;expected output: 学&#x2F;B 习&#x2F;E 出&#x2F;S 一&#x2F;B 个&#x2F;E 模&#x2F;B 型&#x2F;E ，&#x2F;S 然&#x2F;B 后&#x2F;E 再&#x2F;E 预&#x2F;B 测&#x2F;E ……&lt;&#x2F;p&gt;
&lt;p&gt;其中，input里面所有的char构成的字表，形成观测集 $M$ ，因为字序列在inference阶段是我所能看见的；标注集BES构成隐藏状态集 $N$ ，这是我无法直接获取的，也是我的预测任务；至于 $A、B、π$ ，这些概率分布信息（上帝信息）都是我在学习过程中所确定的参数。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;然后一般初次接触的话会疑问：为什么要这样？……好吧，就应该是这样啊，根据具有同时带着隐藏状态节点和观测节点的类型的序列，在HMM下就是这样子建模的。&lt;&#x2F;p&gt;
&lt;p&gt;下面来点高层次的理解：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模 $P(O,I) = \sum_{t=1}^{T}P(I_{t} | I_{t-1})P(O_{t} | I_{t})$ (注意，这个公式不在模型运行的任何阶段能体现出来，只是我们都去这么来表示HMM是个生成式模型，他的联合概率 $P(O,I)$ 就是这么计算的)。&lt;&#x2F;li&gt;
&lt;li&gt;并且B中 $b_{ij} = P(o_{t}|i_{t})$ ，这意味着o对i有依赖性。&lt;&#x2F;li&gt;
&lt;li&gt;在A中， $a_{ij} = P(i_{t+1}|i_{t})$ ，也就是说只遵循了一阶马尔科夫假设，1-gram。试想，如果数据的依赖超过1-gram，那肯定HMM肯定是考虑不进去的。这一点限制了HMM的性能。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;3-2-mo-xing-yun-xing-guo-cheng&quot;&gt;&lt;strong&gt;3.2 模型运行过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;模型的运行过程（工作流程）对应了HMM的3个问题。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-2-1-xue-xi-xun-lian-guo-cheng&quot;&gt;&lt;strong&gt;3.2.1 学习训练过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;对照2.1的讲解，HMM学习训练的过程，就是找出数据的分布情况，也就是模型参数的确定。&lt;&#x2F;p&gt;
&lt;p&gt;主要学习算法按照训练数据除了观测状态序列 $(o_{1}, \cdots, o_{i})$ 是否还有隐状态序列 $(i_{1}, \cdots, i_{i})$ 分为：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;极大似然估计, with 隐状态序列&lt;&#x2F;li&gt;
&lt;li&gt;Baum-Welch(前向后向), without 隐状态序列&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;感觉不用做很多的介绍，都是很实实在在的算法，看懂了就能理解。简要提一下。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 极大似然估计&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的。所以极大似然估计法是非常常用的学习算法，我见过的很多代码里面也是这么计算的。比较简单。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;step1. 算A&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$\hat{a_{ij}} = \frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;step2. 算B&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$\hat{b_{j}}(k) = \frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;step3. 直接估计 $π$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;比如说，在代码里计算完了就是这样的：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;7.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;8.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;9.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. Baum-Welch(前向后向)&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;就是一个EM的过程，如果你对EM的工作流程有经验的话，对这个Baum-Welch一看就懂。EM的过程就是初始化一套值，然后迭代计算，根据结果再调整值，再迭代，最后收敛……好吧，这个理解是没有捷径的，去隔壁钻研EM吧。&lt;&#x2F;p&gt;
&lt;p&gt;这里只提一下核心。因为我们手里没有隐状态序列 $(i_{1}, \cdots, i_{i})$ 信息，所以我先必须给初值 $a_{ij}^{0}, b_{j}(k)^{0}, \pi^{0}$ ，初步确定模型，然后再迭代计算出 $a_{ij}^{n}, b_{j}(k)^{n}, \pi^{n}$ ,中间计算过程会用到给出的观测状态序列 $(o_{1}, \cdots, o_{i})$。另外，收敛性由EM的XXX定理保证。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-2-2-xu-lie-biao-zhu-jie-ma-guo-cheng&quot;&gt;&lt;strong&gt;3.2.2 序列标注（解码）过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;好了，学习完了HMM的分布参数，也就确定了一个HMM模型。需要注意的是，这个HMM是对我这一批全部的数据进行训练所得到的参数。&lt;&#x2F;p&gt;
&lt;p&gt;序列标注问题也就是“预测过程”，通常称为解码过程。对应了序列建模问题3.。对于序列标注问题，我们只需要学习出一个HMM模型即可，后面所有的新的sample我都用这一个HMM去apply。&lt;&#x2F;p&gt;
&lt;p&gt;我们的目的是，在学习后已知了 $P(Q,O)$ ,现在要求出 $P(Q|O)$ ，进一步&lt;&#x2F;p&gt;
&lt;p&gt;$Q_{max} = argmax_{allQ}\frac{P(Q,O)}{P(O)}$&lt;&#x2F;p&gt;
&lt;p&gt;再直白点就是，我现在要在给定的观测序列下找出一条隐状态序列，条件是这个隐状态序列的概率是最大的那个。&lt;&#x2F;p&gt;
&lt;p&gt;具体地，都是用Viterbi算法解码，是用DP思想减少重复的计算。Viterbi也是满大街的，不过要说的是，Viterbi不是HMM的专属，也不是任何模型的专属，他只是恰好被满足了被HMM用来使用的条件。谁知，现在大家都把Viterbi跟HMM捆绑在一起了, shame。&lt;&#x2F;p&gt;
&lt;p&gt;Viterbi计算有向无环图的一条最大路径，应该还好理解。如图：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;418&quot; src=&quot;.&#x2F;10.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;关键是注意，每次工作热点区只涉及到t 与 t-1,这对应了DP的无后效性的条件。如果对某些同学还是很难理解，请参考&lt;a href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20136144&quot;&gt;这个答案&lt;&#x2F;a&gt;下@Kiwee的回答吧。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-2-3-xu-lie-gai-lu-guo-cheng&quot;&gt;&lt;strong&gt;3.2.3 序列概率过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;我通过HMM计算出序列的概率又有什么用？针对这个点我把这个问题详细说一下。&lt;&#x2F;p&gt;
&lt;p&gt;实际上，序列概率过程对应了序列建模问题2.，即序列分类。
在3.2.2第一句话我说，在序列标注问题中，我用一批完整的数据训练出了一支HMM模型即可。好，那在序列分类问题就不是训练一个HMM模型了。我应该这么做（结合语音分类识别例子）：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;目标：识别声音是A发出的还是B发出的。
HMM建模过程：
1. 训练：我将所有A说的语音数据作为dataset_A,将所有B说的语音数据作为dataset_B（当然，先要分别对dataset A ,B做预处理encode为元数据节点，形成sequences）,然后分别用dataset_A、dataset_B去训练出HMM_A&#x2F;HMM_B
2. inference：来了一条新的sample（sequence），我不知道是A的还是B的，没问题，分别用HMM_A&#x2F;HMM_B计算一遍序列的概率得到 $P_{A}(S)、P_{B}(S)$ ，比较两者大小，哪个概率大说明哪个更合理，更大概率作为目标类别。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;所以，本小节的理解重点在于，&lt;strong&gt;如何对一条序列计算其整体的概率&lt;&#x2F;strong&gt;。即目标是计算出 $P(O|λ)$ 。这个问题前辈们在他们的经典中说的非常好了，比如参考李航老师整理的：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;直接计算法（穷举搜索）&lt;&#x2F;li&gt;
&lt;li&gt;前向算法&lt;&#x2F;li&gt;
&lt;li&gt;后向算法&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;后面两个算法采用了DP思想，减少计算量，即每一次直接引用前一个时刻的计算结果以避免重复计算，跟Viterbi一样的技巧。&lt;&#x2F;p&gt;
&lt;p&gt;还是那句，因为这篇文档不是专门讲算法细节的，所以不详细展开这些。毕竟，所有的科普HMM、CRF的博客貌似都是在扯这些算法，妥妥的街货，就不搬运了。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;si-memm&quot;&gt;&lt;strong&gt;四、MEMM&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;MEMM，即最大熵马尔科夫模型，这个是在接触了HMM、CRF之后才知道的一个模型。说到MEMM这一节时，得转换思维了，因为现在这MEMM属于判别式模型。&lt;&#x2F;p&gt;
&lt;p&gt;不过有一点很尴尬，MEMM貌似被使用或者讲解引用的不及HMM、CRF。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-1-li-jie-memm&quot;&gt;&lt;strong&gt;4.1 理解MEMM&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;这里还是啰嗦强调一下，MEMM正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，比如说序列求概率（分类）问题，我直接考虑找出函数分类边界。这一点跟HMM的思维方式发生了很大的变化，如果不对这一点有意识，那么很难理解为什么MEMM、CRF要这么做。&lt;&#x2F;p&gt;
&lt;p&gt;HMM中，观测节点 $o_{i}$ 依赖隐藏状态节点 $i_{i}$ ,也就意味着我的观测节点只依赖当前时刻的隐藏状态。但在更多的实际场景下，观测序列是需要很多的特征来刻画的，比如说，我在做NER时，我的标注 $i_{i}$ 不仅跟当前状态 $o_{i}$ 相关，而且还跟前后标注 $o_{j}(j \neq i)$ 相关，比如字母大小写、词性等等。&lt;&#x2F;p&gt;
&lt;p&gt;为此，提出来的MEMM模型就是能够直接允许**“定义特征”**，直接学习条件概率，即 $P(i_{i}|i_{i-1},o_{i}) (i = 1,\cdots,n)$ , 总体为：&lt;&#x2F;p&gt;
&lt;p&gt;$P(I|O) = \prod_{t=1}^{n}P(i_{i}|i_{i-1},o_{i}), i = 1,\cdots,n$&lt;&#x2F;p&gt;
&lt;p&gt;并且， $P(i|i^{&#x27;},o)$ 这个概率通过最大熵分类器建模（取名MEMM的原因）:&lt;&#x2F;p&gt;
&lt;p&gt;$P(i|i^{&#x27;},o) = \frac{1}{Z(o,i^{&#x27;})} exp(\sum_{a})\lambda_{a}f_{a}(o,i)$&lt;&#x2F;p&gt;
&lt;p&gt;重点来了，这是ME的内容，也是理解MEMM的关键： $Z(o,i^{&#x27;})$ 这部分是归一化； $f_{a}(o,i)$ 是&lt;strong&gt;特征函数&lt;&#x2F;strong&gt;，具体点，这个函数是需要去定义的; $λ$ 是特征函数的权重，这是个未知参数，需要从训练阶段学习而得。&lt;&#x2F;p&gt;
&lt;p&gt;比如我可以这么定义特征函数：&lt;&#x2F;p&gt;
&lt;p&gt;$$\begin{equation} f_{a}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定条件}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;&#x2F;p&gt;
&lt;p&gt;其中，特征函数 $f_{a}(o,i)$ 个数可任意制定， $(a = 1, \cdots, n)$&lt;&#x2F;p&gt;
&lt;p&gt;所以总体上，MEMM的建模公式这样：&lt;&#x2F;p&gt;
&lt;p&gt;$P(I|O) = \prod_{t=1}^{n}\frac{ exp(\sum_{a})\lambda_{a}f_{a}(o,i) }{Z(o,i_{i-1})} , i = 1,\cdots,n$&lt;&#x2F;p&gt;
&lt;p&gt;是的，公式这部分之所以长成这样，是由ME模型决定的。&lt;&#x2F;p&gt;
&lt;p&gt;请务必注意，理解&lt;strong&gt;判别模型&lt;&#x2F;strong&gt;和&lt;strong&gt;定义特征&lt;&#x2F;strong&gt;两部分含义，这已经涉及到CRF的雏形了。&lt;&#x2F;p&gt;
&lt;p&gt;所以说，他是判别式模型，直接对条件概率建模。 上图：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;415&quot; src=&quot;.&#x2F;11.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;MEMM需要两点注意：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;与HMM的 $o_{i}$ 依赖 $i_{i}$ 不一样，MEMM当前隐藏状态 $i_{i}$ 应该是依赖当前时刻的观测节点 $o_{i}$ 和上一时刻的隐藏节点 $i_{i-1}$&lt;&#x2F;li&gt;
&lt;li&gt;需要注意，之所以图的箭头这么画，是由MEMM的公式决定的，而公式是creator定义出来的。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;好了，走一遍完整流程。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;step1. 先预定义特征函数 $f_{a}(o,i)$ ，
step2. 在给定的数据上，训练模型，确定参数，即确定了MEMM模型
step3. 用确定的模型做序列标注问题或者序列求概率问题。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;4-2-mo-xing-yun-xing-guo-cheng&quot;&gt;&lt;strong&gt;4.2 模型运行过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;MEMM模型的工作流程也包括了学习训练问题、序列标注问题、序列求概率问题。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-2-1-xue-xi-xun-lian-guo-cheng&quot;&gt;&lt;strong&gt;4.2.1 学习训练过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;一套MEMM由一套参数唯一确定，同样地，我需要通过训练数据学习这些参数。MEMM模型很自然需要学习里面的特征权重λ。&lt;&#x2F;p&gt;
&lt;p&gt;不过跟HMM不用的是，因为HMM是生成式模型，参数即为各种概率分布元参数，数据量足够可以用最大似然估计。而判别式模型是用函数直接判别，学习边界，MEMM即通过特征函数来界定。但同样，MEMM也有极大似然估计方法、梯度下降、牛顿迭代发、拟牛顿下降、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。&lt;&#x2F;p&gt;
&lt;p&gt;嗯，具体详细求解过程貌似问题不大。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-2-2-xu-lie-biao-zhu-guo-cheng&quot;&gt;&lt;strong&gt;4.2.2 序列标注过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;还是跟HMM一样的，用学习好的MEMM模型，在新的sample（观测序列 $o_{1}, \cdots, o_{i}$ ）上找出一条概率最大最可能的隐状态序列 $i_{1}, \cdots, i_{i}$。&lt;&#x2F;p&gt;
&lt;p&gt;只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-2-3-xu-lie-qiu-gai-lu-guo-cheng&quot;&gt;&lt;strong&gt;4.2.3 序列求概率过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的MEMM，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。&lt;&#x2F;p&gt;
&lt;p&gt;应该可以不用展开，吧……&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-3-biao-zhu-pian-zhi&quot;&gt;&lt;strong&gt;4.3 标注偏置？&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;MEMM讨论的最多的是他的labeling bias 问题。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 现象&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;是从街货上烤过来的……&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;558&quot; src=&quot;.&#x2F;12.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;用Viterbi算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2。 解码过程细节（需要会viterbi算法这个前提）：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;P(1-&amp;gt; 1-&amp;gt; 1-&amp;gt; 1)= 0.4 x 0.45 x 0.5 = 0.09 ，
P(2-&amp;gt;2-&amp;gt;2-&amp;gt;2)= 0.2 X 0.3 X 0.3 = 0.018，
P(1-&amp;gt;2-&amp;gt;1-&amp;gt;2)= 0.6 X 0.2 X 0.5 = 0.06，
P(1-&amp;gt;1-&amp;gt;2-&amp;gt;2)= 0.4 X 0.55 X 0.3 = 0.066&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;但是得到的最优的状态转换路径是1-&amp;gt;1-&amp;gt;1-&amp;gt;1，为什么呢？因为状态2可以转换的状态比状态1要多，从而使转移概率降低,即MEMM倾向于选择拥有更少转移的状态。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. 解释原因&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;直接看MEMM公式：&lt;&#x2F;p&gt;
&lt;p&gt;$P(I|O) = \prod_{t=1}^{n}\frac{ exp[(\sum_{a})\lambda_{a}f_{a}(o,i)] }{Z(o,i_{i-1})} , i = 1,\cdots,n$&lt;&#x2F;p&gt;
&lt;p&gt;$∑$ 求和的作用在概率中是归一化，但是这里归一化放在了指数内部，管这叫local归一化。 来了，viterbi求解过程，是用dp的状态转移公式（MEMM的没展开，请参考CRF下面的公式），因为是局部归一化，所以MEMM的viterbi的转移公式的第二部分出现了问题，导致dp无法正确的递归到全局的最优。&lt;&#x2F;p&gt;
&lt;p&gt;$\delta_{i+1} = max_{1 \le j \le m}\lbrace \delta_{i}(I) + \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i) \rbrace$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;wu-crf&quot;&gt;&lt;strong&gt;五、CRF&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;我觉得一旦有了一个清晰的工作流程，那么按部就班地，没有什么很难理解的地方，因为整体框架已经胸有成竹了，剩下了也只有添砖加瓦小修小补了。有了上面的过程基础，CRF也是类似的，只是有方法论上的细微区别。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-1-li-jie-crf&quot;&gt;&lt;strong&gt;5.1 理解CRF&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;请看第一张概率图模型构架图，CRF上面是马尔科夫随机场（马尔科夫网络），而条件随机场是在给定的随机变量 $X$ （具体，对应观测序列 $o_{1}, \cdots, o_{i}$ ）条件下，随机变量 $Y$ （具体，对应隐状态序列 $i_{1}, \cdots, i_{i}$ ）的马尔科夫随机场。
广义的CRF的定义是： 满足 $P(Y_{v}|X,Y_{w},w \neq v) = P(Y_{v}|X,Y_{w},w \sim v)$ 的马尔科夫随机场叫做条件随机场（CRF）。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen注:
$Y_{w},w \neq v$ 表示除$v$以外观测集中的所有节点，
$Y_{w},w \sim v$ 表示观测集中$v$的邻接节点，&lt;&#x2F;p&gt;
&lt;p&gt;下面是另一种表达方式：
$P(Y_v|X,Y_{V\backslash{v}}) = P(Y_v|X,Y_{n(v)}) \$
其中：
$Y_{V\backslash{v}}$ 表示除$v$以外的$V$中所有节点，
$Y_{n(v)}$ 表示结点$v$的邻接节点&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;不过一般说CRF为序列建模，就专指CRF线性链（linear chain CRF）：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;415&quot; src=&quot;.&#x2F;13.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;在2.1.2中有提到过，概率无向图的联合概率分布可以在因子分解下表示为：&lt;&#x2F;p&gt;
&lt;p&gt;$$P(Y | X)=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c}|X ) = \frac{1}{Z(x)} \prod_{c} e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)} = \frac{1}{Z(x)} e^{\sum_{c}\sum_{k}\lambda_{k}f_{k}(y_{i},y_{i-1},x,i)}$$&lt;&#x2F;p&gt;
&lt;p&gt;而在线性链CRF示意图中，每一个（ $I_{i} \sim O_{i}$ ）对为一个最大团,即在上式中 $c = i$ 。并且线性链CRF满足 $P(I_{i}|O,I_{1},\cdots, I_{n}) = P(I_{i}|O,I_{i-1},I_{i+1})$ 。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;所以CRF的建模公式如下：&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;$$P(I | O)=\frac{1}{Z(O)} \prod_{i}\psi_{i}(I_{i}|O ) = \frac{1}{Z(O)} \prod_{i} e^{\sum_{k}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)} = \frac{1}{Z(O)} e^{\sum_{i}\sum_{k}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;&#x2F;p&gt;
&lt;p&gt;我要敲黑板了，这个公式是非常非常关键的，注意递推过程啊，我是怎么从 $∏$ 跳到 $e^{\sum}$ 的。&lt;&#x2F;p&gt;
&lt;p&gt;不过还是要多啰嗦一句，想要理解CRF，必须判别式模型的概念要深入你心。
正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，因
为我创造出来就是为了这个分边界的目的的。比如说序列求概率（分类）问
题，我直接考虑找出函数分类边界。所以才为什么会有这个公式。所以再看
到这个公式也别懵逼了，he was born for discriminating the given data
from different classes. 就这样。不过待会还会具体介绍特征函数部分的东西。&lt;&#x2F;p&gt;
&lt;p&gt;除了建模总公式，关键的CRF重点概念在MEMM中已强调过：&lt;strong&gt;判别式模型&lt;&#x2F;strong&gt;、&lt;strong&gt;特征函数&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 特征函数&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;上面给出了CRF的建模公式：&lt;&#x2F;p&gt;
&lt;p&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;下标 &lt;em&gt;i&lt;&#x2F;em&gt; 表示我当前所在的节点（token）位置。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;下标 &lt;em&gt;k&lt;&#x2F;em&gt; 表示我这是第几个特征函数，并且每个特征函数都附属一个权重 $\lambda_{k}$ ，也就是这么回事，每个团里面，我将为 $token_{i}$ 构造M个特征，每个特征执行一定的限定作用，然后建模时我再为每个特征函数加权求和。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$Z(O)$ 是用来归一化的，为什么？想想LR以及softmax为何有归一化呢，一样的嘛，形成概率值。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;再来个重要的理解。 $P(I|O)$ 这个表示什么？具体地，表示了在给定的一条观测序列 $O=(o_{1},\cdots, o_{i})$ 条件下，我用CRF所求出来的隐状态序列 $I=(i_{1},\cdots, i_{i})$ 的概率，注意，这里的 $I$ 是一条序列，有多个元素（一组随机变量），而至于观测序列 $O=(o_{1},\cdots, o_{i})$ ，它可以是一整个训练语料的所有的观测序列；也可以是在inference阶段的一句sample，比如说对于序列标注问题，我对一条sample进行预测，可能能得到 $P_{j}(I | O)（j=1,…,J)$,  $J$条隐状态$I$，但我肯定最终选的是最优概率的那条（by viterbi）。这一点希望你能理解。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;对于CRF，可以为他定义两款特征函数：转移特征&amp;amp;状态特征。 我们将建模总公式展开：&lt;&#x2F;p&gt;
&lt;p&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}=\frac{1}{Z(O)} e^{ [ \sum_{i}^{T}\sum_{j}^{J}\lambda_{j}t_{j}(O,I_{i-1},I_{i},i) + \sum_{i}^{T}\sum_{l}^{L}\mu_{l}s_{l}(O,I_{i},i) ] }$$&lt;&#x2F;p&gt;
&lt;p&gt;其中：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$t_{j}$ 为i处的转移特征，对应权重 $\lambda_{j}$ ,每个 $token_{i}$ 都有J个特征,转移特征针对的是前后token之间的限定。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;举个例子：&lt;&#x2F;p&gt;
&lt;p&gt;$$\begin{equation} t_{k=1}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定转移条件，比如前一个token是‘I’}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$s_l$为i 处的状态特征，对应权重$μ_l$，每个$token_i$都有L个特征&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;举个例子：&lt;&#x2F;p&gt;
&lt;p&gt;$$\begin{equation} s_{l=1}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定状态条件，比如当前token的POS是‘V’}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;不过一般情况下，我们不把两种特征区别的那么开，合在一起：&lt;&#x2F;p&gt;
&lt;p&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;&#x2F;p&gt;
&lt;p&gt;满足特征条件就取值为1，否则没贡献，甚至你还可以让他打负分，充分惩罚。&lt;&#x2F;p&gt;
&lt;p&gt;再进一步理解的话，我们需要把特征函数部分抠出来：&lt;&#x2F;p&gt;
&lt;p&gt;$$Score = \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)$$&lt;&#x2F;p&gt;
&lt;p&gt;是的，我们为 $token_{i}$ 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值……完了又扯到了log线性模型。现在稍作解释：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;log-linear models take the following form:
$P(y|x;\omega) = \frac{ exp(\omega·\phi(x,y)) }{ \sum_{y^{&#x27;}\in Y }exp(\omega·\phi(x,y^{‘})) }$&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;我觉得对LR或者sotfmax熟悉的对这个应该秒懂。然后CRF完美地满足这个形式，所以又可以归入到了log-linear models之中。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-2-mo-xing-yun-xing-guo-cheng&quot;&gt;&lt;strong&gt;5.2 模型运行过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;模型的工作流程，跟MEMM是一样的：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;step1. 先预定义特征函数 $f_{a}(o,i)$ ，&lt;&#x2F;li&gt;
&lt;li&gt;step2. 在给定的数据上，训练模型，确定参数 $\lambda_{k}$&lt;&#x2F;li&gt;
&lt;li&gt;step3. 用确定的模型做&lt;code&gt;序列标注问题&lt;&#x2F;code&gt;或者&lt;code&gt;序列求概率问题&lt;&#x2F;code&gt;。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;可能还是没做到100%懂，结合例子说明：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;5-2-1-xue-xi-xun-lian-guo-cheng&quot;&gt;&lt;strong&gt;5.2.1 学习训练过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;一套CRF由一套参数λ唯一确定（先定义好各种特征函数）。&lt;&#x2F;p&gt;
&lt;p&gt;同样，CRF用极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。其实能用在log-linear models上的求参方法都可以用过来。&lt;&#x2F;p&gt;
&lt;p&gt;嗯，具体详细求解过程貌似问题不大。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-2-2-xu-lie-biao-zhu-guo-cheng&quot;&gt;&lt;strong&gt;5.2.2 序列标注过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;还是跟HMM一样的，用学习好的CRF模型，在新的sample（观测序列 $o_{1}, \cdots, o_{i}$ ）上找出一条概率最大最可能的隐状态序列 $i_{1}, \cdots, i_{i}$ 。&lt;&#x2F;p&gt;
&lt;p&gt;只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。&lt;&#x2F;p&gt;
&lt;p&gt;啰嗦一下，我们就定义i处的局部状态为 $\delta_{i}(I)$ ,表示在位置i处的隐状态的各种取值可能为 &lt;em&gt;I&lt;&#x2F;em&gt; ，然后递推位置i+1处的隐状态，写出来的DP转移公式为：&lt;&#x2F;p&gt;
&lt;p&gt;$\delta_{i+1} = max_{1 \le j \le m}\lbrace \delta_{i}(I) + \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i) \rbrace$&lt;&#x2F;p&gt;
&lt;p&gt;这里没写规范因子 $Z(O)$ 是因为不规范化不会影响取最大值后的比较。&lt;&#x2F;p&gt;
&lt;p&gt;具体还是不展开为好。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-2-3-xu-lie-qiu-gai-lu-guo-cheng&quot;&gt;&lt;strong&gt;5.2.3 序列求概率过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的CRF，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。只是貌似很少看到拿CRF或者MEMM来做分类的，直接用网络模型不就完了不……&lt;&#x2F;p&gt;
&lt;p&gt;应该可以不用展开，吧……&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-3-crf-fen-xi&quot;&gt;&lt;strong&gt;5.3 CRF++分析&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;本来做task用CRF++跑过baseline,后来在对CRF做调研时，非常想透析CRF++的工作原理，以identify以及verify做的各种假设猜想。当然，也看过其他的CRF实现源码。&lt;&#x2F;p&gt;
&lt;p&gt;所以干脆写到这里来，结合CRF++实例讲解过程。&lt;&#x2F;p&gt;
&lt;p&gt;有一批语料数据，并且已经tokenized好了：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nuclear
theory
devoted
major
efforts
……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;并且我先确定了13个标注元素：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;B_MAT
B_PRO
B_TAS
E_MAT
E_PRO
E_TAS
I_MAT
I_PRO
I_TAS
O
S_MAT
S_PRO
S_TAS&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;1. 定义模板&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;按道理应该是定义特征函数才对吧？好的，在CRF++下，应该是先定义特征模板，然后用模板自动批量产生大量的特征函数。我之前也蛮confused的，用完CRF++还以为模板就是特征，后面就搞清楚了：每一条模板将在每一个token处生产若干个特征函数。&lt;&#x2F;p&gt;
&lt;p&gt;CRF++的模板（template）有U系列（unigram）、B系列(bigram)，不过我至今搞不清楚B系列的作用，因为U模板都可以完成2-gram的作用。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;U00:%x[-2,0]
U01:%x[-1,0]
U02:%x[0,0]
U03:%x[1,0]
U04:%x[2,0]&lt;&#x2F;p&gt;
&lt;p&gt;U05:%x[-2,0]&#x2F;%x[-1,0]&#x2F;%x[0,0]
U06:%x[-1,0]&#x2F;%x[0,0]&#x2F;%x[1,0]
U07:%x[0,0]&#x2F;%x[1,0]&#x2F;%x[2,0]
U08:%x[-1,0]&#x2F;%x[0,0]
U09:%x[0,0]&#x2F;%x[1,0]&lt;&#x2F;p&gt;
&lt;p&gt;B&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;所以，U00 - U09 我定义了10个模板。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. 产生特征函数&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;是的，会产生大量的特征。 U00 - U04的模板产生的是状态特征函数；U05 - U09的模板产生的是转移特征函数。&lt;&#x2F;p&gt;
&lt;p&gt;在CRF++中，每个特征都会try每个标注label（这里有13个），总共将生成 $N * L = i * k^{&#x27;} * L$ 个特征函数以及对应的权重出来。N表示每一套特征函数 $N= i * k^{&#x27;}$ ，L表示标注集元素个数。&lt;&#x2F;p&gt;
&lt;p&gt;比如训练好的CRF模型的部分特征函数是这样存储的：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;22607 B
790309 U00:%
3453892 U00:%)
2717325 U00:&amp;amp;
2128269 U00:&#x27;t
2826239 U00:(0.3534
2525055 U00:(0.593–1.118
197093 U00:(1)
2079519 U00:(1)L=14w2−12w−FμνaFaμν
2458547 U00:(1)δn=∫−∞En+1ρ˜(E)dE−n
1766024 U00:(1.0g
2679261 U00:(1.1wt%)
1622517 U00:(100)
727701 U00:(1000–5000A)
2626520 U00:(10a)
2626689 U00:(10b)
……
2842814 U07:layer&#x2F;thicknesses&#x2F;Using
2847533 U07:layer&#x2F;thicknesses&#x2F;are
2848651 U07:layer&#x2F;thicknesses&#x2F;in
331539 U07:layer&#x2F;to&#x2F;the
1885871 U07:layer&#x2F;was&#x2F;deposited
……（数量非常庞大）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;其实也就是对应了这样些个特征函数：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;func1 = if (output = B and feature=&quot;U02:一&quot;) return 1 else return 0
func2 = if (output = M and feature=&quot;U02:一&quot;) return 1 else return 0
func3 = if (output = E and feature=&quot;U02:一&quot;) return 1 else return 0
func4 = if (output = S and feature=&quot;U02:一&quot;) return 1 else return 0&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;比如模板U06会从语料中one by one逐句抽出这些各个特征：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;一&#x2F;个&#x2F;人&#x2F;……
个&#x2F;人&#x2F;走&#x2F;……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;3. 求参&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;对上述的各个特征以及初始权重进行迭代参数学习。&lt;&#x2F;p&gt;
&lt;p&gt;在CRF++ 训练好的模型里，权重是这样的：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;0.3972716048310705
0.5078838237171732
0.6715316559507898
-0.4198827647512405
-0.4233310655891150
-0.4176580083832543
-0.4860489836004728
-0.6156475863742051
-0.6997919485753300
0.8309956709647820
0.3749695682658566
0.2627347894057647
0.0169732441379157
0.3972716048310705
0.5078838237171732
0.6715316559507898
……（数量非常庞大，与每个label的特征函数对应，我这有300W个）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;4. 预测解码&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;结果是这样的：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nuclear B_TAS
theory E_TAS
devoted O
major O
efforts O
……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;5-4-lstm-crf&quot;&gt;&lt;strong&gt;5.4 LSTM+CRF&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;LSTM+CRF这个组合其实我在知乎上答过问题，然后顺便可以整合到这里来。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1、perspectively&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;大家都知道，LSTM已经可以胜任序列标注问题了，为每个token预测一个label（LSTM后面接:分类器）；而CRF也是一样的，为每个token预测一个label。&lt;&#x2F;p&gt;
&lt;p&gt;但是，他们的预测机理是不同的。CRF是全局范围内统计归一化的条件状态转移概率矩阵，再预测出一条指定的sample的每个token的label；LSTM（RNNs，不区分here）是依靠神经网络的超强非线性拟合能力，在训练时将samples通过复杂到让你窒息的高阶高纬度异度空间的非线性变换，学习出一个模型，然后再预测出一条指定的sample的每个token的label。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2、LSTM+CRF&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;既然LSTM都OK了，为啥researchers搞一个LSTM+CRF的hybrid model?&lt;&#x2F;p&gt;
&lt;p&gt;哈哈，因为a single LSTM预测出来的标注有问题啊！举个segmentation例子(BES; char level)，plain LSTM 会搞出这样的结果：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;input&lt;&#x2F;strong&gt;: &quot;学习出一个模型，然后再预测出一条指定&quot;
&lt;strong&gt;expected output&lt;&#x2F;strong&gt;: 学&#x2F;B 习&#x2F;E 出&#x2F;S 一&#x2F;B 个&#x2F;E 模&#x2F;B 型&#x2F;E ，&#x2F;S 然&#x2F;B 后&#x2F;E 再&#x2F;E 预&#x2F;B 测&#x2F;E ……
&lt;strong&gt;real output&lt;&#x2F;strong&gt;: 学&#x2F;B 习&#x2F;E 出&#x2F;S 一&#x2F;B 个&#x2F;B 模&#x2F;B 型&#x2F;E ，&#x2F;S 然&#x2F;B 后&#x2F;B 再&#x2F;E 预&#x2F;B 测&#x2F;E ……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;看到不，用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。当然了，CRF还能学到更多的限定特征，那越多越好啊！&lt;&#x2F;p&gt;
&lt;p&gt;好了，那就把CRF接到LSTM上面，把LSTM在time_step上把每一个hidden_state的tensor输入给CRF，让LSTM负责在CRF的特征限定下，依照新的loss function，学习出一套新的非线性变换空间。&lt;&#x2F;p&gt;
&lt;p&gt;最后，不用说，结果还真是好多了呢。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;github.com&#x2F;scofield7419&#x2F;sequence-labeling-BiLSTM-CRF&quot;&gt;LSTM+CRF codes&lt;&#x2F;a&gt;, here. Go just take it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;liu-zong-jie&quot;&gt;&lt;strong&gt;六、总结&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;1-zong-ti-dui-bi&quot;&gt;&lt;strong&gt;1. 总体对比&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;应该看到了熟悉的图了，现在看这个图的话，应该可以很清楚地get到他所表达的含义了。这张图的内容正是按照生成式&amp;amp;判别式来区分的，NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;14.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-hmm-vs-memm-vs-crf&quot;&gt;&lt;strong&gt;2. HMM vs. MEMM vs. CRF&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;将三者放在一块做一个总结：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;HMM -&amp;gt; MEMM： HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;MEMM -&amp;gt; CRF:&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;ul&gt;
&lt;li&gt;CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。&lt;&#x2F;li&gt;
&lt;li&gt;HMM、MEMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去（这点问题应该只有HMM）。
CRF属于无向图，没有这种依赖性，克服此问题。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;3-machine-learning-models-vs-sequential-models&quot;&gt;&lt;strong&gt;3. Machine Learning models vs. Sequential models&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;为了一次将概率图模型理解的深刻到位，我们需要再串一串，更深度与原有的知识体系融合起来。&lt;&#x2F;p&gt;
&lt;p&gt;机器学习模型，按照学习的范式或方法，以及加上自己的理解，给常见的部分的他们整理分了分类（主流上，都喜欢从训练样本的歧义型分，当然也可以从其他角度来）：&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;一、监督：{

1.1 分类算法(线性和非线性)：{

    感知机

    KNN

    概率{
        朴素贝叶斯（NB）
        Logistic Regression（LR）
        最大熵MEM（与LR同属于对数线性分类模型）
    }

    支持向量机(SVM)

    决策树(ID3、CART、C4.5)

    assembly learning{
        Boosting{
            Gradient Boosting{
                GBDT
                xgboost（传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）；xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。）
            }
            AdaBoost
        }
        Bagging{
            随机森林
        }
        Stacking
    }

    ……
}

1.2 概率图模型：{
    HMM
    MEMM（最大熵马尔科夫）
    CRF
    ……
}

1.3 回归预测：{
    线性回归
    树回归
    Ridge岭回归
    Lasso回归
    ……
}

……
}

二、非监督：{
2.1 聚类：{
    1. 基础聚类
        K—mean
        二分k-mean
        K中值聚类
        GMM聚类
    2. 层次聚类
    3. 密度聚类
    4. 谱聚类()
}

2.2 主题模型:{
    pLSA
    LDA隐含狄利克雷分析
}

2.3 关联分析：{
    Apriori算法
    FP-growth算法
}

2.4 降维：{
    PCA算法
    SVD算法
    LDA线性判别分析
    LLE局部线性嵌入
}

2.5 异常检测：
……
}

三、半监督学习

四、迁移学习
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;（注意到，没有把神经网络体系加进来。因为NNs的范式很灵活，不太适用这套分法，largely, off this framework）&lt;&#x2F;p&gt;
&lt;p&gt;Generally speaking，机器学习模型，尤其是有监督学习，一般是为一条sample预测出一个label，作为预测结果。 但与典型常见的机器学习模型不太一样，序列模型（概率图模型）是试图为一条sample里面的每个基本元数据分别预测出一个label。这一点，往往是beginner伊始难以理解的。&lt;&#x2F;p&gt;
&lt;p&gt;具体的实现手段差异，就是：ML models通过直接预测得出label；Sequential models是给每个token预测得出label还没完，还得将他们每个token对应的labels进行组合，具体的话，用viterbi来挑选最好的那个组合。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;over&quot;&gt;&lt;strong&gt;over&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;有了这道开胃菜，接下来，读者可以完成这些事情：完善细节算法、阅读原著相关论文达到彻底理解、理解相关拓展概念、理论创新……&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;hope those hlpe!&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;欢迎留言！&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;有错误之处请多多指正，谢谢！&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;referrences&quot;&gt;&lt;strong&gt;Referrences:&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;《统计学习方法》，李航&lt;&#x2F;p&gt;
&lt;p&gt;《统计自然语言处理》，宗成庆&lt;&#x2F;p&gt;
&lt;p&gt;《 An Introduction to Conditional Random Fields for Relational Learning》， Charles Sutton， Andrew McCallum&lt;&#x2F;p&gt;
&lt;p&gt;《Log-Linear Models, MEMMs, and CRFs》，ichael Collins&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;35866596&quot;&gt;如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;www.cnblogs.com&#x2F;en-heng&#x2F;p&#x2F;6201893.html&quot;&gt;【中文分词】最大熵马尔可夫模型MEMM - Treant - 博客园&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;github.com&#x2F;timvieira&#x2F;crf&quot;&gt;timvieira&#x2F;crf&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;github.com&#x2F;shawntan&#x2F;python-crf&quot;&gt;shawntan&#x2F;python-crf&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=http%3A&#x2F;&#x2F;videolectures.net&#x2F;cikm08_elkan_llmacrf&#x2F;&quot;&gt;Log-linear Models and Conditional Random Fields&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;55755fc649b1&quot;&gt;如何轻松愉快地理解条件随机场（CRF）？&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;www.cnblogs.com&#x2F;pinard&#x2F;p&#x2F;7068574.html&quot;&gt;条件随机场CRF(三) 模型学习与维特比算法解码&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20279019&quot;&gt;crf++里的特征模板得怎么理解？&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=http%3A&#x2F;&#x2F;www.hankcs.com&#x2F;ml&#x2F;crf-code-analysis.html&quot;&gt;CRF++代码分析-码农场&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=http%3A&#x2F;&#x2F;blog.csdn.net&#x2F;aws3217150&#x2F;article&#x2F;details&#x2F;69212445&quot;&gt;CRF++源码解读 - CSDN博客&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=http%3A&#x2F;&#x2F;www.hankcs.com&#x2F;nlp&#x2F;the-crf-model-format-description.html&quot;&gt;CRF++模型格式说明-码农场&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;www.cnblogs.com&#x2F;syx-1987&#x2F;p&#x2F;4077325.html&quot;&gt;标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较&amp;lt;转&amp;gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;……&lt;&#x2F;p&gt;
&lt;p&gt;编辑于 2018-03-21&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;$Y_{w},w \neq v$ 表示除$v$以外观测集中的所有&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Estimator 编程指南</title>
        <published>2018-03-04T00:00:00+00:00</published>
        <updated>2018-04-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/estimator/"/>
        <id>/blog/estimator/</id>
        
        <content type="html" xml:base="/blog/estimator/">&lt;blockquote&gt;
&lt;p&gt;2018-04-12 第一次修订, 新增&quot;多GPU下的写法&quot;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;注：代码适用于 TF1.4 ~ TF1.7 。&lt;&#x2F;p&gt;
&lt;p&gt;为什么要使用Estimator, 仅&lt;a href=&quot;https:&#x2F;&#x2F;www.tensorflow.org&#x2F;programmers_guide&#x2F;estimators&quot;&gt;官方文档&lt;&#x2F;a&gt;里提到的第一条优点就让我不得不重视它。
大意是不管你在本地环境还是分布式环境，不管你用一个或多个CPU、GPU还是TPU训练模型，你的模型代码不需要做任何改变。&lt;&#x2F;p&gt;
&lt;p&gt;但看了一些Estimator教程，不怎么满意。因为大部分介绍的方法过于简单，仅适用于实验环境。当你面对大数据、复杂模型和机能限制时，发现那些方法就不灵了。
所以就自己写了一本，方便自查自检。这篇文章，会随着本人的打怪升级等级进行增补。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;estimator&quot;&gt;Estimator&lt;&#x2F;h2&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;75%&quot; src=&quot;.&#x2F;image2.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;Estimator 作为高层API，可以让我们写出结构清晰的代码。你有两种方法通过 estimator 来构建模型：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-mode Estimator: 创建指定类型的模型，如上图，它们分别是线性分类和回归模型、深度神经网络分类和回归模型，还有线性和深度混合的分类、回归模型。&lt;&#x2F;li&gt;
&lt;li&gt;自定义 Estimator: 按传统的方法写模型，然后用 model_fn 函数封装&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 指定类型模型的估计器举例
classifier = tf.estimator.DNNClassifier(
   feature_columns=feature_columns, # 定义好的特征列
   hidden_units=[10, 10],           # 两个隐藏层, 每层10个神经元
   n_classes=3,                     # 输出3个类别
   model_dir=PATH)                  # 存 checkpoints 的路径

classifier.train(
   input_fn=lambda: input_fn(
        file_path = FILE_TRAIN,     # 训练数据文件路径
        perform_shuffle = True,     # 打乱数据
        repeat_count = 8))          # 重复8次
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 自定义模型的估计器举例
def model_fn(
   features,                       # batch数量的特征，是input_fn 函数的输出
   labels,                         # batch数量的标签，是input_fn 函数的输出
   mode):                          # tf.estimator.ModeKeys.TRAIN &amp;#x2F; EVAL &amp;#x2F; PREDICT

  # 用特征列（feature_columns)定义输入层
  input_layer = tf.feature_column.input_layer(features, feature_columns)

  # 模型定义部分
  ...

  # 返回值被EstimatorSpec封装，返回训练时关心的loss和train_op
  return tf.estimator.EstimatorSpec(
     mode,
     loss=loss,
     train_op=train_op)

classifier = tf.estimator.Estimator(
   model_fn=model_fn,               # 自定义模型的封装函数
   model_dir=PATH)                  # 存 checkpoints 的路径

classifier.train(
  input_fn=lambda: input_fn(FILE_TRAIN, repeat_count=500, shuffle_count=256))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;仔细研究上面两段最简代码，Estimator的编程结构就呼之欲出了，看下图：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;60%&quot; src=&quot;.&#x2F;Unknown.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;input_fn读入数据，预处理后输出到estimator，再由estimator执行训练、评估或预测等任务.
这里的estimator(估计器）就是模型的抽象，它可以直接定义模型或使用外部模型。 特殊的地方在于，
数据送进estimator时，常常被feature_columns（由特征列组成的列表, 与input_fn输出的数据一一对应）
做二次封装. 注意，特征列是使用estimator的主要方法之一，并不是必须.&lt;&#x2F;p&gt;
&lt;p&gt;任务分解如下：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;input_fn: 从原始数据文件读取数据，然后清洗数据、打乱顺序等，用迭代器分批输出特征和对应的标签。&lt;&#x2F;li&gt;
&lt;li&gt;feature_columns: 特征工程，使数据便于模型训练。&lt;&#x2F;li&gt;
&lt;li&gt;模型定义: 简单的模型可考虑用estimator直接定义；自定义模型的话，须封装进model_fn函数，输入层传入feature_columns, 输出用tf.estimator.EstimatorSpec封装。&lt;&#x2F;li&gt;
&lt;li&gt;最后，用estimator把上面三项组织在一起，做训练、评估、预测等任务。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;input-fn&quot;&gt;input_fn&lt;&#x2F;h2&gt;
&lt;p&gt;原始数据一般工整的很少，所以要把input_fn写好，还是蛮难的。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;xiao-shu-ju-de-qing-kuang&quot;&gt;小数据的情况&lt;&#x2F;h3&gt;
&lt;p&gt;通常实验性的项目采用小规模的数据，这时只需要简单把数据载入内存作训练即可, 我们可以用numpy、pandas等通用工具来处理数据。
假如原始数据是csv文件，选用pandas读入并作预处理：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 定义列名
names = [
    &amp;#x27;symboling&amp;#x27;,
    &amp;#x27;normalized-losses&amp;#x27;,
    &amp;#x27;make&amp;#x27;,
     ...
    &amp;#x27;price&amp;#x27;,
]

# 为每列指定类型.
dtypes = {
    &amp;#x27;symboling&amp;#x27;: np.int32,
    &amp;#x27;normalized-losses&amp;#x27;: np.float32,
    &amp;#x27;make&amp;#x27;: str,
     ...
    &amp;#x27;price&amp;#x27;: np.float32,
}

# 读入文件，空数据用 ？号填充.
df = pd.read_csv(&amp;#x27;filename.csv&amp;#x27;, names=names, dtype=dtypes, na_values=&amp;#x27;?&amp;#x27;)

# 清理数据: 如果发现价格为空就删除该行.
df = df.dropna(axis=&amp;#x27;rows&amp;#x27;, how=&amp;#x27;any&amp;#x27;, subset=[&amp;#x27;price&amp;#x27;])

# 补足数据: 把其他列的空值填充为缺省值
# 把float32类型的列放入列表 float_columns
float_columns = [k for k,v in dtypes.items() if v == np.float32]
# 对数值列来说，如果发现空值就填充 0
df[float_columns] = df[float_columns].fillna(value=0., axis=&amp;#x27;columns&amp;#x27;)

# 构建字符串列，如果方向空值(NaN)就填充&amp;#x27;&amp;#x27;(空串).
string_columns = [k for k,v in dtypes.items() if v == str]
df[string_columns] = df[string_columns].fillna(value=&amp;#x27;&amp;#x27;, axis=&amp;#x27;columns&amp;#x27;)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这样，数据变得比较工整了。假设最后一项price是label，前面的都是特征，按照习惯，数据被分割成训练和评估数据,
它们分别被叫做 training_data、training_label 和 eval_data、eval_label, 它们的类型都是dataframe.&lt;&#x2F;p&gt;
&lt;p&gt;定义训练和评估的input_fn&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# num_epochs=None -&amp;gt; 数据无限循环
# shuffle   =True -&amp;gt; 打乱数据
training_input_fn = tf.estimator.inputs.pandas_input_fn(x=training_data, y=training_label, batch_size=64, shuffle=True, num_epochs=None)

# 评估时，数据不需要被打乱，所以shuffle=False
eval_input_fn = tf.estimator.inputs.pandas_input_fn(x=eval_data, y=eval_label, batch_size=64, shuffle=False)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这样，input_fn就快速的写好了。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;da-shu-ju-de-qing-kuang&quot;&gt;大数据的情况&lt;&#x2F;h3&gt;
&lt;p&gt;pandas一次性把数据载入内存中，不适合大数据量的情形。
面对大规模数据时，需要给数据和模型之间接上管道，然后打开水龙头，按照你想要的流量把数据传入模型。
这时，TF提供的 dataset api 就派上用场了。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;dataset-api-de-jie-gou&quot;&gt;Dataset API 的结构&lt;&#x2F;h4&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;70%&quot; src=&quot;.&#x2F;image7.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;TextLineDataset: 从文本文件每次读一行.&lt;&#x2F;li&gt;
&lt;li&gt;TFRecordDataset: 从 TFRecord 文件读取记录.&lt;&#x2F;li&gt;
&lt;li&gt;FixedLengthRecordDataset: 从二进制文件读取固定大小的记录.&lt;&#x2F;li&gt;
&lt;li&gt;Iterator: 从dataset中每次读取一笔(一般为batch条)数据.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;datasetban-de-input-fn&quot;&gt;Dataset版的input_fn&lt;&#x2F;h4&gt;
&lt;p&gt;假设我们手上有一堆人口普查数据，其中年收入是字符串类型，形如“&amp;gt;50k”, 我们的目标是预测人们的年收入是大于5万还是小于等于5万。
input_fn函数的写法如下：&lt;&#x2F;p&gt;
&lt;p&gt;首先定义CSV文件中每行数据的列名和缺省值，字典格式.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;csv_defaults = collections.OrderedDict([
  (&amp;#x27;age&amp;#x27;,[0]),
  (&amp;#x27;workclass&amp;#x27;,[&amp;#x27;&amp;#x27;]),
  (&amp;#x27;fnlwgt&amp;#x27;,[0]),
  (&amp;#x27;education&amp;#x27;,[&amp;#x27;&amp;#x27;]),
  (&amp;#x27;education-num&amp;#x27;,[0]),
  (&amp;#x27;marital-status&amp;#x27;,[&amp;#x27;&amp;#x27;]),
  (&amp;#x27;occupation&amp;#x27;,[&amp;#x27;&amp;#x27;]),
  (&amp;#x27;relationship&amp;#x27;,[&amp;#x27;&amp;#x27;]),
  (&amp;#x27;race&amp;#x27;,[&amp;#x27;&amp;#x27;]),
  (&amp;#x27;sex&amp;#x27;,[&amp;#x27;&amp;#x27;]),
  (&amp;#x27;capital-gain&amp;#x27;,[0]),
  (&amp;#x27;capital-loss&amp;#x27;,[0]),
  (&amp;#x27;hours-per-week&amp;#x27;,[0]),
  (&amp;#x27;native-country&amp;#x27;,[&amp;#x27;&amp;#x27;]),
  (&amp;#x27;income&amp;#x27;,[&amp;#x27;&amp;#x27;]),
])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;接下来是一段通用代码, 具体见代码注释.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 按行解码 CSV 文件.
# 读入一行数据，对于每列如果有数据就用原值，如果没数据就用缺省值;
# 返回字典格式的键值对
def csv_decoder(line):
    parsed = tf.decode_csv(line, list(csv_defaults.values()))
    return dict(zip(csv_defaults.keys(), parsed))

# 过滤器，滤掉空行，该函数后面要用
def filter_empty_lines(line):
    return tf.not_equal(tf.size(tf.string_split([line], &amp;#x27;,&amp;#x27;).values), 0)

# 创建训练的input_fn
def create_train_input_fn(path):
    def input_fn():
        dataset = (
            tf.data.TextLineDataset(path)  # 从文件创建数据集
                .filter(filter_empty_lines)        # 滤掉空行
                .map(csv_decoder)                  # 解析每行
                .shuffle(buffer_size=1000)         # 每1000行打乱顺序
                .repeat()                          # 无限重复
                .batch(32))

        # 迭代器，每次取batch个数据, 这里为32
        features = dataset.make_one_shot_iterator().get_next()

        # 分离出label值，并转成 true&amp;#x2F;false 形式
        labels = tf.equal(features.pop(&amp;#x27;income&amp;#x27;),&amp;quot; &amp;gt;50K&amp;quot;)
        return features, labels

    return input_fn

# 创建测试的input_fn, 注意与前面的区别
def create_test_input_fn(path):
    def input_fn():
        dataset = (
            tf.data.TextLineDataset(path)
                .filter(filter_empty_lines)
                .map(csv_decoder)
                .batch(32))

        features = dataset.make_one_shot_iterator().get_next()

        labels = tf.equal(columns.pop(&amp;#x27;income&amp;#x27;),&amp;quot; &amp;gt;50K&amp;quot;)
        return features, labels

    return input_fn

# 从input_fn中取出数据，每sess.run一次next_batch，就取出一批
train_input_fn = create_train_input_fn(train_path)
next_batch = train_input_fn()

with tf.Session() as sess:
    features, label = sess.run(next_batch)
    print(features[&amp;#x27;education&amp;#x27;])
    print(label)

&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;上面的代码为什么 create_train_input_fn() 套 input_fn() 呢？
回忆这句：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;classifier.train(
  input_fn=lambda: input_fn(FILE_TRAIN, repeat_count=500, shuffle_count=256))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;因为calssifier.train()中，input_fn要求接的是一个函数，而input_fn() 返回的是特征和标签，所以前面要接上&lt;code&gt;lambda:&lt;&#x2F;code&gt;.
如果采用现在函数套函数的结构, 那么这句前面的lambda就可以去掉, 走个例子：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;train_input_fn = create_train_input_fn(FILE_TRAIN)

classifier.train(
  input_fn=train_input_fn, steps=100)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;te-zheng-lie&quot;&gt;特征列&lt;&#x2F;h2&gt;
&lt;p&gt;特征列实质上是对input_fn()输出的数据做的二次封装，它是做特征工程的强力工具之一。
特征列好比一种约定，它规定了estimator使用input_fn传入的数据具备什么样的形式,
主要目的是令特征数据变得更方便机器运算。&lt;&#x2F;p&gt;
&lt;p&gt;关于特征列，一共涉及10个函数（图中底层的3个矩形框, 缺weighted_categorical_column）。
按大类分为类别特征列和密集特征列（以下也简称为“类别列”和“密集列”）。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;70%&quot; src=&quot;.&#x2F;3_.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;其中，&lt;code&gt;buchetized_column&lt;&#x2F;code&gt;位于中间，表示它作为中介把密集列(通常是&lt;code&gt;numeric_column&lt;&#x2F;code&gt;)转为类别列。对于类别列而言，除了&lt;code&gt;categorical_column_with_hash_buchet&lt;&#x2F;code&gt;和&lt;code&gt;crossed_column&lt;&#x2F;code&gt;外，其余三种均把输入的特征数据处理为one-hot结构。&lt;&#x2F;p&gt;
&lt;p&gt;10个函数可对应9种特征列，我们约定中文称谓如下：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;numeric_column : 数值列&lt;&#x2F;li&gt;
&lt;li&gt;bucketized_column : 分区列&lt;&#x2F;li&gt;
&lt;li&gt;indicator_column : 指示列&lt;&#x2F;li&gt;
&lt;li&gt;embedding_column : 嵌入列&lt;&#x2F;li&gt;
&lt;li&gt;categorical_column_with_identity : 类别ID列&lt;&#x2F;li&gt;
&lt;li&gt;categorical_column_with_vocabulary(file or list) : 类别词表列&lt;&#x2F;li&gt;
&lt;li&gt;categorical_column_with_hash_bucket : 类别哈希列&lt;&#x2F;li&gt;
&lt;li&gt;crossed_column : 合成列&lt;&#x2F;li&gt;
&lt;li&gt;weighted_categorical_column : 权重类别列&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;shu-zhi-lie&quot;&gt;数值列&lt;&#x2F;h3&gt;
&lt;p&gt;以鸢尾花分类问题举例，其输入特征 SepalLength, SepalWidth, PetalLength, PetalWidth （萼片的长宽、花瓣的长宽）就是数值类型。用法：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 缺省为tf.float32的标量.
numeric_feature_column = tf.feature_column.numeric_column(key=&amp;quot;SepalLength&amp;quot;)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;数值列的缺省类型为 tf.float32, 如果想指定类型，则：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 用tf.float64的标量表示.
numeric_feature_column = tf.feature_column.numeric_column(key=&amp;quot;SepalLength&amp;quot;, dtype=tf.float64)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;缺省情况下，numeric_column 返回一个单值数据，如果要返回向量数据，则需指定shape值：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 用10维向量来表示，其中每个元素的类型为 tf.float32.
vector_feature_column = tf.feature_column.numeric_column(key=&amp;quot;Bowling&amp;quot;, shape=10)

# 用10x5的矩阵来表示.
matrix_feature_column
   = tf.feature_column.numeric_column(key=&amp;quot;MyMatrix&amp;quot;, shape=[10,5])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;fen-qu-lie&quot;&gt;分区列&lt;&#x2F;h3&gt;
&lt;p&gt;如果要把一个数值分成不同区间，比如按年份划分：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;60%&quot; src=&quot;.&#x2F;4_.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;划分后的结果为one-hot向量形式。&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;区间&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;表示为&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&amp;lt; 1960&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;[1, 0, 0, 0]&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;blockquote&gt;
&lt;p&gt;= 1960 且 &amp;lt; 1980 | [0, 1, 0, 0]
= 1980 且 &amp;lt; 2000 | [0, 0, 1, 0]
2000            | [0, 0, 0, 1]&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 原始输入是一个名为Year的数值列.
numeric_feature_column = tf.feature_column.numeric_column(&amp;quot;Year&amp;quot;)

# 以1960、1980、2000年来划分区间
bucketized_feature_column = tf.feature_column.bucketized_column(
    source_column = numeric_feature_column,
    boundaries = [1960, 1980, 2000])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;lei-bie-idlie&quot;&gt;类别Id列&lt;&#x2F;h3&gt;
&lt;p&gt;如图，所谓类别Id列是指把左边的单值数据转换为右边的one-hot矢量形式。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;40%&quot; src=&quot;.&#x2F;5_.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;比如我们用0、1、2、3分别表示童装、数码、运动和食品四类商品：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;0=“kitchenware”&lt;&#x2F;li&gt;
&lt;li&gt;1=&quot;electronics&quot;&lt;&#x2F;li&gt;
&lt;li&gt;2=&quot;sport&quot;&lt;&#x2F;li&gt;
&lt;li&gt;3=&quot;food&quot;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# key后跟的列名与input_fn()中的列名一致，
# 其值域为[0, num_buckets)间的整数。
identity_feature_column = tf.feature_column.categorical_column_with_identity(
    key=&amp;#x27;procduct_class&amp;#x27;,
    num_buckets=4)

# 本例中, &amp;#x27;Integer_1&amp;#x27; 或 &amp;#x27;Integer_2&amp;#x27; 皆可替换到上句的 key 之后
def input_fn():
    ...&amp;lt;code&amp;gt;...
    return ({ &amp;#x27;Integer_1&amp;#x27;:[values], ..&amp;lt;etc&amp;gt;.., &amp;#x27;Integer_2&amp;#x27;:[values] },
            [Label_values])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;lei-bie-ci-biao-lie&quot;&gt;类别词表列&lt;&#x2F;h3&gt;
&lt;p&gt;在NLP任务中，我们不会把词条直接输入模型，而是首先把它转换成数值或向量。类别词表列可以把词条转换为one-hot向量形式，如下图：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;50%&quot; src=&quot;.&#x2F;6_.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;从列表创建一个词表列：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;vocabulary_feature_column =
    tf.feature_column.categorical_column_with_vocabulary_list(
        key=&amp;quot;feature_name_from_input_fn&amp;quot;,
        vocabulary_list=[&amp;quot;kitchenware&amp;quot;, &amp;quot;electronics&amp;quot;, &amp;quot;sports&amp;quot;])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;从文件创建一个词表列：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;vocabulary_feature_column =
    tf.feature_column.categorical_column_with_vocabulary_file(
        key=&amp;quot;feature_name_from_input_fn&amp;quot;,
        vocabulary_file=&amp;quot;product_class.txt&amp;quot;,
        vocabulary_size=3)

# product_class.txt 的文件内容如下：
kitchenware
electronics
sports
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;lei-bie-ha-xi-lie&quot;&gt;类别哈希列&lt;&#x2F;h3&gt;
&lt;p&gt;如果待分类的数据量很大，势必会消耗很大内存。tensorflow提供一种用哈希表分类的方法。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;hashed_feature_column =
    tf.feature_column.categorical_column_with_hash_bucket(
        key = &amp;quot;feature_name_from_input_fn&amp;quot;,
        hash_buckets_size = 100) # 把特征值哈希分布到100个位置
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;75%&quot; src=&quot;.&#x2F;7_.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
当分类数量大于哈希分布尺寸的时候，必然会有几个特征指向同一个哈希位置。如图所示，`kitchenware`和`sports`的哈希值同为12，这没有关系，模型可以通过你提供的其他特征进一步区分到底是`kitchenware`还是`sports`。
&lt;h3 id=&quot;he-cheng-lie&quot;&gt;合成列&lt;&#x2F;h3&gt;
&lt;p&gt;有时我们需要组合多个特征为一个特征，这种特征叫合成特征。组合方式通常采用相乘或求笛卡尔积，特征组合有助于表示非线性关系。举个例子，假设我们的模型要计算北京的房产价格，而房产价格与它所处的位置密切相关，而对于位置而言，我们需要用经纬度两个数据同时标定，因此这个经纬度就构成了合成特征。假设我们把北京均匀的纵横切100x100刀，这样就会产生10000个可区分的矩形区域。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 将经纬度转换为[0, 100)范围内的整型值
def input_fn():
    # 从数据集读入经纬度
    latitude = ...   # A tf.float32 value
    longitude = ...  # A tf.float32 value

    # 返回的字典包含经纬度及其它特征，经纬度的值为0到99的整型值
    return { &amp;quot;latitude&amp;quot;: latitude, &amp;quot;longitude&amp;quot;: longitude, ...}, labels

# 用np.linspace把纬度区间分成100等份
# 然后把100等份的列表定义为区间列.
latitude_buckets = list(np.linspace(33.641336, 33.887157, 99))
latitude_fc = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column(&amp;#x27;latitude&amp;#x27;),
    latitude_buckets)

longitude_buckets = list(np.linspace(-84.558798, -84.287259, 99))
longitude_fc = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column(&amp;#x27;longitude&amp;#x27;), longitude_buckets)

# 用fc_longitude x fc_latitude创建交叉特征.
fc_beijing_boxed = tf.feature_column.crossed_column(
    keys=[latitude_fc, longitude_fc],
    hash_bucket_size=1000) # 把10000个分区哈希分布到1000个位置
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;创建合成特征的方法为：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;从input_fn的返回值中取得待组合的特征名，本例中为&lt;code&gt;latitude&lt;&#x2F;code&gt;和&lt;code&gt;longitude&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;做组合的这些特征必须先转换成one-hot形式&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;由&lt;code&gt;latitude_fc&lt;&#x2F;code&gt;和&lt;code&gt;longitude_fc&lt;&#x2F;code&gt;组成的合成列的形式如下：&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;(0,0),(0,1)...  (0,99)
(1,0),(1,1)...  (1,99)
…, …,          ...
(99,0),(99,1)...(99, 99)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;注意，使用合成列后，仍需在模型中包含你用来合成特征列的原始特征列，它们负责在哈希冲突时，作为附加特征来进一步做类别区分。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zhi-shi-lie&quot;&gt;指示列&lt;&#x2F;h3&gt;
&lt;p&gt;指示列和后面要说的嵌入列均不能直接作为特征给模型使用，它的数据来源于类别特征列，即类别特征列是它的输入。
为什么要作这样的设计？因为estimator执行深度神经网络的任务时，只能使用密集特征列，而类别特征列为稀疏列，需要用指示列或嵌入列作下变换才能被使用。
至于指示列封装后，数据变成什么样子，我在官方文档中没找到，以后知道了再补充。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;categorical_column = ... # 创建某种类型的类别特征列

# 定义一个指示列，该列中的每个元素为one-hot向量.
indicator_column = tf.feature_column.indicator_column(categorical_column)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;qian-ru-lie&quot;&gt;嵌入列&lt;&#x2F;h3&gt;
&lt;p&gt;如果类别数据量很大，比如上百万、上亿等，这时采用one-hot来表示就不经济了。记得词嵌入模型中的词向量吗，用一组浮点数来代替one-hot形式来表示一个词条，这种形式在这里被叫做嵌入列，这种方法明显的好处就是令向量维度变得很小。&lt;&#x2F;p&gt;
&lt;p&gt;如下图，假设我们有81个不同的单词，采用one-hot形式需要81维的向量，而采用嵌入列则仅需要3维向量就能表达。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;75%&quot; src=&quot;.&#x2F;image9.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;那么，在嵌入列产生的向量中的浮点数是如何确定的呢？通常，由训练数据学得。嵌入列可以提升模型的表达能力，一定程度描述类别间的关系。&lt;&#x2F;p&gt;
&lt;p&gt;如何确定表示81个类别只需要3维呢？有个简单的公式来算出：
$$\frac{1}{2}\log_2(n)$$
&lt;mj&gt;$$n^{0.25}  \tag {等价公式}$$&lt;&#x2F;mj&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 类别数的0.25次方
embedding_dimensions =  number_of_categories**0.25

categorical_column = ... # 创建一个类别列.

# 再把这个类别列转为一个嵌入列.
# 这意味着把one-hot向量转为指定维度的向量.
embedding_column = tf.feature_column.embedding_column(
    categorical_column=categorical_column,
    dimension=embedding_dimensions)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;注意，这仅仅是个一般规则，你也可以自行设定你希望的维度数。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;quan-zhong-lei-bie-lie&quot;&gt;权重类别列&lt;&#x2F;h3&gt;
&lt;p&gt;有时会遇到一种配对特征，特征一是本体，特征二是本体对应的权重（或出现频率）。
这就是权重类别列的使用场景。
下面是从Tensorflow源码里抠出例子，话说有个&lt;code&gt;tf.Example&lt;&#x2F;code&gt;对象，它的proto形式如下：&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# proto
[
  features {
    feature {
      key: &amp;quot;terms&amp;quot;
      value {bytes_list {value: &amp;quot;very&amp;quot; value: &amp;quot;model&amp;quot;}}
    }
    feature {
      key: &amp;quot;frequencies&amp;quot;
      value {float_list {value: 0.3 value: 0.1}}
    }
  },
  features {
    feature {
      key: &amp;quot;terms&amp;quot;
      value {bytes_list {value: &amp;quot;when&amp;quot; value: &amp;quot;course&amp;quot; value: &amp;quot;human&amp;quot;}}
    }
    feature {
      key: &amp;quot;frequencies&amp;quot;
      value {float_list {value: 0.4 value: 0.1 value: 0.2}}
    }
  }
]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;考虑到proto格式熟悉的人不多，我们把上面的内容简化一下：&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;terms      : [&amp;quot;very&amp;quot;, &amp;quot;model&amp;quot;]
frequencies: [  0.3 ,    0.1 ]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;容易看出，这两组数据有伴生关系，下面的代码通过权重类别列函数把该特征组合在一起.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;categorical_column = categorical_column_with_hash_bucket(
  column_name=&amp;#x27;terms&amp;#x27;, hash_bucket_size=1000)

weighted_column = weighted_categorical_column(
  categorical_column=categorical_column, weight_feature_key=&amp;#x27;frequencies&amp;#x27;)

columns = [weighted_column, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;shi-yong-te-zheng-lie&quot;&gt;使用特征列&lt;&#x2F;h3&gt;
&lt;p&gt;我们须把多个特征列封装成一个列表，才能作为参数拿给估计器(estimator)用。
在使用特征列时，要注意区分特征列类型和模型类型。特征列只有两种类型，类别列和密集列；
模型也分两种，线性模型和深度模型。具体如下：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LinearClassifier&lt;&#x2F;code&gt; 和 &lt;code&gt;LinearRegressor&lt;&#x2F;code&gt;:
&lt;ul&gt;
&lt;li&gt;适用所有类型的特征列&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;DNNClassifier&lt;&#x2F;code&gt; 和 &lt;code&gt;DNNRegressor&lt;&#x2F;code&gt;:
&lt;ul&gt;
&lt;li&gt;仅适用于密集列，如要使用类别列，须经过 &lt;code&gt;indicator_column&lt;&#x2F;code&gt; or或&lt;code&gt;embedding_column&lt;&#x2F;code&gt;做二次封装&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;DNNLinearCombinedClassifier&lt;&#x2F;code&gt; 和&lt;code&gt;DNNLinearCombinedRegressor&lt;&#x2F;code&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;linear_feature_columns&lt;&#x2F;code&gt; 参数适用所有类型特征列.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;dnn_feature_columns&lt;&#x2F;code&gt; 参数仅适用密集列, 用法和 &lt;code&gt;DNNClassifier&lt;&#x2F;code&gt; 及 &lt;code&gt;DNNRegressor&lt;&#x2F;code&gt;的用法一致.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;code&gt;DNNLinearCombinedClassifier&lt;&#x2F;code&gt;的代码举例：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;model = tf.estimator.DNNLinearCombinedClassifier(
    model_dir=&amp;#x27;&amp;#x2F;tmp&amp;#x2F;census_model&amp;#x27;,
    linear_feature_columns=base_columns + crossed_columns,
    dnn_feature_columns=deep_columns,
    dnn_hidden_units=[100, 50])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;mo-xing-ding-yi&quot;&gt;模型定义&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;yu-ding-yi&quot;&gt;预定义&lt;&#x2F;h3&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;75%&quot; src=&quot;.&#x2F;image2.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;预定义模型没什么好讲，看看文档就能秒懂，如下面的例子：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 含2个隐藏层的深度神经网络
classifier = tf.estimator.DNNClassifier(
   feature_columns=feature_columns, # 定义好的特征列
   hidden_units=[10, 10],           # 两个隐藏层, 每层10个神经元
   n_classes=3,                     # 输出3个类别
   model_dir=PATH)                  # 存 checkpoints 的路径
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;zi-ding-yi&quot;&gt;自定义&lt;&#x2F;h3&gt;
&lt;p&gt;写自定义模型时，其实和传统的写法差不多，只是有些小地方要注意一下。&lt;&#x2F;p&gt;
&lt;p&gt;基本思路是，定义模型函数，它接收从input_fn()传来的特征和标签，输出由tf.estimator.EstimatorSpec封装后的结果,
函数体主要做两件事情，一件是定义模型，一件是通过分支语句分别实现训练、评估和预测。&lt;&#x2F;p&gt;
&lt;p&gt;我喜欢的结构是把模型单独定义成一个类，然后再用mode_fn()来调用它,
mode_fn()的返回用tf.estimator.EstimatorSpec封装，详见下面的例子：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;class Sample_model(object):
  def __init__(...):
    ... &amp;lt; code &amp;gt; ...
  def __call__(self, inputs, training):
    ... &amp;lt; code &amp;gt; ...

def model_fn(
   features,                       # batch数量的特征，是input_fn 函数的输出
   labels,                         # batch数量的标签，是input_fn 函数的输出
   mode):                          # tf.estimator.ModeKeys.TRAIN &amp;#x2F; EVAL &amp;#x2F; PREDICT

    # 用特征列（feature_columns)定义输入层
    input_layer = tf.feature_column.input_layer(features, feature_columns)

    # 定义模型实例
    model = Sample_model(...)

    if mode == tf.estimator.ModeKeys.TRAIN:
        logits  = model(features, training = True)
        loss = ...
        train_op = ...
        accuracy = ...

        # 给训练准确度命名，并使它被tf日志记录
        tf.identity(accuracy[1], name=&amp;#x27;train_accuracy&amp;#x27;)
        tf.summary.scalar(&amp;#x27;train_accuracy&amp;#x27;, accuracy[1])

        return tf.estimator.EstimatorSpec(
            mode,
            loss=loss,
            train_op=train_op)

    if mode == tf.estimator.ModeKeys.PREDICT:
        logits  = model(features, training = False)
        predictions = ...
        return tf.estimator.EstimatorSpec(
            mode,
            predictions = predictions
            )

    if mode == tf.estimator.ModeKeys.EVAL:
        logits  = model(features, training = False)
        loss = ...

        return tf.estimator.EstimatorSpec(
            mode,
            loss=loss)

&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;你可能发现，我好像没用到特征列。
如果在自定义的模型中想用特征列这个工具（再次强调，不是必须），只需在模型的输入层调用下面这个函数即可：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.feature_column.linear_model(features, feature_columns, ...)&lt;&#x2F;code&gt;：如果定义线性模型的话用这个，输出是预测结果.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;tf.feature_column.input_layer(features, feature_columns, ...)&lt;&#x2F;code&gt;：深度模型用这个.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;其中features 来自input_fn 的输出， feature_columns 是由多个特征列组成的列表。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 自定义一个超简单的模型
# 输入层
input_layer = tf.feature_column.input_layer(features, feature_columns)
# 隐藏层: h1，h2
# 10个神经元，relu激活函数，input_layer作为输入参数
h1 = tf.layers.Dense(10, activation=tf.nn.relu)(input_layer)
h2 = tf.layers.Dense(10, activation=tf.nn.relu)(h1)
# 输出层，3个输出
logits = tf.layers.Dense(3)(h2)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;he-bing-xie-fa&quot;&gt;合并写法&lt;&#x2F;h4&gt;
&lt;p&gt;这是tensorflow 官网给出的一种写法, 只用了一个return, 返回内容的判断放在了前面的 if 分支，你可以根据自己的喜好选择不同写法。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;def model_fn(mode, features, labels):
  if (mode == tf.estimator.ModeKeys.TRAIN or
      mode == tf.estimator.ModeKeys.EVAL):
    loss = ...
  else:
    loss = None
  if mode == tf.estimator.ModeKeys.TRAIN:
    train_op = ...
  else:
    train_op = None
  if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = ...
  else:
    predictions = None

  return tf.estimator.EstimatorSpec(
      mode=mode,
      predictions=predictions,
      loss=loss,
      train_op=train_op)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;zu-zhuang&quot;&gt;组装&lt;&#x2F;h2&gt;
&lt;p&gt;准备好上面的内容后，就可以把model_fn()和input_fn组装在一起了。方法是用estimator实例化
估计器对象，然后用这个对象分别进行训练、评估、预测即可。&lt;&#x2F;p&gt;
&lt;p&gt;在组装时，我们还要加入一些常规的东西。比如设置checkpoint的保存规则，定义一些观测变量，
方便在训练时用TensorBoard观察，还有超参数等等。具体请看示例注释：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# 设置tf输出哪种类别的日志，不同类别详细程度不同
# tf.logging.后的可选值为DEBUG, INFO, WARN, ERROR, or FATAL.
tf.logging.set_verbosity(tf.logging.INFO)

# TensorFlow 版本检查，estimator 要求1.4以上
tf_version = tf.__version__
tf.logging.info(&amp;quot;TensorFlow version: {}&amp;quot;.format(tf_version))
assert &amp;quot;1.4&amp;quot; &amp;lt;= tf_version, &amp;quot;TensorFlow r1.4 or later is needed&amp;quot;

def main(flags, model_function, input_function):
  # flags 携带准备传入模型函数和输入函数的参数。

  # 设置训练时每隔多少秒保存一下checkpoint.
  run_config = tf.estimator.RunConfig().replace(save_checkpoints_secs=1e9)
  # 生成classifier实例
  classifier = tf.estimator.Estimator(
      model_fn=model_function, model_dir=flags.model_dir, config=run_config,
      params={
          &amp;#x27;resnet_size&amp;#x27;: flags.resnet_size,
          &amp;#x27;data_format&amp;#x27;: flags.data_format,
          &amp;#x27;batch_size&amp;#x27;: flags.batch_size,
      })

  # 每训练 flags.epochs_per_eval 轮更新一下日志内容.
  for _ in range(flags.train_epochs &amp;#x2F;&amp;#x2F; flags.epochs_per_eval):
    tensors_to_log = {
        &amp;#x27;learning_rate&amp;#x27;: &amp;#x27;learning_rate&amp;#x27;,
        &amp;#x27;cross_entropy&amp;#x27;: &amp;#x27;cross_entropy&amp;#x27;,
        &amp;#x27;train_accuracy&amp;#x27;: &amp;#x27;train_accuracy&amp;#x27;
    }

    # 设置每跑100个迭代器，打印一下日志。
    logging_hook = tf.train.LoggingTensorHook(
        tensors=tensors_to_log, every_n_iter=100)

    print(&amp;#x27;Starting a training cycle.&amp;#x27;)

    def input_fn_train():
      return input_function(True, flags.data_dir, flags.batch_size,
                            flags.epochs_per_eval, flags.num_parallel_calls)

    classifier.train(input_fn=input_fn_train, hooks=[logging_hook])

    print(&amp;#x27;Starting to evaluate.&amp;#x27;)

    # 评估模型并打印结果
    def input_fn_eval():
      return input_function(False, flags.data_dir, flags.batch_size,
                            1, flags.num_parallel_calls)

    eval_results = classifier.evaluate(input_fn=input_fn_eval)
    print(eval_results)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;duo-gpuxia-de-xie-fa&quot;&gt;多GPU下的写法&lt;&#x2F;h2&gt;
&lt;p&gt;在多GPU的情况下，代码需要做几点小变化。&lt;&#x2F;p&gt;
&lt;p&gt;首先, 检查batch_size的数量，它必须能被GPU的数量整除。其目的是让每批的输入数量被平均分配到各个GPU上。&lt;&#x2F;p&gt;
&lt;p&gt;其次，在model_fn函数中的训练部分封装优化器。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    optimizer = ...          # 原优化器的定义不变
    if params.get(&amp;#x27;multi_gpu&amp;#x27;):
      optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;最后，在main函数部分封装模型函数(model_fn).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;  if flags.multi_gpu:
    model_function = tf.contrib.estimator.replicate_model_fn(
        model_fn, loss_reduction=tf.losses.Reduction.MEAN)

&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;qi-ta&quot;&gt;其他&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;san-ge-import&quot;&gt;三个import&lt;&#x2F;h3&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;from __future__ import print_function
from __future__ import division
from __future__ import absolute_import
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;常看到这三个置于顶上的import 语句，一直没有深究它们有什么用，今天查了下资料，发现这三句都是针对python 2.X版本的情况，分别作用如下：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;print_function: print语句必须使用函数形式，而 print ‘test’这句在这种条件下就会报错。&lt;&#x2F;li&gt;
&lt;li&gt;division: 精确除法，即python2.x版本中，3&#x2F;4=0（截断除法），有了这句, 3&#x2F;4=0.75， 而3&#x2F;&#x2F;4=0&lt;&#x2F;li&gt;
&lt;li&gt;absolute_import: 绝对路径，解决自定义包与缺省包名字冲突的问题，如你不小心自定义了string，有了这句， import string时引用系统的，没有这句就引用你本地的。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;guan-yu-chao-can-shu&quot;&gt;关于超参数&lt;&#x2F;h3&gt;
&lt;p&gt;既然训练始终要调参，不如把参数提前定义好，比如下面这个参数类。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;import argparse
class MymodelArgParser(argparse.ArgumentParser):

  def __init__(self):
    super(MymodelArgParser, self).__init__()

    self.add_argument(
        &amp;#x27;--multi_gpu&amp;#x27;, action=&amp;#x27;store_true&amp;#x27;,
        help=&amp;#x27;If set, run across all available GPUs.&amp;#x27;)
    self.add_argument(
        &amp;#x27;--batch_size&amp;#x27;,
        type=int,
        default=100,
        help=&amp;#x27;Number of images to process in a batch&amp;#x27;)

    ... &amp;lt;code&amp;gt; ...

&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;还有一种方法也比较优雅，它把参数存成json文件，运行时载入参数即可，见下面的几个功能函数：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;import codecs

# 从指定目录载入参数
def load_hparams(model_dir):
  &amp;quot;&amp;quot;&amp;quot;Load hparams from an existing model directory.&amp;quot;&amp;quot;&amp;quot;
  hparams_file = os.path.join(model_dir, &amp;quot;hparams&amp;quot;)
  if tf.gfile.Exists(hparams_file):
    print(&amp;quot;# Loading hparams from %s&amp;quot; % hparams_file)
    with codecs.getreader(&amp;quot;utf-8&amp;quot;)(tf.gfile.GFile(hparams_file, &amp;quot;rb&amp;quot;)) as f:
      try:
        hparams_values = json.load(f)
        hparams = tf.contrib.training.HParams(**hparams_values)
      except ValueError:
        print(&amp;quot;  can&amp;#x27;t load hparams file&amp;quot;)
        return None
    return hparams
  else:
    return None

# 保存参数到json文件
def save_hparams(out_dir, hparams):
  &amp;quot;&amp;quot;&amp;quot;Save hparams.&amp;quot;&amp;quot;&amp;quot;
  hparams_file = os.path.join(out_dir, &amp;quot;hparams&amp;quot;)
  print(&amp;quot;  saving hparams to %s&amp;quot; % hparams_file)
  with codecs.getwriter(&amp;quot;utf-8&amp;quot;)(tf.gfile.GFile(hparams_file, &amp;quot;wb&amp;quot;)) as f:
    f.write(hparams.to_json())

# 用hparams_path里的新值覆盖hparams的老值
def maybe_parse_standard_hparams(hparams, hparams_path):
  if not hparams_path:
    return hparams

  if tf.gfile.Exists(hparams_path):
    print(&amp;quot;# Loading standard hparams from %s&amp;quot; % hparams_path)
    with tf.gfile.GFile(hparams_path, &amp;quot;r&amp;quot;) as f:
      hparams.parse_json(f.read())

  return hparams
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;zhu-cheng-xu-ru-kou&quot;&gt;主程序入口&lt;&#x2F;h3&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;def main(FLAGS, model_function, input_function):
  ... &amp;lt;code&amp;gt; ...

if __name__ == &amp;#x27;__main__&amp;#x27;:
  parser = MymodelArgParser()
  tf.logging.set_verbosity(tf.logging.INFO)
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>TensorFlow 的 MNIST 教程</title>
        <published>2017-08-02T00:00:00+00:00</published>
        <updated>2017-08-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/mnist/"/>
        <id>/blog/mnist/</id>
        
        <content type="html" xml:base="/blog/mnist/">&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.tensorflow.org&#x2F;get_started&#x2F;mnist&#x2F;pros&quot;&gt;原文&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这篇文章对初学tensorflow的朋友来说，有很好的参考作用。曾经在网上看过本教程的翻译稿，但版本偏老。本文翻译时，tensorflow的版本为1.3.0-rc1. 新版相较于老版，示例代码有变化，文字说明也有增补。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。
在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。&lt;&#x2F;p&gt;
&lt;p&gt;这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看新手指南.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;guan-yu-ben-jiao-cheng&quot;&gt;关于本教程&lt;&#x2F;h2&gt;
&lt;p&gt;本教程第一部分讲解 &lt;a href=&quot;https:&#x2F;&#x2F;www.github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;blob&#x2F;r1.2&#x2F;tensorflow&#x2F;examples&#x2F;tutorials&#x2F;mnist&#x2F;mnist_softmax.py&quot;&gt;mnist_softmax.py&lt;&#x2F;a&gt; 代码, 一个简单的tensorflow模型实现。第二部分介绍一些提高精度的方法。&lt;&#x2F;p&gt;
&lt;p&gt;你可以从本教程拷贝和粘贴代码块到你的python环境，或者下载完整代码 &lt;a href=&quot;https:&#x2F;&#x2F;www.github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;blob&#x2F;r1.2&#x2F;tensorflow&#x2F;examples&#x2F;tutorials&#x2F;mnist&#x2F;mnist_deep.py&quot;&gt;mnist_deep.py&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;我们将完成如下目标：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;基于图片像素，创建一个softmax回归函数的模型来识别MNIST数字。&lt;&#x2F;li&gt;
&lt;li&gt;用tensorflow训练模型识别数字。&lt;&#x2F;li&gt;
&lt;li&gt;用测试数据检查模型精度&lt;&#x2F;li&gt;
&lt;li&gt;构建、训练和测试一个多层卷积神经网络来提升模型精度。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;an-zhuang&quot;&gt;安装&lt;&#x2F;h2&gt;
&lt;p&gt;在创建模型之前，我们会先加载MNIST数据集，然后启动一个TensorFlow的session。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;jia-zai-mnistshu-ju&quot;&gt;加载MNIST数据&lt;&#x2F;h3&gt;
&lt;p&gt;为了方便起见，我们已经准备了一个脚本来自动下载和导入MNIST数据集。它会自动创建一个&#x27;MNIST_data&#x27;的目录来存储数据。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&amp;#x27;MNIST_data&amp;#x27;, one_hot=True)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这里，&lt;code&gt;mnist&lt;&#x2F;code&gt;是一个轻量级的类。它以Numpy数组的形式存储着训练、校验和测试数据集。同时提供了一个函数，用于在迭代中获得minibatch，后面我们将会用到。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;yun-xing-tensorflowde-interactivesession&quot;&gt;运行TensorFlow的InteractiveSession&lt;&#x2F;h3&gt;
&lt;p&gt;Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。&lt;&#x2F;p&gt;
&lt;p&gt;这里，我们使用更加方便的InteractiveSession类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些计算图，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用InteractiveSession，那么你需要在启动session之前构建整个计算图，然后启动该计算图。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;import tensorflow as tf
sess = tf.InteractiveSession()
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;ji-suan-tu&quot;&gt;计算图&lt;&#x2F;h3&gt;
&lt;p&gt;为了在Python中进行高效的数值计算，我们通常会使用像NumPy一类的库，将一些诸如矩阵乘法的耗时操作在Python环境的外部来计算，这些计算通常会通过其它语言并用更为高效的代码来实现。&lt;&#x2F;p&gt;
&lt;p&gt;但遗憾的是，每一个操作切换回Python环境时仍需要不小的开销。如果你想在GPU或者分布式环境中计算时，这一开销更加可怖，这一开销主要可能是用来进行数据迁移。&lt;&#x2F;p&gt;
&lt;p&gt;TensorFlow也是在Python外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在Python外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在Python外部。这与Theano或Torch的做法类似。&lt;&#x2F;p&gt;
&lt;p&gt;因此Python代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。详情请查看基本用法中的计算图表一节。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gou-jian-softmax-hui-gui-mo-xing&quot;&gt;构建 Softmax 回归模型&lt;&#x2F;h2&gt;
&lt;p&gt;在这一节中我们将建立一个拥有一个线性层的softmax回归模型。在下一节，我们会将其扩展为一个拥有多层卷积网络的softmax回归模型。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zhan-wei-fu-placeholders&quot;&gt;占位符(placeholders)&lt;&#x2F;h3&gt;
&lt;p&gt;我们通过为输入图像和目标输出类别创建节点，来构建计算图。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;x = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 784])
y_ = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 10])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这里的&lt;code&gt;x&lt;&#x2F;code&gt;和&lt;code&gt;y_&lt;&#x2F;code&gt;并不是特定的值，相反，他们都只是一个&lt;code&gt;占位符&lt;&#x2F;code&gt;，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。&lt;&#x2F;p&gt;
&lt;p&gt;输入图片&lt;code&gt;x&lt;&#x2F;code&gt;是一个2维的浮点数张量。这里，分配给它的shape为[None, 784]，其中784是一张展平的MNIST图片的维度。None表示其值大小不定，在这里作为第一个维度值，用以指代batch的大小，意即x的数量不定。输出类别值y_也是一个2维张量，其中每一行为一个10维的one-hot向量,用于代表对应某一MNIST图片的类别。&lt;&#x2F;p&gt;
&lt;p&gt;虽然placeholder的shape参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;bian-liang&quot;&gt;变量&lt;&#x2F;h3&gt;
&lt;p&gt;我们现在为模型定义权重W和偏置b。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：变量。一个变量代表着TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用Variable来表示。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;我们在调用&lt;code&gt;tf.Variable&lt;&#x2F;code&gt;的时候传入初始值。
在这个例子里，我们把&lt;code&gt;W&lt;&#x2F;code&gt;和&lt;code&gt;b&lt;&#x2F;code&gt;都初始化为零向量。&lt;code&gt;W&lt;&#x2F;code&gt;是一个784x10的矩阵（因为我们有784个特征和10个输出值）。&lt;code&gt;b&lt;&#x2F;code&gt;是一个10维的向量（因为我们有10个分类）。&lt;&#x2F;p&gt;
&lt;p&gt;变量需要通过session初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个变量,可以一次性为所有变量完成此操作。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;sess.run(tf.initialize_all_variables())
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;lei-bie-yu-ce-yu-sun-shi-han-shu&quot;&gt;类别预测与损失函数&lt;&#x2F;h2&gt;
&lt;p&gt;现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片&lt;code&gt;x&lt;&#x2F;code&gt;和权重矩阵&lt;code&gt;W&lt;&#x2F;code&gt;相乘，加上偏置&lt;code&gt;b&lt;&#x2F;code&gt;。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;y = tf.matmul(x,W) + b
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;可以很容易的为训练过程指定最小化误差用的损失函数，损失表示在一个样本上模型预测有多差; 我们试图在所有的样本上最小化这个损失。这里， 我们的损失函数是目标类别和softmax激活函数之间的交叉熵。在教程开始，我们使用如下公式：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;cross_entropy = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;注意，&lt;code&gt;tf.nn.softmax_cross_entropy_with_logits&lt;&#x2F;code&gt;在内部用softmax规范化模型，
并对所有分类求和，而&lt;code&gt;tf.reduce_mean&lt;&#x2F;code&gt;对这些和求平均 。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;xun-lian-mo-xing&quot;&gt;训练模型&lt;&#x2F;h2&gt;
&lt;p&gt;我们已经定义好模型和训练用的损失函数，那么用TensorFlow进行训练就很简单了。因为TensorFlow知道整个计算图，它可以使用自动微分法找到对于各个变量的损失的梯度值。TensorFlow有大量内置的优化算法 这个例子中，我们用最速下降法让交叉熵下降，步长为0.5.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。&lt;&#x2F;p&gt;
&lt;p&gt;返回的&lt;code&gt;train_step&lt;&#x2F;code&gt;操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行&lt;code&gt;train_step&lt;&#x2F;code&gt;来完成。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;for _ in range(1000):
  batch = mnist.train.next_batch(100)
  train_step.run(feed_dict={x: batch[0], y_: batch[1]})
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;每一步迭代，我们都会加载100个训练样本，然后执行一次&lt;code&gt;train_step&lt;&#x2F;code&gt;，并通过&lt;code&gt;feed_dict&lt;&#x2F;code&gt;将&lt;code&gt;x&lt;&#x2F;code&gt; 和 &lt;code&gt;y_&lt;&#x2F;code&gt; 张量占位符用训练训练数据替代。
注意，在计算图中，你可以用&lt;code&gt;feed_dict&lt;&#x2F;code&gt;来替代任何张量，并不仅限于替换占位符。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ping-gu-mo-xing&quot;&gt;评估模型&lt;&#x2F;h3&gt;
&lt;p&gt;那么我们的模型性能如何呢？&lt;&#x2F;p&gt;
&lt;p&gt;首先让我们找出那些预测正确的标签。&lt;code&gt;tf.argmax&lt;&#x2F;code&gt; 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如&lt;code&gt;tf.argmax(y,1)&lt;&#x2F;code&gt;返回的是模型对于任一输入&lt;code&gt;x&lt;&#x2F;code&gt;预测到的标签值，而&lt;code&gt;tf.argmax(y_,1)&lt;&#x2F;code&gt; 代表正确的标签，我们可以用 &lt;code&gt;tf.equal&lt;&#x2F;code&gt; 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：[True, False, True, True]变为[1,0,1,1]，计算出平均值为0.75。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;最后，我们可以计算出在测试数据上的准确率，大概是92%。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;gou-jian-duo-ceng-juan-ji-wang-luo&quot;&gt;构建多层卷积网络&lt;&#x2F;h2&gt;
&lt;p&gt;在MNIST上只有92% 的准确率，实在太糟糕。在这节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;quan-zhong-chu-shi-hua&quot;&gt;权重初始化&lt;&#x2F;h3&gt;
&lt;p&gt;为了创建这个模型，我们需要创建大量的权重和偏置项。
这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。
由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;juan-ji-he-chi-hua&quot;&gt;卷积和池化&lt;&#x2F;h3&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;.&#x2F;convgaus.gif&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：卷积，从数学角度说，指计算一个函数通过另一个函数时，有多少重叠的积分。也可以视为通过相乘的方式将两个函数进行混合。详见上图,
红色曲线（函数f）下的区域是积分，蓝色曲线(函数g)从左向右缓慢移动，绿色曲线下的区域是红色曲线和蓝色曲线的卷积，灰色阴影表示在绿色垂直线位置时，红色曲线和蓝色曲线的卷积, 其值为f(a)*g(x-a)。 &lt;br&#x2F;&gt;
下面再给个我非常喜欢的说明，帮助理解这个公式，写得非常有意思。  &lt;br&#x2F;&gt;
比如说你的老板命令你干活，你却到楼下打台球去了，后来被老板发现，他非常气愤，扇了你一巴掌（注意，这就是输入信号，脉冲），于是你的脸上会渐渐地（贱贱地）鼓起来一个包，你的脸就是一个系统，而鼓起来的包就是你的脸对巴掌的响应，好，这样就和信号系统建立起来意义对应的联系。下面还需要一些假设来保证论证的严谨：假定你的脸是线性时不变系统，也就是说，无论什么时候老板打你一巴掌，打在你脸的同一位置（这似乎要求你的脸足够光滑，如果你说你长了很多青春痘，甚至整个脸皮处处连续处处不可导，那难度太大了，我就无话可说了哈哈），你的脸上总是会在相同的时间间隔内鼓起来一个相同高度的包来，并且假定以鼓起来的包的大小作为系统输出。好了，那么，下面可以进入核心内容——卷积了！  &lt;br&#x2F;&gt;
如果你每天都到楼下去打台球，那么老板每天都要扇你一巴掌，不过当老板打你一巴掌后，你5分钟就消肿了，所以时间长了，你甚至就适应这种生活了……如果有一天，老板忍无可忍，以0.5秒的间隔开始不间断的扇你的过程，这样问题就来了，第一次扇你鼓起来的包还没消肿，第二个巴掌就来了，你脸上的包就可能鼓起来两倍高，老板不断扇你，脉冲不断作用在你脸上，效果不断叠加了，这样这些效果就可以求和了，结果就是你脸上的包的高度随时间变化的一个函数了（注意理解）；如果老板再狠一点，频率越来越高，以至于你都辨别不清时间间隔了，那么，求和就变成积分了。可以这样理解，在这个过程中的某一固定的时刻，你的脸上的包的鼓起程度和什么有关呢？和之前每次打你都有关！但是各次的贡献是不一样的，越早打的巴掌，贡献越小，所以这就是说，某一时刻的输出是之前很多次输入乘以各自的衰减系数之后的叠加而形成某一点的输出，然后再把不同时刻的输出点放在一起，形成一个函数，这就是卷积，卷积之后的函数就是你脸上的包的大小随时间变化的函数。本来你的包几分钟就可以消肿，可是如果连续打，几个小时也消不了肿了，这难道不是一种平滑过程么？反映到剑桥大学的公式上，f(a)就是第a个巴掌，g(x-a)就是第a个巴掌在x时刻的作用程度，乘起来再叠加就ok了，大家说是不是这个道理呢？&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？
在这个实例里，我们会一直使用普通版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。
我们的池化用简单传统的2x2大小的模板做最大池化。为了代码更简洁，我们把这部分抽象成一个函数。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&amp;#x27;SAME&amp;#x27;)

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding=&amp;#x27;SAME&amp;#x27;)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;di-yi-ceng-juan-ji&quot;&gt;第一层卷积&lt;&#x2F;h3&gt;
&lt;p&gt;现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;为了用这一层，我们把&lt;code&gt;x&lt;&#x2F;code&gt;变成一个4维向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，
如果是rgb彩色图，则为3)。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这里&lt;code&gt;x&lt;&#x2F;code&gt;是2维矩阵（m, 784), 其中m表示样本数量，所以下面的reshape中的 &lt;code&gt;-1&lt;&#x2F;code&gt;的位置实质表示的是m值，此处的&lt;code&gt;-1&lt;&#x2F;code&gt;可以推断出m值，在具体执行时，将用m值替换.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;x_image = tf.reshape(x, [-1,28,28,1])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;我们把&lt;code&gt;x_image&lt;&#x2F;code&gt;和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行最大池化。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;di-er-ceng-juan-ji&quot;&gt;第二层卷积&lt;&#x2F;h3&gt;
&lt;p&gt;为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;W_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;quan-lian-jie-ceng&quot;&gt;全连接层&lt;&#x2F;h3&gt;
&lt;p&gt;现在，图片尺寸缩小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;W_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;dropout&quot;&gt;Dropout&lt;&#x2F;h3&gt;
&lt;p&gt;为了减少过拟合，我们在输出层之前加入&lt;a href=&quot;https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~hinton&#x2F;absps&#x2F;JMLRdropout.pdf&quot;&gt;dropout&lt;&#x2F;a&gt;。我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。
这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。
TensorFlow的&lt;code&gt;tf.nn.dropout&lt;&#x2F;code&gt;操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;shu-chu-ceng&quot;&gt;输出层&lt;&#x2F;h3&gt;
&lt;p&gt;最后，我们添加一个softmax层，就像前面的单层 softmax 回归一样。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;xun-lian-he-ping-gu-mo-xing&quot;&gt;训练和评估模型&lt;&#x2F;h3&gt;
&lt;p&gt;这个模型的效果如何呢？&lt;&#x2F;p&gt;
&lt;p&gt;为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，
这个模型和上个模型的不同之处：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;只是我们会用更加复杂的&lt;a href=&quot;http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1412.6980&quot;&gt;ADAM优化器&lt;&#x2F;a&gt;来做梯度最速下降，&lt;&#x2F;li&gt;
&lt;li&gt;在&lt;code&gt;feed_dict&lt;&#x2F;code&gt;中加入额外的参数&lt;code&gt;keep_prob&lt;&#x2F;code&gt;来控制dropout比例。&lt;&#x2F;li&gt;
&lt;li&gt;然后每100次迭代输出一次日志。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;我们使用了 &lt;code&gt;tf.Session&lt;&#x2F;code&gt; 而不是 &lt;code&gt;tf.InteractiveSession&lt;&#x2F;code&gt;. 将创建图(模型规格)和评估图(模型匹配)的处理分开, 这样便产生更清晰的代码。tf.Session被with代码块创建，以至于一旦块退出时它将自动销毁。&lt;&#x2F;p&gt;
&lt;p&gt;随时运行此代码。请注意, 它会进行2万次训练迭代, 并且需要一段时间 (可能长达半小时), 这取决于你的处理器。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;cross_entropy = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  for i in range(20000):
    batch = mnist.train.next_batch(50)
    if i % 100 == 0:
      train_accuracy = accuracy.eval(feed_dict={
          x: batch[0], y_: batch[1], keep_prob: 1.0})
      print(&amp;#x27;step %d, training accuracy %g&amp;#x27; % (i, train_accuracy))
    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

  print(&amp;#x27;test accuracy %g&amp;#x27; % accuracy.eval(feed_dict={
      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;以上代码，在最终测试集上的准确率大概是99.2%。&lt;&#x2F;p&gt;
&lt;p&gt;目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。&lt;&#x2F;p&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;1&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;1&lt;&#x2F;sup&gt;
&lt;p&gt;在小型卷积网络中，有没有dropout对性能影响不大。Dropout 通常可以降低过拟合，它常常被用于大型的神经网络中。&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Neural Machine Translation (seq2seq) 教程</title>
        <published>2017-07-18T00:00:00+00:00</published>
        <updated>2018-01-31T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/blog/nmt/"/>
        <id>/blog/nmt/</id>
        
        <content type="html" xml:base="/blog/nmt/">&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&quot;&gt;原文&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;jie-shao&quot;&gt;介绍&lt;&#x2F;h2&gt;
&lt;p&gt;序列到序列(seq2seq)模型 (&lt;a href=&quot;https:&#x2F;&#x2F;papers.nips.cc&#x2F;paper&#x2F;5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;Sutskever et al., 2014&lt;&#x2F;a&gt;, &lt;a href=&quot;http:&#x2F;&#x2F;emnlp2014.org&#x2F;papers&#x2F;pdf&#x2F;EMNLP2014179.pdf&quot;&gt;Cho et al., 2014&lt;&#x2F;a&gt;) 在诸如机器翻译、语音识别和文本概括等任务中取得了巨大成功. 本教程为读者提供对 seq2seq 模型的全面理解，并展示如何从头构建一个有竞争力的 seq2seq 模型. 我们专注于神经机器翻译（NMT）任务，这是一个很好的、已获得广泛&lt;a href=&quot;https:&#x2F;&#x2F;research.googleblog.com&#x2F;2016&#x2F;09&#x2F;a-neural-network-for-machine.html&quot;&gt;成功&lt;&#x2F;a&gt;的 seq2seq 模型的试验台. 所含的代码轻量、高质、实用，并整合了最新的研究思路。我们通过以下方式达成此目标 :&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;使用最新的 解码器 &#x2F; attention wrapper &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;tree&#x2F;master&#x2F;tensorflow&#x2F;contrib&#x2F;seq2seq&#x2F;python&#x2F;ops&quot;&gt;API&lt;&#x2F;a&gt;, TensorFlow 1.2 数据迭代器&lt;&#x2F;li&gt;
&lt;li&gt;结合我们在建立循环神经网络和序列到序列模型方面的强大专长&lt;&#x2F;li&gt;
&lt;li&gt;提供一些巧思来构建最好的 NMT 模型，并复制一个谷歌神经机器翻译系统 &lt;a href=&quot;https:&#x2F;&#x2F;research.google.com&#x2F;pubs&#x2F;pub45610.html&quot;&gt;Google’s NMT (GNMT) system&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;我们认为，重要的是提供人们可以轻松复制的基准. 因此，我们提供了完整的实验结果，并对以下公开的数据集进行了预训练 :&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;小规模&lt;&#x2F;em&gt;: 英语-越南语平行语料库(133K 句子对,TED 对话), 由 &lt;a href=&quot;https:&#x2F;&#x2F;sites.google.com&#x2F;site&#x2F;iwsltevaluation2015&#x2F;&quot;&gt;IWSLT Evaluation Campaign&lt;&#x2F;a&gt; 提供.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;em&gt;大规模&lt;&#x2F;em&gt;: 德语-英语平行语料库(4.5M 句子对) , 由 &lt;a href=&quot;http:&#x2F;&#x2F;www.statmt.org&#x2F;wmt16&#x2F;translation-task.html&quot;&gt;WMT Evaluation Campaign&lt;&#x2F;a&gt; 提供.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;我们首先建立关于 seq2seq 模型的一些基本知识, 说明如何构建和训练一个普通的 NMT 模型. 第二部分将详细介绍采用注意力机制（attention mechanism) 建立一个较好的 NMT 模型. 然后，我们将讨论构建更好 NMT 模型（包括速度和翻译质量）的技巧，比如 TensorFlow 的最佳实践（batching, bucketing）, 双向 RNNs 和 定向搜索.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ji-chu&quot;&gt;基础&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;shen-jing-ji-qi-fan-yi-de-bei-jing&quot;&gt;神经机器翻译的背景&lt;&#x2F;h3&gt;
&lt;p&gt;回到过去，传统的基于短语的翻译系统通过将语句拆成多个小块，然后再一小块一小块的翻译。这导致不流畅的翻译结果，并不十分像我们人类的翻译。我们是先读懂整个句子，再翻译出来。神经机器翻译(NMT)就是在模仿这种方式！&lt;&#x2F;p&gt;
&lt;p&gt;具体来说,  NMT 系统首先使用 &lt;em&gt;编码器&lt;&#x2F;em&gt; 读取源句来构建一个 &lt;a href=&quot;https:&#x2F;&#x2F;www.theguardian.com&#x2F;science&#x2F;2015&#x2F;may&#x2F;21&#x2F;google-a-step-closer-to-developing-machines-with-human-like-intelligence&quot;&gt;&quot;thought&quot; 向量&lt;&#x2F;a&gt; , 一个表示句子意义的数字序列; 然后，&lt;em&gt;解码器&lt;&#x2F;em&gt; 处理这个向量输出翻译结果
, 如图 1 . 这通常被称为 &lt;em&gt;编码器 - 解码器结构&lt;&#x2F;em&gt;. 以这种方式, NMT 解决了传统的基于短语翻译的遗留问题: 它可以捕获语言的 &lt;em&gt;远程依赖性&lt;&#x2F;em&gt; , 比如，词性、语法结构等，并生成顺畅的翻译，如 &lt;a href=&quot;https:&#x2F;&#x2F;research.googleblog.com&#x2F;2016&#x2F;09&#x2F;a-neural-network-for-machine.html&quot;&gt;Google Neural Machine Translation systems&lt;&#x2F;a&gt; 所示.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：上文的 &lt;em&gt;&quot;thought&quot;&lt;&#x2F;em&gt; 只是个比喻，不要当真. 机器学习建立在统计学方法的基础上，跟“思考”没有半毛钱关系，至于理解人类语言中表达的意义,那更是遥远得离谱的事情.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;encdec.jpg&quot; &#x2F;&gt;
&lt;figcaption&gt;
图1. &lt;b&gt;编码器-解码器结构&lt;&#x2F;b&gt; – NMT 的通用示例. 编码器转换源语句为“含义”向量，再由&lt;i&gt;解码器&lt;&#x2F;i&gt;产生翻译结果.
&lt;&#x2F;figcaption&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;NMT 模型的具体结构有所不同. 对顺序数据而言，大多数 NMT 模型的一个自然选择是采用循环神经网络 (RNN).
通常，RNN 同时使用编码器和解码器. 然而，RNN 模型在以下方面有所不同: (a) &lt;em&gt;方向性&lt;&#x2F;em&gt; –  单向或双向; (b) &lt;em&gt;深度&lt;&#x2F;em&gt; – 单层或多层;  (c) &lt;em&gt;类型&lt;&#x2F;em&gt; – 常见的有 RNN,  Long Short-term Memory (LSTM),  或 gated recurrent unit
(GRU). 有兴趣的读者可以在这篇&lt;a href=&quot;http:&#x2F;&#x2F;colah.github.io&#x2F;posts&#x2F;2015-08-Understanding-LSTMs&#x2F;&quot;&gt;博文&lt;&#x2F;a&gt;上找到有关 RNNs 和 LSTM 的更多信息 .&lt;&#x2F;p&gt;
&lt;p&gt;在本教程中, 我们将考察一个 &lt;em&gt;深度多层 RNN&lt;&#x2F;em&gt; 的例子，它是单向的，并使用 LSTM 作为循环单元. 如图 2. 在这个例子中，我们将 &quot;I am a student&quot;  翻译成 &quot;Je suis étudiant&quot;. 在高层上, 这个 NMT 模型由两个循环神经网络组成: &lt;em&gt;编码器&lt;&#x2F;em&gt;
RNN 简单的吃进输入文字，不做任何预测;
另一方面，&lt;em&gt;解码器&lt;&#x2F;em&gt;在预测下一个单词时处理目标句子.&lt;&#x2F;p&gt;
&lt;p&gt;更多信息, 请参阅 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lmthang&#x2F;thesis&quot;&gt;Luong (2016)&lt;&#x2F;a&gt; .&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;58%&quot; src=&quot;.&#x2F;seq2seq.jpg&quot; &#x2F;&gt;
&lt;figcaption&gt;
图2. &lt;b&gt;神经机器翻译&lt;&#x2F;b&gt; – 一个深度循环网络的例子，把源语句 &quot;I am a student&quot; 翻译成目标语句
 &quot;Je suis étudiant&quot;. 这里, &quot;&amp;lt;s&amp;gt;&quot; 表示解码处理的开始,
而 &quot;&amp;lt;&#x2F;s&amp;gt;&quot; 表示解码结束.
&lt;&#x2F;figcaption&gt;
&lt;&#x2F;p&gt;
&lt;h3 id=&quot;an-zhuang-jiao-cheng&quot;&gt;安装教程&lt;&#x2F;h3&gt;
&lt;p&gt;要安装本教程, 你需要在系统上安装 TensorFlow. 本教程撰写时 TensorFlow 的版本为 &lt;strong&gt;1.2.1&lt;&#x2F;strong&gt; .
安装 TensorFlow 请参阅 &lt;a href=&quot;https:&#x2F;&#x2F;www.tensorflow.org&#x2F;install&#x2F;&quot;&gt;installation instructions here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;一旦安装了 TensorFlow, 你就可以运行以下脚本下载本教程的源码了:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;git clone https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;tensorflow&amp;#x2F;nmt&amp;#x2F;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;xun-lian-ru-he-jian-li-wo-men-de-di-yi-ge-nmt-xi-tong&quot;&gt;训练 – 如何建立我们的第一个 NMT 系统&lt;&#x2F;h3&gt;
&lt;p&gt;让我们先走进构建 NMT 模型的核心代码，一会儿我们将详细解释图 2 . 我们晚点再来看完整代码及数据准备部分. 这部分的代码文件为
&lt;em&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;model.py&quot;&gt;model.py&lt;&#x2F;a&gt;&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;如图 2 的底层, 编码器和解码器的循环神经网络接收下列输入: 首先, 是待翻译的句子, 接着是一个边界标记 &quot;&amp;lt;s&amp;gt;”，表示从编码到解码模式的转换, 最后是翻译好的句子.  为了&lt;em&gt;训练&lt;&#x2F;em&gt;, 我们将为系统提供以下张量,
它们包含词汇索引和时序格式（time-major format）:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;encoder_inputs&lt;&#x2F;strong&gt; [max_encoder_time, batch_size]: 原始输入字.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;decoder_inputs&lt;&#x2F;strong&gt; [max_decoder_time, batch_size]: 目标输入字.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;decoder_outputs&lt;&#x2F;strong&gt; [max_decoder_time, batch_size]: 目标输出字.
这里目标输入字 &lt;em&gt;decoder_inputs&lt;&#x2F;em&gt; 向左移动一个时间步长，并在右边附加一个句末标记.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;为了效率，我们一次训练多个句子 (batch_size). 测试时略有不同，我们稍后再行讨论.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;embedding&quot;&gt;Embedding&lt;&#x2F;h4&gt;
&lt;p&gt;根据词汇的含义，模型必须首先找出源词和目标词对应的词向量表达。为使 &lt;em&gt;embedding layer&lt;&#x2F;em&gt; 工作，首先为每种语言选择一个词表。
通常, 把词表的长度设为 V (即不重复的词汇数量). 而其它词则设为“unknown”标记，并赋予同样的词向量值。一种语言一套词向量, 一般通过训练学到。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Embedding
embedding_encoder = variable_scope.get_variable(
    &amp;quot;embedding_encoder&amp;quot;, [src_vocab_size, embedding_size], ...)
# Look up embedding:
#   encoder_inputs: [max_time, batch_size]
#   encoder_emp_inp: [max_time, batch_size, embedding_size]
encoder_emb_inp = embedding_ops.embedding_lookup(
    embedding_encoder, encoder_inputs)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;类似的，我们可以构建 &lt;em&gt;embedding_decoder&lt;&#x2F;em&gt; 和 &lt;em&gt;decoder_emb_inp&lt;&#x2F;em&gt; 。注意，可以使用已训练好的词向量, 如 word2vec 或  Glove vectors, 来初始化我们的词向量. 通常，如果有大量的训练数据，我们也可以从头开始训练这些词向量。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;bian-ma-qi&quot;&gt;编码器&lt;&#x2F;h4&gt;
&lt;p&gt;一旦检索到这个词，就将其对应的词向量作为输入发送到主网络，该网络由两个多层 RNN 组成 ，一个针对源语言的编码器和一个针对目标语言的解码器。
这两个 RNN 原则上可以共享相同的权重；然而，实践中，我们经常使用不同的参数（这样的模型在拟合大规模训练数据集时做得更好）.
这个 RNN &lt;em&gt;编码器&lt;&#x2F;em&gt;使用 0 向量作为初始状态，建立如下:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Build RNN cell
encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

# Run Dynamic RNN
#   encoder_outpus: [max_time, batch_size, num_units]
#   encoder_state: [batch_size, num_units]
encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_emb_inp,
    sequence_length=source_seqence_length, time_major=True)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;注意, 句子有不同的长度，我们通过 &lt;em&gt;source_sequence_length&lt;&#x2F;em&gt; 来告诉 &lt;em&gt;dynamic_rnn&lt;&#x2F;em&gt; 确切的源句长度，以避免计算上的浪费. 由于我们的输入有时序，我们设置 &lt;em&gt;time_major=True&lt;&#x2F;em&gt; . 这里，我们仅建立一个单层的 LSTM &lt;em&gt;编码器单元&lt;&#x2F;em&gt;. 我们将在后面的章节说明如何构建多层LSTM，增加 dropout, 和使用 attention.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;jie-ma-qi&quot;&gt;解码器&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;em&gt;decoder&lt;&#x2F;em&gt; 也需要访问源信息，一个简单的方法是用编码器最后的隐藏状态来初始化它。如图2，我们把源语言的“student”的隐藏状态传递到解码器端。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Build RNN cell
decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Helper
helper = tf.contrib.seq2seq.TrainingHelper(
    decoder_emb_inp, decoder_lengths, time_major=True)
# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
logits = outputs.rnn_output
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这里，这个代码的核心部分是 &lt;em&gt;BasicDecoder&lt;&#x2F;em&gt;，它接收 &lt;em&gt;decoder_cell&lt;&#x2F;em&gt; (类似 encoder_cell)、&lt;em&gt;helper&lt;&#x2F;em&gt;、前一个 &lt;em&gt;encoder_state&lt;&#x2F;em&gt; 作为输入输出到 &lt;em&gt;decoder&lt;&#x2F;em&gt; 对象。通过分开 decoder 和 helper，我们可以在不同代码中重用。例如，&lt;em&gt;TrainingHelper&lt;&#x2F;em&gt; 可以被 &lt;em&gt;GreedyEmbeddingHelper&lt;&#x2F;em&gt; 取代做贪心解码。更多内容请看&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;blob&#x2F;master&#x2F;tensorflow&#x2F;contrib&#x2F;seq2seq&#x2F;python&#x2F;ops&#x2F;helper.py&quot;&gt;helper.py&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;最后，我们没提到的 &lt;em&gt;projection_layer&lt;&#x2F;em&gt; 是个密集矩阵，它将顶部的隐藏状态转为 V 个维度的 logit 向量。我们在图 2 的顶部展示了这个过程。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;projection_layer = layers_core.Dense(
    tgt_vocab_size, use_bias=False)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;wu-chai&quot;&gt;误差&lt;&#x2F;h4&gt;
&lt;p&gt;根据上面给定的 &lt;em&gt;logits&lt;&#x2F;em&gt; ，我们现在准备计算我们的训练误差:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=decoder_outputs, logits=logits)
train_loss = (tf.reduce_sum(crossent * target_weights) &amp;#x2F;
    batch_size)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这里，&lt;em&gt;target_weights&lt;&#x2F;em&gt; 是一个和 &lt;em&gt;decoder_outputs&lt;&#x2F;em&gt; 维度一样的 0-1 矩阵.它把超出目标序列长度之外的地方填充为0。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;重要注意事项&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;: 值得指出的是，我们将误差除以 &lt;em&gt;batch_size&lt;&#x2F;em&gt;, 所以我们的超参数对 batch_size 是不变的. 有些人将误差除以(&lt;em&gt;batch_size&lt;&#x2F;em&gt; * &lt;em&gt;num_time_steps&lt;&#x2F;em&gt;)，它可以降低短句的错误. 更微妙的是，我们的超参数（用于前一种方法）不能被用于后一种方法.  例如，如果两个方法都使用 1.0 为学习率的 SGD（随机梯度下降算法），后一种方法会更有效, 因为它采用了更小的 1 &#x2F; &lt;em&gt;num_time_steps&lt;&#x2F;em&gt; 作为学习率。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ti-du-ji-suan-you-hua&quot;&gt;梯度计算 &amp;amp; 优化&lt;&#x2F;h4&gt;
&lt;p&gt;我们现在已经定义了正向传播的 NMT 模型。计算反向传播只是几行代码的问题:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Calculate and clip gradients
params = tf.trainable_variables()
gradients = tf.gradients(train_loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(
    gradients, max_gradient_norm)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;训练 RNN 的一个重要步骤是梯度调整。这里，我们按照惯例来调整。&lt;em&gt;max_gradient_norm&lt;&#x2F;em&gt; 的最大值, 通常设为 5 或 1. 最后一步是选择优化器. Adam 优化器是常用的选择.  我们也选择一个学习率，这个值常在 0.0001 到 0.001 之间; 并且可以随训练进度而减小.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Optimization
optimizer = tf.train.AdamOptimizer(learning_rate)
update_step = optimizer.apply_gradients(
    zip(clipped_gradients, params))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在我们自己的实验中，我们采用可自动降低学习率的标准 SGD 优化器（tf.train.GradientDescentOptimizer），从而产生更好的性能。参见 &lt;a href=&quot;&#x2F;blog&#x2F;nmt&#x2F;#ping-ce&quot;&gt;评测&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;dong-shou-rang-wo-men-xun-lian-yi-ge-nmtmo-xing&quot;&gt;动手 - 让我们训练一个NMT模型&lt;&#x2F;h3&gt;
&lt;p&gt;让我们训练我们的第一个 NMT 模型，把越南语翻译成英语！我们代码的入口是
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;nmt.py&quot;&gt;&lt;em&gt;nmt.py&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;我们将使用 &lt;em&gt;small-scale parallel corpus of TED talks&lt;&#x2F;em&gt; (133K training
examples) 进行此练习. 所有数据可在:
&lt;a href=&quot;https:&#x2F;&#x2F;nlp.stanford.edu&#x2F;projects&#x2F;nmt&#x2F;&quot;&gt;https:&#x2F;&#x2F;nlp.stanford.edu&#x2F;projects&#x2F;nmt&#x2F;&lt;&#x2F;a&gt;找到. 我们将使用 tst2012 作为训练数据集,  tst2013 作为测试数据集.&lt;&#x2F;p&gt;
&lt;p&gt;运行下列命令下载训练 NMT 模型的数据:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;nmt&#x2F;scripts&#x2F;download_iwslt15.sh &#x2F;tmp&#x2F;nmt_data&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;运行如下命令开始训练:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;mkdir &amp;#x2F;tmp&amp;#x2F;nmt_model
python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=&amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;vocab  \
    --train_prefix=&amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;train \
    --dev_prefix=&amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;tst2012  \
    --test_prefix=&amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;tst2013 \
    --out_dir=&amp;#x2F;tmp&amp;#x2F;nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;上述命令训练一个 2 层 LSTM seq2seq 模型，含 128 个隐藏单元和 12 轮的 embedding 操作。我们使用的 dropout 值为 0.2（维持概率在0.8 ）。如果没有错误，我们应该在我们训练时看到类似于下面的日志。&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# First evaluation, global step 0
  eval dev: perplexity 17193.66
  eval test: perplexity 17193.27
# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017
  sample train data:
    src_reverse: &amp;lt;&amp;#x2F;s&amp;gt; &amp;lt;&amp;#x2F;s&amp;gt; Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .
    ref: That , of course , was the &amp;lt;unk&amp;gt; distilled from the theories of Karl Marx . &amp;lt;&amp;#x2F;s&amp;gt; &amp;lt;&amp;#x2F;s&amp;gt; &amp;lt;&amp;#x2F;s&amp;gt;
  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00
  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00
  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00
  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00
  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;有关详细信息，请参阅 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;train.py&quot;&gt;&lt;em&gt;train.py&lt;&#x2F;em&gt;&lt;&#x2F;a&gt; .&lt;&#x2F;p&gt;
&lt;p&gt;我们可以在训练期间启动 Tensorboard 查看模型的统计：:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;tensorboard --port 22222 --logdir &amp;#x2F;tmp&amp;#x2F;nmt_model&amp;#x2F;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;从英语到越南语的训练可以简单地改变:
&lt;code&gt;--src=en --tgt=vi&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;tui-li-ru-he-chan-sheng-fan-yi&quot;&gt;推理 – 如何产生翻译&lt;&#x2F;h3&gt;
&lt;p&gt;当你训练你的 NMT 模型（一旦你已训练好模型），你可以得到以前未见过的源语句的翻译。这个过程称为推理。训练和推理（测试）之间有明确的区别：在推理时，我们只能访问源语句，即 encoder_inputs。解码有很多种方法, 包括贪心、采样和定向搜索几种。在这里，我们将讨论贪心解码法。&lt;&#x2F;p&gt;
&lt;p&gt;这个想法很简单，我们在图 3 中说明:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;我们仍然以与训练期间相同的方式对源语句进行编码以获得 encoder_state，并使用该 encoder_state 来初始化解码器。&lt;&#x2F;li&gt;
&lt;li&gt;一旦解码器接收到起始符号“&amp;lt;s&amp;gt;”（参见我们代码中的 tgt_sos_id ），解码（翻译）处理就开始&lt;&#x2F;li&gt;
&lt;li&gt;对于解码器侧的每个时间步长，我们将 RNN 的输出视为一组 logit。我们选择最可能的字，与最大 logit 值相关联的 id 作为译出的字（这是“贪婪”行为）。例如在图 3 中，在第一个解码步骤中，词“moi”具有最高的翻译概率。然后，我们将这个词作为输入提供给下一个时间步。(译者注：由于输出的logit向量维度为词表长度V，当词表很大时计算量很大)&lt;&#x2F;li&gt;
&lt;li&gt;继续第3步直到遇到句子的结尾标记“&amp;lt;&#x2F;s&amp;gt;”（参见我们的代码中的 tgt_eos_id ）。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;50%&quot; src=&quot;.&#x2F;greedy_dec.jpg&quot; &#x2F;&gt;
&lt;figcaption&gt;
图3. &lt;b&gt;贪心解码&lt;&#x2F;b&gt; – 如何用贪心搜索法训练 NMT 模型, 使源语句产生&quot;Je suis étudiant&quot;的翻译
&lt;&#x2F;figcaption&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;第 3 步的推理与训练不同。不是总是将正确的目标词作为输入，推理使用模型预测的单词。以下是实现贪心解码的代码。它与训练解码器非常相似。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Helper
helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
    embedding_decoder,
    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)

# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder, maximum_iterations=maximum_iterations)
translations = outputs.sample_id
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这里，我们使用 &lt;em&gt;GreedyEmbeddingHelper&lt;&#x2F;em&gt; 代替 &lt;em&gt;TrainingHelper&lt;&#x2F;em&gt;。由于我们预先不知道目标序列的长度，所以我们使用 &lt;em&gt;maximum_iterations&lt;&#x2F;em&gt; 来限制翻译的长度。一个启发式的用法是采用源语句长度的两倍来解码。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;训练好一个模型后，我们现在可以创建一个推理文件并翻译一些句子：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;cat &amp;gt; &amp;#x2F;tmp&amp;#x2F;my_infer_file.vi
# (copy and paste some sentences from &amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;tst2013.vi)

python -m nmt.nmt \
    --model_dir=&amp;#x2F;tmp&amp;#x2F;nmt_model \
    --inference_input_file=&amp;#x2F;tmp&amp;#x2F;my_infer_file.vi \
    --inference_output_file=&amp;#x2F;tmp&amp;#x2F;nmt_model&amp;#x2F;output_infer

cat &amp;#x2F;tmp&amp;#x2F;nmt_model&amp;#x2F;output_infer # To view the inference as output
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;注意，只要存在训练检查点，即使模型仍在训练中，也可以运行上述命令。详见 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;inference.py&quot;&gt;&lt;em&gt;inference.py&lt;&#x2F;em&gt;&lt;&#x2F;a&gt; 。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;jin-jie&quot;&gt;进阶&lt;&#x2F;h2&gt;
&lt;p&gt;经历了最基本的 seq2seq 模型，让我们进一步完善它！为了建立最先进的神经机器翻译系统，我们还需要更多的“内功心法”：
&lt;em&gt;注意力机制(attention mechanism)&lt;&#x2F;em&gt;，这是由 &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473&quot;&gt;Bahdanau等人2015年&lt;&#x2F;a&gt;首次引入，然后由 &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1508.04025&quot;&gt;Luong等人2015年&lt;&#x2F;a&gt;完善。&lt;em&gt;注意力机制&lt;&#x2F;em&gt;的关键在于，通过在翻译过程中对相关的源内容进行“关注”，建立目标和源之间的直接连接。注意力机制 的一个很好的副产品是在源和目标句子之间生成一个易于查看的对齐矩阵（如图4所示）&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;50%&quot; src=&quot;.&#x2F;attention_vis.jpg&quot; &#x2F;&gt;
&lt;figcaption&gt;
图4. &lt;b&gt;Attention 可视化&lt;&#x2F;b&gt; – 源语句与目标语句的词汇对齐矩阵示例， 图片来自 (Bahdanau et al., 2015).
&lt;&#x2F;figcaption&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;请记住，在 seq2seq 模型中，当开始解码时，我们将最后的源状态从编码器传递到解码器。这对中短句的效果很好; 而对于长句，单个固定大小的隐藏状态会成为信息瓶颈。
注意力机制不是放弃在源 RNN 中计算的所有隐藏状态，而是提供了一种允许解码器窥视它们的方法（将它们视为源信息的动态存储器）。
通过这样做，注意力机制改善了较长句子的翻译质量。现在，注意力机制已成为事实上的标准，并已成功应用于许多其他任务（包括图像字幕生成，语音识别和文本摘要等）。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zhu-yi-li-ji-zhi-de-bei-jing&quot;&gt;注意力机制的背景&lt;&#x2F;h3&gt;
&lt;p&gt;我们现在讲讲（Luong等人，2015年）中提出的注意力机制(attention mechanism)的一个实例，该实例已被用于包括 &lt;a href=&quot;http:&#x2F;&#x2F;opennmt.net&#x2F;about&#x2F;&quot;&gt;OpenNMT&lt;&#x2F;a&gt; 等开源工具包在内的多个最先进的系统，以及本教程的 TF seq2seq API 中。我们还将提供注意力机制其他变种的连接。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;58%&quot; src=&quot;.&#x2F;attention_mechanism.jpg&quot; &#x2F;&gt;
&lt;figcaption&gt;
图5. &lt;b&gt;Attention mechanism&lt;&#x2F;b&gt; – (Luong et al., 2015)中提到的基于 attention NMT 系统的示例. 我们重点突出 attention 计算的第一个步骤. 为了清晰，我们没像图2那样显示 embedding 层和 projection 层.
&lt;&#x2F;figcaption&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;如图 5 所示，attention 计算发生在每个解码器的时间步长。它包括以下步骤：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;将当前目标的隐藏状态与所有源的状态进行比较以获得 &lt;em&gt;attention weights&lt;&#x2F;em&gt;（如图4所示）。&lt;&#x2F;li&gt;
&lt;li&gt;基于 attention weights ，我们计算&lt;em&gt;上下文矢量&lt;&#x2F;em&gt;（&lt;em&gt;context vector&lt;&#x2F;em&gt;） 作为源状态的加权平均值。&lt;&#x2F;li&gt;
&lt;li&gt;将上下文矢量与当前目标的隐藏状态组合以产生最终的 &lt;em&gt;attention vector&lt;&#x2F;em&gt;。&lt;&#x2F;li&gt;
&lt;li&gt;attention vector 作为输入发送到下一个时间步（&lt;em&gt;input feeding&lt;&#x2F;em&gt;）。前三个步骤通过以下等式来总结：&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;attention_equation_0.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;这里，函数 &lt;code&gt;score&lt;&#x2F;code&gt; 用于将目标隐藏状态 $h_t$ 与每个源隐藏状态 $\overline{h}_s$ 进行比较，并将结果归一化以产生 attention weights（一个基于源语言的位置分布）。这里的 score 函数有多种选择; 流行的 score 函数包括等式（4）列出的乘法和加法形式。一旦完成计算，Attention vector $a_t$ 被用来导出softmax logit 和 loss。这类似于 seq2seq 模型顶层的目标隐藏状态。这个函数 &lt;code&gt;f&lt;&#x2F;code&gt; 也可以采取其他形式。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;attention_equation_1.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;attention mechanisms 的各种实现可以在 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;blob&#x2F;master&#x2F;tensorflow&#x2F;contrib&#x2F;seq2seq&#x2F;python&#x2F;ops&#x2F;attention_wrapper.py&quot;&gt;attention_wrapper.py&lt;&#x2F;a&gt; 中找到.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;注意力机制有多重要?&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;如上述方程式所示，有许多不同的 attention 变体。这些变体取决于 score 函数和 attention 函数的形式，以及在 score 函数中是否使用上一个时间步的状态 $h_{t-1}$ 而不是 $h_t$ （源自 Bahdanau et al.,2015 的建议）。经验上，我们发现只有某些选择很重要。首先，是  attention 的基本形式，即目标语言和源语言之间的直接联系。第二，重要的是把 attention vector 送到下一个时间步，以通报网络关于过去的 attention 决定（源自Luong et al., 2015 中的示范）。最后，score 函数的选择常常会导致不同的表现。更多内容请看&lt;a href=&quot;&#x2F;blog&#x2F;nmt&#x2F;#ping-ce&quot;&gt;评测结果&lt;&#x2F;a&gt;部分。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;attention-wrapper-api&quot;&gt;Attention Wrapper API&lt;&#x2F;h3&gt;
&lt;p&gt;在实现 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;blob&#x2F;master&#x2F;tensorflow&#x2F;contrib&#x2F;seq2seq&#x2F;python&#x2F;ops&#x2F;attention_wrapper.py&quot;&gt;AttentionWrapper&lt;&#x2F;a&gt; 时，我们借鉴了 &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473&quot;&gt;(Weston et al., 2015)&lt;&#x2F;a&gt; 在 &lt;em&gt;memory networks&lt;&#x2F;em&gt; 方面的一些术语。本教程介绍的 attention mechanism 是只读 memory，而不是可读写的 memory。具体来说，一组源语隐藏状态（或其转换版本，如：Luong 的评分中的  $W\overline{h}_s$，或 Bahong 的评分中的 $W_2\overline{h}_s$）被作为 &lt;em&gt;“memory”&lt;&#x2F;em&gt; 。在每个时间步骤中，我们使用当前的目标隐藏状态作为 &lt;em&gt;“query”&lt;&#x2F;em&gt; 来决定要读取 memory 的哪个部分。通常，查询需要与对应于各个 memory 插槽的键值进行比较。在上述 attention mechanism 的介绍中，我们恰好将源语隐藏状态（或其转换版本，例如，Bahdanau 的评分中的 $W_1h_t$）作为“键”值。可以通过这种记忆网络术语来启发其他形式的  attention！&lt;&#x2F;p&gt;
&lt;p&gt;多亏了对 attention 的封装，使得我们扩展原始 seq2seq 代码时很方便。这部分文件参见 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;attention_model.py&quot;&gt;&lt;em&gt;attention_model.py&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;首先，我们需要定义一个attention mechanism，例如(Luong et al., 2015):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# attention_states: [batch_size, max_time, num_units]
attention_states = tf.transpose(encoder_outputs, [1, 0, 2])

# Create an attention mechanism
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    num_units, attention_states,
    memory_sequence_length=source_sequence_length)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在前面的&lt;a href=&quot;&#x2F;blog&#x2F;nmt&#x2F;#bian-ma-qi&quot;&gt;编码器&lt;&#x2F;a&gt; 部分，&lt;em&gt;encoder_outputs&lt;&#x2F;em&gt; 是顶层所有源语隐藏状态的集合，其形状为 &lt;em&gt;[max_time，batch_size，num_units]&lt;&#x2F;em&gt; （因为我们使用 &lt;em&gt;dynamic_rnn&lt;&#x2F;em&gt;，&lt;em&gt;time_major&lt;&#x2F;em&gt; 设置为 &lt;em&gt;True&lt;&#x2F;em&gt; 以获得效率）。对于 attention mechanism，我们需要确保传递的“memory”是批处理的，所以我们需要转置 &lt;em&gt;attention_states&lt;&#x2F;em&gt;。我们将 &lt;em&gt;source_sequence_length&lt;&#x2F;em&gt; 传递给 attention machanism，以确保 attention weight 适当归一化（仅在非填充位置上）。&lt;&#x2F;p&gt;
&lt;p&gt;定义了 attention mechanism 后，我们使用 &lt;em&gt;AttentionWrapper&lt;&#x2F;em&gt; 来包装 decoding_cell：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    decoder_cell, attention_mechanism,
    attention_layer_size=num_units)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;代码的其余部分与&lt;a href=&quot;&#x2F;blog&#x2F;nmt&#x2F;#bian-ma-qi&quot;&gt;解码器&lt;&#x2F;a&gt;部分几乎相同！&lt;&#x2F;p&gt;
&lt;h3 id=&quot;dong-shou-jian-li-ji-yu-attention-de-nmt-mo-xing&quot;&gt;动手 – 建立基于 attention 的 NMT 模型&lt;&#x2F;h3&gt;
&lt;p&gt;要启用的 attention ，在训练时我们需要使用 &lt;code&gt;luong&lt;&#x2F;code&gt;、&lt;code&gt;scaled_luong&lt;&#x2F;code&gt;、&lt;code&gt;bahdanau&lt;&#x2F;code&gt;  或 &lt;code&gt;normed_bahdanau&lt;&#x2F;code&gt; 中的一个作为 &lt;code&gt;attention&lt;&#x2F;code&gt; 标志的值。该标志指定了我们将要使用的 attention mechanism。此外，我们需要为 attention 模型创建一个新的目录，所以我们不能重复使用前面训练过的简单 NMT 模型。&lt;&#x2F;p&gt;
&lt;p&gt;运行以下命令开始训练:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;mkdir &amp;#x2F;tmp&amp;#x2F;nmt_attention_model

python -m nmt.nmt \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=&amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;vocab  \
    --train_prefix=&amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;train \
    --dev_prefix=&amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;tst2012  \
    --test_prefix=&amp;#x2F;tmp&amp;#x2F;nmt_data&amp;#x2F;tst2013 \
    --out_dir=&amp;#x2F;tmp&amp;#x2F;nmt_attention_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;训练后，我们可以使用相同的推理命令与新的 model_dir 进行推理：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;python -m nmt.nmt \
    --model_dir=&amp;#x2F;tmp&amp;#x2F;nmt_attention_model \
    --inference_input_file=&amp;#x2F;tmp&amp;#x2F;my_infer_file.vi \
    --inference_output_file=&amp;#x2F;tmp&amp;#x2F;nmt_attention_model&amp;#x2F;output_infer
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;ti-shi-ji-qiao&quot;&gt;提示 &amp;amp; 技巧&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;xun-lian-ping-gu-he-tui-li-tu&quot;&gt;训练, 评估, 和推理图&lt;&#x2F;h3&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：“图”的定义原文，A Graph contains a set of Operation objects, which represent units of computation; and Tensor objects, which represent the units of data that flow between operations. 简单说, “图”相当于一个数学公式，其中的 tensor 相当于变量 x  ，而定义的 op 相当于连接变量的运算符号.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;在 TensorFlow 中构建机器学习模型时，最好建立三个独立的图：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;训练图, 其中:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;把来自文件或外部导入的数据批次化（即分成数量相同的很多组，每次处理一组），作为输入数据&lt;&#x2F;li&gt;
&lt;li&gt;包含正向和反向传播操作.&lt;&#x2F;li&gt;
&lt;li&gt;构建优化器，并加到训练中.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;评估图, 其中:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;批次化输入数据.&lt;&#x2F;li&gt;
&lt;li&gt;包含在训练图中用过的正向传播操作，增加了一个没在训练图中用过的评估操作&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;推理图, 其中:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;可能不批量输入数据.&lt;&#x2F;li&gt;
&lt;li&gt;不对输入数据进行子采样或批次化.&lt;&#x2F;li&gt;
&lt;li&gt;从占位符读取输入数据（可以通过 &lt;em&gt;feed_dict&lt;&#x2F;em&gt; 或 C ++ TensorFlow serving binary 将数据直接提供给推理图）.&lt;&#x2F;li&gt;
&lt;li&gt;包括模型正向传播操作的一个子集，和一个额外的在 session.run 调用之间存储状态的特殊输入&#x2F;输出（或有）.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;分别构建图有几个好处:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;推理图通常与其他两个很不同，因此分开构建是有意义的.&lt;&#x2F;li&gt;
&lt;li&gt;由于没有反向转播操作，评估图变得更简单.&lt;&#x2F;li&gt;
&lt;li&gt;数据分别提供给每个图.&lt;&#x2F;li&gt;
&lt;li&gt;复用简单得多.  比如, 在评估图中，不需要用 &lt;em&gt;reuse=True&lt;&#x2F;em&gt; 来打开变量的作用域，因为训练模型已经创建了这些变量.  所以相同的代码不需要加上 &lt;em&gt;reuse=&lt;&#x2F;em&gt; 参数就能被复用.&lt;&#x2F;li&gt;
&lt;li&gt;在分布式训练中，把训练、评估和推理的工作分开是很平常的。这就需要它们各自建立自己的图。因此，以这种方式构建的系统将为你提供分布式训练的准备。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;复杂性的主要来源是如何在单个计算机设置中跨三个图共享变量. 可以为每个图使用单独的会话来解决. 训练会话定期的保存检查点，评估会话和推断会话从检查点导入参数. 下面的例子显示了两种方法的主要区别.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;之前: 三个模型在一个图中并共享一个会话&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;with tf.variable_scope(&amp;#x27;root&amp;#x27;):
  train_inputs = tf.placeholder()
  train_op, loss = BuildTrainModel(train_inputs)
  initializer = tf.global_variables_initializer()

with tf.variable_scope(&amp;#x27;root&amp;#x27;, reuse=True):
  eval_inputs = tf.placeholder()
  eval_loss = BuildEvalModel(eval_inputs)

with tf.variable_scope(&amp;#x27;root&amp;#x27;, reuse=True):
  infer_inputs = tf.placeholder()
  inference_output = BuildInferenceModel(infer_inputs)

sess = tf.Session()

sess.run(initializer)

for i in itertools.count():
  train_input_data = ...
  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})

  if i % EVAL_STEPS == 0:
    while data_to_eval:
      eval_input_data = ...
      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})

  if i % INFER_STEPS == 0:
    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;之后: 三个模型在三个图中，有三个会话并共享同样的变量&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;train_graph = tf.Graph()
eval_graph = tf.Graph()
infer_graph = tf.Graph()

with train_graph.as_default():
  train_iterator = ...
  train_model = BuildTrainModel(train_iterator)
  initializer = tf.global_variables_initializer()

with eval_graph.as_default():
  eval_iterator = ...
  eval_model = BuildEvalModel(eval_iterator)

with infer_graph.as_default():
  infer_iterator, infer_inputs = ...
  infer_model = BuildInferenceModel(infer_iterator)

checkpoints_path = &amp;quot;&amp;#x2F;tmp&amp;#x2F;model&amp;#x2F;checkpoints&amp;quot;

train_sess = tf.Session(graph=train_graph)
eval_sess = tf.Session(graph=eval_graph)
infer_sess = tf.Session(graph=infer_graph)

train_sess.run(initializer)
train_sess.run(train_iterator.initializer)

for i in itertools.count():

  train_model.train(train_sess)

  if i % EVAL_STEPS == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)
    eval_model.saver.restore(eval_sess, checkpoint_path)
    eval_sess.run(eval_iterator.initializer)
    while data_to_eval:
      eval_model.eval(eval_sess)

  if i % INFER_STEPS == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)
    infer_model.saver.restore(infer_sess, checkpoint_path)
    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})
    while data_to_infer:
      infer_model.infer(infer_sess)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;注意后一种方法是如何转换为分布式版本的。&lt;&#x2F;p&gt;
&lt;p&gt;新方法的另一个区别在于，在每个 &lt;em&gt;session.run&lt;&#x2F;em&gt; 调用时，我们使用有状态的迭代器对象来代替 &lt;em&gt;feed_dicts&lt;&#x2F;em&gt; 提供数据（从而我们能自己批处理、切分和操作数据）。这些迭代器使&quot;输入管道&quot;在设置单机和分布式时，都容易很多。我们将在下一节中介绍新的输入数据管道（ 限TensorFlow 1.2 ）。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shu-ju-shu-ru-guan-dao&quot;&gt;数据输入管道&lt;&#x2F;h3&gt;
&lt;p&gt;在 TensorFlow 1.2 之前, 用户有三种方式把数据提供给 TensorFlow 进行训练和评估:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;在每次调用 &lt;em&gt;session.run&lt;&#x2F;em&gt; 时，用 &lt;em&gt;feed_dict&lt;&#x2F;em&gt; 提供数据.&lt;&#x2F;li&gt;
&lt;li&gt;在 &lt;em&gt;tf.train&lt;&#x2F;em&gt; (e.g. &lt;em&gt;tf.train.batch&lt;&#x2F;em&gt;) 和 &lt;em&gt;tf.contrib.train&lt;&#x2F;em&gt; 中使用排队机制 .&lt;&#x2F;li&gt;
&lt;li&gt;使用象 &lt;em&gt;tf.contrib.learn&lt;&#x2F;em&gt; 或 &lt;em&gt;tf.contrib.slim&lt;&#x2F;em&gt; 等高级框架中的 helper (用 #2 效率更高).&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;第一种方法对于不熟悉 TensorFlow 的用户或只能在 Python 中需要做个性化输入（如：自己的小批次队列）的用户更容易。第二和第三种方法更标准，但灵活性稍差一些；他们还需要启动多个 python 线程（queue runners）。此外，如果使用错误的队列可能会导致死锁或不可知的错误。然而，队列比使用 &lt;em&gt;feed_dict&lt;&#x2F;em&gt; 更高效，也是单机和分布式训练的标准方式。&lt;&#x2F;p&gt;
&lt;p&gt;从 TensorFlow 1.2 开始，有一个新的方法可用于将数据读入 TensorFlow 模型：数据集迭代器（dataset iterators），可在 &lt;strong&gt;tf.contrib.data&lt;&#x2F;strong&gt;  模块中找到。数据迭代器是灵活的、易理解的和可定制的，并能依赖 TensorFlow C ++ 运行库提供高效和多线程的读入操作。&lt;&#x2F;p&gt;
&lt;p&gt;一个&lt;strong&gt;数据集&lt;&#x2F;strong&gt;可以从一批数据张量、一个文件名，或包含多个文件名的一个张量来创建。举例如下：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Training dataset consists of multiple files.
train_dataset = tf.contrib.data.TextLineDataset(train_files)

# Evaluation dataset uses a single file, but we may
# point to a different file for each evaluation round.
eval_file = tf.placeholder(tf.string, shape=())
eval_dataset = tf.contrib.data.TextLineDataset(eval_file)

# For inference, feed input data to the dataset directly via feed_dict.
infer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))
infer_dataset = tf.contrib.data.Dataset.from_tensor_slices(infer_batch)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;所有数据集的输入处理都是类似的。包括数据的读取和清理，数据的切分（在训练和评估时）、过滤及批处理等。&lt;&#x2F;p&gt;
&lt;p&gt;把每个句子转换成单词的字符串向量，例如，我们对数据集做 map 转换：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;dataset = dataset.map(lambda string: tf.string_split([string]).values)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;然后，我们把每个句子向量切换成一个包含向量及其动态长度的元组：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;dataset = dataset.map(lambda words: (words, tf.size(words))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;最后，我们对每个句子执行词汇查找。根据给定的查找表，把元组中的元素从字符串向量转换为整数向量。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;dataset = dataset.map(lambda words, size: (table.lookup(words), size))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;拼接两个数据集也很容易。如果有两个彼此逐行互译的文件，且每个文件都有自己的数据集，则可按以下方式创建一个新数据集来合并这两个数据集：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;source_target_dataset = tf.contrib.data.Dataset.zip((source_dataset, target_dataset))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;不定长句子的批处理是简单明了的。接下来对 &lt;em&gt;source_target_dataset&lt;&#x2F;em&gt; 数据集中的元素按 &lt;em&gt;batch_size&lt;&#x2F;em&gt; 的大小规格做批次转换， 同时在每批中，把源向量和目标向量进行填充，使其长度与它们当中最长的向量长度一致。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这句话不是人翻的，不信你去看原文，我是看了半天下面的 python 源码才搞出来，我在怀疑是我的语文不好还是作者的语文不好，哎！&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;batched_dataset = source_target_dataset.padded_batch(
    batch_size,
    padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size
                    tf.TensorShape([])),     # size(source)
                   (tf.TensorShape([None]),  # target vectors of unknown size
                    tf.TensorShape([]))),    # size(target)
    padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id
                     0),          # size(source) -- unused
                    (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id
                     0)))         # size(target) -- unused
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;从这个数据集传出的值是一个嵌套元组，其张量最左边的维度就是 &lt;em&gt;batch_size&lt;&#x2F;em&gt; 的大小。该结构为:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;iterator[0][0] 是分好批和填充好的源句子矩阵.&lt;&#x2F;li&gt;
&lt;li&gt;iterator[0][1] 是分好批的源句子长度向量.&lt;&#x2F;li&gt;
&lt;li&gt;iterator[1][0] 是分好批和填充好的目标句子矩阵.&lt;&#x2F;li&gt;
&lt;li&gt;iterator[1][1] 是分好批的目标句子长度向量.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;最后, 尽量把这些长度相似的源句子打包在一起。更多内容及完整实现请参阅 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;utils&#x2F;iterator_utils.py&quot;&gt;utils&#x2F;iterator_utils.py&lt;&#x2F;a&gt; .&lt;&#x2F;p&gt;
&lt;p&gt;从数据集读取数据仅需三行代码：创建迭代器，获取其值，和初始化。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;batched_iterator = batched_dataset.make_initializable_iterator()

((source, source_lengths), (target, target_lenghts)) = batched_iterator.get_next()

# At initialization time.
session.run(batched_iterator.initializer, feed_dict={...})
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;一旦迭代器被初始化，每个 &lt;em&gt;session.run&lt;&#x2F;em&gt; 调用（访问源或目标张量）将从底层数据集请求下一批数据。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;guan-yu-zeng-qiang-nmt-mo-xing-de-qi-ta-xi-jie&quot;&gt;关于增强 NMT 模型的其它细节&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;shuang-xiang-rnns&quot;&gt;双向 RNNs&lt;&#x2F;h4&gt;
&lt;p&gt;在编码器端双向化通常会带来更好的性能(随着层数的增加速度会有所降低). 这里, 我们给个简单的例子，建立一个双向单层的编码器：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Construct forward and backward cells
forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)
backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(
    forward_cell, backward_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=True)
encoder_outputs = tf.concat(bi_outputs, -1)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;变量 &lt;em&gt;encoder_outputs&lt;&#x2F;em&gt; 和 &lt;em&gt;encoder_state&lt;&#x2F;em&gt; 的用法与编码器那节中说的一样. 注意, 对多个双向层, 我们需要对 encoder_state 做些修改, 详见 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;model.py&quot;&gt;model.py&lt;&#x2F;a&gt;,中的 &lt;em&gt;_build_bidirectional_rnn()&lt;&#x2F;em&gt; 方法 .&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ding-xiang-sou-suo&quot;&gt;定向搜索&lt;&#x2F;h4&gt;
&lt;p&gt;虽然用贪心法解码能带给我们较满意的翻译质量，但用定向搜索解码能进一步提高性能。定向搜索的想法是，在翻译的同时，保留一个小小的最佳候选集以便更容易检索。这个定向的范围称为 &lt;em&gt;beam width&lt;&#x2F;em&gt;; 一般这个值设为 10 就够了. 更多信息请参阅 &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1703.01619&quot;&gt;Neubig, (2017)&lt;&#x2F;a&gt; 第 7.2.3 节。举例如下：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;# Replicate encoder infos beam_width times
decoder_initial_state = tf.contrib.seq2seq.tile_batch(
    encoder_state, multiplier=hparams.beam_width)

# Define a beam-search decoder
decoder = tf.contrib.seq2seq.BeamSearchDecoder(
        cell=decoder_cell,
        embedding=embedding_decoder,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=decoder_initial_state,
        beam_width=beam_width,
        output_layer=projection_layer,
        length_penalty_weight=0.0)

# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;注意这里对 &lt;em&gt;dynamic_decode()&lt;&#x2F;em&gt; API 的调用和&lt;a href=&quot;&#x2F;blog&#x2F;nmt&#x2F;#jie-ma-qi&quot;&gt;解码器&lt;&#x2F;a&gt;那节一样。解码后，我们就能向下面那样获取翻译结果了：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;translations = outputs.predicted_ids
# Make sure translations shape is [batch_size, beam_width, time]
if self.time_major:
   translations = tf.transpose(translations, perm=[1, 2, 0])
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;关于 &lt;em&gt;_build_decoder()&lt;&#x2F;em&gt; 方法的更多信息，详见 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;model.py&quot;&gt;model.py&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;chao-can-shu&quot;&gt;超参数&lt;&#x2F;h4&gt;
&lt;p&gt;有几个超参数可以提高性能。这里，我们根据自己的经验列出一些［免责声明：其它人可能不同意我说的］。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;优化&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;: Adam 优化器有些不太寻常的结构, 如果你用 SGD （随机梯度下降算法）训练，它一般会带来更好的性能。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Attention&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;: 在编码器端，Bahdanau-style attention 通常需要双向 RNN 才运行良好; 而 Luong-style attention 适用于不同的设置. 在本教程中, 我们推荐使用 Luong 和 Bahdanau-style attentions 的改进版本： &lt;em&gt;scaled_luong&lt;&#x2F;em&gt; 和 &lt;em&gt;normed bahdanau&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;duo-gpu-xun-lian&quot;&gt;多 GPU 训练&lt;&#x2F;h4&gt;
&lt;p&gt;训练一个 NMT 模型可能要好几天。把不同的 RNN 层放到不同的 GPU 上，能提高训练速度。下面是在多 GPU 上创建 RNN 层的例子。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;cells = []
for i in range(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      &amp;quot;&amp;#x2F;gpu:%d&amp;quot; % (num_layers % num_gpus)))
cell = tf.contrib.rnn.MultiRNNCell(cells)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;另外，我们要在&lt;code&gt;tf.gradients&lt;&#x2F;code&gt; 中启用 &lt;code&gt;colocate_gradients_with_ops&lt;&#x2F;code&gt;选项，才可以对梯度进行并行计算。&lt;&#x2F;p&gt;
&lt;p&gt;你可能注意到即使增加GPU，对基于 attention 的 NMT 模型的速度提升也很小。attention 架构的一个主要缺点是，在每一个时间步，采用顶层（即最后一层）输出来查询 attention。这就意味着每次解码时必须等前面的所以步骤完成才行；因此，我们不能简单的把 RNN 层放在多个 GPU 上来并行解码。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1609.08144.pdf&quot;&gt;GNMT attention architecture&lt;&#x2F;a&gt; 提出，通过使用底层（即第一层）输出来查询 attention 来并行解码运算。这样，每次解码就能在前面的第一层完成后开始。我们在 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;gnmt_model.py&quot;&gt;GNMTAttentionMultiCell&lt;&#x2F;a&gt; 中的子类 &lt;em&gt;tf.contrib.rnn.MultiRNNCell&lt;&#x2F;em&gt;上实现此架构，下面是用 &lt;em&gt;GNMTAttentionMultiCell&lt;&#x2F;em&gt; 创建一个解码单元的示例。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;cells = []
for i in range(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      &amp;quot;&amp;#x2F;gpu:%d&amp;quot; % (num_layers % num_gpus)))
attention_cell = cells.pop(0)
attention_cell = tf.contrib.seq2seq.AttentionWrapper(
    attention_cell,
    attention_mechanism,
    attention_layer_size=None,  # don&amp;#x27;t add an additional dense layer.
    output_attention=False,)
cell = GNMTAttentionMultiCell(attention_cell, cells)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;ping-ce&quot;&gt;评测&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;iwslt-ying-yu-yue-nan-yu&quot;&gt;IWSLT 英语-越南语&lt;&#x2F;h3&gt;
&lt;p&gt;样本集: 133K examples, 训练集=tst2012, 测试集=tst2013,
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;nmt&#x2F;blob&#x2F;master&#x2F;nmt&#x2F;scripts&#x2F;download_iwslt15.sh&quot;&gt;下载脚本&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;训练细节&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;. 我们用双向编码器（编码器有一个双向层）训练 512 单元的 2 层 LSTM，embedding 维度为 512。LuongAttention (scale=True) 与 keep_prob 为 0.8 的 dropout 一起使用。所有参数均匀。我们采用学习率为 1.0 的 SGD 算法：训练12K 步（12 轮）；8K 步后，我们开始每 1K 步减半学习率。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;结果&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;.
TODO(rzhao): 添加英语-越南语模型的 URL。&lt;&#x2F;p&gt;
&lt;p&gt;以下是两个模型的平均结果
(&lt;a href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;models&#x2F;nmt&#x2F;envi_model_1.zip&quot;&gt;model 1&lt;&#x2F;a&gt;, &lt;a href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;models&#x2F;nmt&#x2F;envi_model_2.zip&quot;&gt;model 2&lt;&#x2F;a&gt;).
我们用 BLEU 评分来评估翻译质量 &lt;a href=&quot;http:&#x2F;&#x2F;www.aclweb.org&#x2F;anthology&#x2F;P02-1040.pdf&quot;&gt;(Papineni et al., 2002)&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Systems&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;tst2012 (dev)&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;test2013 (test)&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;NMT (greedy)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;23.2&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;25.5&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;NMT (beam=10)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;23.8&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;26.1&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;http:&#x2F;&#x2F;stanford.edu&#x2F;~lmthang&#x2F;data&#x2F;papers&#x2F;iwslt15.pdf&quot;&gt;(Luong &amp;amp; Manning, 2015)&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;23.3&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;&lt;strong&gt;训练速度&lt;&#x2F;strong&gt;: 在 &lt;em&gt;K40m&lt;&#x2F;em&gt; 上 (0.37 秒每步 , 15.3K 字每秒)  &amp;amp; 在 &lt;em&gt;TitanX&lt;&#x2F;em&gt; 上 (0.17 秒每步, 32.2K 字每秒) .
这里，每步时间指运行一个小批量（128个）所需的时间。对于字每秒，我们统计的是源和目标上的单词。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wmt-de-yu-ying-yu&quot;&gt;WMT 德语-英语&lt;&#x2F;h3&gt;
&lt;p&gt;样本集: 4.5M examples, 训练集=newstest2013, 测试集=newstest2015
&lt;a href=&quot;&#x2F;blog&#x2F;nmt&#x2F;nmt&#x2F;scripts&#x2F;wmt16_en_de.sh&quot;&gt;下载脚本&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;训练细节&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;. 我们训练的超参数与英语-越南语的实验类似，除了以下细节. 采用 &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;rsennrich&#x2F;subword-nmt&quot;&gt;BPE&lt;&#x2F;a&gt;(32K 操作)将数据分成子字单元. 我们用双向编码器 1024（编码器有2个双向层）训练 1024 单元的 4 层 LSTM , embedding 维度为 1024. 我们训练了 359K 步 (10轮); 170K 步后, 我们开始每 17K 步减半学习率.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;结果&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;.
TODO(rzhao): 添加德语-英语模型的 URL.&lt;&#x2F;p&gt;
&lt;p&gt;前 2 行是模型 1、2 的评价结果(&lt;a href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;models&#x2F;nmt&#x2F;deen_model_1.zip&quot;&gt;model 1&lt;&#x2F;a&gt;,&lt;a href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;models&#x2F;nmt&#x2F;deen_model_2.zip&quot;&gt;model 2&lt;&#x2F;a&gt;).
第 4 行是运行在 4 个 GPU 上的 GNMT attention &lt;a href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;models&#x2F;nmt&#x2F;deen_gnmt_model_4_layer.zip&quot;&gt;模型&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Systems&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;newstest2013 (dev)&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;newstest2015&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;NMT (greedy)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;27.1&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;27.6&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;NMT (beam=10)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;28.0&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;28.9&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;NMT + GNMT attention (beam=10)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;29.0&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;29.9&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;http:&#x2F;&#x2F;matrix.statmt.org&#x2F;&quot;&gt;WMT SOTA&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;29.3&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;这些结果表明，我们的代码为 NMT 建立了强大的基线系统。
(请注意，WMT 系统通常会使用大量的单种语料数据，我们目前没有。)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;训练速度&lt;&#x2F;strong&gt;: 在 &lt;em&gt;Nvidia K40m&lt;&#x2F;em&gt; 上 (2.1 秒每步, 3.4K 字每秒)  &amp;amp; 在 &lt;em&gt;Nvidia TitanX&lt;&#x2F;em&gt; 上(0.7 秒每步, 8.7K 字每秒) 。
为看 GNMT attention 的速度提升效果, 我们仅在 &lt;em&gt;K40m&lt;&#x2F;em&gt; 上做测试:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Systems&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;1 gpu&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;4 gpus&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;8 gpus&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;NMT (4 layers)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;2.2s, 3.4K&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;1.9s, 3.9K&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;NMT (8 layers)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;3.5s, 2.0K&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;2.9s, 2.4K&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;NMT + GNMT attention (4 layers)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;2.6s, 2.8K&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;1.7s, 4.3K&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;NMT + GNMT attention (8 layers)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;4.2s, 1.7K&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;1.9s, 3.8K&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;这些结果表明，没有 GNMT attention，使用多 GPU 获得的效果很小。而使用 GNMT attention，我们从多 GPU 上获得了 50%-100% 的速度提升。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wmt-ying-yu-de-yu-wan-quan-bi-jiao&quot;&gt;WMT 英语-德语 — 完全比较&lt;&#x2F;h3&gt;
&lt;p&gt;前 2 行是 GNMT attention 模型: &lt;a href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;models&#x2F;nmt&#x2F;ende_gnmt_model_4_layer.zip&quot;&gt;model 1 (4 层)&lt;&#x2F;a&gt;,&lt;a href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;models&#x2F;nmt&#x2F;ende_gnmt_model_8_layer.zip&quot;&gt;model 2 (8 层)&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Systems&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;newstest2014&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;newstest2015&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;em&gt;Ours&lt;&#x2F;em&gt; — NMT + GNMT attention (4 layers)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;23.7&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;26.5&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;em&gt;Ours&lt;&#x2F;em&gt; — NMT + GNMT attention (8 layers)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;24.4&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;27.6&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;http:&#x2F;&#x2F;matrix.statmt.org&#x2F;&quot;&gt;WMT SOTA&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;20.6&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;24.9&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;OpenNMT &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1701.02810&quot;&gt;(Klein et al., 2017)&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;19.3&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;tf-seq2seq &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1703.03906&quot;&gt;(Britz et al., 2017)&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;22.2&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;25.2&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;GNMT &lt;a href=&quot;https:&#x2F;&#x2F;research.google.com&#x2F;pubs&#x2F;pub45610.html&quot;&gt;(Wu et al., 2016)&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;24.6&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;上面的结果表明，我们的模型在类似架构中有很强的竞争力。
注意，OpenNMT 使用较小的模型，而目前在 Transformer network &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762&quot;&gt;Vaswani et al., 2017&lt;&#x2F;a&gt;中获得的最佳结果为 28.4，不过这是明显不同的架构.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;qi-ta-zi-yuan&quot;&gt;其它资源&lt;&#x2F;h2&gt;
&lt;p&gt;为深入了解神经机器翻译和序列到序列模型，我们强烈推荐下面的材料
&lt;a href=&quot;https:&#x2F;&#x2F;sites.google.com&#x2F;site&#x2F;acl16nmt&#x2F;&quot;&gt;Luong, Cho, Manning, (2016)&lt;&#x2F;a&gt;;
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lmthang&#x2F;thesis&quot;&gt;Luong, (2016)&lt;&#x2F;a&gt;;
and &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1703.01619&quot;&gt;Neubig, (2017)&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;可用不同的工具构建 seq2seq 模型，我们每样选了一种:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Stanford NMT &lt;a href=&quot;https:&#x2F;&#x2F;nlp.stanford.edu&#x2F;projects&#x2F;nmt&#x2F;&quot;&gt;https:&#x2F;&#x2F;nlp.stanford.edu&#x2F;projects&#x2F;nmt&#x2F;&lt;&#x2F;a&gt;&lt;em&gt;[Matlab]&lt;&#x2F;em&gt;&lt;&#x2F;li&gt;
&lt;li&gt;tf-seq2seq &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;seq2seq&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;seq2seq&lt;&#x2F;a&gt;&lt;em&gt;[TensorFlow]&lt;&#x2F;em&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Nemantus &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;rsennrich&#x2F;nematus&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;rsennrich&#x2F;nematus&lt;&#x2F;a&gt;&lt;em&gt;[Theano]&lt;&#x2F;em&gt;&lt;&#x2F;li&gt;
&lt;li&gt;OpenNMT &lt;a href=&quot;http:&#x2F;&#x2F;opennmt.net&#x2F;&quot;&gt;http:&#x2F;&#x2F;opennmt.net&#x2F;&lt;&#x2F;a&gt; &lt;em&gt;[Torch]&lt;&#x2F;em&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;zhi-xie&quot;&gt;致谢&lt;&#x2F;h2&gt;
&lt;p&gt;我们要感谢 Denny Britz, Anna Goldie, Derek Murray, 和 Cinjon Resnick 为 TensorFlow 和 seq2seq 库带来的新特性. 还要感谢 Lukasz Kaiser 在 seq2seq 代码库上最初的帮助; Quoc Le 提议复现一个 GNMT; Yonghui Wu 和 Zhifeng Chen 负责 GNMT 系统的细节; 同时还要感谢谷歌大脑 （Google Brain ）团队的支持和反馈!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;can-kao&quot;&gt;参考&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015.&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1409.0473.pdf&quot;&gt; Neural machine translation by jointly learning to align and translate&lt;&#x2F;a&gt;. ICLR.&lt;&#x2F;li&gt;
&lt;li&gt;Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015.&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1508.04025.pdf&quot;&gt; Effective approaches to attention-based neural machine translation&lt;&#x2F;a&gt;. EMNLP.&lt;&#x2F;li&gt;
&lt;li&gt;Ilya Sutskever, Oriol Vinyals, and Quoc
V. Le. 2014.&lt;a href=&quot;https:&#x2F;&#x2F;papers.nips.cc&#x2F;paper&#x2F;5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt; Sequence to sequence learning with neural networks&lt;&#x2F;a&gt;. NIPS.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
