<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Freeopen</title><link href="https://freeopen.github.io/" rel="alternate"></link><link href="https://freeopen.github.io/feeds/all.atom.xml" rel="self"></link><id>https://freeopen.github.io/</id><updated>2018-05-08T00:00:00+08:00</updated><entry><title>概率图模型 HMM、MEMM、CRF</title><link href="https://freeopen.github.io/posts/hmm-memm-crf" rel="alternate"></link><published>2018-05-08T00:00:00+08:00</published><updated>2018-05-08T00:00:00+08:00</updated><author><name>Scofield</name></author><id>tag:freeopen.github.io,2018-05-08:/posts/hmm-memm-crf</id><summary type="html">&lt;p&gt;&lt;a href="https://www.zhihu.com/question/35866596/answer/236886066"&gt;转自知乎&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 此文不错，有些小错，顺手改了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="yi , preface"&gt;&lt;strong&gt;一、Preface&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;之前刚接触NLP时做相关的任务，也必然地涉及到了序列处理任务，然后自然要接触到概率图模型。当时在全网搜中文资料，陆续失望地发现竟然真的没有讲得清楚的博文，发现基本是把李航老师书里或CRF tutorial等资料的文字论述和公式抄来抄去的。当然，没有说别人讲的是错的，只是觉得，要是没有把东西说的让读者看得懂，那也是没意义啊。或者有些吧，就是讲了一大堆的东西，貌似也明白了啥，但还是不能让我很好的理解CRF这些模型究竟是个啥，完了还是有一头雾水散不开的感觉。试想，一堆公式扔过来，没有个感性理解的过渡，怎么可能理解的了。我甚至觉得，如果博客让人看不懂，那说明要么自己没理解透要么就是思维不清晰讲不清楚。所以默想，深水区攻坚还是要靠自己，然后去做调研做research，所以就写了个这个学习记录。&lt;/p&gt;
&lt;p&gt;所以概率图的研究学习思考列入了我的任务清单。不过平时的时间又非常的紧，只能陆陆续续的思考着，所以时间拖得也真是长啊。&lt;/p&gt;
&lt;p&gt;这是个学习笔记。相比其他的学习模型，概率图貌似确实是比较难以理解的。这里我基本全部用自己的理解加上自己的语言习惯表达出来，off the official form，表达尽量接地气。我会尽量将我所有理解过程中的每个关键小细节都详细描述出来，以使对零基础的初学者友好 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://www.zhihu.com/question/35866596/answer/236886066"&gt;转自知乎&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 此文不错，有些小错，顺手改了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="yi , preface"&gt;&lt;strong&gt;一、Preface&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;之前刚接触NLP时做相关的任务，也必然地涉及到了序列处理任务，然后自然要接触到概率图模型。当时在全网搜中文资料，陆续失望地发现竟然真的没有讲得清楚的博文，发现基本是把李航老师书里或CRF tutorial等资料的文字论述和公式抄来抄去的。当然，没有说别人讲的是错的，只是觉得，要是没有把东西说的让读者看得懂，那也是没意义啊。或者有些吧，就是讲了一大堆的东西，貌似也明白了啥，但还是不能让我很好的理解CRF这些模型究竟是个啥，完了还是有一头雾水散不开的感觉。试想，一堆公式扔过来，没有个感性理解的过渡，怎么可能理解的了。我甚至觉得，如果博客让人看不懂，那说明要么自己没理解透要么就是思维不清晰讲不清楚。所以默想，深水区攻坚还是要靠自己，然后去做调研做research，所以就写了个这个学习记录。&lt;/p&gt;
&lt;p&gt;所以概率图的研究学习思考列入了我的任务清单。不过平时的时间又非常的紧，只能陆陆续续的思考着，所以时间拖得也真是长啊。&lt;/p&gt;
&lt;p&gt;这是个学习笔记。相比其他的学习模型，概率图貌似确实是比较难以理解的。这里我基本全部用自己的理解加上自己的语言习惯表达出来，off the official form，表达尽量接地气。我会尽量将我所有理解过程中的每个关键小细节都详细描述出来，以使对零基础的初学者友好。包括理论的来龙去脉，抽象具象化，模型的构成，模型的训练过程，会注重类比的学习。&lt;/p&gt;
&lt;p&gt;根据现有资料，我是按照概率图模型将HMM，MEMM，CRF放在这里一起对比学习。之所以把他们拿在一起，是因为他们都用于标注问题。并且之所以放在概率图框架下，是完全因为自己top-down思维模式使然。另外，概率图下还有很多的模型，这儿只学习标注模型。&lt;/p&gt;
&lt;p&gt;正儿八经的，我对这些个概率图模型有了彻悟，是从我明白了生成式模型与判别式模型的那一刻。一直在思考从概率图模型角度讲他们的区别到底在哪。&lt;/p&gt;
&lt;p&gt;另外，篇幅略显长，但咱们不要急躁，好好看完这篇具有良好的上下文的笔记，那肯定是能理解的，或者就多看几遍。&lt;/p&gt;
&lt;p&gt;个人学习习惯就是，&lt;strong&gt;要尽可能地将一群没有结构的知识点融会贯通，再用一条树状结构的绳将之串起来，结构化，就是说要成体系，这样把绳子头一拎所有的东西都能拿起来&lt;/strong&gt;。学习嘛，应该要是一个熵减的过程，卓有成效的学习应该是混乱度越来越小！这个思维方式对我影响还是蛮大的。&lt;/p&gt;
&lt;p&gt;在正式内容之前，还是先要明确下面这一点，最好脑子里形成一个定势：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;统计机器学习所有的模型（个别instant model和优化算法以及其他的特种工程知识点除外）的工作流程都是如此：&lt;br/&gt;
a.训练模型参数，得到模型（由参数唯一确定），&lt;br/&gt;
b.预测给定的测试数据。&lt;br/&gt;
拿这个流程去挨个学习模型，思路上会非常顺畅。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;除此之外，对初学者的关于机器学习的入门学习方式也顺带表达一下(empirical speaking)：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a.完整特征工程竞赛&lt;br/&gt;
b.野博客理论入门理解&lt;br/&gt;
c.再回到代码深入理解模型内部&lt;br/&gt;
d.再跨理论，查阅经典理论巨作。这时感性理性都有一定高度，会遇到很多很大的理解上的疑惑，这时3大经典可能就可以发挥到最大作用了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;很多beginer，就比如说学CRF模型，然后一上来就摆一套复杂的公式，什么我就问，这能理解的了吗？这是正确的开启姿势吗？当然了，也要怪那些博主，直接整一大堆核心公式，实际上读者的理解门槛可能就是一个过渡性的细枝末节而已。没有上下文的教育肯定是失败的（这一点我又想吐槽国内绝大部分本科的院校教育模式）。所以说带有完整上下文信息以及过程来龙去脉交代清楚才算到位吧。&lt;/p&gt;
&lt;p&gt;而不是一上来就死啃被人推荐的&amp;ldquo;经典资料&amp;rdquo;，这一点相信部分同学会理解。好比以前本科零基础学c++ JAVA，上来就看primr TIJ，结果浪费了时间精力一直在门外兜圈。总结方法吸取教训，应该快速上手代码，才是最高效的。经典最好是用来查阅的工具书，我目前是李航周志华和经典的那3本迭代轮询看了好多轮，经常会反复查询某些model或理论的来龙去脉；有时候要查很多相关的东西，看这些书还是难以贯通，然后发现有些人的博客写的会更容易去理解。所以另外，学习资料渠道也要充分才行。&lt;/p&gt;
&lt;p&gt;最后提示一下，&lt;strong&gt;请务必按照标题层级结构和目录一级一级阅读，防止跟丢。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="er , prerequisite"&gt;&lt;strong&gt;二、Prerequisite&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id="2.1 gai lu tu"&gt;&lt;strong&gt;2.1 概率图&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;之前刚接触CRF时，一上来试图越过一堆繁琐的概率图相关概念，不过sad to say, 这是后面的前驱知识，后面还得反过来补这个点。所以若想整体把握，系统地拿下这一块，应该还是要越过这块门槛的。&lt;/p&gt;
&lt;p&gt;当然了，一开始只需略略快速看一篇，后面可再返过来补查。&lt;/p&gt;
&lt;h3 id="2.1.1 gai lan"&gt;&lt;strong&gt;2.1.1 概览&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构（个人非常喜欢这种类型的图）：  &lt;/p&gt;
&lt;p&gt;&lt;img data-caption="" data-rawheight="336" data-rawwidth="631" data-size="normal" src="https://freeopen.github.io/images/v2.jpg" width="631"/&gt;&lt;/p&gt;
&lt;p&gt;在概率图模型中，数据(样本)由公式 &lt;span class="math"&gt;\(G=(V,E)\)&lt;/span&gt; 建模表示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(V\)&lt;/span&gt; 表示节点，即随机变量（放在此处的，可以是一个token或者一个label），具体地，用 &lt;span class="math"&gt;\(Y = (y_1, \cdots, y_n)\)&lt;/span&gt; 为随机变量建模，注意 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 现在是代表了一批随机变量（想象对应一条sequence，包含了很多的token）， &lt;span class="math"&gt;\(P(Y)\)&lt;/span&gt; 为这些随机变量的分布；  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(E\)&lt;/span&gt; 表示边，即概率依赖关系。具体咋理解，还是要在后面结合HMM或CRF的graph具体解释。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="2.1.2 you xiang tu  vs. wu xiang tu"&gt;&lt;strong&gt;2.1.2 有向图 vs. 无向图&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;上图可以看到，贝叶斯网络（信念网络）都是有向的，马尔科夫网络无向。所以，贝叶斯网络适合为有单向依赖的数据建模，马尔科夫网络适合实体之间互相依赖的建模。具体地，他们的核心差异表现在如何求 &lt;span class="math"&gt;\(P=(Y)\)&lt;/span&gt; ，即怎么表示 &lt;span class="math"&gt;\(Y=(y_1,\cdots,y_n)\)&lt;/span&gt; 这个的联合概率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 有向图&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于有向图模型，这么求联合概率： &lt;span class="math"&gt;\(P(x_1, \cdots, x_n )=\prod_{i=0}P(x_i | \pi(x_{i}))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;举个例子，对于下面的这个有向图的随机变量(注意，这个图我画的还是比较广义的)：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/2.jpg" width="453"&gt;
&lt;br/&gt;
&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;应该这样表示他们的联合概率:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(x_1, \cdots, x_n )=P(x_1)&amp;middot;P(x_2|x_1 )&amp;middot;P(x_3|x_2 )&amp;middot;P(x_4|x_2 )&amp;middot;P(x_5|x_3,x_4 )\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;应该很好理解吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 无向图&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于无向图，我看资料一般就指马尔科夫网络(注意，这个图我画的也是比较广义的)。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/3.jpg" width="260"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;如果一个graph太大，可以用因子分解将 &lt;img alt="P=(Y)" src="https://www.zhihu.com/equation?tex=P%3D%28Y%29"/&gt; 写为若干个联合概率的乘积。咋分解呢，将一个图分为若干个&amp;ldquo;小团&amp;rdquo;，注意每个团必须是&amp;ldquo;最大团&amp;rdquo;（就是里面任何两个点连在了一块，具体&amp;hellip;&amp;hellip;算了不解释，有点&amp;ldquo;最大连通子图&amp;rdquo;的感觉），则有：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Y )=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c} )\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;其中, &lt;span class="math"&gt;\(Z(x) = \sum_{Y} \prod_{c}\psi_{c}(Y_{c} )\)&lt;/span&gt;，公式应该不难理解吧，归一化是为了让结果算作概率。&lt;/p&gt;
&lt;p&gt;所以像上面的无向图：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Y )=\frac{1}{Z(x)} ( \psi_{1}(X_{1}, X_{3}, X_{4} ) &amp;middot; \psi_{2}(X_{2}, X_{3}, X_{4} ) )\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中， &lt;span class="math"&gt;\(\psi_{c}(Y_{c} )\)&lt;/span&gt; 是一个最大团 &lt;img alt="C" src="https://www.zhihu.com/equation?tex=C"/&gt; 上随机变量们的联合概率，一般取指数函数的：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\psi_{c}(Y_{c} ) = e^{-E(Y_{c})} =e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)}\)&lt;/span&gt;
好了，管这个东西叫做&lt;code&gt;势函数&lt;/code&gt;。注意 &lt;span class="math"&gt;\(e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)}\)&lt;/span&gt; 是否有看到CRF的影子。&lt;/p&gt;
&lt;p&gt;那么概率无向图的联合概率分布可以在因子分解下表示为：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Y )=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c} ) = \frac{1}{Z(x)} \prod_{c} e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)} = \frac{1}{Z(x)} e^{\sum_{c}\sum_{k}\lambda_{k}f_{k}(y_{i},y_{i-1},x,i)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意，这里的理解还蛮重要的，注意递推过程，敲黑板，这是CRF的开端！&lt;br/&gt;
这个由&lt;code&gt;Hammersly-Clifford law&lt;/code&gt;保证，具体不展开。&lt;/p&gt;
&lt;h3 id="2.1.3 ma er ke fu jia she &amp;amp;ma er ke fu xing"&gt;&lt;strong&gt;2.1.3 马尔科夫假设&amp;amp;马尔科夫性&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;这个也属于前馈知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 马尔科夫假设&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;额应该是齐次马尔科夫假设，这样假设：马尔科夫链 &lt;span class="math"&gt;\((x_{1},\cdots,x_{n})\)&lt;/span&gt; 里的 &lt;span class="math"&gt;\(x_{i}\)&lt;/span&gt; 总是只受 &lt;span class="math"&gt;\(x_{i-1}\)&lt;/span&gt; 一个人的影响。&lt;br/&gt;
马尔科夫假设这里相当于就是个1-gram。&lt;/p&gt;
&lt;p&gt;马尔科夫过程呢？即，在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。最简单的马尔科夫过程是一阶的，即只依赖于前一个状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 马尔科夫性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;马尔科夫性是是保证或者判断概率图是否为概率无向图的条件。&lt;/p&gt;
&lt;p&gt;三点内容：a. 成对，b. 局部，c. 全局。&lt;/p&gt;
&lt;p&gt;我觉得这个不用展开。&lt;/p&gt;
&lt;h3 id="2.2 pan bie shi (discriminative)mo xing  vs. sheng cheng shi (generative)mo xing"&gt;&lt;strong&gt;2.2 判别式(discriminative)模型 vs. 生成式(generative)模型&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在监督学习下，模型可以分为判别式模型与生成式模型。&lt;/p&gt;
&lt;p&gt;重点来了。上面有提到，我理解了HMM、CRF模型的区别是从理解了判别式模型与生成式模型的那刻，并且瞬间对其他的模型有一个恍然大悟。我记得是一年前就开始纠结这两者的区别，但我只能说，栽在了一些烂博客上，大部分都没有自己的insightful理解，也就是一顿官话，也真是难以理解。后来在知乎上一直琢磨别人的答案，然后某日早晨终于豁然开朗，就是这种感觉。&lt;/p&gt;
&lt;p&gt;好了，我要用自己的理解来转述两者的区别了below。&lt;/p&gt;
&lt;p&gt;先问个问题，根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT&amp;hellip;&amp;hellip;）与B批模型（NB、LDA&amp;hellip;&amp;hellip;），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。  &lt;/p&gt;
&lt;p&gt;2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; (注意 &lt;span class="math"&gt;\(X\)&lt;/span&gt; 包含所有的特征 &lt;span class="math"&gt;\(x_{i}\)&lt;/span&gt; ， &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; ，再结合新样本给的 &lt;span class="math"&gt;\(X\)&lt;/span&gt; ，通过条件概率就能出来 &lt;span class="math"&gt;\(Y\)&lt;/span&gt;：&lt;br/&gt;
&lt;span class="math"&gt;\(P(Y|X) = \frac{P(X,Y)}{P(X)}\)&lt;/span&gt;
好了，应该说清楚了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;1. 判别式模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：&lt;strong&gt;判别模型是直接对&lt;/strong&gt; &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; &lt;strong&gt;建模&lt;/strong&gt;，就是说，直接根据X特征来对Y建模训练。&lt;/p&gt;
&lt;p&gt;具体地，我的训练过程是确定构件 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; 模型里面&amp;ldquo;复杂映射关系&amp;rdquo;中的参数，完了再去inference一批新的sample。&lt;/p&gt;
&lt;p&gt;所以判别式模型的特征总结如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; 建模&lt;/li&gt;
&lt;li&gt;对所有的样本只构建一个模型，确认总体判别边界&lt;/li&gt;
&lt;li&gt;观测到输入什么特征，就预测最可能的label&lt;/li&gt;
&lt;li&gt;另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;2. 生成式模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; ，也就是说，&lt;strong&gt;我在训练阶段是只对&lt;/strong&gt; &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt;&lt;strong&gt;建模&lt;/strong&gt;，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt;，导出 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; ,但这已经不属于建模阶段了。&lt;/p&gt;
&lt;p&gt;结合NB过一遍生成式模型的工作流程。学习阶段，建模： &lt;span class="math"&gt;\(P(X,Y)=P(X|Y)P(Y)\)&lt;/span&gt; （当然，NB具体流程去隔壁参考）,然后 &lt;span class="math"&gt;\(P(Y|X) = \frac{P(X,Y)}{P(X)}\)&lt;/span&gt; 。&lt;br/&gt;
另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。&lt;/p&gt;
&lt;p&gt;所以生成式总结下有如下特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; 建模&lt;/li&gt;
&lt;li&gt;这里我们主要讲分类问题，所以是要对每个label(&lt;span class="math"&gt;\(y_{i}\)&lt;/span&gt;) 都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）&lt;/li&gt;
&lt;li&gt;中间生成联合分布，并可生成采样数据。&lt;/li&gt;
&lt;li&gt;生成式模型的优点在于，所包含的信息非常齐全，我称之为&amp;ldquo;上帝信息&amp;rdquo;，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。&lt;br/&gt;
最后identity the picture below:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/4.jpg" width="80%"&gt;
&lt;br/&gt;
&lt;/img&gt;&lt;/p&gt;
&lt;h3 id="2.3 xu lie jian mo"&gt;&lt;strong&gt;2.3 序列建模&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;为了号召零门槛理解，现在解释如何为序列问题建模。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/5.jpg" width="80%"&gt;
&lt;br/&gt;
&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;序列包括时间序列以及general sequence，但两者无异。连续的序列在分析时也会先离散化处理。常见的序列有如：时序数据、本文句子、语音数据、等等。&lt;/p&gt;
&lt;p&gt;广义下的序列有这些特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点之间有关联依赖性/无关联依赖性&lt;/li&gt;
&lt;li&gt;序列的节点是随机的/确定的&lt;/li&gt;
&lt;li&gt;序列是线性变化/非线性的&lt;/li&gt;
&lt;li&gt;&amp;hellip;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对不同的序列有不同的问题需求，常见的序列建模方法总结有如下：&lt;/p&gt;
&lt;p&gt;1. 拟合，预测未来节点（或走势分析）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a. 常规序列建模方法：AR、MA、ARMA、ARIMA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;b. 回归拟合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;c. Neural Networks&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2. 判断不同序列类别，即分类问题：HMM、CRF、General Classifier（ML models、NN models）&lt;/p&gt;
&lt;p&gt;3. 不同时序对应的状态的分析，即序列标注问题：HMM、CRF、RecurrentNNs&lt;/p&gt;
&lt;p&gt;在本篇文字中，我们只关注在2. &amp;amp; 3.类问题下的建模过程和方法。&lt;/p&gt;
&lt;h2 id="san , hmm_1"&gt;&lt;strong&gt;三、HMM&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;最早接触的是HMM。较早做过一个项目，关于声波手势识别，跟声音识别的机制一样，使用的正是HMM的一套方法。后来又用到了 &lt;em&gt;kalman filter&lt;/em&gt;，之后做序列标注任务接触到了CRF，所以整个概率图模型还是接触的方面还蛮多。&lt;/p&gt;
&lt;h3 id="3.1 li jie hmm"&gt;&lt;strong&gt;3.1 理解HMM&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在2.2、2.3中提序列的建模问题时，我们只是讨论了常规的序列数据，e.g., &lt;span class="math"&gt;\((X_{1},\cdots,X_{n})\)&lt;/span&gt; ,像2.3的图片那样。像这种序列一般用马尔科夫模型就可以胜任。实际上我们碰到的更多的使用HMM的场景是每个节点 &lt;span class="math"&gt;\(X_{i}\)&lt;/span&gt; 下还附带着另一个节点 &lt;span class="math"&gt;\(Y_{i}\)&lt;/span&gt; ，正所谓&lt;strong&gt;隐含&lt;/strong&gt;马尔科夫模型，那么除了正常的节点，还要将&lt;strong&gt;隐含状态节点&lt;/strong&gt;也得建模进去。正儿八经地，将 &lt;span class="math"&gt;\(X_{i} 、 Y_{i}\)&lt;/span&gt; 换成 &lt;span class="math"&gt;\(i_{i} 、o_{i}\)&lt;/span&gt; ,并且他们的名称变为状态节点、观测节点。状态节点正是我的隐状态。&lt;/p&gt;
&lt;p&gt;HMM属于典型的生成式模型。对照2.1的讲解，应该是要从训练数据中学到数据的各种分布，那么有哪些分布呢以及是什么呢？直接正面回答的话，正是&lt;strong&gt;HMM的5要素&lt;/strong&gt;，其中有3个就是整个数据的不同角度的概率分布：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt; ，隐藏状态集 &lt;span class="math"&gt;\(N = \lbrace q_{1}, \cdots, q_{N} \rbrace\)&lt;/span&gt; , 我的隐藏节点不能随意取，只能限定取包含在隐藏状态集中的符号。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M\)&lt;/span&gt;，观测集 &lt;span class="math"&gt;\(M = \lbrace v_{1}, \cdots, v_{M} \rbrace\)&lt;/span&gt; , 同样我的观测节点不能随意取，只能限定取包含在观测状态集中的符号。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A\)&lt;/span&gt; ，状态转移概率矩阵，这个就是其中一个概率分布。他是个矩阵， &lt;span class="math"&gt;\(A= [a_{ij}]_{N \times N}\)&lt;/span&gt; （N为隐藏状态集元素个数），其中 &lt;span class="math"&gt;\(a_{ij} = P(i_{t+1}|i_{t})， i_{t}\)&lt;/span&gt; 即第i个隐状态节点,即所谓的状态转移嘛。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(B\)&lt;/span&gt; ，观测概率矩阵，这个就是另一个概率分布。他是个矩阵， &lt;span class="math"&gt;\(B = [b_{ij}]_{N \times M}\)&lt;/span&gt; （&lt;span class="math"&gt;\(N\)&lt;/span&gt;为隐藏状态集元素个数，&lt;span class="math"&gt;\(M\)&lt;/span&gt;为观测集元素个数），其中 &lt;span class="math"&gt;\(b_{ij} = P(o_{t}|i_{t})， o_{t}\)&lt;/span&gt; 即第&lt;span class="math"&gt;\(i\)&lt;/span&gt;个观测节点，&lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 即第&lt;span class="math"&gt;\(i\)&lt;/span&gt;个隐状态节点，即所谓的观测概率（发射概率）嘛。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(&amp;pi;\)&lt;/span&gt; ，指模型在初始时刻各状态(来自状态集&lt;span class="math"&gt;\(N\)&lt;/span&gt;)出现的概率。通常，第一个隐状态节点 &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt;的隐状态可由EM方法学得,故&lt;span class="math"&gt;\(&amp;pi;\)&lt;/span&gt;在初始化时可随机给定。(&lt;em&gt;这里原句读不通，由freeopenn修订&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以图看起来是这样的：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/6.jpg" width="415"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;看的很清楚，我的模型先去学习要确定以上5要素，之后在inference阶段的工作流程是：首先，隐状态节点 &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 是不能直接观测到的数据节点， &lt;span class="math"&gt;\(o_{t}\)&lt;/span&gt; 才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系， &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 在A的指导下生成下一个隐状态节点 &lt;span class="math"&gt;\(i_{t+1}\)&lt;/span&gt; ，并且 &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 在 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 的指导下生成依赖于该 &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 的观测节点 &lt;span class="math"&gt;\(o_{t}\)&lt;/span&gt; , 并且我只能观测到序列 &lt;span class="math"&gt;\((o_{1}, \cdots, o_{i})\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;好，举例子说明（序列标注问题，POS，标注集BES）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;input: "学习出一个模型，然后再预测出一条指定"  &lt;/p&gt;
&lt;p&gt;expected output: 学/B 习/E 出/S 一/B 个/E 模/B 型/E ，/S 然/B 后/E 再/E 预/B 测/E &amp;hellip;&amp;hellip;  &lt;/p&gt;
&lt;p&gt;其中，input里面所有的char构成的字表，形成观测集 &lt;span class="math"&gt;\(M\)&lt;/span&gt; ，因为字序列在inference阶段是我所能看见的；标注集BES构成隐藏状态集 &lt;span class="math"&gt;\(N\)&lt;/span&gt; ，这是我无法直接获取的，也是我的预测任务；至于 &lt;span class="math"&gt;\(A、B、&amp;pi;\)&lt;/span&gt; ，这些概率分布信息（上帝信息）都是我在学习过程中所确定的参数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;然后一般初次接触的话会疑问：为什么要这样？&amp;hellip;&amp;hellip;好吧，就应该是这样啊，根据具有同时带着隐藏状态节点和观测节点的类型的序列，在HMM下就是这样子建模的。&lt;/p&gt;
&lt;p&gt;下面来点高层次的理解：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模 &lt;span class="math"&gt;\(P(O,I) = \sum_{t=1}^{T}P(I_{t} | I_{t-1})P(O_{t} | I_{t})\)&lt;/span&gt; (注意，这个公式不在模型运行的任何阶段能体现出来，只是我们都去这么来表示HMM是个生成式模型，他的联合概率 &lt;span class="math"&gt;\(P(O,I)\)&lt;/span&gt; 就是这么计算的)。&lt;/li&gt;
&lt;li&gt;并且B中 &lt;span class="math"&gt;\(b_{ij} = P(o_{t}|i_{t})\)&lt;/span&gt; ，这意味着o对i有依赖性。&lt;/li&gt;
&lt;li&gt;在A中， &lt;span class="math"&gt;\(a_{ij} = P(i_{t+1}|i_{t})\)&lt;/span&gt; ，也就是说只遵循了一阶马尔科夫假设，1-gram。试想，如果数据的依赖超过1-gram，那肯定HMM肯定是考虑不进去的。这一点限制了HMM的性能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="3.2 mo xing yun xing guo cheng"&gt;&lt;strong&gt;3.2 模型运行过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;模型的运行过程（工作流程）对应了HMM的3个问题。&lt;/p&gt;
&lt;h3 id="3.2.1 xue xi xun lian guo cheng"&gt;&lt;strong&gt;3.2.1 学习训练过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;对照2.1的讲解，HMM学习训练的过程，就是找出数据的分布情况，也就是模型参数的确定。&lt;/p&gt;
&lt;p&gt;主要学习算法按照训练数据除了观测状态序列 &lt;span class="math"&gt;\((o_{1}, \cdots, o_{i})\)&lt;/span&gt; 是否还有隐状态序列 &lt;span class="math"&gt;\((i_{1}, \cdots, i_{i})\)&lt;/span&gt; 分为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;极大似然估计, with 隐状态序列&lt;/li&gt;
&lt;li&gt;Baum-Welch(前向后向), without 隐状态序列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感觉不用做很多的介绍，都是很实实在在的算法，看懂了就能理解。简要提一下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 极大似然估计&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的。所以极大似然估计法是非常常用的学习算法，我见过的很多代码里面也是这么计算的。比较简单。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step1. 算A&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$\hat{a_{ij}} = \frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;step2. 算B&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$\hat{b_{j}}(k) = \frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;step3. 直接估计 &lt;span class="math"&gt;\(&amp;pi;\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;比如说，在代码里计算完了就是这样的：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/7.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/8.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/9.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Baum-Welch(前向后向)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;就是一个EM的过程，如果你对EM的工作流程有经验的话，对这个Baum-Welch一看就懂。EM的过程就是初始化一套值，然后迭代计算，根据结果再调整值，再迭代，最后收敛&amp;hellip;&amp;hellip;好吧，这个理解是没有捷径的，去隔壁钻研EM吧。&lt;/p&gt;
&lt;p&gt;这里只提一下核心。因为我们手里没有隐状态序列 &lt;span class="math"&gt;\((i_{1}, \cdots, i_{i})\)&lt;/span&gt; 信息，所以我先必须给初值 &lt;span class="math"&gt;\(a_{ij}^{0}, b_{j}(k)^{0}, \pi^{0}\)&lt;/span&gt; ，初步确定模型，然后再迭代计算出 &lt;span class="math"&gt;\(a_{ij}^{n}, b_{j}(k)^{n}, \pi^{n}\)&lt;/span&gt; ,中间计算过程会用到给出的观测状态序列 &lt;span class="math"&gt;\((o_{1}, \cdots, o_{i})\)&lt;/span&gt;。另外，收敛性由EM的XXX定理保证。&lt;/p&gt;
&lt;h3 id="3.2.2 xu lie biao zhu (jie ma )guo cheng"&gt;&lt;strong&gt;3.2.2 序列标注（解码）过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;好了，学习完了HMM的分布参数，也就确定了一个HMM模型。需要注意的是，这个HMM是对我这一批全部的数据进行训练所得到的参数。&lt;/p&gt;
&lt;p&gt;序列标注问题也就是&amp;ldquo;预测过程&amp;rdquo;，通常称为解码过程。对应了序列建模问题3.。对于序列标注问题，我们只需要学习出一个HMM模型即可，后面所有的新的sample我都用这一个HMM去apply。&lt;/p&gt;
&lt;p&gt;我们的目的是，在学习后已知了 &lt;span class="math"&gt;\(P(Q,O)\)&lt;/span&gt; ,现在要求出 &lt;span class="math"&gt;\(P(Q|O)\)&lt;/span&gt; ，进一步&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Q_{max} = argmax_{allQ}\frac{P(Q,O)}{P(O)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再直白点就是，我现在要在给定的观测序列下找出一条隐状态序列，条件是这个隐状态序列的概率是最大的那个。&lt;/p&gt;
&lt;p&gt;具体地，都是用Viterbi算法解码，是用DP思想减少重复的计算。Viterbi也是满大街的，不过要说的是，Viterbi不是HMM的专属，也不是任何模型的专属，他只是恰好被满足了被HMM用来使用的条件。谁知，现在大家都把Viterbi跟HMM捆绑在一起了, shame。&lt;/p&gt;
&lt;p&gt;Viterbi计算有向无环图的一条最大路径，应该还好理解。如图：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/10.jpg" width="418"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;关键是注意，每次工作热点区只涉及到t 与 t-1,这对应了DP的无后效性的条件。如果对某些同学还是很难理解，请参考&lt;a href="https://www.zhihu.com/question/20136144"&gt;这个答案&lt;/a&gt;下@Kiwee的回答吧。&lt;/p&gt;
&lt;h3 id="3.2.3 xu lie gai lu guo cheng"&gt;&lt;strong&gt;3.2.3 序列概率过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;我通过HMM计算出序列的概率又有什么用？针对这个点我把这个问题详细说一下。&lt;/p&gt;
&lt;p&gt;实际上，序列概率过程对应了序列建模问题2.，即序列分类。&lt;br/&gt;
在3.2.2第一句话我说，在序列标注问题中，我用一批完整的数据训练出了一支HMM模型即可。好，那在序列分类问题就不是训练一个HMM模型了。我应该这么做（结合语音分类识别例子）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目标：识别声音是A发出的还是B发出的。&lt;br/&gt;
HMM建模过程：&lt;br/&gt;
1. 训练：我将所有A说的语音数据作为dataset_A,将所有B说的语音数据作为dataset_B（当然，先要分别对dataset A ,B做预处理encode为元数据节点，形成sequences）,然后分别用dataset_A、dataset_B去训练出HMM_A/HMM_B&lt;br/&gt;
2. inference：来了一条新的sample（sequence），我不知道是A的还是B的，没问题，分别用HMM_A/HMM_B计算一遍序列的概率得到 &lt;span class="math"&gt;\(P_{A}(S)、P_{B}(S)\)&lt;/span&gt; ，比较两者大小，哪个概率大说明哪个更合理，更大概率作为目标类别。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以，本小节的理解重点在于，&lt;strong&gt;如何对一条序列计算其整体的概率&lt;/strong&gt;。即目标是计算出 &lt;span class="math"&gt;\(P(O|&amp;lambda;)\)&lt;/span&gt; 。这个问题前辈们在他们的经典中说的非常好了，比如参考李航老师整理的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接计算法（穷举搜索）&lt;/li&gt;
&lt;li&gt;前向算法&lt;/li&gt;
&lt;li&gt;后向算法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后面两个算法采用了DP思想，减少计算量，即每一次直接引用前一个时刻的计算结果以避免重复计算，跟Viterbi一样的技巧。&lt;/p&gt;
&lt;p&gt;还是那句，因为这篇文档不是专门讲算法细节的，所以不详细展开这些。毕竟，所有的科普HMM、CRF的博客貌似都是在扯这些算法，妥妥的街货，就不搬运了。&lt;/p&gt;
&lt;h2 id="si , memm_1"&gt;&lt;strong&gt;四、MEMM&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;MEMM，即最大熵马尔科夫模型，这个是在接触了HMM、CRF之后才知道的一个模型。说到MEMM这一节时，得转换思维了，因为现在这MEMM属于判别式模型。&lt;/p&gt;
&lt;p&gt;不过有一点很尴尬，MEMM貌似被使用或者讲解引用的不及HMM、CRF。&lt;/p&gt;
&lt;h3 id="4.1 li jie memm"&gt;&lt;strong&gt;4.1 理解MEMM&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;这里还是啰嗦强调一下，MEMM正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，比如说序列求概率（分类）问题，我直接考虑找出函数分类边界。这一点跟HMM的思维方式发生了很大的变化，如果不对这一点有意识，那么很难理解为什么MEMM、CRF要这么做。&lt;/p&gt;
&lt;p&gt;HMM中，观测节点 &lt;span class="math"&gt;\(o_{i}\)&lt;/span&gt; 依赖隐藏状态节点 &lt;span class="math"&gt;\(i_{i}\)&lt;/span&gt; ,也就意味着我的观测节点只依赖当前时刻的隐藏状态。但在更多的实际场景下，观测序列是需要很多的特征来刻画的，比如说，我在做NER时，我的标注 &lt;span class="math"&gt;\(i_{i}\)&lt;/span&gt; 不仅跟当前状态 &lt;span class="math"&gt;\(o_{i}\)&lt;/span&gt; 相关，而且还跟前后标注 &lt;span class="math"&gt;\(o_{j}(j \neq i)\)&lt;/span&gt; 相关，比如字母大小写、词性等等。&lt;/p&gt;
&lt;p&gt;为此，提出来的MEMM模型就是能够直接允许&lt;strong&gt;&amp;ldquo;定义特征&amp;rdquo;&lt;/strong&gt;，直接学习条件概率，即 &lt;span class="math"&gt;\(P(i_{i}|i_{i-1},o_{i}) (i = 1,\cdots,n)\)&lt;/span&gt; , 总体为：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(I|O) = \prod_{t=1}^{n}P(i_{i}|i_{i-1},o_{i}), i = 1,\cdots,n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;并且， &lt;span class="math"&gt;\(P(i|i^{'},o)\)&lt;/span&gt; 这个概率通过最大熵分类器建模（取名MEMM的原因）:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(i|i^{'},o) = \frac{1}{Z(o,i^{'})} exp(\sum_{a})\lambda_{a}f_{a}(o,i)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;重点来了，这是ME的内容，也是理解MEMM的关键： &lt;span class="math"&gt;\(Z(o,i^{'})\)&lt;/span&gt; 这部分是归一化； &lt;span class="math"&gt;\(f_{a}(o,i)\)&lt;/span&gt; 是&lt;strong&gt;特征函数&lt;/strong&gt;，具体点，这个函数是需要去定义的; &lt;span class="math"&gt;\(&amp;lambda;\)&lt;/span&gt; 是特征函数的权重，这是个未知参数，需要从训练阶段学习而得。&lt;/p&gt;
&lt;p&gt;比如我可以这么定义特征函数：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation} f_{a}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定条件}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;/div&gt;
&lt;p&gt;其中，特征函数 &lt;span class="math"&gt;\(f_{a}(o,i)\)&lt;/span&gt; 个数可任意制定， &lt;span class="math"&gt;\((a = 1, \cdots, n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以总体上，MEMM的建模公式这样：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(I|O) = \prod_{t=1}^{n}\frac{ exp(\sum_{a})\lambda_{a}f_{a}(o,i) }{Z(o,i_{i-1})} , i = 1,\cdots,n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;是的，公式这部分之所以长成这样，是由ME模型决定的。&lt;/p&gt;
&lt;p&gt;请务必注意，理解&lt;strong&gt;判别模型&lt;/strong&gt;和&lt;strong&gt;定义特征&lt;/strong&gt;两部分含义，这已经涉及到CRF的雏形了。&lt;/p&gt;
&lt;p&gt;所以说，他是判别式模型，直接对条件概率建模。 上图：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/11.jpg" width="415"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;MEMM需要两点注意：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;与HMM的 &lt;span class="math"&gt;\(o_{i}\)&lt;/span&gt; 依赖 &lt;span class="math"&gt;\(i_{i}\)&lt;/span&gt; 不一样，MEMM当前隐藏状态 &lt;span class="math"&gt;\(i_{i}\)&lt;/span&gt; 应该是依赖当前时刻的观测节点 &lt;span class="math"&gt;\(o_{i}\)&lt;/span&gt; 和上一时刻的隐藏节点 &lt;span class="math"&gt;\(i_{i-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;需要注意，之所以图的箭头这么画，是由MEMM的公式决定的，而公式是creator定义出来的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;好了，走一遍完整流程。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;step1. 先预定义特征函数 &lt;span class="math"&gt;\(f_{a}(o,i)\)&lt;/span&gt; ，&lt;br/&gt;
step2. 在给定的数据上，训练模型，确定参数，即确定了MEMM模型&lt;br/&gt;
step3. 用确定的模型做序列标注问题或者序列求概率问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="4.2 mo xing yun xing guo cheng"&gt;&lt;strong&gt;4.2 模型运行过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;MEMM模型的工作流程也包括了学习训练问题、序列标注问题、序列求概率问题。&lt;/p&gt;
&lt;h3 id="4.2.1 xue xi xun lian guo cheng"&gt;&lt;strong&gt;4.2.1 学习训练过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;一套MEMM由一套参数唯一确定，同样地，我需要通过训练数据学习这些参数。MEMM模型很自然需要学习里面的特征权重&amp;lambda;。&lt;/p&gt;
&lt;p&gt;不过跟HMM不用的是，因为HMM是生成式模型，参数即为各种概率分布元参数，数据量足够可以用最大似然估计。而判别式模型是用函数直接判别，学习边界，MEMM即通过特征函数来界定。但同样，MEMM也有极大似然估计方法、梯度下降、牛顿迭代发、拟牛顿下降、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。&lt;/p&gt;
&lt;p&gt;嗯，具体详细求解过程貌似问题不大。&lt;/p&gt;
&lt;h3 id="4.2.2 xu lie biao zhu guo cheng"&gt;&lt;strong&gt;4.2.2 序列标注过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;还是跟HMM一样的，用学习好的MEMM模型，在新的sample（观测序列 &lt;span class="math"&gt;\(o_{1}, \cdots, o_{i}\)&lt;/span&gt; ）上找出一条概率最大最可能的隐状态序列 &lt;span class="math"&gt;\(i_{1}, \cdots, i_{i}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。&lt;/p&gt;
&lt;h3 id="4.2.3 xu lie qiu gai lu guo cheng"&gt;&lt;strong&gt;4.2.3 序列求概率过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的MEMM，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。&lt;/p&gt;
&lt;p&gt;应该可以不用展开，吧&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;h3 id="4.3 biao zhu pian zhi ?"&gt;&lt;strong&gt;4.3 标注偏置？&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;MEMM讨论的最多的是他的labeling bias 问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 现象&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;是从街货上烤过来的&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/12.jpg" width="558"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;用Viterbi算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2。 解码过程细节（需要会viterbi算法这个前提）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;P(1-&amp;gt; 1-&amp;gt; 1-&amp;gt; 1)= 0.4 x 0.45 x 0.5 = 0.09 ，&lt;br/&gt;
P(2-&amp;gt;2-&amp;gt;2-&amp;gt;2)= 0.2 X 0.3 X 0.3 = 0.018，&lt;br/&gt;
P(1-&amp;gt;2-&amp;gt;1-&amp;gt;2)= 0.6 X 0.2 X 0.5 = 0.06，&lt;br/&gt;
P(1-&amp;gt;1-&amp;gt;2-&amp;gt;2)= 0.4 X 0.55 X 0.3 = 0.066&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但是得到的最优的状态转换路径是1-&amp;gt;1-&amp;gt;1-&amp;gt;1，为什么呢？因为状态2可以转换的状态比状态1要多，从而使转移概率降低,即MEMM倾向于选择拥有更少转移的状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 解释原因&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;直接看MEMM公式：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(I|O) = \prod_{t=1}^{n}\frac{ exp[(\sum_{a})\lambda_{a}f_{a}(o,i)] }{Z(o,i_{i-1})} , i = 1,\cdots,n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(&amp;sum;\)&lt;/span&gt; 求和的作用在概率中是归一化，但是这里归一化放在了指数内部，管这叫local归一化。 来了，viterbi求解过程，是用dp的状态转移公式（MEMM的没展开，请参考CRF下面的公式），因为是局部归一化，所以MEMM的viterbi的转移公式的第二部分出现了问题，导致dp无法正确的递归到全局的最优。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\delta_{i+1} = max_{1 \le j \le m}\lbrace \delta_{i}(I) + \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i) \rbrace\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="wu , crf_1"&gt;&lt;strong&gt;五、CRF&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;我觉得一旦有了一个清晰的工作流程，那么按部就班地，没有什么很难理解的地方，因为整体框架已经胸有成竹了，剩下了也只有添砖加瓦小修小补了。有了上面的过程基础，CRF也是类似的，只是有方法论上的细微区别。&lt;/p&gt;
&lt;h3 id="5.1 li jie crf"&gt;&lt;strong&gt;5.1 理解CRF&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;请看第一张概率图模型构架图，CRF上面是马尔科夫随机场（马尔科夫网络），而条件随机场是在给定的随机变量 &lt;span class="math"&gt;\(X\)&lt;/span&gt; （具体，对应观测序列 &lt;span class="math"&gt;\(o_{1}, \cdots, o_{i}\)&lt;/span&gt; ）条件下，随机变量 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; （具体，对应隐状态序列 &lt;span class="math"&gt;\(i_{1}, \cdots, i_{i}\)&lt;/span&gt; ）的马尔科夫随机场。&lt;br/&gt;
广义的CRF的定义是： 满足 &lt;span class="math"&gt;\(P(Y_{v}|X,Y_{w},w \neq v) = P(Y_{v}|X,Y_{w},w \sim v)\)&lt;/span&gt; 的马尔科夫随机场叫做条件随机场（CRF）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen注:&lt;br/&gt;
&lt;span class="math"&gt;\(Y_{w},w \neq v\)&lt;/span&gt; 表示除&lt;span class="math"&gt;\(v\)&lt;/span&gt;以外观测集中的所有节点，&lt;br/&gt;
&lt;span class="math"&gt;\(Y_{w},w \sim v\)&lt;/span&gt; 表示观测集中&lt;span class="math"&gt;\(v\)&lt;/span&gt;的邻接节点，  &lt;/p&gt;
&lt;p&gt;下面是另一种表达方式：&lt;br/&gt;
&lt;span class="math"&gt;\(P(Y_v|X,Y_{V\backslash\{v\}}) = P(Y_v|X,Y_{n(v)}) \\\)&lt;/span&gt; 
其中：&lt;br/&gt;
&lt;span class="math"&gt;\(Y_{V\backslash\{v\}}\)&lt;/span&gt; 表示除&lt;span class="math"&gt;\(v\)&lt;/span&gt;以外的&lt;span class="math"&gt;\(V\)&lt;/span&gt;中所有节点， &lt;br/&gt;
&lt;span class="math"&gt;\(Y_{n(v)}\)&lt;/span&gt; 表示结点&lt;span class="math"&gt;\(v\)&lt;/span&gt;的邻接节点  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;不过一般说CRF为序列建模，就专指CRF线性链（linear chain CRF）：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/13.jpg" width="415"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;在2.1.2中有提到过，概率无向图的联合概率分布可以在因子分解下表示为：&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y | X)=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c}|X ) = \frac{1}{Z(x)} \prod_{c} e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)} = \frac{1}{Z(x)} e^{\sum_{c}\sum_{k}\lambda_{k}f_{k}(y_{i},y_{i-1},x,i)}$$&lt;/div&gt;
&lt;p&gt;而在线性链CRF示意图中，每一个（ &lt;span class="math"&gt;\(I_{i} \sim O_{i}\)&lt;/span&gt; ）对为一个最大团,即在上式中 &lt;span class="math"&gt;\(c = i\)&lt;/span&gt; 。并且线性链CRF满足 &lt;span class="math"&gt;\(P(I_{i}|O,I_{1},\cdots, I_{n}) = P(I_{i}|O,I_{i-1},I_{i+1})\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;所以CRF的建模公式如下：&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$P(I | O)=\frac{1}{Z(O)} \prod_{i}\psi_{i}(I_{i}|O ) = \frac{1}{Z(O)} \prod_{i} e^{\sum_{k}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)} = \frac{1}{Z(O)} e^{\sum_{i}\sum_{k}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;/div&gt;
&lt;p&gt;我要敲黑板了，这个公式是非常非常关键的，注意递推过程啊，我是怎么从 &lt;span class="math"&gt;\(&amp;prod;\)&lt;/span&gt; 跳到 &lt;span class="math"&gt;\(e^{\sum}\)&lt;/span&gt; 的。&lt;/p&gt;
&lt;p&gt;不过还是要多啰嗦一句，想要理解CRF，必须判别式模型的概念要深入你心。
正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，因
为我创造出来就是为了这个分边界的目的的。比如说序列求概率（分类）问
题，我直接考虑找出函数分类边界。所以才为什么会有这个公式。所以再看
到这个公式也别懵逼了，he was born for discriminating the given data
from different classes. 就这样。不过待会还会具体介绍特征函数部分的东西。&lt;/p&gt;
&lt;p&gt;除了建模总公式，关键的CRF重点概念在MEMM中已强调过：&lt;strong&gt;判别式模型&lt;/strong&gt;、&lt;strong&gt;特征函数&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 特征函数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上面给出了CRF的建模公式：&lt;/p&gt;
&lt;div class="math"&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;下标 &lt;em&gt;i&lt;/em&gt; 表示我当前所在的节点（token）位置。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;下标 &lt;em&gt;k&lt;/em&gt; 表示我这是第几个特征函数，并且每个特征函数都附属一个权重 &lt;span class="math"&gt;\(\lambda_{k}\)&lt;/span&gt; ，也就是这么回事，每个团里面，我将为 &lt;span class="math"&gt;\(token_{i}\)&lt;/span&gt; 构造M个特征，每个特征执行一定的限定作用，然后建模时我再为每个特征函数加权求和。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Z(O)\)&lt;/span&gt; 是用来归一化的，为什么？想想LR以及softmax为何有归一化呢，一样的嘛，形成概率值。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;再来个重要的理解。 &lt;span class="math"&gt;\(P(I|O)\)&lt;/span&gt; 这个表示什么？具体地，表示了在给定的一条观测序列 &lt;span class="math"&gt;\(O=(o_{1},\cdots, o_{i})\)&lt;/span&gt; 条件下，我用CRF所求出来的隐状态序列 &lt;span class="math"&gt;\(I=(i_{1},\cdots, i_{i})\)&lt;/span&gt; 的概率，注意，这里的 &lt;span class="math"&gt;\(I\)&lt;/span&gt; 是一条序列，有多个元素（一组随机变量），而至于观测序列 &lt;span class="math"&gt;\(O=(o_{1},\cdots, o_{i})\)&lt;/span&gt; ，它可以是一整个训练语料的所有的观测序列；也可以是在inference阶段的一句sample，比如说对于序列标注问题，我对一条sample进行预测，可能能得到 &lt;span class="math"&gt;\(P_{j}(I | O)（j=1,&amp;hellip;,J)\)&lt;/span&gt;,  &lt;span class="math"&gt;\(J\)&lt;/span&gt;条隐状态&lt;span class="math"&gt;\(I\)&lt;/span&gt;，但我肯定最终选的是最优概率的那条（by viterbi）。这一点希望你能理解。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于CRF，可以为他定义两款特征函数：转移特征&amp;amp;状态特征。 我们将建模总公式展开：&lt;/p&gt;
&lt;div class="math"&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}=\frac{1}{Z(O)} e^{ [ \sum_{i}^{T}\sum_{j}^{J}\lambda_{j}t_{j}(O,I_{i-1},I_{i},i) + \sum_{i}^{T}\sum_{l}^{L}\mu_{l}s_{l}(O,I_{i},i) ] }$$&lt;/div&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(t_{j}\)&lt;/span&gt; 为i处的转移特征，对应权重 &lt;span class="math"&gt;\(\lambda_{j}\)&lt;/span&gt; ,每个 &lt;span class="math"&gt;\(token_{i}\)&lt;/span&gt; 都有J个特征,转移特征针对的是前后token之间的限定。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\begin{equation} t_{k=1}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定转移条件，比如前一个token是&amp;lsquo;I&amp;rsquo;}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(s_l\)&lt;/span&gt;为i 处的状态特征，对应权重&lt;span class="math"&gt;\(&amp;mu;_l\)&lt;/span&gt;，每个&lt;span class="math"&gt;\(token_i\)&lt;/span&gt;都有L个特征  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\begin{equation} s_{l=1}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定状态条件，比如当前token的POS是&amp;lsquo;V&amp;rsquo;}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不过一般情况下，我们不把两种特征区别的那么开，合在一起：&lt;/p&gt;
&lt;div class="math"&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;/div&gt;
&lt;p&gt;满足特征条件就取值为1，否则没贡献，甚至你还可以让他打负分，充分惩罚。&lt;/p&gt;
&lt;p&gt;再进一步理解的话，我们需要把特征函数部分抠出来：&lt;/p&gt;
&lt;div class="math"&gt;$$Score = \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)$$&lt;/div&gt;
&lt;p&gt;是的，我们为 &lt;span class="math"&gt;\(token_{i}\)&lt;/span&gt; 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值&amp;hellip;&amp;hellip;完了又扯到了log线性模型。现在稍作解释：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;log-linear models take the following form:&lt;br/&gt;
&lt;span class="math"&gt;\(P(y|x;\omega) = \frac{ exp(\omega&amp;middot;\phi(x,y)) }{ \sum_{y^{'}\in Y }exp(\omega&amp;middot;\phi(x,y^{&amp;lsquo;})) }\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我觉得对LR或者sotfmax熟悉的对这个应该秒懂。然后CRF完美地满足这个形式，所以又可以归入到了log-linear models之中。&lt;/p&gt;
&lt;h3 id="5.2 mo xing yun xing guo cheng"&gt;&lt;strong&gt;5.2 模型运行过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;模型的工作流程，跟MEMM是一样的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step1. 先预定义特征函数 &lt;span class="math"&gt;\(f_{a}(o,i)\)&lt;/span&gt; ，&lt;/li&gt;
&lt;li&gt;step2. 在给定的数据上，训练模型，确定参数 &lt;span class="math"&gt;\(\lambda_{k}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;step3. 用确定的模型做&lt;code&gt;序列标注问题&lt;/code&gt;或者&lt;code&gt;序列求概率问题&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可能还是没做到100%懂，结合例子说明：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="5.2.1 xue xi xun lian guo cheng"&gt;&lt;strong&gt;5.2.1 学习训练过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;一套CRF由一套参数&amp;lambda;唯一确定（先定义好各种特征函数）。&lt;/p&gt;
&lt;p&gt;同样，CRF用极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。其实能用在log-linear models上的求参方法都可以用过来。&lt;/p&gt;
&lt;p&gt;嗯，具体详细求解过程貌似问题不大。&lt;/p&gt;
&lt;h3 id="5.2.2 xu lie biao zhu guo cheng"&gt;&lt;strong&gt;5.2.2 序列标注过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;还是跟HMM一样的，用学习好的CRF模型，在新的sample（观测序列 &lt;span class="math"&gt;\(o_{1}, \cdots, o_{i}\)&lt;/span&gt; ）上找出一条概率最大最可能的隐状态序列 &lt;span class="math"&gt;\(i_{1}, \cdots, i_{i}\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。&lt;/p&gt;
&lt;p&gt;啰嗦一下，我们就定义i处的局部状态为 &lt;span class="math"&gt;\(\delta_{i}(I)\)&lt;/span&gt; ,表示在位置i处的隐状态的各种取值可能为 &lt;em&gt;I&lt;/em&gt; ，然后递推位置i+1处的隐状态，写出来的DP转移公式为：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\delta_{i+1} = max_{1 \le j \le m}\lbrace \delta_{i}(I) + \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i) \rbrace\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里没写规范因子 &lt;span class="math"&gt;\(Z(O)\)&lt;/span&gt; 是因为不规范化不会影响取最大值后的比较。&lt;/p&gt;
&lt;p&gt;具体还是不展开为好。&lt;/p&gt;
&lt;h3 id="5.2.3 xu lie qiu gai lu guo cheng"&gt;&lt;strong&gt;5.2.3 序列求概率过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的CRF，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。只是貌似很少看到拿CRF或者MEMM来做分类的，直接用网络模型不就完了不&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;应该可以不用展开，吧&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;h3 id="5.3 crf++fen xi"&gt;&lt;strong&gt;5.3 CRF++分析&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;本来做task用CRF++跑过baseline,后来在对CRF做调研时，非常想透析CRF++的工作原理，以identify以及verify做的各种假设猜想。当然，也看过其他的CRF实现源码。&lt;/p&gt;
&lt;p&gt;所以干脆写到这里来，结合CRF++实例讲解过程。&lt;/p&gt;
&lt;p&gt;有一批语料数据，并且已经tokenized好了：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nuclear&lt;br/&gt;
theory&lt;br/&gt;
devoted&lt;br/&gt;
major&lt;br/&gt;
efforts&lt;br/&gt;
&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;并且我先确定了13个标注元素：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;B_MAT&lt;br/&gt;
B_PRO&lt;br/&gt;
B_TAS&lt;br/&gt;
E_MAT&lt;br/&gt;
E_PRO&lt;br/&gt;
E_TAS&lt;br/&gt;
I_MAT&lt;br/&gt;
I_PRO&lt;br/&gt;
I_TAS&lt;br/&gt;
O&lt;br/&gt;
S_MAT&lt;br/&gt;
S_PRO&lt;br/&gt;
S_TAS&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;1. 定义模板&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按道理应该是定义特征函数才对吧？好的，在CRF++下，应该是先定义特征模板，然后用模板自动批量产生大量的特征函数。我之前也蛮confused的，用完CRF++还以为模板就是特征，后面就搞清楚了：每一条模板将在每一个token处生产若干个特征函数。&lt;/p&gt;
&lt;p&gt;CRF++的模板（template）有U系列（unigram）、B系列(bigram)，不过我至今搞不清楚B系列的作用，因为U模板都可以完成2-gram的作用。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;U00:%x[-2,0]&lt;br/&gt;
U01:%x[-1,0]&lt;br/&gt;
U02:%x[0,0]&lt;br/&gt;
U03:%x[1,0]&lt;br/&gt;
U04:%x[2,0]  &lt;/p&gt;
&lt;p&gt;U05:%x[-2,0]/%x[-1,0]/%x[0,0]&lt;br/&gt;
U06:%x[-1,0]/%x[0,0]/%x[1,0]&lt;br/&gt;
U07:%x[0,0]/%x[1,0]/%x[2,0]&lt;br/&gt;
U08:%x[-1,0]/%x[0,0]&lt;br/&gt;
U09:%x[0,0]/%x[1,0]  &lt;/p&gt;
&lt;p&gt;B&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以，U00 - U09 我定义了10个模板。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 产生特征函数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;是的，会产生大量的特征。 U00 - U04的模板产生的是状态特征函数；U05 - U09的模板产生的是转移特征函数。&lt;/p&gt;
&lt;p&gt;在CRF++中，每个特征都会try每个标注label（这里有13个），总共将生成 &lt;span class="math"&gt;\(N * L = i * k^{'} * L\)&lt;/span&gt; 个特征函数以及对应的权重出来。N表示每一套特征函数 &lt;span class="math"&gt;\(N= i * k^{'}\)&lt;/span&gt; ，L表示标注集元素个数。&lt;/p&gt;
&lt;p&gt;比如训练好的CRF模型的部分特征函数是这样存储的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;22607 B&lt;br/&gt;
790309 U00:%&lt;br/&gt;
3453892 U00:%)&lt;br/&gt;
2717325 U00:&amp;amp;&lt;br/&gt;
2128269 U00:'t&lt;br/&gt;
2826239 U00:(0.3534&lt;br/&gt;
2525055 U00:(0.593&amp;ndash;1.118&lt;br/&gt;
197093 U00:(1)&lt;br/&gt;
2079519 U00:(1)L=14w2&amp;minus;12w&amp;minus;F&amp;mu;&amp;nu;aFa&amp;mu;&amp;nu;&lt;br/&gt;
2458547 U00:(1)&amp;delta;n=&amp;int;&amp;minus;&amp;infin;En+1&amp;rho;&amp;tilde;(E)dE&amp;minus;n&lt;br/&gt;
1766024 U00:(1.0g&lt;br/&gt;
2679261 U00:(1.1wt%)&lt;br/&gt;
1622517 U00:(100)&lt;br/&gt;
727701 U00:(1000&amp;ndash;5000A)&lt;br/&gt;
2626520 U00:(10a)&lt;br/&gt;
2626689 U00:(10b)&lt;br/&gt;
&amp;hellip;&amp;hellip;&lt;br/&gt;
2842814 U07:layer/thicknesses/Using&lt;br/&gt;
2847533 U07:layer/thicknesses/are&lt;br/&gt;
2848651 U07:layer/thicknesses/in&lt;br/&gt;
331539 U07:layer/to/the&lt;br/&gt;
1885871 U07:layer/was/deposited&lt;br/&gt;
&amp;hellip;&amp;hellip;（数量非常庞大）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其实也就是对应了这样些个特征函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;func1 = if (output = B and feature="U02:一") return 1 else return 0&lt;br/&gt;
func2 = if (output = M and feature="U02:一") return 1 else return 0&lt;br/&gt;
func3 = if (output = E and feature="U02:一") return 1 else return 0&lt;br/&gt;
func4 = if (output = S and feature="U02:一") return 1 else return 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;比如模板U06会从语料中one by one逐句抽出这些各个特征：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一/个/人/&amp;hellip;&amp;hellip;&lt;br/&gt;
个/人/走/&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;3. 求参&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对上述的各个特征以及初始权重进行迭代参数学习。&lt;/p&gt;
&lt;p&gt;在CRF++ 训练好的模型里，权重是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;0.3972716048310705&lt;br/&gt;
0.5078838237171732&lt;br/&gt;
0.6715316559507898&lt;br/&gt;
-0.4198827647512405&lt;br/&gt;
-0.4233310655891150&lt;br/&gt;
-0.4176580083832543&lt;br/&gt;
-0.4860489836004728&lt;br/&gt;
-0.6156475863742051&lt;br/&gt;
-0.6997919485753300&lt;br/&gt;
0.8309956709647820&lt;br/&gt;
0.3749695682658566&lt;br/&gt;
0.2627347894057647&lt;br/&gt;
0.0169732441379157&lt;br/&gt;
0.3972716048310705&lt;br/&gt;
0.5078838237171732&lt;br/&gt;
0.6715316559507898&lt;br/&gt;
&amp;hellip;&amp;hellip;（数量非常庞大，与每个label的特征函数对应，我这有300W个）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;4. 预测解码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;结果是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nuclear B_TAS&lt;br/&gt;
theory E_TAS&lt;br/&gt;
devoted O&lt;br/&gt;
major O&lt;br/&gt;
efforts O&lt;br/&gt;
&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="5.4 lstm+crf"&gt;&lt;strong&gt;5.4 LSTM+CRF&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;LSTM+CRF这个组合其实我在知乎上答过问题，然后顺便可以整合到这里来。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、perspectively&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大家都知道，LSTM已经可以胜任序列标注问题了，为每个token预测一个label（LSTM后面接:分类器）；而CRF也是一样的，为每个token预测一个label。&lt;/p&gt;
&lt;p&gt;但是，他们的预测机理是不同的。CRF是全局范围内统计归一化的条件状态转移概率矩阵，再预测出一条指定的sample的每个token的label；LSTM（RNNs，不区分here）是依靠神经网络的超强非线性拟合能力，在训练时将samples通过复杂到让你窒息的高阶高纬度异度空间的非线性变换，学习出一个模型，然后再预测出一条指定的sample的每个token的label。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2、LSTM+CRF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;既然LSTM都OK了，为啥researchers搞一个LSTM+CRF的hybrid model?&lt;/p&gt;
&lt;p&gt;哈哈，因为a single LSTM预测出来的标注有问题啊！举个segmentation例子(BES; char level)，plain LSTM 会搞出这样的结果：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;input&lt;/strong&gt;: "学习出一个模型，然后再预测出一条指定"&lt;br/&gt;
&lt;strong&gt;expected output&lt;/strong&gt;: 学/B 习/E 出/S 一/B 个/E 模/B 型/E ，/S 然/B 后/E 再/E 预/B 测/E &amp;hellip;&amp;hellip;&lt;br/&gt;
&lt;strong&gt;real output&lt;/strong&gt;: 学/B 习/E 出/S 一/B 个/B 模/B 型/E ，/S 然/B 后/B 再/E 预/B 测/E &amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看到不，用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。当然了，CRF还能学到更多的限定特征，那越多越好啊！&lt;/p&gt;
&lt;p&gt;好了，那就把CRF接到LSTM上面，把LSTM在time_step上把每一个hidden_state的tensor输入给CRF，让LSTM负责在CRF的特征限定下，依照新的loss function，学习出一套新的非线性变换空间。&lt;/p&gt;
&lt;p&gt;最后，不用说，结果还真是好多了呢。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//github.com/scofield7419/sequence-labeling-BiLSTM-CRF"&gt;LSTM+CRF codes&lt;/a&gt;, here. Go just take it.&lt;/p&gt;
&lt;h2 id="liu , zong jie_1"&gt;&lt;strong&gt;六、总结&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id="1. zong ti dui bi"&gt;&lt;strong&gt;1. 总体对比&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;应该看到了熟悉的图了，现在看这个图的话，应该可以很清楚地get到他所表达的含义了。这张图的内容正是按照生成式&amp;amp;判别式来区分的，NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/14.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;h3 id="2. hmm vs. memm vs. crf"&gt;&lt;strong&gt;2. HMM vs. MEMM vs. CRF&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;将三者放在一块做一个总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;HMM -&amp;gt; MEMM： HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MEMM -&amp;gt; CRF:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;HMM、MEMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去（这点问题应该只有HMM）。
CRF属于无向图，没有这种依赖性，克服此问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="3. machine learning models vs. sequential models"&gt;&lt;strong&gt;3. Machine Learning models vs. Sequential models&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;为了一次将概率图模型理解的深刻到位，我们需要再串一串，更深度与原有的知识体系融合起来。&lt;/p&gt;
&lt;p&gt;机器学习模型，按照学习的范式或方法，以及加上自己的理解，给常见的部分的他们整理分了分类（主流上，都喜欢从训练样本的歧义型分，当然也可以从其他角度来）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;一、监督：{

1.1 分类算法(线性和非线性)：{

    感知机

    KNN

    概率{
        朴素贝叶斯（NB）
        Logistic Regression（LR）
        最大熵MEM（与LR同属于对数线性分类模型）
    }

    支持向量机(SVM)

    决策树(ID3、CART、C4.5)

    assembly learning{
        Boosting{
            Gradient Boosting{
                GBDT
                xgboost（传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）；xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。）
            }
            AdaBoost
        }   
        Bagging{
            随机森林
        }
        Stacking
    }

    &amp;hellip;&amp;hellip;
}

1.2 概率图模型：{
    HMM
    MEMM（最大熵马尔科夫）
    CRF
    &amp;hellip;&amp;hellip;
}

1.3 回归预测：{
    线性回归
    树回归
    Ridge岭回归
    Lasso回归
    &amp;hellip;&amp;hellip;
}

&amp;hellip;&amp;hellip;  
}

二、非监督：{
2.1 聚类：{
    1. 基础聚类
        K&amp;mdash;mean
        二分k-mean
        K中值聚类
        GMM聚类
    2. 层次聚类
    3. 密度聚类
    4. 谱聚类()
}

2.2 主题模型:{
    pLSA
    LDA隐含狄利克雷分析
}

2.3 关联分析：{
    Apriori算法
    FP-growth算法
}

2.4 降维：{
    PCA算法
    SVD算法
    LDA线性判别分析
    LLE局部线性嵌入
}

2.5 异常检测：
&amp;hellip;&amp;hellip;
}

三、半监督学习

四、迁移学习
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;（注意到，没有把神经网络体系加进来。因为NNs的范式很灵活，不太适用这套分法，largely, off this framework）&lt;/p&gt;
&lt;p&gt;Generally speaking，机器学习模型，尤其是有监督学习，一般是为一条sample预测出一个label，作为预测结果。 但与典型常见的机器学习模型不太一样，序列模型（概率图模型）是试图为一条sample里面的每个基本元数据分别预测出一个label。这一点，往往是beginner伊始难以理解的。&lt;/p&gt;
&lt;p&gt;具体的实现手段差异，就是：ML models通过直接预测得出label；Sequential models是给每个token预测得出label还没完，还得将他们每个token对应的labels进行组合，具体的话，用viterbi来挑选最好的那个组合。&lt;/p&gt;
&lt;h2 id="over_1"&gt;&lt;strong&gt;over&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;有了这道开胃菜，接下来，读者可以完成这些事情：完善细节算法、阅读原著相关论文达到彻底理解、理解相关拓展概念、理论创新&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;hope those hlpe!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欢迎留言！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有错误之处请多多指正，谢谢！&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="referrences:"&gt;&lt;strong&gt;Referrences:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;《统计学习方法》，李航&lt;/p&gt;
&lt;p&gt;《统计自然语言处理》，宗成庆&lt;/p&gt;
&lt;p&gt;《 An Introduction to Conditional Random Fields for Relational Learning》， Charles Sutton， Andrew McCallum&lt;/p&gt;
&lt;p&gt;《Log-Linear Models, MEMMs, and CRFs》，ichael Collins&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/question/35866596"&gt;如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/en-heng/p/6201893.html"&gt;【中文分词】最大熵马尔可夫模型MEMM - Treant - 博客园&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//github.com/timvieira/crf"&gt;timvieira/crf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//github.com/shawntan/python-crf"&gt;shawntan/python-crf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=http%3A//videolectures.net/cikm08_elkan_llmacrf/"&gt;Log-linear Models and Conditional Random Fields&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/55755fc649b1"&gt;如何轻松愉快地理解条件随机场（CRF）？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/7068574.html"&gt;条件随机场CRF(三) 模型学习与维特比算法解码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/question/20279019"&gt;crf++里的特征模板得怎么理解？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=http%3A//www.hankcs.com/ml/crf-code-analysis.html"&gt;CRF++代码分析-码农场&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=http%3A//blog.csdn.net/aws3217150/article/details/69212445"&gt;CRF++源码解读 - CSDN博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=http%3A//www.hankcs.com/nlp/the-crf-model-format-description.html"&gt;CRF++模型格式说明-码农场&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/syx-1987/p/4077325.html"&gt;标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较&amp;lt;转&amp;gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;编辑于 2018-03-21&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Y_{w},w \neq v\)&lt;/span&gt; 表示除&lt;span class="math"&gt;\(v\)&lt;/span&gt;以外观测集中的所有&lt;/p&gt;
&lt;/blockquote&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Estimator 编程指南</title><link href="https://freeopen.github.io/posts/estimator-bian-cheng-zhi-nan" rel="alternate"></link><published>2018-03-04T00:00:00+08:00</published><updated>2018-03-04T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2018-03-04:/posts/estimator-bian-cheng-zhi-nan</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;2018-04-12 第一次修订, 新增"多GPU下的写法"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：代码适用于 TF1.4 ~ TF1.7 。&lt;/p&gt;
&lt;p&gt;为什么要使用Estimator, 仅&lt;a href="https://www.tensorflow.org/programmers_guide/estimators"&gt;官方文档&lt;/a&gt;里提到的第一条优点就让我不得不重视它。
大意是不管你在本地环境还是分布式环境，不管你用一个或多个CPU、GPU还是TPU训练模型，你的模型代码不需要做任何改变。&lt;/p&gt;
&lt;p&gt;但看了一些Estimator教程，不怎么满意。因为大部分介绍的方法过于简单，仅适用于实验环境。当你面对大数据、复杂模型和机能限制时，发现那些方法就不灵了。
所以就自己写了一本，方便自查自检。这篇文章，会随着本人的打怪升级等级进行增补。&lt;/p&gt;
&lt;h2 id="estimator"&gt;Estimator&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;img src="/images/image2.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;Estimator 作为高层API，可以让我们写出结构清晰的代码。你有两种方法通过 estimator 来构建模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-mode Estimator: 创建指定类型的模型，如上图，它们分别是线性分类和回归模型、深度神经网络分类和回归模型，还有线性和深度混合的分类、回归模型。&lt;/li&gt;
&lt;li&gt;自定义 Estimator: 按传统的方法写模型，然后用 model_fn 函数封装 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;2018-04-12 第一次修订, 新增"多GPU下的写法"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：代码适用于 TF1.4 ~ TF1.7 。&lt;/p&gt;
&lt;p&gt;为什么要使用Estimator, 仅&lt;a href="https://www.tensorflow.org/programmers_guide/estimators"&gt;官方文档&lt;/a&gt;里提到的第一条优点就让我不得不重视它。
大意是不管你在本地环境还是分布式环境，不管你用一个或多个CPU、GPU还是TPU训练模型，你的模型代码不需要做任何改变。&lt;/p&gt;
&lt;p&gt;但看了一些Estimator教程，不怎么满意。因为大部分介绍的方法过于简单，仅适用于实验环境。当你面对大数据、复杂模型和机能限制时，发现那些方法就不灵了。
所以就自己写了一本，方便自查自检。这篇文章，会随着本人的打怪升级等级进行增补。&lt;/p&gt;
&lt;h2 id="estimator"&gt;Estimator&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;img src="/images/image2.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;Estimator 作为高层API，可以让我们写出结构清晰的代码。你有两种方法通过 estimator 来构建模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-mode Estimator: 创建指定类型的模型，如上图，它们分别是线性分类和回归模型、深度神经网络分类和回归模型，还有线性和深度混合的分类、回归模型。&lt;/li&gt;
&lt;li&gt;自定义 Estimator: 按传统的方法写模型，然后用 model_fn 函数封装&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 指定类型模型的估计器举例&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNNClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# 定义好的特征列 &lt;/span&gt;
   &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;           &lt;span class="c1"&gt;# 两个隐藏层, 每层10个神经元&lt;/span&gt;
   &lt;span class="n"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                     &lt;span class="c1"&gt;# 输出3个类别&lt;/span&gt;
   &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# 存 checkpoints 的路径&lt;/span&gt;

&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;file_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FILE_TRAIN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;     &lt;span class="c1"&gt;# 训练数据文件路径&lt;/span&gt;
        &lt;span class="n"&gt;perform_shuffle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;     &lt;span class="c1"&gt;# 打乱数据&lt;/span&gt;
        &lt;span class="n"&gt;repeat_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;          &lt;span class="c1"&gt;# 重复8次&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 自定义模型的估计器举例&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                       &lt;span class="c1"&gt;# batch数量的特征，是input_fn 函数的输出&lt;/span&gt;
   &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                         &lt;span class="c1"&gt;# batch数量的标签，是input_fn 函数的输出&lt;/span&gt;
   &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;                          &lt;span class="c1"&gt;# tf.estimator.ModeKeys.TRAIN / EVAL / PREDICT&lt;/span&gt;

  &lt;span class="c1"&gt;# 用特征列（feature_columns)定义输入层 &lt;/span&gt;
  &lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;# 模型定义部分&lt;/span&gt;
  &lt;span class="o"&gt;...&lt;/span&gt;

  &lt;span class="c1"&gt;# 返回值被EstimatorSpec封装，返回训练时关心的loss和train_op&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
     &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Estimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;model_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;               &lt;span class="c1"&gt;# 自定义模型的封装函数&lt;/span&gt;
   &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# 存 checkpoints 的路径&lt;/span&gt;

&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILE_TRAIN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;repeat_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;仔细研究上面两段最简代码，Estimator的编程结构就呼之欲出了，看下图：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/Unknown.jpg" width="60%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;input_fn读入数据，预处理后输出到estimator，再由estimator执行训练、评估或预测等任务.
这里的estimator(估计器）就是模型的抽象，它可以直接定义模型或使用外部模型。 特殊的地方在于，
数据送进estimator时，常常被feature_columns（由特征列组成的列表, 与input_fn输出的数据一一对应）
做二次封装. 注意，特征列是使用estimator的主要方法之一，并不是必须.&lt;/p&gt;
&lt;p&gt;任务分解如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input_fn: 从原始数据文件读取数据，然后清洗数据、打乱顺序等，用迭代器分批输出特征和对应的标签。  &lt;/li&gt;
&lt;li&gt;feature_columns: 特征工程，使数据便于模型训练。&lt;/li&gt;
&lt;li&gt;模型定义: 简单的模型可考虑用estimator直接定义；自定义模型的话，须封装进model_fn函数，输入层传入feature_columns, 输出用tf.estimator.EstimatorSpec封装。&lt;/li&gt;
&lt;li&gt;最后，用estimator把上面三项组织在一起，做训练、评估、预测等任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="input_fn"&gt;input_fn&lt;/h2&gt;
&lt;p&gt;原始数据一般工整的很少，所以要把input_fn写好，还是蛮难的。&lt;/p&gt;
&lt;h3 id="xiao shu ju de qing kuang"&gt;小数据的情况&lt;/h3&gt;
&lt;p&gt;通常实验性的项目采用小规模的数据，这时只需要简单把数据载入内存作训练即可, 我们可以用numpy、pandas等通用工具来处理数据。
假如原始数据是csv文件，选用pandas读入并作预处理：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 定义列名&lt;/span&gt;
&lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;'symboling'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="s1"&gt;'normalized-losses'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="s1"&gt;'make'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
     &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="s1"&gt;'price'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# 为每列指定类型.&lt;/span&gt;
&lt;span class="n"&gt;dtypes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;'symboling'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="s1"&gt;'normalized-losses'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="s1"&gt;'make'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
     &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="s1"&gt;'price'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# 读入文件，空数据用 ？号填充.&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'filename.csv'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;na_values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'?'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 清理数据: 如果发现价格为空就删除该行.&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rows'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'any'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;subset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'price'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# 补足数据: 把其他列的空值填充为缺省值&lt;/span&gt;
&lt;span class="c1"&gt;# 把float32类型的列放入列表 float_columns&lt;/span&gt;
&lt;span class="n"&gt;float_columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# 对数值列来说，如果发现空值就填充 0 &lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;float_columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;float_columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'columns'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 构建字符串列，如果方向空值(NaN)就填充''(空串).&lt;/span&gt;
&lt;span class="n"&gt;string_columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;string_columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;string_columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'columns'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样，数据变得比较工整了。假设最后一项price是label，前面的都是特征，按照习惯，数据被分割成训练和评估数据, 
它们分别被叫做 training_data、training_label 和 eval_data、eval_label, 它们的类型都是dataframe.&lt;/p&gt;
&lt;p&gt;定义训练和评估的input_fn&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# num_epochs=None -&amp;gt; 数据无限循环&lt;/span&gt;
&lt;span class="c1"&gt;# shuffle   =True -&amp;gt; 打乱数据&lt;/span&gt;
&lt;span class="n"&gt;training_input_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pandas_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 评估时，数据不需要被打乱，所以shuffle=False &lt;/span&gt;
&lt;span class="n"&gt;eval_input_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pandas_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eval_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eval_label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样，input_fn就快速的写好了。&lt;/p&gt;
&lt;h3 id="da shu ju de qing kuang"&gt;大数据的情况&lt;/h3&gt;
&lt;p&gt;pandas一次性把数据载入内存中，不适合大数据量的情形。
面对大规模数据时，需要给数据和模型之间接上管道，然后打开水龙头，按照你想要的流量把数据传入模型。
这时，TF提供的 dataset api 就派上用场了。&lt;/p&gt;
&lt;h4&gt;Dataset API 的结构&lt;/h4&gt;
&lt;p&gt;&lt;p align="center"&gt;
&lt;img src="/images/image7.jpg" width="70%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TextLineDataset: 从文本文件每次读一行.&lt;/li&gt;
&lt;li&gt;TFRecordDataset: 从 TFRecord 文件读取记录.&lt;/li&gt;
&lt;li&gt;FixedLengthRecordDataset: 从二进制文件读取固定大小的记录.&lt;/li&gt;
&lt;li&gt;Iterator: 从dataset中每次读取一笔(一般为batch条)数据.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Dataset版的input_fn&lt;/h4&gt;
&lt;p&gt;假设我们手上有一堆人口普查数据，其中年收入是字符串类型，形如&amp;ldquo;&amp;gt;50k&amp;rdquo;, 我们的目标是预测人们的年收入是大于5万还是小于等于5万。
input_fn函数的写法如下：&lt;/p&gt;
&lt;p&gt;首先定义CSV文件中每行数据的列名和缺省值，字典格式.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;csv_defaults&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collections&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderedDict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'age'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'workclass'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fnlwgt'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'education'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'education-num'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'marital-status'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'occupation'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'relationship'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'race'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'sex'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'capital-gain'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'capital-loss'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'hours-per-week'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'native-country'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'income'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;接下来是一段通用代码, 具体见代码注释.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 按行解码 CSV 文件.&lt;/span&gt;
&lt;span class="c1"&gt;# 读入一行数据，对于每列如果有数据就用原值，如果没数据就用缺省值;&lt;/span&gt;
&lt;span class="c1"&gt;# 返回字典格式的键值对&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;csv_decoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;parsed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_defaults&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_defaults&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;parsed&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# 过滤器，滤掉空行，该函数后面要用&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;filter_empty_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;not_equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string_split&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;','&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 创建训练的input_fn&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;create_train_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;    
        &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextLineDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# 从文件创建数据集&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_empty_lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="c1"&gt;# 滤掉空行&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_decoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# 解析每行&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;buffer_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;# 每1000行打乱顺序&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;                          &lt;span class="c1"&gt;# 无限重复&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 

        &lt;span class="c1"&gt;# 迭代器，每次取batch个数据, 这里为32&lt;/span&gt;
        &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_one_shot_iterator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# 分离出label值，并转成 true/false 形式&lt;/span&gt;
        &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'income'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s2"&gt;" &amp;gt;50K"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;

&lt;span class="c1"&gt;# 创建测试的input_fn, 注意与前面的区别&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;create_test_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;    
        &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextLineDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_empty_lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_decoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_one_shot_iterator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'income'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s2"&gt;" &amp;gt;50K"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; 

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;

&lt;span class="c1"&gt;# 从input_fn中取出数据，每sess.run一次next_batch，就取出一批&lt;/span&gt;
&lt;span class="n"&gt;train_input_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_train_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;next_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_input_fn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'education'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面的代码为什么 create_train_input_fn() 套 input_fn() 呢？
回忆这句：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILE_TRAIN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;repeat_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;因为calssifier.train()中，input_fn要求接的是一个函数，而input_fn() 返回的是特征和标签，所以前面要接上&lt;code&gt;lambda:&lt;/code&gt;.
如果采用现在函数套函数的结构, 那么这句前面的lambda就可以去掉, 走个例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_input_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_train_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILE_TRAIN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_input_fn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="te zheng lie_1"&gt;特征列&lt;/h2&gt;
&lt;p&gt;特征列实质上是对input_fn()输出的数据做的二次封装，它是做特征工程的强力工具之一。
特征列好比一种约定，它规定了estimator使用input_fn传入的数据具备什么样的形式, 
主要目的是令特征数据变得更方便机器运算。&lt;/p&gt;
&lt;p&gt;关于特征列，一共涉及10个函数（图中底层的3个矩形框, 缺weighted_categorical_column）。
按大类分为类别特征列和密集特征列（以下也简称为&amp;ldquo;类别列&amp;rdquo;和&amp;ldquo;密集列&amp;rdquo;）。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/3_.jpg" width="70%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;其中，&lt;code&gt;buchetized_column&lt;/code&gt;位于中间，表示它作为中介把密集列(通常是&lt;code&gt;numeric_column&lt;/code&gt;)转为类别列。对于类别列而言，除了&lt;code&gt;categorical_column_with_hash_buchet&lt;/code&gt;和&lt;code&gt;crossed_column&lt;/code&gt;外，其余三种均把输入的特征数据处理为one-hot结构。&lt;/p&gt;
&lt;p&gt;10个函数可对应9种特征列，我们约定中文称谓如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;numeric_column : 数值列&lt;/li&gt;
&lt;li&gt;bucketized_column : 分区列 &lt;/li&gt;
&lt;li&gt;indicator_column : 指示列&lt;/li&gt;
&lt;li&gt;embedding_column : 嵌入列&lt;/li&gt;
&lt;li&gt;categorical_column_with_identity : 类别ID列&lt;/li&gt;
&lt;li&gt;categorical_column_with_vocabulary(file or list) : 类别词表列&lt;/li&gt;
&lt;li&gt;categorical_column_with_hash_bucket : 类别哈希列&lt;/li&gt;
&lt;li&gt;crossed_column : 合成列&lt;/li&gt;
&lt;li&gt;weighted_categorical_column : 权重类别列&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="shu zhi lie"&gt;数值列&lt;/h3&gt;
&lt;p&gt;以鸢尾花分类问题举例，其输入特征 SepalLength, SepalWidth, PetalLength, PetalWidth （萼片的长宽、花瓣的长宽）就是数值类型。用法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 缺省为tf.float32的标量.&lt;/span&gt;
&lt;span class="n"&gt;numeric_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"SepalLength"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;数值列的缺省类型为 tf.float32, 如果想指定类型，则：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 用tf.float64的标量表示.&lt;/span&gt;
&lt;span class="n"&gt;numeric_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"SepalLength"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;缺省情况下，numeric_column 返回一个单值数据，如果要返回向量数据，则需指定shape值：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 用10维向量来表示，其中每个元素的类型为 tf.float32.&lt;/span&gt;
&lt;span class="n"&gt;vector_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Bowling"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 用10x5的矩阵来表示.&lt;/span&gt;
&lt;span class="n"&gt;matrix_feature_column&lt;/span&gt;
   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"MyMatrix"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="fen qu lie"&gt;分区列&lt;/h3&gt;
&lt;p&gt;如果要把一个数值分成不同区间，比如按年份划分：
&lt;p align="center"&gt;
&lt;img src="/images/4_.jpg" width="60%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;划分后的结果为one-hot向量形式。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;区间&lt;/th&gt;
&lt;th align="center"&gt;表示为&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 1960&lt;/td&gt;
&lt;td align="center"&gt;[1, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;= 1960 且 &amp;lt; 1980&lt;/td&gt;
&lt;td align="center"&gt;[0, 1, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;= 1980 且 &amp;lt; 2000&lt;/td&gt;
&lt;td align="center"&gt;[0, 0, 1, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt; 2000&lt;/td&gt;
&lt;td align="center"&gt;[0, 0, 0, 1]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 原始输入是一个名为Year的数值列.&lt;/span&gt;
&lt;span class="n"&gt;numeric_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Year"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 以1960、1980、2000年来划分区间&lt;/span&gt;
&lt;span class="n"&gt;bucketized_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bucketized_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;source_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numeric_feature_column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;boundaries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1960&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1980&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="lei bie idlie"&gt;类别Id列&lt;/h3&gt;
&lt;p&gt;如图，所谓类别Id列是指把左边的单值数据转换为右边的one-hot矢量形式。
&lt;p align="center"&gt;
&lt;img src="/images/5_.jpg" width="40%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;比如我们用0、1、2、3分别表示童装、数码、运动和食品四类商品：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0=&amp;ldquo;kitchenware&amp;rdquo;&lt;/li&gt;
&lt;li&gt;1="electronics"&lt;/li&gt;
&lt;li&gt;2="sport"&lt;/li&gt;
&lt;li&gt;3="food"&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# key后跟的列名与input_fn()中的列名一致，&lt;/span&gt;
&lt;span class="c1"&gt;# 其值域为[0, num_buckets)间的整数。&lt;/span&gt;
&lt;span class="n"&gt;identity_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical_column_with_identity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'procduct_class'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;num_buckets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="c1"&gt;# 本例中, 'Integer_1' 或 'Integer_2' 皆可替换到上句的 key 之后&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="o"&gt;...&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;...&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;({&lt;/span&gt; &lt;span class="s1"&gt;'Integer_1'&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;..&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;etc&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;..&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Integer_2'&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Label_values&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="lei bie ci biao lie"&gt;类别词表列&lt;/h3&gt;
&lt;p&gt;在NLP任务中，我们不会把词条直接输入模型，而是首先把它转换成数值或向量。类别词表列可以把词条转换为one-hot向量形式，如下图：
&lt;p align="center"&gt;
&lt;img src="/images/6_.jpg" width="50%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;从列表创建一个词表列：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;vocabulary_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical_column_with_vocabulary_list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"feature_name_from_input_fn"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;vocabulary_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"kitchenware"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"electronics"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"sports"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从文件创建一个词表列：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;vocabulary_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical_column_with_vocabulary_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"feature_name_from_input_fn"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;vocabulary_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"product_class.txt"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;vocabulary_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# product_class.txt 的文件内容如下：&lt;/span&gt;
&lt;span class="n"&gt;kitchenware&lt;/span&gt;
&lt;span class="n"&gt;electronics&lt;/span&gt;
&lt;span class="n"&gt;sports&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="lei bie ha xi lie"&gt;类别哈希列&lt;/h3&gt;
&lt;p&gt;如果待分类的数据量很大，势必会消耗很大内存。tensorflow提供一种用哈希表分类的方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hashed_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical_column_with_hash_bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"feature_name_from_input_fn"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;hash_buckets_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 把特征值哈希分布到100个位置&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
&lt;img src="/images/7_.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;当分类数量大于哈希分布尺寸的时候，必然会有几个特征指向同一个哈希位置。如图所示，&lt;code&gt;kitchenware&lt;/code&gt;和&lt;code&gt;sports&lt;/code&gt;的哈希值同为12，这没有关系，模型可以通过你提供的其他特征进一步区分到底是&lt;code&gt;kitchenware&lt;/code&gt;还是&lt;code&gt;sports&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id="he cheng lie"&gt;合成列&lt;/h3&gt;
&lt;p&gt;有时我们需要组合多个特征为一个特征，这种特征叫合成特征。组合方式通常采用相乘或求笛卡尔积，特征组合有助于表示非线性关系。举个例子，假设我们的模型要计算北京的房产价格，而房产价格与它所处的位置密切相关，而对于位置而言，我们需要用经纬度两个数据同时标定，因此这个经纬度就构成了合成特征。假设我们把北京均匀的纵横切100x100刀，这样就会产生10000个可区分的矩形区域。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 将经纬度转换为[0, 100)范围内的整型值&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# 从数据集读入经纬度&lt;/span&gt;
    &lt;span class="n"&gt;latitude&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;   &lt;span class="c1"&gt;# A tf.float32 value&lt;/span&gt;
    &lt;span class="n"&gt;longitude&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# A tf.float32 value&lt;/span&gt;

    &lt;span class="c1"&gt;# 返回的字典包含经纬度及其它特征，经纬度的值为0到99的整型值&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;"latitude"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;latitude&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"longitude"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;longitude&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;

&lt;span class="c1"&gt;# 用np.linspace把纬度区间分成100等份&lt;/span&gt;
&lt;span class="c1"&gt;# 然后把100等份的列表定义为区间列.&lt;/span&gt;
&lt;span class="n"&gt;latitude_buckets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;33.641336&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;33.887157&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;latitude_fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bucketized_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'latitude'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;latitude_buckets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;longitude_buckets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;84.558798&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;84.287259&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;longitude_fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bucketized_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'longitude'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;longitude_buckets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 用fc_longitude x fc_latitude创建交叉特征.&lt;/span&gt;
&lt;span class="n"&gt;fc_beijing_boxed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;crossed_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;latitude_fc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;longitude_fc&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;hash_bucket_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 把10000个分区哈希分布到1000个位置&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;创建合成特征的方法为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从input_fn的返回值中取得待组合的特征名，本例中为&lt;code&gt;latitude&lt;/code&gt;和&lt;code&gt;longitude&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;做组合的这些特征必须先转换成one-hot形式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由&lt;code&gt;latitude_fc&lt;/code&gt;和&lt;code&gt;longitude_fc&lt;/code&gt;组成的合成列的形式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(0,0),(0,1)...  (0,99)
(1,0),(1,1)...  (1,99)
&amp;hellip;, &amp;hellip;,          ...
(99,0),(99,1)...(99, 99)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，使用合成列后，仍需在模型中包含你用来合成特征列的原始特征列，它们负责在哈希冲突时，作为附加特征来进一步做类别区分。&lt;/p&gt;
&lt;h3 id="zhi shi lie"&gt;指示列&lt;/h3&gt;
&lt;p&gt;指示列和后面要说的嵌入列均不能直接作为特征给模型使用，它的数据来源于类别特征列，即类别特征列是它的输入。
为什么要作这样的设计？因为estimator执行深度神经网络的任务时，只能使用密集特征列，而类别特征列为稀疏列，需要用指示列或嵌入列作下变换才能被使用。
至于指示列封装后，数据变成什么样子，我在官方文档中没找到，以后知道了再补充。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# 创建某种类型的类别特征列&lt;/span&gt;

&lt;span class="c1"&gt;# 定义一个指示列，该列中的每个元素为one-hot向量. &lt;/span&gt;
&lt;span class="n"&gt;indicator_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;indicator_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="qian ru lie"&gt;嵌入列&lt;/h3&gt;
&lt;p&gt;如果类别数据量很大，比如上百万、上亿等，这时采用one-hot来表示就不经济了。记得词嵌入模型中的词向量吗，用一组浮点数来代替one-hot形式来表示一个词条，这种形式在这里被叫做嵌入列，这种方法明显的好处就是令向量维度变得很小。&lt;/p&gt;
&lt;p&gt;如下图，假设我们有81个不同的单词，采用one-hot形式需要81维的向量，而采用嵌入列则仅需要3维向量就能表达。
&lt;p align="center"&gt;
&lt;img src="/images/image9.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;那么，在嵌入列产生的向量中的浮点数是如何确定的呢？通常，由训练数据学得。嵌入列可以提升模型的表达能力，一定程度描述类别间的关系。&lt;/p&gt;
&lt;p&gt;如何确定表示81个类别只需要3维呢？有个简单的公式来算出：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{2}\log_2(n)$$&lt;/div&gt;
&lt;p&gt;
&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$n^{0.25}  \tag {等价公式}$$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 类别数的0.25次方&lt;/span&gt;
&lt;span class="n"&gt;embedding_dimensions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;number_of_categories&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mf"&gt;0.25&lt;/span&gt;

&lt;span class="n"&gt;categorical_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# 创建一个类别列.&lt;/span&gt;

&lt;span class="c1"&gt;# 再把这个类别列转为一个嵌入列.&lt;/span&gt;
&lt;span class="c1"&gt;# 这意味着把one-hot向量转为指定维度的向量.&lt;/span&gt;
&lt;span class="n"&gt;embedding_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dimension&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;embedding_dimensions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，这仅仅是个一般规则，你也可以自行设定你希望的维度数。&lt;/p&gt;
&lt;h3 id="quan zhong lei bie lie"&gt;权重类别列&lt;/h3&gt;
&lt;p&gt;有时会遇到一种配对特征，特征一是本体，特征二是本体对应的权重（或出现频率）。
这就是权重类别列的使用场景。
下面是从Tensorflow源码里抠出例子，话说有个&lt;code&gt;tf.Example&lt;/code&gt;对象，它的proto形式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# proto 
[
  features {
    feature {
      key: "terms"
      value {bytes_list {value: "very" value: "model"}}
    }
    feature {
      key: "frequencies"
      value {float_list {value: 0.3 value: 0.1}}
    }
  },
  features {
    feature {
      key: "terms"
      value {bytes_list {value: "when" value: "course" value: "human"}}
    }
    feature {
      key: "frequencies"
      value {float_list {value: 0.4 value: 0.1 value: 0.2}}
    }
  }
]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;考虑到proto格式熟悉的人不多，我们把上面的内容简化一下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;terms      : ["very", "model"]
frequencies: [  0.3 ,    0.1 ]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;容易看出，这两组数据有伴生关系，下面的代码通过权重类别列函数把该特征组合在一起.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;categorical_column_with_hash_bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;column_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'terms'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hash_bucket_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;weighted_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weighted_categorical_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight_feature_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'frequencies'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;weighted_column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;make_parse_example_spec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;linear_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;linear_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="shi yong te zheng lie"&gt;使用特征列&lt;/h3&gt;
&lt;p&gt;我们须把多个特征列封装成一个列表，才能作为参数拿给估计器(estimator)用。
在使用特征列时，要注意区分特征列类型和模型类型。特征列只有两种类型，类别列和密集列；
模型也分两种，线性模型和深度模型。具体如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LinearClassifier&lt;/code&gt; 和 &lt;code&gt;LinearRegressor&lt;/code&gt;:&lt;ul&gt;
&lt;li&gt;适用所有类型的特征列&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DNNClassifier&lt;/code&gt; 和 &lt;code&gt;DNNRegressor&lt;/code&gt;:&lt;ul&gt;
&lt;li&gt;仅适用于密集列，如要使用类别列，须经过 &lt;code&gt;indicator_column&lt;/code&gt; or或&lt;code&gt;embedding_column&lt;/code&gt;做二次封装&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DNNLinearCombinedClassifier&lt;/code&gt; 和&lt;code&gt;DNNLinearCombinedRegressor&lt;/code&gt;:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;linear_feature_columns&lt;/code&gt; 参数适用所有类型特征列.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dnn_feature_columns&lt;/code&gt; 参数仅适用密集列, 用法和 &lt;code&gt;DNNClassifier&lt;/code&gt; 及 &lt;code&gt;DNNRegressor&lt;/code&gt;的用法一致.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;DNNLinearCombinedClassifier&lt;/code&gt;的代码举例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNNLinearCombinedClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'/tmp/census_model'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;linear_feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;base_columns&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;crossed_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dnn_feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;deep_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dnn_hidden_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="mo xing ding yi_1"&gt;模型定义&lt;/h2&gt;
&lt;h3 id="yu ding yi"&gt;预定义&lt;/h3&gt;
&lt;p align="center"&gt;
&lt;img src="/images/image2.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;预定义模型没什么好讲，看看文档就能秒懂，如下面的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 含2个隐藏层的深度神经网络&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNNClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# 定义好的特征列 &lt;/span&gt;
   &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;           &lt;span class="c1"&gt;# 两个隐藏层, 每层10个神经元&lt;/span&gt;
   &lt;span class="n"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                     &lt;span class="c1"&gt;# 输出3个类别&lt;/span&gt;
   &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# 存 checkpoints 的路径&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="zi ding yi"&gt;自定义&lt;/h3&gt;
&lt;p&gt;写自定义模型时，其实和传统的写法差不多，只是有些小地方要注意一下。&lt;/p&gt;
&lt;p&gt;基本思路是，定义模型函数，它接收从input_fn()传来的特征和标签，输出由tf.estimator.EstimatorSpec封装后的结果, 
函数体主要做两件事情，一件是定义模型，一件是通过分支语句分别实现训练、评估和预测。&lt;/p&gt;
&lt;p&gt;我喜欢的结构是把模型单独定义成一个类，然后再用mode_fn()来调用它, 
mode_fn()的返回用tf.estimator.EstimatorSpec封装，详见下面的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Sample_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                       &lt;span class="c1"&gt;# batch数量的特征，是input_fn 函数的输出&lt;/span&gt;
   &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                         &lt;span class="c1"&gt;# batch数量的标签，是input_fn 函数的输出&lt;/span&gt;
   &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;                          &lt;span class="c1"&gt;# tf.estimator.ModeKeys.TRAIN / EVAL / PREDICT&lt;/span&gt;

    &lt;span class="c1"&gt;# 用特征列（feature_columns)定义输入层 &lt;/span&gt;
    &lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# 定义模型实例 &lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Sample_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TRAIN&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

        &lt;span class="c1"&gt;# 给训练准确度命名，并使它被tf日志记录&lt;/span&gt;
        &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;identity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'train_accuracy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'train_accuracy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PREDICT&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EVAL&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你可能发现，我好像没用到特征列。
如果在自定义的模型中想用特征列这个工具（再次强调，不是必须），只需在模型的输入层调用下面这个函数即可：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.feature_column.linear_model(features, feature_columns, ...)&lt;/code&gt;：如果定义线性模型的话用这个，输出是预测结果.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.feature_column.input_layer(features, feature_columns, ...)&lt;/code&gt;：深度模型用这个.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中features 来自input_fn 的输出， feature_columns 是由多个特征列组成的列表。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 自定义一个超简单的模型&lt;/span&gt;
&lt;span class="c1"&gt;# 输入层&lt;/span&gt;
&lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 隐藏层: h1，h2&lt;/span&gt;
&lt;span class="c1"&gt;# 10个神经元，relu激活函数，input_layer作为输入参数&lt;/span&gt;
&lt;span class="n"&gt;h1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 输出层，3个输出&lt;/span&gt;
&lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;合并写法&lt;/h4&gt;
&lt;p&gt;这是tensorflow 官网给出的一种写法, 只用了一个return, 返回内容的判断放在了前面的 if 分支，你可以根据自己的喜好选择不同写法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TRAIN&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt;
      &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EVAL&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TRAIN&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PREDICT&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="zu zhuang_1"&gt;组装&lt;/h2&gt;
&lt;p&gt;准备好上面的内容后，就可以把model_fn()和input_fn组装在一起了。方法是用estimator实例化
估计器对象，然后用这个对象分别进行训练、评估、预测即可。&lt;/p&gt;
&lt;p&gt;在组装时，我们还要加入一些常规的东西。比如设置checkpoint的保存规则，定义一些观测变量，
方便在训练时用TensorBoard观察，还有超参数等等。具体请看示例注释：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 设置tf输出哪种类别的日志，不同类别详细程度不同&lt;/span&gt;
&lt;span class="c1"&gt;# tf.logging.后的可选值为DEBUG, INFO, WARN, ERROR, or FATAL.&lt;/span&gt;
&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# TensorFlow 版本检查，estimator 要求1.4以上 &lt;/span&gt;
&lt;span class="n"&gt;tf_version&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__version__&lt;/span&gt;
&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"TensorFlow version: {}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf_version&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="s2"&gt;"1.4"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;tf_version&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"TensorFlow r1.4 or later is needed"&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_function&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="c1"&gt;# flags 携带准备传入模型函数和输入函数的参数。 &lt;/span&gt;

  &lt;span class="c1"&gt;# 设置训练时每隔多少秒保存一下checkpoint.&lt;/span&gt;
  &lt;span class="n"&gt;run_config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RunConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;save_checkpoints_secs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="c1"&gt;# 生成classifier实例&lt;/span&gt;
  &lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Estimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;model_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run_config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="s1"&gt;'resnet_size'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;'data_format'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_format&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;'batch_size'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="c1"&gt;# 每训练 flags.epochs_per_eval 轮更新一下日志内容.&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_epochs&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs_per_eval&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tensors_to_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;'learning_rate'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'learning_rate'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;'cross_entropy'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'cross_entropy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;'train_accuracy'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'train_accuracy'&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="c1"&gt;# 设置每跑100个迭代器，打印一下日志。&lt;/span&gt;
    &lt;span class="n"&gt;logging_hook&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LoggingTensorHook&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;tensors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tensors_to_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Starting a training cycle.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn_train&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs_per_eval&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_fn_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hooks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;logging_hook&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Starting to evaluate.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# 评估模型并打印结果&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn_eval&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;eval_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_fn_eval&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="duo gpuxia de xie fa"&gt;多GPU下的写法&lt;/h2&gt;
&lt;p&gt;在多GPU的情况下，代码需要做几点小变化。&lt;/p&gt;
&lt;p&gt;首先, 检查batch_size的数量，它必须能被GPU的数量整除。其目的是让每批的输入数量被平均分配到各个GPU上。&lt;/p&gt;
&lt;p&gt;其次，在model_fn函数中的训练部分封装优化器。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;          &lt;span class="c1"&gt;# 原优化器的定义不变&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'multi_gpu'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
      &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TowerOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，在main函数部分封装模型函数(model_fn).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multi_gpu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;model_function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replicate_model_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss_reduction&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Reduction&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MEAN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="qi ta"&gt;其他&lt;/h2&gt;
&lt;h3 id="san ge import"&gt;三个import&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;absolute_import&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;常看到这三个置于顶上的import 语句，一直没有深究它们有什么用，今天查了下资料，发现这三句都是针对python 2.X版本的情况，分别作用如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;print_function: print语句必须使用函数形式，而 print &amp;lsquo;test&amp;rsquo;这句在这种条件下就会报错。&lt;/li&gt;
&lt;li&gt;division: 精确除法，即python2.x版本中，3/4=0（截断除法），有了这句, 3/4=0.75， 而3//4=0&lt;/li&gt;
&lt;li&gt;absolute_import: 绝对路径，解决自定义包与缺省包名字冲突的问题，如你不小心自定义了string，有了这句， import string时引用系统的，没有这句就引用你本地的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="guan yu chao can shu"&gt;关于超参数&lt;/h3&gt;
&lt;p&gt;既然训练始终要调参，不如把参数提前定义好，比如下面这个参数类。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MymodelArgParser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;argparse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MymodelArgParser&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s1"&gt;'--multi_gpu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'store_true'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'If set, run across all available GPUs.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s1"&gt;'--batch_size'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Number of images to process in a batch'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;还有一种方法也比较优雅，它把参数存成json文件，运行时载入参数即可，见下面的几个功能函数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;codecs&lt;/span&gt;

&lt;span class="c1"&gt;# 从指定目录载入参数&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;load_hparams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;"""Load hparams from an existing model directory."""&lt;/span&gt;
  &lt;span class="n"&gt;hparams_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"hparams"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"# Loading hparams from &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;codecs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getreader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"utf-8"&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"rb"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;hparams_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;hparams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HParams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;hparams_values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"  can't load hparams file"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;hparams&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

&lt;span class="c1"&gt;# 保存参数到json文件&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;save_hparams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;"""Save hparams."""&lt;/span&gt;
  &lt;span class="n"&gt;hparams_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"hparams"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"  saving hparams to &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;codecs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getwriter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"utf-8"&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"wb"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_json&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# 用hparams_path里的新值覆盖hparams的老值&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maybe_parse_standard_hparams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;hparams&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"# Loading standard hparams from &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"r"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;hparams&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="zhu cheng xu ru kou"&gt;主程序入口&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FLAGS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_function&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;parser&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MymodelArgParser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;FLAGS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;unparsed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_known_args&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;unparsed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>机器学习术语表</title><link href="https://freeopen.github.io/posts/ji-qi-xue-xi-zhu-yu-biao" rel="alternate"></link><published>2018-02-26T00:00:00+08:00</published><updated>2018-02-26T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2018-02-26:/posts/ji-qi-xue-xi-zhu-yu-biao</id><summary type="html">&lt;p&gt;&lt;a href="https://developers.google.com/machine-learning/glossary/"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为什么做双语版本？当你读论文发现陌生英文术语时，就知道它的好处了。
有不准确的地方请来信告知，我会即时更正。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本术语表定义了一般机器学习术语以及特定于 TensorFlow 的术语。&lt;/p&gt;
&lt;h2 id="a"&gt;A&lt;/h2&gt;
&lt;h3 id="a/b testing"&gt;A/B testing&lt;/h3&gt;
&lt;p&gt;A statistical way of comparing two (or more) techniques, typically an incumbent against a new rival. A/B testing aims to determine not only which technique performs better but also to understand whether the difference is statistically …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://developers.google.com/machine-learning/glossary/"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为什么做双语版本？当你读论文发现陌生英文术语时，就知道它的好处了。
有不准确的地方请来信告知，我会即时更正。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本术语表定义了一般机器学习术语以及特定于 TensorFlow 的术语。&lt;/p&gt;
&lt;h2 id="a"&gt;A&lt;/h2&gt;
&lt;h3 id="a/b testing"&gt;A/B testing&lt;/h3&gt;
&lt;p&gt;A statistical way of comparing two (or more) techniques, typically an incumbent against a new rival. A/B testing aims to determine not only which technique performs better but also to understand whether the difference is statistically significant. A/B testing usually considers only two techniques using one measurement, but it can be applied to any finite number of techniques and measures.&lt;/p&gt;
&lt;p&gt;一种统计方法，用于将两种或多种技术进行比较，通常是将当前采用的技术与新技术进行比较。A/B 测试不仅旨在确定哪种技术的效果更好，而且还有助于了解相应差异是否具有显著的统计意义。A/B 测试通常是采用一种衡量方式对两种技术进行比较，但也适用于任意有限数量的技术和衡量方式。&lt;/p&gt;
&lt;h3 id="accuracy"&gt;accuracy&lt;/h3&gt;
&lt;p&gt;The fraction of predictions that a &lt;a href="#classification model"&gt;&lt;strong&gt;classification model&lt;/strong&gt;&lt;/a&gt; got right. In &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification&lt;/strong&gt;&lt;/a&gt;, accuracy is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$ \text{Accuracy} = \frac{\text{Correct Predictions}} {\text{Total Number Of Examples}} $$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt;, accuracy has the following definition:&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}} {\text{Total Number Of Examples}}$$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;See &lt;a href="#true positive (tp)"&gt;&lt;strong&gt;true positive&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#true negative (tn)"&gt;&lt;strong&gt;true negative&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;的正确预测所占的比例。在&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类&lt;/strong&gt;&lt;/a&gt;中，准确率的定义如下：&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{准确率} = \frac{\text{正确的预测数}} {\text{样本总数}}$$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;中，准确率的定义如下：&lt;/p&gt;
&lt;p&gt;&lt;mj&gt; &lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{准确率} = \frac{\text{真正例数} + \text{真负例数}} {\text{样本总数}}$$&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;请参阅&lt;a href="#true positive (tp)"&gt;&lt;strong&gt;真正例&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#true negative (tn)"&gt;&lt;strong&gt;真负例&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="activation function"&gt;activation function&lt;/h3&gt;
&lt;p&gt;A function (for example, &lt;a href="#rectified linear unit (relu)"&gt;&lt;strong&gt;ReLU&lt;/strong&gt;&lt;/a&gt; or &lt;a href="#sigmoid function"&gt;&lt;strong&gt;sigmoid&lt;/strong&gt;&lt;/a&gt;) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer.&lt;/p&gt;
&lt;p&gt;一种函数（例如 &lt;a href="#rectified linear unit (relu)"&gt;&lt;strong&gt;ReLU&lt;/strong&gt;&lt;/a&gt; 或 &lt;a href="#sigmoid function"&gt;&lt;strong&gt;S 型&lt;/strong&gt;&lt;/a&gt;函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。&lt;/p&gt;
&lt;h3 id="adagrad"&gt;AdaGrad&lt;/h3&gt;
&lt;p&gt;A sophisticated gradient descent algorithm that rescales the gradients of each parameter, effectively giving each parameter an independent &lt;a href="#learning rate"&gt;&lt;strong&gt;learning rate&lt;/strong&gt;&lt;/a&gt;. For a full explanation, see &lt;a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种先进的梯度下降法，用于重新调整每个参数的梯度，以便有效地为每个参数指定独立的&lt;a href="#learning rate"&gt;&lt;strong&gt;学习速率&lt;/strong&gt;&lt;/a&gt;。如需查看完整的解释，请参阅&lt;a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf"&gt;这篇论文&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="auc (area under the roc curve)"&gt;AUC (Area under the ROC Curve)&lt;/h3&gt;
&lt;p&gt;An evaluation metric that considers all possible &lt;a href="#classification threshold"&gt;&lt;strong&gt;classification thresholds&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Area Under the &lt;a href="#roc-receiver-operating-characteristic-curve"&gt;ROC curve&lt;/a&gt; is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.&lt;/p&gt;
&lt;p&gt;一种会考虑所有可能&lt;a href="#classification threshold"&gt;&lt;strong&gt;分类阈值&lt;/strong&gt;&lt;/a&gt;的评估指标。&lt;/p&gt;
&lt;p&gt;&lt;a href="#roc-receiver-operating-characteristic-curve"&gt;ROC 曲线&lt;/a&gt;下面积是，对于随机选择的正类别样本确实为正类别，以及随机选择的负类别样本为正类别，分类器更确信前者的概率。&lt;/p&gt;
&lt;h2 id="b_1"&gt;B&lt;/h2&gt;
&lt;h3 id="backpropagation"&gt;backpropagation&lt;/h3&gt;
&lt;p&gt;The primary algorithm for performing &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; on &lt;a href="#neural network"&gt;&lt;strong&gt;neural networks&lt;/strong&gt;&lt;/a&gt;. First, the output values of each node are calculated (and cached) in a forward pass. Then, the &lt;a href="https://en.wikipedia.org/wiki/Partial_derivative"&gt;partial derivative&lt;/a&gt; of the error with respect to each parameter is calculated in a backward pass through the graph.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;上执行&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的&lt;a href="https://en.wikipedia.org/wiki/Partial_derivative"&gt;偏导数&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="baseline"&gt;baseline&lt;/h3&gt;
&lt;p&gt;A simple &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; or heuristic used as reference point for comparing how well a model is performing. A baseline helps model developers quantify the minimal, expected performance on a particular problem.&lt;/p&gt;
&lt;p&gt;一种简单的&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;或启发法，用作比较模型效果时的参考点。基准有助于模型开发者针对特定问题量化最低预期效果。&lt;/p&gt;
&lt;h3 id="batch"&gt;batch&lt;/h3&gt;
&lt;p&gt;The set of examples used in one &lt;a href="#iteration"&gt;&lt;strong&gt;iteration&lt;/strong&gt;&lt;/a&gt; (that is, one &lt;a href="#gradient"&gt;&lt;strong&gt;gradient&lt;/strong&gt;&lt;/a&gt; update) of &lt;a href="#model training"&gt;&lt;strong&gt;model training&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See also &lt;a href="#batch size"&gt;&lt;strong&gt;batch size&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#model training"&gt;&lt;strong&gt;模型训练&lt;/strong&gt;&lt;/a&gt;的一次&lt;a href="#iteration"&gt;&lt;strong&gt;迭代&lt;/strong&gt;&lt;/a&gt;（即一次&lt;a href="#gradient"&gt;&lt;strong&gt;梯度&lt;/strong&gt;&lt;/a&gt;更新）中使用的样本集。&lt;/p&gt;
&lt;p&gt;另请参阅&lt;a href="#batch size"&gt;&lt;strong&gt;批次规模&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="batch size"&gt;batch size&lt;/h3&gt;
&lt;p&gt;The number of examples in a &lt;a href="#batch"&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/a&gt;. For example, the batch size of &lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;SGD&lt;/strong&gt;&lt;/a&gt; is 1, while the batch size of a &lt;a href="#mini-batch"&gt;&lt;strong&gt;mini-batch&lt;/strong&gt;&lt;/a&gt; is usually between 10 and 1000. Batch size is usually fixed during training and inference; however, TensorFlow does permit dynamic batch sizes.&lt;/p&gt;
&lt;p&gt;一个&lt;a href="#batch"&gt;&lt;strong&gt;批次&lt;/strong&gt;&lt;/a&gt;中的样本数。例如，&lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;SGD&lt;/strong&gt;&lt;/a&gt; 的批次规模为 1，而&lt;a href="#mini-batch"&gt;&lt;strong&gt;小批次&lt;/strong&gt;&lt;/a&gt;的规模通常介于 10 到 1000 之间。批次规模在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次规模。&lt;/p&gt;
&lt;h3 id="bias"&gt;bias&lt;/h3&gt;
&lt;p&gt;An intercept or offset from an origin. Bias (also known as the &lt;strong&gt;bias term&lt;/strong&gt;) is referred to as &lt;span class="math"&gt;\(b\)&lt;/span&gt; or &lt;span class="math"&gt;\(w_0\)&lt;/span&gt; in machine learning models. For example, bias is the &lt;em&gt;b&lt;/em&gt; in the following formula:&lt;/p&gt;
&lt;p&gt;距离原点的截距或偏移。偏差（也称为&lt;strong&gt;偏差项&lt;/strong&gt;）在机器学习模型中以 &lt;span class="math"&gt;\(b\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(w_0\)&lt;/span&gt; 表示。例如，在下面的公式中，偏差为 b：&lt;/p&gt;
&lt;div class="math"&gt;$$y' = b + w_1x_1 + w_2x_2 + &amp;hellip; w_nx_n$$&lt;/div&gt;
&lt;p&gt;Not to be confused with &lt;a href="#prediction bias"&gt;&lt;strong&gt;prediction bias&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;请勿与&lt;a href="#prediction bias"&gt;&lt;strong&gt;预测偏差&lt;/strong&gt;&lt;/a&gt;混淆。&lt;/p&gt;
&lt;h3 id="binary classification"&gt;binary classification&lt;/h3&gt;
&lt;p&gt;A type of classification task that outputs one of two mutually exclusive classes. For example, a machine learning model that evaluates email messages and outputs either "spam" or "not spam" is a binary classifier.&lt;/p&gt;
&lt;p&gt;一种分类任务，可输出两种互斥类别之一。例如，对电子邮件进行评估并输出&amp;ldquo;垃圾邮件&amp;rdquo;或&amp;ldquo;非垃圾邮件&amp;rdquo;的机器学习模型就是一个二元分类器。&lt;/p&gt;
&lt;h3 id="binning"&gt;binning&lt;/h3&gt;
&lt;p&gt;See &lt;a href="#bucketing"&gt;&lt;strong&gt;bucketing&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="bucketing"&gt;bucketing&lt;/h3&gt;
&lt;p&gt;Converting a (usually &lt;a href="#continuous feature"&gt;&lt;strong&gt;continuous&lt;/strong&gt;&lt;/a&gt;) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin.&lt;/p&gt;
&lt;p&gt;将一个特征（通常是&lt;a href="#continuous feature"&gt;&lt;strong&gt;连续&lt;/strong&gt;&lt;/a&gt;特征）转换成多个二元特征（称为桶或箱），通常是根据值区间进行转换。例如，您可以将温度区间分割为离散分箱，而不是将温度表示成单个连续的浮点特征。假设温度数据可精确到小数点后一位，则可以将介于 0.0 到 15.0 度之间的所有温度都归入一个分箱，将介于 15.1 到 30.0 度之间的所有温度归入第二个分箱，并将介于 30.1 到 50.0 度之间的所有温度归入第三个分箱。&lt;/p&gt;
&lt;h2 id="c_1"&gt;C&lt;/h2&gt;
&lt;h3 id="calibration layer"&gt;calibration layer&lt;/h3&gt;
&lt;p&gt;A post-prediction adjustment, typically to account for &lt;a href="#prediction bias"&gt;&lt;strong&gt;prediction bias&lt;/strong&gt;&lt;/a&gt;. The adjusted predictions and probabilities should match the distribution of an observed set of labels.&lt;/p&gt;
&lt;p&gt;一种预测后调整，通常是为了降低&lt;a href="#prediction bias"&gt;&lt;strong&gt;预测偏差&lt;/strong&gt;&lt;/a&gt;。调整后的预测和概率应与观察到的标签集的分布一致。&lt;/p&gt;
&lt;h3 id="candidate sampling"&gt;candidate sampling&lt;/h3&gt;
&lt;p&gt;A training-time optimization in which a probability is calculated for all the positive labels, using, for example, softmax, but only for a random sample of negative labels. For example, if we have an example labeled &lt;em&gt;beagle&lt;/em&gt; and &lt;em&gt;dog&lt;/em&gt; candidate sampling computes the predicted probabilities and corresponding loss terms for the &lt;em&gt;beagle&lt;/em&gt; and &lt;em&gt;dog&lt;/em&gt; class outputs in addition to a random subset of the remaining classes (&lt;em&gt;cat&lt;/em&gt;, &lt;em&gt;lollipop&lt;/em&gt;, &lt;em&gt;fence&lt;/em&gt;). The idea is that the &lt;a href="#negative class"&gt;&lt;strong&gt;negative classes&lt;/strong&gt;&lt;/a&gt; can learn from less frequent negative reinforcement as long as &lt;a href="#positive class"&gt;&lt;strong&gt;positive classes&lt;/strong&gt;&lt;/a&gt; always get proper positive reinforcement, and this is indeed observed empirically. The motivation for candidate sampling is a computational efficiency win from not computing predictions for all negatives.&lt;/p&gt;
&lt;p&gt;一种训练时进行的优化，会使用某种函数（例如 softmax）针对所有正类别标签计算概率，但对于负类别标签，则仅针对其随机样本计算概率。例如，如果某个样本的标签为&amp;ldquo;小猎犬&amp;rdquo;和&amp;ldquo;狗&amp;rdquo;，则候选采样将针对&amp;ldquo;小猎犬&amp;rdquo;和&amp;ldquo;狗&amp;rdquo;类别输出以及其他类别（猫、棒棒糖、栅栏）的随机子集计算预测概率和相应的损失项。这种采样基于的想法是，只要&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;始终得到适当的正增强，&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;就可以从频率较低的负增强中进行学习，这确实是在实际中观察到的情况。候选采样的目的是，通过不针对所有负类别计算预测结果来提高计算效率。&lt;/p&gt;
&lt;h3 id="categorical data"&gt;categorical data&lt;/h3&gt;
&lt;p&gt;&lt;a href="#feature"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/a&gt; having a discrete set of possible values. For example, consider a categorical feature named &lt;code&gt;house style&lt;/code&gt;, which has a discrete set of three possible values: &lt;code&gt;Tudor, ranch, colonial&lt;/code&gt;. By representing &lt;code&gt;house style&lt;/code&gt; as categorical data, the model can learn the separate impacts of &lt;code&gt;Tudor&lt;/code&gt;, &lt;code&gt;ranch&lt;/code&gt;, and &lt;code&gt;colonial&lt;/code&gt; on house price.&lt;/p&gt;
&lt;p&gt;Sometimes, values in the discrete set are mutually exclusive, and only one value can be applied to a given example. For example, a &lt;code&gt;car maker&lt;/code&gt; categorical feature would probably permit only a single value (&lt;code&gt;Toyota&lt;/code&gt;) per example. Other times, more than one value may be applicable. A single car could be painted more than one different color, so a &lt;code&gt;car color&lt;/code&gt; categorical feature would likely permit a single example to have multiple values (for example, &lt;code&gt;red&lt;/code&gt; and &lt;code&gt;white&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Categorical features are sometimes called &lt;a href="#discrete feature"&gt;&lt;strong&gt;discrete features&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#numerical data"&gt;&lt;strong&gt;numerical data&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，拥有一组离散的可能值。以某个名为 &lt;code&gt;house style&lt;/code&gt; 的分类特征为例，该特征拥有一组离散的可能值（共三个），即 &lt;code&gt;Tudor, ranch, colonial&lt;/code&gt;。通过将 &lt;code&gt;house style&lt;/code&gt; 表示成分类数据，相应模型可以学习 &lt;code&gt;Tudor&lt;/code&gt;、&lt;code&gt;ranch&lt;/code&gt; 和 &lt;code&gt;colonial&lt;/code&gt; 分别对房价的影响。&lt;/p&gt;
&lt;p&gt;有时，离散集中的值是互斥的，只能将其中一个值应用于指定样本。例如，&lt;code&gt;car maker&lt;/code&gt; 分类特征可能只允许一个样本有一个值 (&lt;code&gt;Toyota&lt;/code&gt;)。在其他情况下，则可以应用多个值。一辆车可能会被喷涂多种不同的颜色，因此，&lt;code&gt;car color&lt;/code&gt; 分类特征可能会允许单个样本具有多个值（例如 &lt;code&gt;red&lt;/code&gt; 和 &lt;code&gt;white&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;分类特征有时称为&lt;a href="#discrete feature"&gt;&lt;strong&gt;离散特征&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#numerical data"&gt;&lt;strong&gt;数值数据&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="checkpoint"&gt;checkpoint&lt;/h3&gt;
&lt;p&gt;Data that captures the state of the variables of a model at a particular time. Checkpoints enable exporting model &lt;a href="#weight"&gt;&lt;strong&gt;weights&lt;/strong&gt;&lt;/a&gt;, as well as performing training across multiple sessions. Checkpoints also enable training to continue past errors (for example, job preemption). Note that the &lt;a href="#graph"&gt;&lt;strong&gt;graph&lt;/strong&gt;&lt;/a&gt; itself is not included in a checkpoint.&lt;/p&gt;
&lt;p&gt;一种数据，用于捕获模型变量在特定时间的状态。借助检查点，可以导出模型&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;，跨多个会话执行训练，以及使训练在发生错误之后得以继续（例如作业抢占）。请注意，&lt;a href="#graph"&gt;&lt;strong&gt;图&lt;/strong&gt;&lt;/a&gt;本身不包含在检查点中。&lt;/p&gt;
&lt;h3 id="class"&gt;class&lt;/h3&gt;
&lt;p&gt;One of a set of enumerated target values for a label. For example, in a &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt; model that detects spam, the two classes are &lt;em&gt;spam&lt;/em&gt; and &lt;em&gt;not spam&lt;/em&gt;. In a &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification&lt;/strong&gt;&lt;/a&gt; model that identifies dog breeds, the classes would be &lt;em&gt;poodle&lt;/em&gt;, &lt;em&gt;beagle&lt;/em&gt;, &lt;em&gt;pug&lt;/em&gt;, and so on.&lt;/p&gt;
&lt;p&gt;为标签枚举的一组目标值中的一个。例如，在检测垃圾邮件的&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;模型中，两种类别分别是&amp;ldquo;垃圾邮件&amp;rdquo;和&amp;ldquo;非垃圾邮件&amp;rdquo;。在识别狗品种的&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类&lt;/strong&gt;&lt;/a&gt;模型中，类别可以是&amp;ldquo;贵宾犬&amp;rdquo;、&amp;ldquo;小猎犬&amp;rdquo;、&amp;ldquo;哈巴犬&amp;rdquo;等等。&lt;/p&gt;
&lt;h3 id="class-imbalanced data set"&gt;class-imbalanced data set&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt; problem in which the &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt; for the two classes have significantly different frequencies. For example, a disease data set in which 0.0001 of examples have positive labels and 0.9999 have negative labels is a class-imbalanced problem, but a football game predictor in which 0.51 of examples label one team winning and 0.49 label the other team winning is &lt;em&gt;not&lt;/em&gt; a class-imbalanced problem.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;问题问题中，两种类别的&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;在出现频率方面具有很大的差距。例如，在某个疾病数据集中，0.0001 的样本具有正类别标签，0.9999 的样本具有负类别标签，这就属于分类不平衡问题；但在某个足球比赛预测器中，0.51 的样本的标签为其中一个球队赢，0.49 的样本的标签为另一个球队赢，这就不属于分类不平衡问题。&lt;/p&gt;
&lt;h3 id="classification model"&gt;classification model&lt;/h3&gt;
&lt;p&gt;A type of machine learning model for distinguishing among two or more discrete classes. For example, a natural language processing classification model could determine whether an input sentence was in French, Spanish, or Italian. Compare with &lt;a href="#regression model"&gt;&lt;strong&gt;regression model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种机器学习模型，用于区分两种或多种离散类别。例如，某个自然语言处理分类模型可以确定输入的句子是法语、西班牙语还是意大利语。请与&lt;a href="#regression model"&gt;&lt;strong&gt;回归模型&lt;/strong&gt;&lt;/a&gt;进行比较。&lt;/p&gt;
&lt;h3 id="classification threshold"&gt;classification threshold&lt;/h3&gt;
&lt;p&gt;A scalar-value criterion that is applied to a model's predicted score in order to separate the &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt; from the &lt;a href="#negative class"&gt;&lt;strong&gt;negative class&lt;/strong&gt;&lt;/a&gt;. Used when mapping &lt;a href="#logistic regression"&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt; results to &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt;. For example, consider a logistic regression model that determines the probability of a given email message being spam. If the classification threshold is 0.9, then logistic regression values above 0.9 are classified as &lt;em&gt;spam&lt;/em&gt; and those below 0.9 are classified as &lt;em&gt;not spam&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;一种标量值条件，应用于模型预测的得分，旨在将&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;与&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;区分开。将&lt;a href="#logistic regression"&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/a&gt;结果映射到&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;时使用。以某个逻辑回归模型为例，该模型用于确定指定电子邮件是垃圾邮件的概率。如果分类阈值为 0.9，那么逻辑回归值高于 0.9 的电子邮件将被归类为&amp;ldquo;垃圾邮件&amp;rdquo;，低于 0.9 的则被归类为&amp;ldquo;非垃圾邮件&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="collaborative filtering"&gt;collaborative filtering&lt;/h3&gt;
&lt;p&gt;Making predictions about the interests of one user based on the interests of many other users. Collaborative filtering is often used in recommendation systems.&lt;/p&gt;
&lt;p&gt;根据很多其他用户的兴趣来预测某位用户的兴趣。协同过滤通常用在推荐系统中。&lt;/p&gt;
&lt;h3 id="confusion matrix"&gt;confusion matrix&lt;/h3&gt;
&lt;p&gt;An NxN table that summarizes how successful a &lt;a href="#classification model"&gt;&lt;strong&gt;classification model's&lt;/strong&gt;&lt;/a&gt; predictions were; that is, the correlation between the label and the model's classification. One axis of a confusion matrix is the label that the model predicted, and the other axis is the actual label. N represents the number of classes. In a &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt; problem, N=2. For example, here is a sample confusion matrix for a binary classification problem:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;Tumor (predicted)&lt;/th&gt;
&lt;th align="center"&gt;Non-Tumor (predicted)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Tumor (actual)&lt;/td&gt;
&lt;td align="center"&gt;18&lt;/td&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Non-Tumor (actual)&lt;/td&gt;
&lt;td align="center"&gt;6&lt;/td&gt;
&lt;td align="center"&gt;452&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The preceding confusion matrix shows that of the 19 samples that actually had tumors, the model correctly classified 18 as having tumors (18 true positives), and incorrectly classified 1 as not having a tumor (1 false negative). Similarly, of 458 samples that actually did not have tumors, 452 were correctly classified (452 true negatives) and 6 were incorrectly classified (6 false positives).&lt;/p&gt;
&lt;p&gt;The confusion matrix for a multi-class classification problem can help you determine mistake patterns. For example, a confusion matrix could reveal that a model trained to recognize handwritten digits tends to mistakenly predict 9 instead of 4, or 1 instead of 7.&lt;/p&gt;
&lt;p&gt;Confusion matrices contain sufficient information to calculate a variety of performance metrics, including &lt;a href="#precision"&gt;&lt;strong&gt;precision&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#recall"&gt;&lt;strong&gt;recall&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种 NxN 表格，用于总结&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;的预测成效；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;问题中，N=2。例如，下面显示了一个二元分类问题的混淆矩阵示例：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;肿瘤(预测)&lt;/th&gt;
&lt;th align="center"&gt;非肿瘤(预测)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;肿瘤  (实际)&lt;/td&gt;
&lt;td align="center"&gt;18&lt;/td&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;非肿瘤(实际)&lt;/td&gt;
&lt;td align="center"&gt;6&lt;/td&gt;
&lt;td align="center"&gt;452&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;上面的混淆矩阵显示，在 19 个实际有肿瘤的样本中，该模型正确地将 18 个归类为有肿瘤（18 个真正例），错误地将 1 个归类为没有肿瘤（1 个假负例）。同样，在 458 个实际没有肿瘤的样本中，模型归类正确的有 452 个（452 个真负例），归类错误的有 6 个（6 个假正例）。&lt;/p&gt;
&lt;p&gt;多类别分类问题的混淆矩阵有助于确定出错模式。例如，某个混淆矩阵可以揭示，某个经过训练以识别手写数字的模型往往会将 4 错误地预测为 9，将 7 错误地预测为 1。&lt;/p&gt;
&lt;p&gt;混淆矩阵包含计算各种效果指标（包括&lt;a href="#precision"&gt;&lt;strong&gt;精确率&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#recall"&gt;&lt;strong&gt;召回率&lt;/strong&gt;&lt;/a&gt;）所需的充足信息。&lt;/p&gt;
&lt;h3 id="continuous feature"&gt;continuous feature&lt;/h3&gt;
&lt;p&gt;A floating-point feature with an infinite range of possible values. Contrast with &lt;a href="#discrete feature"&gt;&lt;strong&gt;discrete feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种浮点特征，可能值的区间不受限制。与&lt;a href="#discrete feature"&gt;&lt;strong&gt;离散特征&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="convergence"&gt;convergence&lt;/h3&gt;
&lt;p&gt;Informally, often refers to a state reached during training in which training &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence.&lt;/p&gt;
&lt;p&gt;See also &lt;a href="#early stopping"&gt;&lt;strong&gt;early stopping&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See also Boyd and Vandenberghe, &lt;a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"&gt;Convex Optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。&lt;/p&gt;
&lt;p&gt;另请参阅&lt;a href="#early stopping"&gt;&lt;strong&gt;早停法&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另请参阅 Boyd 和 Vandenberghe 合著的 &lt;a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"&gt;Convex Optimization&lt;/a&gt;（《凸优化》）。&lt;/p&gt;
&lt;h3 id="convex function"&gt;convex function&lt;/h3&gt;
&lt;p&gt;A function in which the region above the graph of the function is a &lt;a href="#convex set"&gt;&lt;strong&gt;convex set&lt;/strong&gt;&lt;/a&gt;. The prototypical convex function is shaped something like the letter &lt;strong&gt;U&lt;/strong&gt;. For example, the following are all convex functions:&lt;/p&gt;
&lt;p&gt;一种函数，函数图像以上的区域为&lt;a href="#convex set"&gt;&lt;strong&gt;凸集&lt;/strong&gt;&lt;/a&gt;。典型凸函数的形状类似于字母 &lt;strong&gt;U&lt;/strong&gt;。例如，以下都是凸函数：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/convex_functions.png" width="90%"/&gt;
&lt;br/&gt;
A typical convex function is shaped like the letter 'U'.
&lt;/p&gt;
&lt;p&gt;By contrast, the following function is not convex. Notice how the region above the graph is not a convex set:&lt;/p&gt;
&lt;p&gt;相反，以下函数则不是凸函数。请注意图像上方的区域如何不是凸集：&lt;/p&gt;
&lt;p&gt;
&lt;svg height="299.19577" id="svg2" version="1.1" viewbox="0 0 379.11578 299.19577" width="379.11578" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg"&gt;
&lt;title&gt;Nonconvex function&lt;/title&gt;
&lt;desc&gt;A nonconvex function that looks like a curved "W" character, with two local minima&lt;/desc&gt;
&lt;defs id="defs6"&gt;&lt;clippath clippathunits="userSpaceOnUse" id="clipPath20"&gt;&lt;path d="M 0,0 H 365760 V 274320 H 0 Z" id="path18"&gt;&lt;/path&gt;&lt;/clippath&gt;&lt;/defs&gt;&lt;g id="g10" transform="matrix(1.3333333,0,0,-1.3333333,-345.28663,532.80533)"&gt;&lt;path d="M 0,540 H 719.99856 V 0.00108 H 0 Z" id="path28" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="m 278.33212,392.51998 c 12.48685,-27.90513 53.26204,-155.55219 74.92111,-167.43077 21.65907,-11.87858 37.99042,86.40436 55.03336,96.15926 17.04293,9.7549 33.05274,-49.79059 47.22431,-37.62985 14.17157,12.16073 31.50421,92.1619 37.80504,110.59427" id="path36" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="m 278.33212,392.51998 c 12.48685,-27.90513 53.26204,-155.55219 74.92111,-167.43077 21.65907,-11.87858 37.99042,86.40436 55.03336,96.15926 17.04293,9.7549 33.05274,-49.79059 47.22431,-37.62985 14.17157,12.16073 31.50421,92.1619 37.80504,110.59427" id="path38" style="fill:none;stroke:#4285f4;stroke-width:1.49999702;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 482.16811,278.04383 h 61.13372 v -37.91331 h -61.13372 z" id="path46" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text56" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="488.91809" y="-260.73386"&gt;&lt;tspan id="tspan54" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="488.91809 491.36011 497.47607 502.97607 509.09207" y="-260.73386"&gt;local&lt;/tspan&gt;&lt;/text&gt;
&lt;text id="text60" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="488.91809" y="-247.23389"&gt;&lt;tspan id="tspan58" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="488.91809 498.08109 500.52307 506.63907 509.08105 518.24402 524.36005" y="-247.23389"&gt;minimum&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 452.82959,279.16588 29.33852,-20.0787" id="path70" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="m 459.19298,274.8109 22.97513,-15.72372" id="path72" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 459.19298,274.8109 3.51718,0.65906 -7.16613,1.8382 4.308,-6.01443 z" id="path74" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 264.28807,179.58871 v 220.0153" id="path82" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 264.28807,179.58871 V 391.89309" id="path84" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 264.28807,391.89309 2.53033,-2.53033 -2.53033,6.95198 -2.53033,-6.95198 z" id="path86" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 263.27841,180.53025 H 543.2936" id="path94" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 263.27841,180.53025 H 535.58265" id="path96" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 535.58265,180.53025 -2.53033,-2.53031 6.95195,2.53031 -6.95195,2.53031 z" id="path98" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 382.99136,219.87466 H 444.1251 V 181.96135 H 382.99136 Z" id="path106" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text116" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="389.74133" y="-202.56468"&gt;&lt;tspan id="tspan114" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="389.74133 392.18335 398.29932 403.79932 409.91531 412.3573" y="-202.56468"&gt;local &lt;/tspan&gt;&lt;/text&gt;
&lt;text id="text120" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="389.74133" y="-189.06471"&gt;&lt;tspan id="tspan118" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="389.74133 398.90433 401.34631 407.46231 409.9043 419.06729 425.18326" y="-189.06471"&gt;minimum&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 358.85873,221.41495 388.19726,205.6827" id="path130" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 365.65429,217.77094 388.19726,205.6827" id="path132" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 365.65429,217.77094 3.42568,1.03417 -7.32245,1.05539 4.93094,-5.51524 z" id="path134" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 280.99156,219.87466 H 342.1253 V 181.96135 H 280.99156 Z" id="path142" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text152" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="287.74155" y="-202.56468"&gt;&lt;tspan id="tspan150" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="287.74155 293.85754 296.29953 302.41553 308.53149 314.64749 317.08948" y="-202.56468"&gt;global &lt;/tspan&gt;&lt;/text&gt;
&lt;text id="text156" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="287.74155" y="-189.06471"&gt;&lt;tspan id="tspan154" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="287.74155 296.90454 299.34653 305.46252 307.90451 317.0675 323.18347" y="-189.06471"&gt;minimum&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 354.92356,221.42398 324.80551,206.63661" id="path166" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 348.00192,218.02558 324.80551,206.63661" id="path168" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 348.00192,218.02558 -1.15616,-3.38646 5.12521,5.33518 -7.35556,-0.79256 z" id="path170" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;/g&gt;&lt;/svg&gt;
&lt;/p&gt;
&lt;p&gt;Nonconvex function A nonconvex function that looks like a curved "W" character, with two local minimum.&lt;/p&gt;
&lt;p&gt;非凸函数的形状类似于字母&lt;strong&gt;W&lt;/strong&gt;, 有两个局部最低点.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;strictly convex function&lt;/strong&gt; has exactly one local minimum point, which is also the global minimum point. The classic U-shaped functions are strictly convex functions. However, some convex functions (for example, straight lines) are not.&lt;/p&gt;
&lt;p&gt;A lot of the common &lt;strong&gt;loss functions&lt;/strong&gt;, including the following, are convex functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#l2 loss"&gt;&lt;strong&gt;L2 loss&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#log loss"&gt;&lt;strong&gt;Log Loss&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many variations of &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; are guaranteed to find a point close to the minimum of a strictly convex function. Similarly, many variations of &lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;stochastic gradient descent&lt;/strong&gt;&lt;/a&gt; have a high probability (though, not a guarantee) of finding a point close to the minimum of a strictly convex function.&lt;/p&gt;
&lt;p&gt;The sum of two convex functions (for example, L2 loss + L1 regularization) is a convex function.&lt;/p&gt;
&lt;p&gt;&lt;a href="#deep model"&gt;&lt;strong&gt;Deep models&lt;/strong&gt;&lt;/a&gt; are never convex functions. Remarkably, algorithms designed for &lt;a href="#convex optimization"&gt;&lt;strong&gt;convex optimization&lt;/strong&gt;&lt;/a&gt; tend to find reasonably good solutions on deep networks anyway, even though those solutions are not guaranteed to be a global minimum.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;严格凸函数&lt;/strong&gt;只有一个局部最低点，该点也是全局最低点。经典的 U 形函数都是严格凸函数。不过，有些凸函数（例如直线）则不是这样。&lt;/p&gt;
&lt;p&gt;很多常见的&lt;strong&gt;损失函数&lt;/strong&gt;（包括下列函数）都是凸函数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#l2 loss"&gt;&lt;strong&gt;L2 损失函数&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#log loss"&gt;&lt;strong&gt;对数损失函数&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;的很多变体都一定能找到一个接近严格凸函数最小值的点。同样，&lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;随机梯度下降法&lt;/strong&gt;&lt;/a&gt;的很多变体都有很高的可能性能够找到接近严格凸函数最小值的点（但并非一定能找到）。&lt;/p&gt;
&lt;p&gt;两个凸函数的和（例如 L2 损失函数 + L1 正则化）也是凸函数。&lt;/p&gt;
&lt;p&gt;&lt;a href="#deep model"&gt;&lt;strong&gt;深度模型&lt;/strong&gt;&lt;/a&gt;绝不会是凸函数。值得注意的是，专门针对&lt;a href="#convex optimization"&gt;&lt;strong&gt;凸优化&lt;/strong&gt;&lt;/a&gt;设计的算法往往总能在深度网络上找到非常好的解决方案，虽然这些解决方案并不一定对应于全局最小值。&lt;/p&gt;
&lt;h3 id="convex optimization"&gt;convex optimization&lt;/h3&gt;
&lt;p&gt;The process of using mathematical techniques such as &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; to find the minimum of a &lt;a href="#convex function"&gt;&lt;strong&gt;convex function&lt;/strong&gt;&lt;/a&gt;. A great deal of research in machine learning has focused on formulating various problems as convex optimization problems and in solving those problems more efficiently.&lt;/p&gt;
&lt;p&gt;For complete details, see Boyd and Vandenberghe, &lt;a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"&gt;Convex Optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;使用数学方法（例如&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;）寻找&lt;a href="#convex function"&gt;&lt;strong&gt;凸函数&lt;/strong&gt;&lt;/a&gt;最小值的过程。机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。&lt;/p&gt;
&lt;p&gt;如需完整的详细信息，请参阅 Boyd 和 Vandenberghe 合著的 &lt;a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"&gt;Convex Optimization&lt;/a&gt;（《凸优化》）。&lt;/p&gt;
&lt;h3 id="convex set"&gt;convex set&lt;/h3&gt;
&lt;p&gt;A subset of Euclidean space such that a line drawn between any two points in the subset remains completely within the subset. For instance, the following two shapes are convex sets:&lt;/p&gt;
&lt;p&gt;欧几里德空间的一个子集，其中任意两点之间的连线仍完全落在该子集内。例如，下面的两个图形都是凸集：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/convex_set.png" width="80%"/&gt;
&lt;br/&gt;
A rectangle and a semi-ellipse are both convex sets.
矩形和半椭圆形都是凸集
&lt;/p&gt;
&lt;p&gt;By contrast, the following two shapes are not convex sets:&lt;/p&gt;
&lt;p&gt;相反，下面的两个图形都不是凸集：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/nonconvex_set.png" width="80%"/&gt;
&lt;br/&gt;
A pie-chart with a missing slice and a firework are both nonconvex sets.
缺少一块的饼图以及烟花图都是非凸集
&lt;/p&gt;
&lt;h3 id="cost"&gt;cost&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="cross-entropy"&gt;cross-entropy&lt;/h3&gt;
&lt;p&gt;A generalization of &lt;a href="#log loss"&gt;&lt;strong&gt;Log Loss&lt;/strong&gt;&lt;/a&gt; to &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification problems&lt;/strong&gt;&lt;/a&gt;. Cross-entropy quantifies the difference between two probability distributions. See also &lt;a href="#perplexity"&gt;&lt;strong&gt;perplexity&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#log loss"&gt;&lt;strong&gt;对数损失函数&lt;/strong&gt;&lt;/a&gt;向&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类问题&lt;/strong&gt;&lt;/a&gt;进行的一种泛化。交叉熵可以量化两种概率分布之间的差异。另请参阅&lt;a href="#perplexity"&gt;&lt;strong&gt;困惑度&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="custom estimator"&gt;custom Estimator&lt;/h3&gt;
&lt;p&gt;An &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt; that you write yourself by following &lt;a href="https://www.tensorflow.org/extend/estimators"&gt;these directions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#pre-made estimator"&gt;&lt;strong&gt;pre-made Estimators&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="d_1"&gt;D&lt;/h2&gt;
&lt;h3 id="data set"&gt;data set&lt;/h3&gt;
&lt;p&gt;A collection of &lt;a href="#example"&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一组&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;的集合。&lt;/p&gt;
&lt;h3 id="dataset api (tf.data)"&gt;Dataset API (tf.data)&lt;/h3&gt;
&lt;p&gt;A high-level TensorFlow API for reading data and transforming it into a form that a machine learning algorithm requires. A &lt;code&gt;tf.data.Dataset&lt;/code&gt; object represents a sequence of elements, in which each element contains one or more &lt;a href="#tensor"&gt;&lt;strong&gt;Tensors&lt;/strong&gt;&lt;/a&gt;. A &lt;code&gt;tf.data.Iterator&lt;/code&gt; object provides access to the elements of a &lt;code&gt;Dataset&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For details about the Dataset API, see &lt;a href="https://www.tensorflow.org/programmers_guide/datasets"&gt;Importing Data&lt;/a&gt; in the TensorFlow Programmer's Guide.&lt;/p&gt;
&lt;p&gt;一种高级别的 TensorFlow API，用于读取数据并将其转换为机器学习算法所需的格式。&lt;code&gt;tf.data.Dataset&lt;/code&gt; 对象表示一系列元素，其中每个元素都包含一个或多个&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;。&lt;code&gt;tf.data.Iterator&lt;/code&gt; 对象可获取 &lt;code&gt;Dataset&lt;/code&gt; 中的元素。&lt;/p&gt;
&lt;p&gt;如需详细了解 Dataset API，请参阅《TensorFlow 编程人员指南》中的&lt;a href="https://www.tensorflow.org/programmers_guide/datasets"&gt;导入数据&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="decision boundary"&gt;decision boundary&lt;/h3&gt;
&lt;p&gt;The separator between classes learned by a model in a &lt;a href="#binary classification"&gt;&lt;strong&gt;binary class&lt;/strong&gt;&lt;/a&gt; or &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification problems&lt;/strong&gt;&lt;/a&gt;. For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange class and the blue class:&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;或&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类问题&lt;/strong&gt;&lt;/a&gt;中，模型学到的类别之间的分界线。例如，在以下表示某个二元分类问题的图片中，决策边界是橙色类别和蓝色类别之间的分界线：&lt;/p&gt;
&lt;p&gt;&lt;img alt="A
well-defined boundary between one class and another." src="/images/decision_boundary.png"/&gt;&lt;/p&gt;
&lt;h3 id="dense layer"&gt;dense layer&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#fully connected layer"&gt;&lt;strong&gt;fully connected layer&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#fully connected layer"&gt;&lt;strong&gt;全连接层&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="deep model"&gt;deep model&lt;/h3&gt;
&lt;p&gt;A type of &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt; containing multiple &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layers&lt;/strong&gt;&lt;/a&gt;. Deep models rely on trainable nonlinearities.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#wide model"&gt;&lt;strong&gt;wide model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;，其中包含多个&lt;a href="#hidden layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;。深度模型依赖于可训练的非线性关系。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#wide model"&gt;&lt;strong&gt;宽度模型&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="dense feature"&gt;dense feature&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#feature"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/a&gt; in which most values are non-zero, typically a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt; of floating-point values. Contrast with &lt;a href="#sparse feature"&gt;&lt;strong&gt;sparse feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种大部分数值是非零值的&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，通常是一个浮点值&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;。参照&lt;a href="#sparse features"&gt;&lt;strong&gt;稀疏特征&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="derived feature"&gt;derived feature&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#synthetic feature"&gt;&lt;strong&gt;synthetic feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#synthetic feature"&gt;&lt;strong&gt;合成特征&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="discrete feature"&gt;discrete feature&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#feature"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/a&gt; with a finite set of possible values. For example, a feature whose values may only be &lt;em&gt;animal&lt;/em&gt;, &lt;em&gt;vegetable&lt;/em&gt;, or &lt;em&gt;mineral&lt;/em&gt; is a discrete (or categorical) feature. Contrast with &lt;a href="#continuous feature"&gt;&lt;strong&gt;continuous feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，包含有限个可能值。例如，某个值只能是&amp;ldquo;动物&amp;rdquo;、&amp;ldquo;蔬菜&amp;rdquo;或&amp;ldquo;矿物&amp;rdquo;的特征便是一个离散特征（或分类特征）。与&lt;a href="#continuous feature"&gt;&lt;strong&gt;连续特征&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="dropout regularization"&gt;dropout regularization&lt;/h3&gt;
&lt;p&gt;A form of &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt; useful in training &lt;a href="#neural network"&gt;&lt;strong&gt;neural networks&lt;/strong&gt;&lt;/a&gt;. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. For full details, see &lt;a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf"&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种形式的&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;，在训练&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;方面非常有用。丢弃正则化的运作机制是，在神经网络层的一个梯度步长中移除随机选择的固定数量的单元。丢弃的单元越多，正则化效果就越强。这类似于训练神经网络以模拟较小网络的指数级规模集成学习。如需完整的详细信息，请参阅 &lt;a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf"&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;（《丢弃：一种防止神经网络过拟合的简单方法》）。&lt;/p&gt;
&lt;h3 id="dynamic model"&gt;dynamic model&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; that is trained online in a continuously updating fashion. That is, data is continuously entering the model.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;，以持续更新的方式在线接受训练。也就是说，数据会源源不断地进入这种模型。&lt;/p&gt;
&lt;h2 id="e_1"&gt;E&lt;/h2&gt;
&lt;h3 id="early stopping"&gt;early stopping&lt;/h3&gt;
&lt;p&gt;A method for &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt; that involves ending model training &lt;em&gt;before&lt;/em&gt; training loss finishes decreasing. In early stopping, you end model training when the loss on a &lt;a href="#validation set"&gt;&lt;strong&gt;validation data set&lt;/strong&gt;&lt;/a&gt; starts to increase, that is, when &lt;a href="#generalization"&gt;&lt;strong&gt;generalization&lt;/strong&gt;&lt;/a&gt; performance worsens.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;方法，涉及在训练损失仍可以继续减少之前结束模型训练。使用早停法时，您会在基于&lt;a href="#validation set"&gt;&lt;strong&gt;验证数据集&lt;/strong&gt;&lt;/a&gt;的损失开始增加（也就是&lt;a href="#generalization"&gt;&lt;strong&gt;泛化&lt;/strong&gt;&lt;/a&gt;效果变差）时结束模型训练。&lt;/p&gt;
&lt;h3 id="embeddings"&gt;embeddings&lt;/h3&gt;
&lt;p&gt;A categorical feature represented as a continuous-valued feature. Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. For example, you can represent the words in an English sentence in either of the following two ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As a million-element (high-dimensional) &lt;a href="#sparse feature"&gt;&lt;strong&gt;sparse vector&lt;/strong&gt;&lt;/a&gt; in which all elements are integers. Each cell in the vector represents a separate English word; the value in a cell represents the number of times that word appears in a sentence. Since a single English sentence is unlikely to contain more than 50 words, nearly every cell in the vector will contain a 0. The few cells that aren't 0 will contain a low integer (usually 1) representing the number of times that word appeared in the sentence.&lt;/li&gt;
&lt;li&gt;As a several-hundred-element (low-dimensional) &lt;a href="#dense feature"&gt;&lt;strong&gt;dense vector&lt;/strong&gt;&lt;/a&gt; in which each element holds a floating-point value between 0 and 1. This is an embedding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In TensorFlow, embeddings are trained by &lt;a href="#backpropagation"&gt;&lt;strong&gt;backpropagating&lt;/strong&gt;&lt;/a&gt; &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; just like any other parameter in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种分类特征，以连续值特征表示。通常，嵌套是指将高维度向量映射到低维度的空间。例如，您可以采用以下两种方式之一来表示英文句子中的单词：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;表示成包含百万个元素（高维度）的&lt;a href="#sparse feature"&gt;&lt;strong&gt;稀疏向量&lt;/strong&gt;&lt;/a&gt;，其中所有元素都是整数。向量中的每个单元格都表示一个单独的英文单词，单元格中的值表示相应单词在句子中出现的次数。由于单个英文句子包含的单词不太可能超过 50 个，因此向量中几乎每个单元格都包含 0。少数非 0 的单元格中将包含一个非常小的整数（通常为 1），该整数表示相应单词在句子中出现的次数。&lt;/li&gt;
&lt;li&gt;表示成包含数百个元素（低维度）的&lt;a href="#dense feature"&gt;&lt;strong&gt;密集向量&lt;/strong&gt;&lt;/a&gt;，其中每个元素都包含一个介于 0 到 1 之间的浮点值。这就是一种嵌套。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 TensorFlow 中，会按&lt;a href="#backpropagation"&gt;&lt;strong&gt;反向传播&lt;/strong&gt;&lt;/a&gt;&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;训练嵌套，和训练&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的任何其他参数时一样。&lt;/p&gt;
&lt;h3 id="empirical risk minimization (erm)"&gt;empirical risk minimization (ERM)&lt;/h3&gt;
&lt;p&gt;Choosing the model function that minimizes loss on the training set. Contrast with &lt;a href="#structural risk minimization (srm)"&gt;&lt;strong&gt;structural risk minimization&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;用于选择可以将基于训练集的损失降至最低的模型函数。与&lt;a href="#structural risk minimization (srm)"&gt;&lt;strong&gt;结构风险最小化&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="ensemble"&gt;ensemble&lt;/h3&gt;
&lt;p&gt;A merger of the predictions of multiple &lt;a href="#model"&gt;&lt;strong&gt;models&lt;/strong&gt;&lt;/a&gt;. You can create an ensemble via one or more of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;different initializations&lt;/li&gt;
&lt;li&gt;different &lt;a href="#hyperparameter"&gt;&lt;strong&gt;hyperparameters&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;different overall structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/tutorials/wide_and_deep"&gt;Deep and wide models&lt;/a&gt; are a kind of ensemble.&lt;/p&gt;
&lt;p&gt;多个&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;的预测结果的并集。您可以通过以下一项或多项来创建集成学习：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同的初始化&lt;/li&gt;
&lt;li&gt;不同的&lt;a href="#hyperparameter"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;不同的整体结构&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/tutorials/wide_and_deep"&gt;深度模型和宽度模型&lt;/a&gt;属于一种集成学习。&lt;/p&gt;
&lt;h3 id="epoch"&gt;epoch&lt;/h3&gt;
&lt;p&gt;A full training pass over the entire data set such that each example has been seen once. Thus, an epoch represents &lt;code&gt;N&lt;/code&gt;/&lt;a href="#batch size"&gt;&lt;strong&gt;batch size&lt;/strong&gt;&lt;/a&gt; training &lt;a href="#iteration"&gt;&lt;strong&gt;iterations&lt;/strong&gt;&lt;/a&gt;, where &lt;code&gt;N&lt;/code&gt; is the total number of examples.&lt;/p&gt;
&lt;p&gt;在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。因此，一个周期表示（&lt;code&gt;N&lt;/code&gt;/&lt;a href="#batch size"&gt;&lt;strong&gt;批次规模&lt;/strong&gt;&lt;/a&gt;）次训练&lt;a href="#iteration"&gt;&lt;strong&gt;迭代&lt;/strong&gt;&lt;/a&gt;，其中 &lt;code&gt;N&lt;/code&gt; 是样本总数。&lt;/p&gt;
&lt;h3 id="estimator"&gt;Estimator&lt;/h3&gt;
&lt;p&gt;An instance of the &lt;code&gt;tf.Estimator&lt;/code&gt; class, which encapsulates logic that builds a TensorFlow graph and runs a TensorFlow session. You may create your own &lt;a href="#custom estimator"&gt;&lt;strong&gt;custom Estimators&lt;/strong&gt;&lt;/a&gt; (as described &lt;a href="https://www.tensorflow.org/extend/estimators"&gt;here&lt;/a&gt;) or instantiate &lt;a href="#pre-made estimator"&gt;&lt;strong&gt;pre-made Estimators&lt;/strong&gt;&lt;/a&gt; created by others.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tf.Estimator&lt;/code&gt; 类的一个实例，用于封装负责构建 TensorFlow 图并运行 TensorFlow 会话的逻辑。您可以创建自己的&lt;a href="#custom estimator"&gt;&lt;strong&gt;自定义 Estimator&lt;/strong&gt;&lt;/a&gt;（如需相关介绍，请&lt;a href="https://www.tensorflow.org/extend/estimators"&gt;点击此处&lt;/a&gt;），也可以将其他人&lt;a href="#pre-made estimator"&gt;&lt;strong&gt;预创建的 Estimator&lt;/strong&gt;&lt;/a&gt; 实例化。&lt;/p&gt;
&lt;h3 id="example"&gt;example&lt;/h3&gt;
&lt;p&gt;One row of a data set. An example contains one or more &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; and possibly a &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. See also &lt;a href="#labeled example"&gt;&lt;strong&gt;labeled example&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#unlabeled example"&gt;&lt;strong&gt;unlabeled example&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;数据集的一行。一个样本包含一个或多个&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，此外还可能包含一个&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;。另请参阅&lt;a href="#labeled example"&gt;&lt;strong&gt;有标签样本&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#unlabeled example"&gt;&lt;strong&gt;无标签样本&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="f_1"&gt;F&lt;/h2&gt;
&lt;h3 id="false negative (fn)"&gt;false negative (FN)&lt;/h3&gt;
&lt;p&gt;An example in which the model mistakenly predicted the &lt;a href="#negative class"&gt;&lt;strong&gt;negative class&lt;/strong&gt;&lt;/a&gt;. For example, the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam.&lt;/p&gt;
&lt;p&gt;被模型错误地预测为&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;的样本。例如，模型推断出某封电子邮件不是垃圾邮件（负类别），但该电子邮件其实是垃圾邮件。&lt;/p&gt;
&lt;h3 id="false positive (fp)"&gt;false positive (FP)&lt;/h3&gt;
&lt;p&gt;An example in which the model mistakenly predicted the &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt;. For example, the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam.&lt;/p&gt;
&lt;p&gt;被模型错误地预测为&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;的样本。例如，模型推断出某封电子邮件是垃圾邮件（正类别），但该电子邮件其实不是垃圾邮件。&lt;/p&gt;
&lt;h3 id="false positive rate (fp rate)"&gt;false positive rate (FP rate)&lt;/h3&gt;
&lt;p&gt;The x-axis in an &lt;a href="#roc (receiver operating characteristic) curve"&gt;&lt;strong&gt;ROC curve&lt;/strong&gt;&lt;/a&gt;. The FP rate is defined as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$\text{False Positive Rate} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}$$&lt;/div&gt;
&lt;p&gt;&lt;a href="#roc (receiver operating characteristic) curve"&gt;&lt;strong&gt;ROC 曲线&lt;/strong&gt;&lt;/a&gt;中的 x 轴。FP 率的定义如下：&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{假正例率} = \frac{\text{假正例数}}{\text{假正例数} + \text{真负例数}}$$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 id="feature"&gt;feature&lt;/h3&gt;
&lt;p&gt;An input variable used in making &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在进行&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;时使用的输入变量。&lt;/p&gt;
&lt;h3 id="feature columns (featurecolumns)"&gt;feature columns (FeatureColumns)&lt;/h3&gt;
&lt;p&gt;A set of related features, such as the set of all possible countries in which users might live. An example may have one or more features present in a feature column.&lt;/p&gt;
&lt;p&gt;Feature columns in TensorFlow also encapsulate metadata such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the feature's data type&lt;/li&gt;
&lt;li&gt;whether a feature is fixed length or should be converted to an embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A feature column can contain a single feature.&lt;/p&gt;
&lt;p&gt;"Feature column" is Google-specific terminology. A feature column is referred to as a "namespace" in the &lt;a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit"&gt;VW&lt;/a&gt; system (at Yahoo/Microsoft), or a &lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libffm/"&gt;field&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一组相关特征，例如用户可能居住的所有国家/地区的集合。样本的特征列中可能包含一个或多个特征。&lt;/p&gt;
&lt;p&gt;TensorFlow 中的特征列内还封装了元数据，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征的数据类型&lt;/li&gt;
&lt;li&gt;特征是固定长度还是应转换为嵌套&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;特征列可以包含单个特征。&lt;/p&gt;
&lt;p&gt;&amp;ldquo;特征列&amp;rdquo;是 Google 专用的术语。特征列在 Yahoo/Microsoft 使用的 &lt;a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit"&gt;VW&lt;/a&gt; 系统中称为&amp;ldquo;命名空间&amp;rdquo;，也称为&lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libffm/"&gt;场&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="feature cross"&gt;feature cross&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#synthetic feature"&gt;&lt;strong&gt;synthetic feature&lt;/strong&gt;&lt;/a&gt; formed by crossing (multiplying or taking a Cartesian product of) individual features. Feature crosses help represent nonlinear relationships.&lt;/p&gt;
&lt;p&gt;通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的&lt;a href="#synthetic feature"&gt;&lt;strong&gt;合成特征&lt;/strong&gt;&lt;/a&gt;。特征组合有助于表示非线性关系。&lt;/p&gt;
&lt;h3 id="feature engineering"&gt;feature engineering&lt;/h3&gt;
&lt;p&gt;The process of determining which &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; might be useful in training a model, and then converting raw data from log files and other sources into said features. In TensorFlow, feature engineering often means converting raw log file entries to &lt;a href="#tf.example"&gt;&lt;strong&gt;tf.Example&lt;/strong&gt;&lt;/a&gt; protocol buffers. See also &lt;a href="https://github.com/tensorflow/transform"&gt;tf.Transform&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feature engineering is sometimes called &lt;strong&gt;feature extraction&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;指以下过程：确定哪些&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。在 TensorFlow 中，特征工程通常是指将原始日志文件条目转换为 &lt;a href="#tf.Example"&gt;&lt;strong&gt;tf.Example&lt;/strong&gt;&lt;/a&gt; proto buffer。另请参阅 &lt;a href="https://github.com/tensorflow/transform"&gt;tf.Transform&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;特征工程有时称为&lt;strong&gt;特征提取&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="feature set"&gt;feature set&lt;/h3&gt;
&lt;p&gt;The group of &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; your machine learning model trains on. For example, postal code, property size, and property condition might comprise a simple feature set for a model that predicts housing prices.&lt;/p&gt;
&lt;p&gt;训练机器学习模型时采用的一组&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;。例如，对于某个用于预测房价的模型，邮政编码、房屋面积以及房屋状况可以组成一个简单的特征集。&lt;/p&gt;
&lt;h3 id="feature spec"&gt;feature spec&lt;/h3&gt;
&lt;p&gt;Describes the information required to extract &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; data from the &lt;a href="#tf.example"&gt;&lt;strong&gt;tf.Example&lt;/strong&gt;&lt;/a&gt; protocol buffer. Because the tf.Example protocol buffer is just a container for data, you must specify the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the data to extract (that is, the keys for the features)&lt;/li&gt;
&lt;li&gt;the data type (for example, float or int)&lt;/li&gt;
&lt;li&gt;The length (fixed or variable)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator API&lt;/strong&gt;&lt;/a&gt; provides facilities for producing a feature spec from a list of &lt;a href="#feature columns (featurecolumns)"&gt;&lt;strong&gt;FeatureColumns&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;用于描述如何从 &lt;a href="#tf.example"&gt;&lt;strong&gt;tf.Example&lt;/strong&gt;&lt;/a&gt; proto buffer 提取&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;数据。由于 tf.Example proto buffer 只是一个数据容器，因此您必须指定以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;要提取的数据（即特征的键）&lt;/li&gt;
&lt;li&gt;数据类型（例如 float 或 int）&lt;/li&gt;
&lt;li&gt;长度（固定或可变）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#estimator"&gt;&lt;strong&gt;Estimator API&lt;/strong&gt;&lt;/a&gt; 提供了一些可用来根据给定 &lt;a href="#feature columns (featurecolumns)"&gt;&lt;strong&gt;FeatureColumns&lt;/strong&gt;&lt;/a&gt; 列表生成特征规范的工具。&lt;/p&gt;
&lt;h3 id="full softmax"&gt;full softmax&lt;/h3&gt;
&lt;p&gt;See &lt;a href="#softmax"&gt;&lt;strong&gt;softmax&lt;/strong&gt;&lt;/a&gt;. Contrast with &lt;a href="#candidate sampling"&gt;&lt;strong&gt;candidate sampling&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;请参阅 &lt;a href="#softmax"&gt;&lt;strong&gt;softmax&lt;/strong&gt;&lt;/a&gt;。与&lt;a href="#candidate sampling"&gt;&lt;strong&gt;候选采样&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="fully connected layer"&gt;fully connected layer&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layer&lt;/strong&gt;&lt;/a&gt; in which each &lt;a href="#node"&gt;&lt;strong&gt;node&lt;/strong&gt;&lt;/a&gt; is connected to &lt;em&gt;every&lt;/em&gt; node in the subsequent hidden layer.&lt;/p&gt;
&lt;p&gt;A fully connected layer is also known as a &lt;a href="#dense layer"&gt;&lt;strong&gt;dense layer&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#hidden layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;，其中的每个&lt;a href="#node"&gt;&lt;strong&gt;节点&lt;/strong&gt;&lt;/a&gt;均与下一个隐藏层中的每个节点相连。&lt;/p&gt;
&lt;p&gt;全连接层又称为&lt;a href="#dense layer"&gt;&lt;strong&gt;密集层&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="g_1"&gt;G&lt;/h2&gt;
&lt;h3 id="generalization"&gt;generalization&lt;/h3&gt;
&lt;p&gt;Refers to your model's ability to make correct predictions on new, previously unseen data as opposed to the data used to train the model.&lt;/p&gt;
&lt;p&gt;指的是模型依据训练时采用的数据，针对以前未见过的新数据做出正确预测的能力。&lt;/p&gt;
&lt;h3 id="generalized linear model"&gt;generalized linear model&lt;/h3&gt;
&lt;p&gt;A generalization of &lt;a href="https://developers.google.com/machine-learning/glossary/least_squares_regression"&gt;&lt;strong&gt;least squares regression&lt;/strong&gt;&lt;/a&gt; models, which are based on &lt;a href="https://en.wikipedia.org/wiki/Gaussian_noise"&gt;Gaussian noise&lt;/a&gt;, to other types of models based on other types of noise, such as &lt;a href="https://en.wikipedia.org/wiki/Shot_noise"&gt;Poisson noise&lt;/a&gt; or categorical noise. Examples of generalized linear models include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#logistic regression"&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;multi-class regression&lt;/li&gt;
&lt;li&gt;least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The parameters of a generalized linear model can be found through &lt;a href="https://en.wikipedia.org/wiki/Convex_optimization"&gt;convex optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Generalized linear models exhibit the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The average prediction of the optimal least squares regression model is equal to the average label on the training data.&lt;/li&gt;
&lt;li&gt;The average probability predicted by the optimal logistic regression model is equal to the average label on the training data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The power of a generalized linear model is limited by its features. Unlike a deep model, a generalized linear model cannot "learn new features."&lt;/p&gt;
&lt;p&gt;&lt;a href="https://developers.google.com/machine-learning/glossary/least_squares_regression"&gt;&lt;strong&gt;最小二乘回归&lt;/strong&gt;&lt;/a&gt;模型（基于&lt;a href="https://en.wikipedia.org/wiki/Gaussian_noise"&gt;高斯噪声&lt;/a&gt;）向其他类型的模型（基于其他类型的噪声，例如&lt;a href="https://en.wikipedia.org/wiki/Shot_noise"&gt;泊松噪声&lt;/a&gt;或分类噪声）进行的一种泛化。广义线性模型的示例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#logistic regression"&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;多类别回归&lt;/li&gt;
&lt;li&gt;最小二乘回归&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以通过&lt;a href="https://en.wikipedia.org/wiki/Convex_optimization"&gt;凸优化&lt;/a&gt;找到广义线性模型的参数。&lt;/p&gt;
&lt;p&gt;广义线性模型具有以下特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最优的最小二乘回归模型的平均预测结果等于训练数据的平均标签。&lt;/li&gt;
&lt;li&gt;最优的逻辑回归模型预测的平均概率等于训练数据的平均标签。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;广义线性模型的功能受其特征的限制。与深度模型不同，广义线性模型无法&amp;ldquo;学习新特征&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="gradient"&gt;gradient&lt;/h3&gt;
&lt;p&gt;The vector of &lt;a href="#partial derivative"&gt;&lt;strong&gt;partial derivatives&lt;/strong&gt;&lt;/a&gt; with respect to all of the independent variables. In machine learning, the gradient is the the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent.&lt;/p&gt;
&lt;p&gt;&lt;a href="#partial derivative"&gt;&lt;strong&gt;偏导数&lt;/strong&gt;&lt;/a&gt;相对于所有自变量的向量。在机器学习中，梯度是模型函数偏导数的向量。梯度指向最速上升的方向。&lt;/p&gt;
&lt;h3 id="gradient clipping"&gt;gradient clipping&lt;/h3&gt;
&lt;p&gt;Capping &lt;a href="#gradient"&gt;&lt;strong&gt;gradient&lt;/strong&gt;&lt;/a&gt; values before applying them. Gradient clipping helps ensure numerical stability and prevents &lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf"&gt;exploding gradients&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在应用&lt;a href="#gradient"&gt;&lt;strong&gt;梯度&lt;/strong&gt;&lt;/a&gt;值之前先设置其上限。梯度裁剪有助于确保数值稳定性以及防止&lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf"&gt;梯度爆炸&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="gradient descent"&gt;gradient descent&lt;/h3&gt;
&lt;p&gt;A technique to minimize &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of &lt;a href="#weight"&gt;&lt;strong&gt;weights&lt;/strong&gt;&lt;/a&gt; and bias to minimize loss.&lt;/p&gt;
&lt;p&gt;一种通过计算并且减小梯度将&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;和偏差的最佳组合，从而将损失降至最低。&lt;/p&gt;
&lt;h3 id="graph"&gt;graph&lt;/h3&gt;
&lt;p&gt;In TensorFlow, a computation specification. Nodes in the graph represent operations. Edges are directed and represent passing the result of an operation (a &lt;a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"&gt;Tensor&lt;/a&gt;) as an operand to another operation. Use &lt;a href="#tensorboard"&gt;&lt;strong&gt;TensorBoard&lt;/strong&gt;&lt;/a&gt; to visualize a graph.&lt;/p&gt;
&lt;p&gt;TensorFlow 中的一种计算规范。图中的节点表示操作。边缘具有方向，表示将某项操作的结果（一个&lt;a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"&gt;张量&lt;/a&gt;）作为一个操作数传递给另一项操作。可以使用 &lt;a href="#tensorboard"&gt;&lt;strong&gt;TensorBoard&lt;/strong&gt;&lt;/a&gt; 直观呈现图。&lt;/p&gt;
&lt;h2 id="h_1"&gt;H&lt;/h2&gt;
&lt;h3 id="heuristic"&gt;heuristic&lt;/h3&gt;
&lt;p&gt;A practical and nonoptimal solution to a problem, which is sufficient for making progress or for learning from.&lt;/p&gt;
&lt;p&gt;一种非最优但实用的问题解决方案，足以用于进行改进或从中学习。&lt;/p&gt;
&lt;h3 id="hidden layer"&gt;hidden layer&lt;/h3&gt;
&lt;p&gt;A synthetic layer in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt; between the &lt;a href="#input layer"&gt;&lt;strong&gt;input layer&lt;/strong&gt;&lt;/a&gt; (that is, the features) and the &lt;a href="#output layer"&gt;&lt;strong&gt;output layer&lt;/strong&gt;&lt;/a&gt; (the prediction). A neural network contains one or more hidden layers.&lt;/p&gt;
&lt;p&gt;&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的合成层，介于&lt;a href="#input layer"&gt;&lt;strong&gt;输入层&lt;/strong&gt;&lt;/a&gt;（即特征）和&lt;a href="#output layer"&gt;&lt;strong&gt;输出层&lt;/strong&gt;&lt;/a&gt;（即预测）之间。神经网络包含一个或多个隐藏层。&lt;/p&gt;
&lt;h3 id="hinge loss"&gt;hinge loss&lt;/h3&gt;
&lt;p&gt;A family of &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; functions for &lt;a href="#classification model"&gt;&lt;strong&gt;classification&lt;/strong&gt;&lt;/a&gt; designed to find the &lt;a href="#decision boundary"&gt;&lt;strong&gt;decision boundary&lt;/strong&gt;&lt;/a&gt; as distant as possible from each training example, thus maximizing the margin between examples and the boundary. &lt;a href="#kernel support vector machines (ksvms)"&gt;&lt;strong&gt;KSVMs&lt;/strong&gt;&lt;/a&gt; use hinge loss (or a related function, such as squared hinge loss). For binary classification, the hinge loss function is defined as follows:&lt;/p&gt;
&lt;p&gt;一系列用于&lt;a href="#classification model"&gt;&lt;strong&gt;分类&lt;/strong&gt;&lt;/a&gt;的&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;函数，旨在找到距离每个训练样本都尽可能远的&lt;a href="#decision boundary"&gt;&lt;strong&gt;决策边界&lt;/strong&gt;&lt;/a&gt;，从而使样本和边界之间的裕度最大化。 &lt;a href="#kernel support vector machines (ksvms)"&gt;&lt;strong&gt;KSVM&lt;/strong&gt;&lt;/a&gt; 使用合页损失函数（或相关函数，例如平方合页损失函数）。对于二元分类，合页损失函数的定义如下：&lt;/p&gt;
&lt;div class="math"&gt;$$\text{loss} = \text{max}(0, 1 - (y' * y))$$&lt;/div&gt;
&lt;p&gt;where &lt;em&gt;y'&lt;/em&gt; is the raw output of the classifier model:&lt;/p&gt;
&lt;p&gt;其中&amp;ldquo;y'&amp;rdquo;表示分类器模型的原始输出：&lt;/p&gt;
&lt;div class="math"&gt;$$y' = b + w_1x_1 + w_2x_2 + &amp;hellip; w_nx_n$$&lt;/div&gt;
&lt;p&gt;and &lt;em&gt;y&lt;/em&gt; is the true label, either -1 or +1.&lt;/p&gt;
&lt;p&gt;Consequently, a plot of hinge loss vs. (y * y') looks as follows:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;y&amp;rdquo;表示真标签，值为 -1 或 +1。&lt;/p&gt;
&lt;p&gt;因此，合页损失与 (y * y') 的关系图如下所示：&lt;/p&gt;
&lt;p&gt;
&lt;svg height="566.00629" id="svg2" version="1.1" viewbox="0 0 659.45264 566.00629" width="659.45264" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg"&gt;
&lt;title&gt;Hinge loss vs. (y * y')&lt;/title&gt;
&lt;desc&gt;A plot of hinge loss vs. raw classifier score shows a distinct hinge at the
coordinate (1,0).&lt;/desc&gt;
&lt;defs id="defs6"&gt;&lt;clippath clippathunits="userSpaceOnUse" id="clipPath20"&gt;&lt;path d="M 0,0 H 365760 V 274320 H 0 Z" id="path18"&gt;&lt;/path&gt;&lt;/clippath&gt;&lt;/defs&gt;&lt;g id="g10" transform="matrix(1.3333333,0,0,-1.3333333,-22.422343,676.03376)"&gt;&lt;path d="M 0,540 H 719.99856 V 0.00108 H 0 Z" id="path28" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 95.419101,498.0867 V 157.52832" id="path36" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 95.419101,498.0867 V 157.52832" id="path38" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 95.419101,157.76328 H 503.46553" id="path46" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 95.419101,157.76328 H 503.46553" id="path48" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 294.60847,173.49483 V 142.03032" id="path56" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 294.60847,173.49483 V 142.03032" id="path58" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 362.51227,173.49483 V 142.03032" id="path66" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 362.51227,173.49483 V 142.03032" id="path68" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 430.41607,173.49483 V 142.03032" id="path76" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 430.41607,173.49483 V 142.03032" id="path78" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 226.70466,173.49483 V 142.03032" id="path86" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 226.70466,173.49483 V 142.03032" id="path88" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 158.80086,173.49483 V 142.03032" id="path96" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 158.80086,173.49483 V 142.03032" id="path98" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 498.31987,173.49483 V 142.03032" id="path106" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 498.31987,173.49483 V 142.03032" id="path108" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 282.09479,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path116" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text126" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="292.508" y="-122.12723"&gt;&lt;tspan id="tspan124" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="292.508" y="-122.12723"&gt;0&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 124.85634,138.47718 h 66.992 v -31.4645 h -66.992 z" id="path136" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text146" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="153.90829" y="-122.12723"&gt;&lt;tspan id="tspan144" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="153.90829 157.23828" y="-122.12723"&gt;-2&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 198.28295,138.47718 h 56.50382 v -31.4645 h -56.50382 z" id="path156" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text166" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="222.09081" y="-122.12723"&gt;&lt;tspan id="tspan164" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="222.09081 225.42079" y="-122.12723"&gt;-1&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 350.12473,138.47718 h 26.38578 v -31.4645 h -26.38578 z" id="path176" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text186" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="360.53796" y="-122.12723"&gt;&lt;tspan id="tspan184" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="360.53796" y="-122.12723"&gt;1&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 417.22358,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path196" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text206" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="427.63678" y="-122.12723"&gt;&lt;tspan id="tspan204" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="427.63678" y="-122.12723"&gt;2&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 485.02047,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path216" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text226" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="495.43369" y="-122.12723"&gt;&lt;tspan id="tspan224" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="495.43369" y="-122.12723"&gt;3&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 111.32608,415.76206 H 79.861573" id="path236" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,415.76206 H 79.861573" id="path238" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 111.32608,347.85826 H 79.861573" id="path246" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,347.85826 H 79.861573" id="path248" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 111.32608,279.95446 H 79.861573" id="path256" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,279.95446 H 79.861573" id="path258" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 111.32608,483.66586 H 79.861573" id="path266" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,483.66586 H 79.861573" id="path268" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 50.757773,302.93551 H 77.143547 V 255.21907 H 50.757773 Z" id="path276" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text286" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="61.170982" y="-275.47729"&gt;&lt;tspan id="tspan284" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="61.170982" y="-275.47729"&gt;1&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 53.622428,371.98048 H 80.008206 V 324.26403 H 53.622428 Z" id="path296" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text306" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-344.52228"&gt;&lt;tspan id="tspan304" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-344.52228"&gt;2&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 53.622428,438.99539 H 80.008206 V 391.27895 H 53.622428 Z" id="path316" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text326" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-411.53717"&gt;&lt;tspan id="tspan324" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-411.53717"&gt;3&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 53.622428,507.02533 H 80.008206 V 459.30889 H 53.622428 Z" id="path336" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text346" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-479.56711"&gt;&lt;tspan id="tspan344" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-479.56711"&gt;4&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 95.053282,483.16711 360.44646,217.77393" id="path356" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 95.053282,483.16711 360.44646,217.77393" id="path358" style="fill:none;stroke:#ff0000;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 359.32928,218.08535 H 501.43923" id="path366" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 359.32928,218.08535 H 501.43923" id="path368" style="fill:none;stroke:#ff0000;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 50.757773,241.9206 H 77.143547 V 194.20416 H 50.757773 Z" id="path376" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text386" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="61.170982" y="-214.46239"&gt;&lt;tspan id="tspan384" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="61.170982" y="-214.46239"&gt;0&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 111.32608,217.92452 H 79.861573" id="path396" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,217.92452 H 79.861573" id="path398" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 9.299194,394.88415 H 62.614048 V 344.4748 H 9.299194 Z" id="path406" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text416" style="font-variant:normal;font-weight:normal;font-size:11.99997616px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="16.049181" y="-376.6142"&gt;&lt;tspan id="tspan414" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="16.049181 22.721169 25.385162 32.057148 38.729134 45.401123" y="-376.6142"&gt;hinge &lt;/tspan&gt;&lt;/text&gt;
&lt;text id="text420" style="font-variant:normal;font-weight:normal;font-size:11.99997616px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="16.049181" y="-362.36423"&gt;&lt;tspan id="tspan418" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="16.049181 18.713175 25.385162 31.385151" y="-362.36423"&gt;loss&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 95.419101,110.41818 H 498.31593 V 82.5206 H 95.419101 Z" id="path430" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="m 266.48095,103.66819 h 60.77311 V 88.068225 h -60.77311 z" id="path438" style="fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851"&gt;&lt;/path&gt;&lt;text id="text442" style="font-variant:normal;font-weight:normal;font-size:12.99997425px;font-family:Consolas;-inkscape-font-specification:Consolas;writing-mode:lr-tb;fill:#222222;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="270.09167" y="-91.188217"&gt;&lt;tspan id="tspan440" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="270.09167 277.22867 284.36563 291.50262 298.63962 305.77661 312.91357 320.05057" y="-91.188217"&gt;(y * y')&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;/svg&gt;
&lt;/p&gt;
&lt;p&gt;Hinge loss vs. (y * y') A plot of hinge loss vs. raw classifier score shows a distinct hinge at the coordinate (1,0). &lt;/p&gt;
&lt;h3 id="holdout data"&gt;holdout data&lt;/h3&gt;
&lt;p&gt;&lt;a href="#example"&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; intentionally not used ("held out") during training. The &lt;a href="#validation set"&gt;&lt;strong&gt;validation data set&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#test set"&gt;&lt;strong&gt;test data set&lt;/strong&gt;&lt;/a&gt; are examples of holdout data. Holdout data helps evaluate your model's ability to generalize to data other than the data it was trained on. The loss on the holdout set provides a better estimate of the loss on an unseen data set than does the loss on the training set.&lt;/p&gt;
&lt;p&gt;训练期间故意不使用（&amp;ldquo;维持&amp;rdquo;）的&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;。&lt;a href="#validation set"&gt;&lt;strong&gt;验证数据集&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#test set"&gt;&lt;strong&gt;测试数据集&lt;/strong&gt;&lt;/a&gt;都属于维持数据。维持数据有助于评估模型向训练时所用数据之外的数据进行泛化的能力。与基于训练数据集的损失相比，基于维持数据集的损失有助于更好地估算基于未见过的数据集的损失。&lt;/p&gt;
&lt;h3 id="hyperparameter"&gt;hyperparameter&lt;/h3&gt;
&lt;p&gt;The "knobs" that you tweak during successive runs of training a model. For example, &lt;a href="#learning rate"&gt;&lt;strong&gt;learning rate&lt;/strong&gt;&lt;/a&gt; is a hyperparameter.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#parameter"&gt;&lt;strong&gt;parameter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在模型训练的连续过程中，您调节的&amp;ldquo;旋钮&amp;rdquo;。例如，&lt;a href="#learning rate"&gt;&lt;strong&gt;学习速率&lt;/strong&gt;&lt;/a&gt;就是一种超参数。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#parameter"&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="hyperplane"&gt;hyperplane&lt;/h3&gt;
&lt;p&gt;A boundary that separates a space into two subspaces. For example, a line is a hyperplane in two dimensions and a plane is a hyperplane in three dimensions. More typically in machine learning, a hyperplane is the boundary separating a high-dimensional space. &lt;a href="#kernel support vector machines (ksvms)"&gt;&lt;strong&gt;Kernel Support Vector Machines&lt;/strong&gt;&lt;/a&gt; use hyperplanes to separate positive classes from negative classes, often in a very high-dimensional space.&lt;/p&gt;
&lt;p&gt;将一个空间划分为两个子空间的边界。例如，在二维空间中，直线就是一个超平面，在三维空间中，平面则是一个超平面。在机器学习中更典型的是：超平面是分隔高维度空间的边界。&lt;a href="#kernel support vector machines (ksvms)"&gt;&lt;strong&gt;核支持向量机&lt;/strong&gt;&lt;/a&gt;利用超平面将正类别和负类别区分开来（通常是在极高维度空间中）。&lt;/p&gt;
&lt;h2 id="i_1"&gt;I&lt;/h2&gt;
&lt;h3 id="independently and identically distributed (i.i.d)"&gt;independently and identically distributed (i.i.d)&lt;/h3&gt;
&lt;p&gt;Data drawn from a distribution that doesn't change, and where each value drawn doesn't depend on values that have been drawn previously. An i.i.d. is the &lt;a href="https://en.wikipedia.org/wiki/Ideal_gas"&gt;ideal gas&lt;/a&gt; of machine learning&amp;mdash;a useful mathematical construct but almost never exactly found in the real world. For example, the distribution of visitors to a web page may be i.i.d. over a brief window of time; that is, the distribution doesn't change during that brief window and one person's visit is generally independent of another's visit. However, if you expand that window of time, seasonal differences in the web page's visitors may appear.&lt;/p&gt;
&lt;p&gt;从不会改变的分布中提取的数据，其中提取的每个值都不依赖于之前提取的值。i.i.d. 是机器学习的&lt;a href="https://en.wikipedia.org/wiki/Ideal_gas"&gt;理想气体&lt;/a&gt; - 一种实用的数学结构，但在现实世界中几乎从未发现过。例如，某个网页的访问者在短时间内的分布可能为 i.i.d.，即分布在该短时间内没有变化，且一位用户的访问行为通常与另一位用户的访问行为无关。不过，如果将时间窗口扩大，网页访问者的分布可能呈现出季节性变化。&lt;/p&gt;
&lt;h3 id="inference"&gt;inference&lt;/h3&gt;
&lt;p&gt;In machine learning, often refers to the process of making predictions by applying the trained model to &lt;a href="#unlabeled example"&gt;&lt;strong&gt;unlabeled examples&lt;/strong&gt;&lt;/a&gt;. In statistics, inference refers to the process of fitting the parameters of a distribution conditioned on some observed data. (See the &lt;a href="https://en.wikipedia.org/wiki/Statistical_inference"&gt;Wikipedia article on statistical inference&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;在机器学习中，推断通常指以下过程：通过将训练过的模型应用于&lt;a href="#unlabeled example"&gt;&lt;strong&gt;无标签样本&lt;/strong&gt;&lt;/a&gt;来做出预测。在统计学中，推断是指在某些观测数据条件下拟合分布参数的过程。（请参阅&lt;a href="https://en.wikipedia.org/wiki/Statistical_inference"&gt;维基百科中有关统计学推断的文章&lt;/a&gt;。）&lt;/p&gt;
&lt;h3 id="input function"&gt;input function&lt;/h3&gt;
&lt;p&gt;In TensorFlow, a function that returns input data to the training, evaluation, or prediction method of an &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt;. For example, the training input function returns a &lt;a href="#batch"&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/a&gt; of features and labels from the &lt;a href="#training set"&gt;&lt;strong&gt;training set&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在 TensorFlow 中，用于将输入数据返回到 &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt; 的训练、评估或预测方法的函数。例如，训练输入函数用于返回&lt;a href="#training set"&gt;&lt;strong&gt;训练集&lt;/strong&gt;&lt;/a&gt;中的&lt;a href="#batch"&gt;&lt;strong&gt;批次&lt;/strong&gt;&lt;/a&gt;特征和标签。&lt;/p&gt;
&lt;h3 id="input layer"&gt;input layer&lt;/h3&gt;
&lt;p&gt;The first layer (the one that receives the input data) in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的第一层（接收输入数据的层）。&lt;/p&gt;
&lt;h3 id="instance"&gt;instance&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#example"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="interpretability"&gt;interpretability&lt;/h3&gt;
&lt;p&gt;The degree to which a model's predictions can be readily explained. Deep models are often non-interpretable; that is, a deep model's different layers can be hard to decipher. By contrast, linear regression models and &lt;a href="#wide model"&gt;&lt;strong&gt;wide models&lt;/strong&gt;&lt;/a&gt; are typically far more interpretable.&lt;/p&gt;
&lt;p&gt;模型的预测可解释的难易程度。深度模型通常不可解释，也就是说，很难对深度模型的不同层进行解释。相比之下，线性回归模型和&lt;a href="#wide model"&gt;&lt;strong&gt;宽度模型&lt;/strong&gt;&lt;/a&gt;的可解释性通常要好得多。&lt;/p&gt;
&lt;h3 id="inter-rater agreement"&gt;inter-rater agreement&lt;/h3&gt;
&lt;p&gt;A measurement of how often human raters agree when doing a task. If raters disagree, the task instructions may need to be improved. Also sometimes called &lt;strong&gt;inter-annotator agreement&lt;/strong&gt; or &lt;strong&gt;inter-rater reliability&lt;/strong&gt;. See also &lt;a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa"&gt;Cohen's kappa&lt;/a&gt;, which is one of the most popular inter-rater agreement measurements.&lt;/p&gt;
&lt;p&gt;一种衡量指标，用于衡量在执行某项任务时评分者达成一致的频率。如果评分者未达成一致，则可能需要改进任务说明。有时也称为&lt;strong&gt;注释者间一致性信度&lt;/strong&gt;或&lt;strong&gt;评分者间可靠性信度&lt;/strong&gt;。另请参阅 &lt;a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa"&gt;Cohen's kappa&lt;/a&gt;（最热门的评分者间一致性信度衡量指标之一）。&lt;/p&gt;
&lt;h3 id="iteration"&gt;iteration&lt;/h3&gt;
&lt;p&gt;A single update of a model's weights during training. An iteration consists of computing the gradients of the parameters with respect to the loss on a single &lt;a href="#batch"&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/a&gt; of data.&lt;/p&gt;
&lt;p&gt;模型的权重在训练期间的一次更新。迭代包含计算参数在单个&lt;a href="#batch"&gt;&lt;strong&gt;批量&lt;/strong&gt;&lt;/a&gt;数据上的梯度损失。&lt;/p&gt;
&lt;h2 id="k_1"&gt;K&lt;/h2&gt;
&lt;h3 id="keras"&gt;Keras&lt;/h3&gt;
&lt;p&gt;A popular Python machine learning API. &lt;a href="https://keras.io"&gt;Keras&lt;/a&gt; runs on several deep learning frameworks, including TensorFlow, where it is made available as &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras"&gt;&lt;strong&gt;tf.keras&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种热门的 Python 机器学习 API。&lt;a href="https://keras.io"&gt;Keras&lt;/a&gt; 能够在多种深度学习框架上运行，其中包括 TensorFlow（在该框架上，Keras 作为 &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras"&gt;&lt;strong&gt;tf.keras&lt;/strong&gt;&lt;/a&gt; 提供）。&lt;/p&gt;
&lt;h3 id="kernel support vector machines (ksvms)"&gt;Kernel Support Vector Machines (KSVMs)&lt;/h3&gt;
&lt;p&gt;A classification algorithm that seeks to maximize the margin between &lt;a href="#positive class"&gt;&lt;strong&gt;positive&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#negative class"&gt;&lt;strong&gt;negative classes&lt;/strong&gt;&lt;/a&gt; by mapping input data vectors to a higher dimensional space. For example, consider a classification problem in which the input data set consists of a hundred features. In order to maximize the margin between positive and negative classes, KSVMs could internally map those features into a million-dimension space. KSVMs uses a loss function called &lt;a href="#hinge-loss"&gt;hinge loss&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种分类算法，旨在通过将输入数据向量映射到更高维度的空间，来最大化&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;之间的裕度。以某个输入数据集包含一百个特征的分类问题为例。为了最大化正类别和负类别之间的裕度，KSVM 可以在内部将这些特征映射到百万维度的空间。KSVM 使用&lt;a href="#hinge loss"&gt;&lt;strong&gt;合页损失函数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="l_1"&gt;L&lt;/h2&gt;
&lt;h3 id="l1 loss"&gt;L1 loss&lt;/h3&gt;
&lt;p&gt;&lt;a href="#loss"&gt;&lt;strong&gt;Loss&lt;/strong&gt;&lt;/a&gt; function based on the absolute value of the difference between the values that a model is predicting and the actual values of the &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt;. L1 loss is less sensitive to outliers than &lt;a href="#squared loss"&gt;&lt;strong&gt;L2 loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;函数，基于模型预测的值与&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的实际值之差的绝对值。与 &lt;a href="#squared loss"&gt;&lt;strong&gt;L2 损失函数&lt;/strong&gt;&lt;/a&gt;相比，L1 损失函数对离群值的敏感性弱一些。&lt;/p&gt;
&lt;h3 id="l1 regularization"&gt;L1 regularization&lt;/h3&gt;
&lt;p&gt;A type of &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt; that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on &lt;a href="#sparse feature"&gt;&lt;strong&gt;sparse features&lt;/strong&gt;&lt;/a&gt;, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with &lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;，根据权重的绝对值的总和来惩罚权重。在依赖&lt;a href="#sparse feature"&gt;&lt;strong&gt;稀疏特征&lt;/strong&gt;&lt;/a&gt;的模型中，L1 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 &lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 正则化&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="l2 loss"&gt;L2 loss&lt;/h3&gt;
&lt;p&gt;See &lt;a href="#squared loss"&gt;&lt;strong&gt;squared loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;请参阅&lt;a href="#squared loss"&gt;&lt;strong&gt;平方损失函数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="l2 regularization"&gt;L2 regularization&lt;/h3&gt;
&lt;p&gt;A type of &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt; that penalizes weights in proportion to the sum of the &lt;em&gt;squares&lt;/em&gt; of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with &lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 regularization&lt;/strong&gt;&lt;/a&gt;.) L2 regularization always improves generalization in linear models.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;，根据权重的平方和来惩罚权重。L2 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 &lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 正则化&lt;/strong&gt;&lt;/a&gt;相对。）在线性模型中，L2 正则化始终可以改进泛化。&lt;/p&gt;
&lt;h3 id="label"&gt;label&lt;/h3&gt;
&lt;p&gt;In supervised learning, the "answer" or "result" portion of an &lt;a href="#example"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;. Each example in a labeled data set consists of one or more features and a label. For instance, in a housing data set, the features might include the number of bedrooms, the number of bathrooms, and the age of the house, while the label might be the house's price. in a spam detection dataset, the features might include the subject line, the sender, and the email message itself, while the label would probably be either "spam" or "not spam."&lt;/p&gt;
&lt;p&gt;在监督式学习中，标签指&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;的&amp;ldquo;答案&amp;rdquo;或&amp;ldquo;结果&amp;rdquo;部分。有标签数据集中的每个样本都包含一个或多个特征以及一个标签。例如，在房屋数据集中，特征可以包括卧室数、卫生间数以及房龄，而标签则可以是房价。在垃圾邮件检测数据集中，特征可以包括主题行、发件人以及电子邮件本身，而标签则可以是&amp;ldquo;垃圾邮件&amp;rdquo;或&amp;ldquo;非垃圾邮件&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="labeled example"&gt;labeled example&lt;/h3&gt;
&lt;p&gt;An example that contains &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; and a &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. In supervised training, models learn from labeled examples.&lt;/p&gt;
&lt;p&gt;包含&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的样本。在监督式训练中，模型从有标签样本中进行学习。&lt;/p&gt;
&lt;h3 id="lambda"&gt;lambda&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#regularization rate"&gt;&lt;strong&gt;regularization rate&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(This is an overloaded term. Here we're focusing on the term's definition within &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;是&lt;a href="#regularization rate"&gt;&lt;strong&gt;正则化率&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;p&gt;（多含义术语，我们在此关注的是该术语在&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;中的定义。）&lt;/p&gt;
&lt;h3 id="layer"&gt;layer&lt;/h3&gt;
&lt;p&gt;A set of &lt;a href="#neuron"&gt;&lt;strong&gt;neurons&lt;/strong&gt;&lt;/a&gt; in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt; that process a set of input features, or the output of those neurons.&lt;/p&gt;
&lt;p&gt;Also, an abstraction in TensorFlow. Layers are Python functions that take &lt;a href="#tensor"&gt;&lt;strong&gt;Tensors&lt;/strong&gt;&lt;/a&gt; and configuration options as input and produce other tensors as output. Once the necessary Tensors have been composed, the user can convert the result into an &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt; via a model function.&lt;/p&gt;
&lt;p&gt;&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的一组&lt;a href="#neuron"&gt;&lt;strong&gt;神经元&lt;/strong&gt;&lt;/a&gt;，处理一组输入特征，或一组神经元的输出。&lt;/p&gt;
&lt;p&gt;此外还指 TensorFlow 中的抽象层。层是 Python 函数，以&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;和配置选项作为输入，然后生成其他张量作为输出。当必要的张量组合起来，用户便可以通过模型函数将结果转换为 &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="layers api (tf.layers)"&gt;Layers API (tf.layers)&lt;/h3&gt;
&lt;p&gt;A TensorFlow API for constructing a &lt;a href="#deep model"&gt;&lt;strong&gt;deep&lt;/strong&gt;&lt;/a&gt; neural network as a composition of layers. The Layers API enables you to build different types of &lt;a href="#layer"&gt;&lt;strong&gt;layers&lt;/strong&gt;&lt;/a&gt;, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.layers.Dense&lt;/code&gt; for a &lt;a href="#fully connected layer"&gt;&lt;strong&gt;fully-connected layer&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.layers.Conv2D&lt;/code&gt; for a convolutional layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When writing a &lt;a href="#custom estimator"&gt;&lt;strong&gt;custom Estimator&lt;/strong&gt;&lt;/a&gt;, you compose Layers objects to define the characteristics of all the &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Layers API follows the &lt;a href="#keras"&gt;&lt;strong&gt;Keras&lt;/strong&gt;&lt;/a&gt; layers API conventions. That is, aside from a different prefix, all functions in the Layers API have the same names and signatures as their counterparts in the Keras layers API.&lt;/p&gt;
&lt;p&gt;一种 TensorFlow API，用于以层组合的方式构建&lt;a href="#deep model"&gt;&lt;strong&gt;深度&lt;/strong&gt;&lt;/a&gt;神经网络。通过 Layers API，您可以构建不同类型的&lt;a href="#layer"&gt;&lt;strong&gt;层&lt;/strong&gt;&lt;/a&gt;，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过 &lt;code&gt;tf.layers.Dense&lt;/code&gt; 构建&lt;a href="#fully connected layer"&gt;&lt;strong&gt;全连接层&lt;/strong&gt;&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;通过 &lt;code&gt;tf.layers.Conv2D&lt;/code&gt; 构建卷积层。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在编写&lt;a href="#custom estimator"&gt;&lt;strong&gt;自定义 Estimator&lt;/strong&gt;&lt;/a&gt; 时，您可以编写&amp;ldquo;层&amp;rdquo;对象来定义所有&lt;a href="#hidden layers"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;的特征。&lt;/p&gt;
&lt;p&gt;Layers API 遵循 &lt;a href="#keras"&gt;&lt;strong&gt;Keras&lt;/strong&gt;&lt;/a&gt; layers API 规范。也就是说，除了前缀不同以外，Layers API 中的所有函数均与 Keras layers API 中的对应函数具有相同的名称和签名。&lt;/p&gt;
&lt;h3 id="learning rate"&gt;learning rate&lt;/h3&gt;
&lt;p&gt;A scalar used to train a model via gradient descent. During each iteration, the &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; algorithm multiplies the learning rate by the gradient. The resulting product is called the &lt;strong&gt;gradient step&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Learning rate is a key &lt;a href="#hyperparameter"&gt;&lt;strong&gt;hyperparameter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在训练模型时用于梯度下降的一个变量。在每次迭代期间，&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;都会将学习速率与梯度相乘。得出的乘积称为&lt;strong&gt;梯度步长&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;学习速率是一个重要的&lt;a href="#hyperparameter"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="least squares regression"&gt;least squares regression&lt;/h3&gt;
&lt;p&gt;A linear regression model trained by minimizing &lt;a href="#l2 loss"&gt;&lt;strong&gt;L2 Loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种通过最小化 &lt;a href="#l2 loss"&gt;&lt;strong&gt;L2 损失&lt;/strong&gt;&lt;/a&gt;训练出的线性回归模型。&lt;/p&gt;
&lt;h3 id="linear regression"&gt;linear regression&lt;/h3&gt;
&lt;p&gt;A type of &lt;a href="#regression model"&gt;&lt;strong&gt;regression model&lt;/strong&gt;&lt;/a&gt; that outputs a continuous value from a linear combination of input features.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#regression model"&gt;&lt;strong&gt;回归模型&lt;/strong&gt;&lt;/a&gt;，通过将输入特征进行线性组合，以连续值作为输出。&lt;/p&gt;
&lt;h3 id="logistic regression"&gt;logistic regression&lt;/h3&gt;
&lt;p&gt;A model that generates a probability for each possible discrete label value in classification problems by applying a &lt;a href="#sigmoid function"&gt;&lt;strong&gt;sigmoid function&lt;/strong&gt;&lt;/a&gt; to a linear prediction. Although logistic regression is often used in &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt; problems, it can also be used in &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class&lt;/strong&gt;&lt;/a&gt; classification problems (where it becomes called &lt;strong&gt;multi-class logistic regression&lt;/strong&gt; or &lt;strong&gt;multinomial regression&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;一种模型，通过将 &lt;a href="#sigmoid function"&gt;&lt;strong&gt;S 型函数&lt;/strong&gt;&lt;/a&gt;应用于线性预测，生成分类问题中每个可能的离散标签值的概率。虽然逻辑回归经常用于&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;问题，但也可用于&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别&lt;/strong&gt;&lt;/a&gt;分类问题（其叫法变为&lt;strong&gt;多类别逻辑回归&lt;/strong&gt;或&lt;strong&gt;多项回归&lt;/strong&gt;）。&lt;/p&gt;
&lt;h3 id="log loss"&gt;Log Loss&lt;/h3&gt;
&lt;p&gt;The &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; function used in binary &lt;a href="#logistic regression"&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;二元&lt;a href="#logistic regression"&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/a&gt;中使用的&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;函数。&lt;/p&gt;
&lt;h3 id="loss"&gt;loss&lt;/h3&gt;
&lt;p&gt;A measure of how far a model's &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt; are from its &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use &lt;a href="#mean squared error (mse)"&gt;&lt;strong&gt;mean squared error&lt;/strong&gt;&lt;/a&gt; for a loss function, while logistic regression models use &lt;a href="#log loss"&gt;&lt;strong&gt;Log Loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种衡量指标，用于衡量模型的&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;偏离其&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将&lt;a href="#mean squared error (mse)"&gt;&lt;strong&gt;均方误差&lt;/strong&gt;&lt;/a&gt;用于损失函数，而逻辑回归模型则使用&lt;a href="#log loss"&gt;&lt;strong&gt;对数损失函数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="m_1"&gt;M&lt;/h2&gt;
&lt;h3 id="machine learning"&gt;machine learning&lt;/h3&gt;
&lt;p&gt;A program or system that builds (trains) a predictive model from input data. The system uses the learned model to make useful predictions from new (never-before-seen) data drawn from the same distribution as the one used to train the model. Machine learning also refers to the field of study concerned with these programs or systems.&lt;/p&gt;
&lt;p&gt;一种程序或系统，用于根据输入数据构建（训练）预测模型。这种系统会利用学到的模型根据从分布（训练该模型时使用的同一分布）中提取的新数据（以前从未见过的数据）进行实用的预测。机器学习还指与这些程序或系统相关的研究领域。&lt;/p&gt;
&lt;h3 id="mean squared error (mse)"&gt;Mean Squared Error (MSE)&lt;/h3&gt;
&lt;p&gt;The average squared loss per example. MSE is calculated by dividing the &lt;a href="#squared loss"&gt;&lt;strong&gt;squared loss&lt;/strong&gt;&lt;/a&gt; by the number of &lt;a href="#example"&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt;. The values that &lt;a href="#tensorflow playground"&gt;&lt;strong&gt;TensorFlow Playground&lt;/strong&gt;&lt;/a&gt; displays for "Training loss" and "Test loss" are MSE.&lt;/p&gt;
&lt;p&gt;每个样本的平均平方损失。MSE 的计算方法是&lt;a href="#squared loss"&gt;&lt;strong&gt;平方损失&lt;/strong&gt;&lt;/a&gt;除以&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;数。&lt;a href="#tensorflow playground"&gt;&lt;strong&gt;TensorFlow Playground&lt;/strong&gt;&lt;/a&gt; 显示的&amp;ldquo;训练损失&amp;rdquo;值和&amp;ldquo;测试损失&amp;rdquo;值都是 MSE。&lt;/p&gt;
&lt;h3 id="metric"&gt;metric&lt;/h3&gt;
&lt;p&gt;A number that you care about. May or may not be directly optimized in a machine-learning system. A metric that your system tries to optimize is called an &lt;a href="#objective"&gt;&lt;strong&gt;objective&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;您关心的一个数值。可能可以也可能不可以直接在机器学习系统中得到优化。您的系统尝试优化的指标称为&lt;a href="#objective"&gt;&lt;strong&gt;目标&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="metrics api (tf.metrics)"&gt;Metrics API (tf.metrics)&lt;/h3&gt;
&lt;p&gt;A TensorFlow API for evaluating models. For example, &lt;code&gt;tf.metrics.accuracy&lt;/code&gt; determines how often a model's predictions match labels. When writing a &lt;a href="#custom estimator"&gt;&lt;strong&gt;custom Estimator&lt;/strong&gt;&lt;/a&gt;, you invoke Metrics API functions to specify how your model should be evaluated.&lt;/p&gt;
&lt;p&gt;一种用于评估模型的 TensorFlow API。例如，&lt;code&gt;tf.metrics.accuracy&lt;/code&gt; 用于确定模型的预测与标签匹配的频率。在编写&lt;a href="#custom estimator"&gt;&lt;strong&gt;自定义 Estimator&lt;/strong&gt;&lt;/a&gt; 时，您可以调用 Metrics API 函数来指定应如何评估您的模型。&lt;/p&gt;
&lt;h3 id="mini-batch"&gt;mini-batch&lt;/h3&gt;
&lt;p&gt;A small, randomly selected subset of the entire batch of &lt;a href="#example"&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt; run together in a single iteration of training or inference. The &lt;a href="#batch size"&gt;&lt;strong&gt;batch size&lt;/strong&gt;&lt;/a&gt; of a mini-batch is usually between 10 and 1,000. It is much more efficient to calculate the loss on a mini-batch than on the full training data.&lt;/p&gt;
&lt;p&gt;从训练或推断过程的一次迭代中一起运行的整批&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;内随机选择的一小部分。小批次的&lt;a href="#batch size"&gt;&lt;strong&gt;规模&lt;/strong&gt;&lt;/a&gt;通常介于 10 到 1000 之间。与基于完整的训练数据计算损失相比，基于小批次数据计算损失要高效得多。&lt;/p&gt;
&lt;h3 id="mini-batch stochastic gradient descent (sgd)"&gt;mini-batch stochastic gradient descent (SGD)&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; algorithm that uses &lt;a href="#mini-batch"&gt;&lt;strong&gt;mini-batches&lt;/strong&gt;&lt;/a&gt;. In other words, mini-batch SGD estimates the gradient based on a small subset of the training data. &lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;Vanilla SGD&lt;/strong&gt;&lt;/a&gt; uses a mini-batch of size 1.&lt;/p&gt;
&lt;p&gt;一种采用&lt;a href="#mini-batch"&gt;&lt;strong&gt;小批次&lt;/strong&gt;&lt;/a&gt;样本的&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;。也就是说，小批次 SGD 会根据一小部分训练数据来估算梯度。&lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;Vanilla SGD&lt;/strong&gt;&lt;/a&gt; 使用的小批次的规模为 1。&lt;/p&gt;
&lt;h3 id="ml"&gt;ML&lt;/h3&gt;
&lt;p&gt;Abbreviation for &lt;a href="#machine learning"&gt;&lt;strong&gt;machine learning&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#machine learning"&gt;&lt;strong&gt;机器学习&lt;/strong&gt;&lt;/a&gt;的缩写。&lt;/p&gt;
&lt;h3 id="model"&gt;model&lt;/h3&gt;
&lt;p&gt;The representation of what an ML system has learned from the training data. This is an overloaded term, which can have either of the following two related meanings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href="#tensorflow"&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/a&gt; graph that expresses the structure of how a prediction will be computed.&lt;/li&gt;
&lt;li&gt;The particular weights and biases of that TensorFlow graph, which are determined by &lt;a href="#model training"&gt;&lt;strong&gt;training&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;机器学习系统从训练数据学到的内容的表示形式。多含义术语，可以理解为下列两种相关含义之一：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一种 &lt;a href="#tensorflow"&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/a&gt; 图，用于表示预测计算结构。&lt;/li&gt;
&lt;li&gt;该 TensorFlow 图的特定权重和偏差，通过&lt;a href="#model training"&gt;&lt;strong&gt;训练&lt;/strong&gt;&lt;/a&gt;决定。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="model training"&gt;model training&lt;/h3&gt;
&lt;p&gt;The process of determining the best &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;确定最佳&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;的过程。&lt;/p&gt;
&lt;h3 id="momentum"&gt;Momentum&lt;/h3&gt;
&lt;p&gt;A sophisticated gradient descent algorithm in which a learning step depends not only on the derivative in the current step, but also on the derivatives of the step(s) that immediately preceded it. Momentum involves computing an exponentially weighted moving average of the gradients over time, analogous to momentum in physics. Momentum sometimes prevents learning from getting stuck in local minima.&lt;/p&gt;
&lt;p&gt;一种先进的梯度下降法，其中学习步长不仅取决于当前步长的导数，还取决于之前一步或多步的步长的导数。动量涉及计算梯度随时间而变化的指数级加权移动平均值，与物理学中的动量类似。动量有时可以防止学习过程被卡在局部最小的情况。&lt;/p&gt;
&lt;h3 id="multi-class classification"&gt;multi-class classification&lt;/h3&gt;
&lt;p&gt;Classification problems that distinguish among more than two classes. For example, there are approximately 128 species of maple trees, so a model that categorized maple tree species would be multi-class. Conversely, a model that divided emails into only two categories (&lt;em&gt;spam&lt;/em&gt; and &lt;em&gt;not spam&lt;/em&gt;) would be a &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;区分两种以上类别的分类问题。例如，枫树大约有 128 种，因此，确定枫树种类的模型就属于多类别模型。反之，仅将电子邮件分为两类（&amp;ldquo;垃圾邮件&amp;rdquo;和&amp;ldquo;非垃圾邮件&amp;rdquo;）的模型属于&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类模型&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="multinomial classification"&gt;multinomial classification&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h2 id="n_1"&gt;N&lt;/h2&gt;
&lt;h3 id="nan trap"&gt;NaN trap&lt;/h3&gt;
&lt;p&gt;When one number in your model becomes a &lt;a href="https://en.wikipedia.org/wiki/NaN"&gt;NaN&lt;/a&gt; during training, which causes many or all other numbers in your model to eventually become a NaN.&lt;/p&gt;
&lt;p&gt;NaN is an abbreviation for "Not a Number."&lt;/p&gt;
&lt;p&gt;模型中的一个数字在训练期间变成 &lt;a href="https://en.wikipedia.org/wiki/NaN"&gt;NaN&lt;/a&gt;，这会导致模型中的很多或所有其他数字最终也会变成 NaN。&lt;/p&gt;
&lt;p&gt;NaN 是&amp;ldquo;非数字&amp;rdquo;的缩写。&lt;/p&gt;
&lt;h3 id="negative class"&gt;negative class&lt;/h3&gt;
&lt;p&gt;In &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt;, one class is termed positive and the other is termed negative. The positive class is the thing we're looking for and the negative class is the other possibility. For example, the negative class in a medical test might be "not tumor." The negative class in an email classifier might be "not spam." See also &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;中，一种类别称为正类别，另一种类别称为负类别。正类别是我们要寻找的类别，负类别则是另一种可能性。例如，在医学检查中，负类别可以是&amp;ldquo;非肿瘤&amp;rdquo;。在电子邮件分类器中，负类别可以是&amp;ldquo;非垃圾邮件&amp;rdquo;。另请参阅&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="neural network"&gt;neural network&lt;/h3&gt;
&lt;p&gt;A model that, taking inspiration from the brain, is composed of layers (at least one of which is &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden&lt;/strong&gt;&lt;/a&gt;) consisting of simple connected units or &lt;a href="#neuron"&gt;&lt;strong&gt;neurons&lt;/strong&gt;&lt;/a&gt; followed by nonlinearities.&lt;/p&gt;
&lt;p&gt;一种模型，灵感来源于脑部结构，由多个层构成（至少有一个是&lt;a href="#hidden layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;），每个层都包含简单相连的单元或&lt;a href="#neuron"&gt;&lt;strong&gt;神经元&lt;/strong&gt;&lt;/a&gt;（具有非线性关系）。&lt;/p&gt;
&lt;h3 id="neuron"&gt;neuron&lt;/h3&gt;
&lt;p&gt;A node in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt;, typically taking in multiple input values and generating one output value. The neuron calculates the output value by applying an &lt;a href="#activation function"&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/a&gt; (nonlinear transformation) to a weighted sum of input values.&lt;/p&gt;
&lt;p&gt;&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的节点，通常是接收多个输入值并生成一个输出值。神经元通过将&lt;a href="#activation function"&gt;&lt;strong&gt;激活函数&lt;/strong&gt;&lt;/a&gt;（非线性转换）应用于输入值的加权和来计算输出值。&lt;/p&gt;
&lt;h3 id="node"&gt;node&lt;/h3&gt;
&lt;p&gt;An overloaded term that means either of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A neuron in a &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layer&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;An operation in a TensorFlow &lt;a href="#graph"&gt;&lt;strong&gt;graph&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;多含义术语，可以理解为下列两种含义之一：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hidden layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;中的神经元。&lt;/li&gt;
&lt;li&gt;TensorFlow &lt;a href="#graph"&gt;&lt;strong&gt;图&lt;/strong&gt;&lt;/a&gt;中的操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="normalization"&gt;normalization&lt;/h3&gt;
&lt;p&gt;The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. For example, suppose the natural range of a certain feature is 800 to 6,000. Through subtraction and division, you can normalize those values into the range -1 to +1.&lt;/p&gt;
&lt;p&gt;See also &lt;a href="#scaling"&gt;&lt;strong&gt;scaling&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;将实际的值区间转换为标准的值区间（通常为 -1 到 +1 或 0 到 1）的过程。例如，假设某个特征的自然区间是 800 到 6000。通过减法和除法运算，您可以将这些值标准化为位于 -1 到 +1 区间内。&lt;/p&gt;
&lt;p&gt;另请参阅&lt;a href="#scaling"&gt;&lt;strong&gt;缩放&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="numerical data"&gt;numerical data&lt;/h3&gt;
&lt;p&gt;&lt;a href="#feature"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/a&gt; represented as integers or real-valued numbers. For example, in a real estate model, you would probably represent the size of a house (in square feet or square meters) as numerical data. Representing a feature as numerical data indicates that the feature's values have a &lt;em&gt;mathematical&lt;/em&gt; relationship to each other and possibly to the label. For example, representing the size of a house as numerical data indicates that a 200 square-meter house is twice as large as a 100 square-meter house. Furthermore, the number of square meters in a house probably has some mathematical relationship to the price of the house.&lt;/p&gt;
&lt;p&gt;Not all integer data should be represented as numerical data. For example, postal codes in some parts of the world are integers; however, integer postal codes should not be represented as numerical data in models. That's because a postal code of &lt;code&gt;20000&lt;/code&gt; is not twice (or half) as potent as a postal code of 10000. Furthermore, although different postal codes &lt;em&gt;do&lt;/em&gt; correlate to different real estate values, we can't assume that real estate values at postal code 20000 are twice as valuable as real estate values at postal code 10000. Postal codes should be represented as &lt;a href="#categorical data"&gt;&lt;strong&gt;categorical data&lt;/strong&gt;&lt;/a&gt; instead.&lt;/p&gt;
&lt;p&gt;Numerical features are sometimes called &lt;a href="#continuous feature"&gt;&lt;strong&gt;continuous features&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;用整数或实数表示的&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;。例如，在房地产模型中，您可能会用数值数据表示房子大小（以平方英尺或平方米为单位）。如果用数值数据表示特征，则可以表明特征的值相互之间具有数学关系，并且与标签可能也有数学关系。例如，如果用数值数据表示房子大小，则可以表明面积为 200 平方米的房子是面积为 100 平方米的房子的两倍。此外，房子面积的平方米数可能与房价存在一定的数学关系。&lt;/p&gt;
&lt;p&gt;并非所有整数数据都应表示成数值数据。例如，世界上某些地区的邮政编码是整数，但在模型中，不应将整数邮政编码表示成数值数据。这是因为邮政编码 &lt;code&gt;20000&lt;/code&gt; 在效力上并不是邮政编码 10000 的两倍（或一半）。此外，虽然不同的邮政编码确实与不同的房地产价值有关，但我们也不能假设邮政编码为 20000 的房地产在价值上是邮政编码为 10000 的房地产的两倍。邮政编码应表示成&lt;a href="#categorical data"&gt;&lt;strong&gt;分类数据&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;数值特征有时称为&lt;a href="#continuous feature"&gt;&lt;strong&gt;连续特征&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="numpy"&gt;numpy&lt;/h3&gt;
&lt;p&gt;An &lt;a href="http://www.numpy.org/"&gt;open-source math library&lt;/a&gt; that provides efficient array operations in Python. &lt;a href="#pandas"&gt;&lt;strong&gt;pandas&lt;/strong&gt;&lt;/a&gt; is built on numpy.&lt;/p&gt;
&lt;p&gt;一个&lt;a href="http://www.numpy.org/"&gt;开放源代码数学库&lt;/a&gt;，在 Python 中提供高效的数组操作。&lt;a href="#pandas"&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/a&gt; 就建立在 Numpy 之上。&lt;/p&gt;
&lt;h2 id="o_1"&gt;O&lt;/h2&gt;
&lt;h3 id="objective"&gt;objective&lt;/h3&gt;
&lt;p&gt;A metric that your algorithm is trying to optimize.&lt;/p&gt;
&lt;p&gt;算法尝试优化的指标。&lt;/p&gt;
&lt;h3 id="offline inference"&gt;offline inference&lt;/h3&gt;
&lt;p&gt;Generating a group of &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt;, storing those predictions, and then retrieving those predictions on demand. Contrast with &lt;a href="#online inference"&gt;&lt;strong&gt;online inference&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;生成一组&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;，存储这些预测，然后根据需求检索这些预测。与&lt;a href="#online inference"&gt;&lt;strong&gt;在线推断&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="one-hot encoding"&gt;one-hot encoding&lt;/h3&gt;
&lt;p&gt;A sparse vector in which:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One element is set to 1.&lt;/li&gt;
&lt;li&gt;All other elements are set to 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One-hot encoding is commonly used to represent strings or identifiers that have a finite set of possible values. For example, suppose a given botany data set chronicles 15,000 different species, each denoted with a unique string identifier. As part of feature engineering, you'll probably encode those string identifiers as one-hot vectors in which the vector has a size of 15,000.&lt;/p&gt;
&lt;p&gt;一种稀疏向量，其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个元素设为 1。&lt;/li&gt;
&lt;li&gt;所有其他元素均设为 0。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;one-hot 编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为 one-hot 向量，向量的大小为 15000。&lt;/p&gt;
&lt;h3 id="one-vs.-all"&gt;one-vs.-all&lt;/h3&gt;
&lt;p&gt;Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classifiers&lt;/strong&gt;&lt;/a&gt;&amp;mdash;one binary classifier for each possible outcome. For example, given a model that classifies examples as animal, vegetable, or mineral, a one-vs.-all solution would provide the following three separate binary classifiers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;animal vs. not animal&lt;/li&gt;
&lt;li&gt;vegetable vs. not vegetable&lt;/li&gt;
&lt;li&gt;mineral vs. not mineral&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设某个分类问题有 N 种可能的解决方案，一对多解决方案将包含 N 个单独的&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类器&lt;/strong&gt;&lt;/a&gt; - 一个二元分类器对应一种可能的结果。例如，假设某个模型用于区分样本属于动物、蔬菜还是矿物，一对多解决方案将提供下列三个单独的二元分类器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;动物和非动物&lt;/li&gt;
&lt;li&gt;蔬菜和非蔬菜&lt;/li&gt;
&lt;li&gt;矿物和非矿物&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="online inference"&gt;online inference&lt;/h3&gt;
&lt;p&gt;Generating &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt; on demand. Contrast with &lt;a href="#offline inference"&gt;&lt;strong&gt;offline inference&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;根据需求生成&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;。与&lt;a href="#offline inference"&gt;&lt;strong&gt;离线推断&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="operation (op)"&gt;Operation (op)&lt;/h3&gt;
&lt;p&gt;A node in the TensorFlow graph. In TensorFlow, any procedure that creates, manipulates, or destroys a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt; is an operation. For example, a matrix multiply is an operation that takes two Tensors as input and generates one Tensor as output.&lt;/p&gt;
&lt;p&gt;TensorFlow 图中的节点。在 TensorFlow 中，任何创建、操纵或销毁&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;的过程都属于操作。例如，矩阵相乘就是一种操作，该操作以两个张量作为输入，并生成一个张量作为输出。&lt;/p&gt;
&lt;h3 id="optimizer"&gt;optimizer&lt;/h3&gt;
&lt;p&gt;A specific implementation of the &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; algorithm. TensorFlow's base class for optimizers is &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"&gt;tf.train.Optimizer&lt;/a&gt;. Different optimizers may leverage one or more of the following concepts to enhance the effectiveness of gradient descent on a given &lt;a href="#training set"&gt;&lt;strong&gt;training set&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer"&gt;momentum&lt;/a&gt; (Momentum)&lt;/li&gt;
&lt;li&gt;update frequency (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer"&gt;AdaGrad&lt;/a&gt; = ADAptive GRADient descent; &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer"&gt;Adam&lt;/a&gt; = ADAptive with Momentum; RMSProp)&lt;/li&gt;
&lt;li&gt;sparsity/regularization (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer"&gt;Ftrl&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;more complex math (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer"&gt;Proximal&lt;/a&gt;, and others)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might even imagine an &lt;a href="https://arxiv.org/abs/1606.04474"&gt;NN-driven optimizer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;的一种具体实现。TensorFlow 的优化器基类是 &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"&gt;tf.train.Optimizer&lt;/a&gt;。不同的优化器（&lt;code&gt;tf.train.Optimizer&lt;/code&gt; 的子类）会考虑如下概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer"&gt;动量&lt;/a&gt; (Momentum)&lt;/li&gt;
&lt;li&gt;更新频率 （&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer"&gt;AdaGrad&lt;/a&gt; = ADAptive GRADient descent； &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer"&gt;Adam&lt;/a&gt; = ADAptive with Momentum；RMSProp）&lt;/li&gt;
&lt;li&gt;稀疏性/正则化 (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer"&gt;Ftrl&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;更复杂的计算方法 （&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer"&gt;Proximal&lt;/a&gt;， 等等）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;甚至还包括 &lt;a href="https://arxiv.org/abs/1606.04474"&gt;NN 驱动的优化器&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="outliers"&gt;outliers&lt;/h3&gt;
&lt;p&gt;Values distant from most other values. In machine learning, any of the following are outliers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#weight"&gt;&lt;strong&gt;Weights&lt;/strong&gt;&lt;/a&gt; with high absolute values.&lt;/li&gt;
&lt;li&gt;Predicted values relatively far away from the actual values.&lt;/li&gt;
&lt;li&gt;Input data whose values are more than roughly 3 standard deviations from the mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Outliers often cause problems in model training.&lt;/p&gt;
&lt;p&gt;与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;绝对值很高的&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;与实际值相差很大的预测值。&lt;/li&gt;
&lt;li&gt;值比平均值高大约 3 个标准偏差的输入数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;离群值常常会导致模型训练出现问题。&lt;/p&gt;
&lt;h3 id="output layer"&gt;output layer&lt;/h3&gt;
&lt;p&gt;The "final" layer of a neural network. The layer containing the answer(s).&lt;/p&gt;
&lt;p&gt;神经网络的&amp;ldquo;最后&amp;rdquo;一层，也是包含答案的层。&lt;/p&gt;
&lt;h3 id="overfitting"&gt;overfitting&lt;/h3&gt;
&lt;p&gt;Creating a model that matches the &lt;a href="#training set"&gt;&lt;strong&gt;training data&lt;/strong&gt;&lt;/a&gt; so closely that the model fails to make correct predictions on new data.&lt;/p&gt;
&lt;p&gt;创建的模型与&lt;a href="#training set"&gt;&lt;strong&gt;训练数据&lt;/strong&gt;&lt;/a&gt;过于匹配，以致于模型无法根据新数据做出正确的预测。&lt;/p&gt;
&lt;h2 id="p_1"&gt;P&lt;/h2&gt;
&lt;h3 id="pandas"&gt;pandas&lt;/h3&gt;
&lt;p&gt;A column-oriented data analysis API. Many ML frameworks, including TensorFlow, support pandas data structures as input. See &lt;a href="http://pandas.pydata.org/"&gt;pandas documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;面向列的数据分析 API。很多机器学习框架（包括 TensorFlow）都支持将 Pandas 数据结构作为输入。请参阅 &lt;a href="http://pandas.pydata.org/"&gt;Pandas 文档&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="parameter"&gt;parameter&lt;/h3&gt;
&lt;p&gt;A variable of a model that the ML system trains on its own. For example, &lt;a href="#weight"&gt;&lt;strong&gt;weights&lt;/strong&gt;&lt;/a&gt; are parameters whose values the ML system gradually learns through successive training iterations. Contrast with &lt;a href="#hyperparameter"&gt;&lt;strong&gt;hyperparameter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;机器学习系统自行训练的模型的变量。例如，&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;就是一种参数，它们的值是机器学习系统通过连续的训练迭代逐渐学习到的。与&lt;a href="#hyperparameter"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="parameter server (ps)"&gt;Parameter Server (PS)&lt;/h3&gt;
&lt;p&gt;A job that keeps track of a model's &lt;a href="#parameter"&gt;&lt;strong&gt;parameters&lt;/strong&gt;&lt;/a&gt; in a distributed setting.&lt;/p&gt;
&lt;p&gt;一种作业，负责在分布式设置中跟踪模型&lt;a href="#parameter"&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="parameter update"&gt;parameter update&lt;/h3&gt;
&lt;p&gt;The operation of adjusting a model's &lt;a href="#parameter"&gt;&lt;strong&gt;parameters&lt;/strong&gt;&lt;/a&gt; during training, typically within a single iteration of &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在训练期间（通常是在&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;的单次迭代中）调整模型&lt;a href="#parameter"&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/a&gt;的操作。&lt;/p&gt;
&lt;h3 id="partial derivative"&gt;partial derivative&lt;/h3&gt;
&lt;p&gt;A derivative in which all but one of the variables is considered a constant. For example, the partial derivative of &lt;em&gt;f(x, y)&lt;/em&gt; with respect to &lt;em&gt;x&lt;/em&gt; is the derivative of &lt;em&gt;f&lt;/em&gt; considered as a function of &lt;em&gt;x&lt;/em&gt; alone (that is, keeping &lt;em&gt;y&lt;/em&gt; constant). The partial derivative of &lt;em&gt;f&lt;/em&gt; with respect to &lt;em&gt;x&lt;/em&gt; focuses only on how &lt;em&gt;x&lt;/em&gt; is changing and ignores all other variables in the equation.&lt;/p&gt;
&lt;p&gt;一种导数，除一个变量之外的所有变量都被视为常量。例如，f(x, y) 对 x 的偏导数就是 f(x) 的导数（即，使 y 保持恒定）。f 对 x 的偏导数仅关注 x 如何变化，而忽略公式中的所有其他变量。&lt;/p&gt;
&lt;h3 id="partitioning strategy"&gt;partitioning strategy&lt;/h3&gt;
&lt;p&gt;The algorithm by which variables are divided across &lt;a href="#parameter server (ps)"&gt;&lt;strong&gt;parameter servers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#parameter server (ps)"&gt;&lt;strong&gt;参数服务器&lt;/strong&gt;&lt;/a&gt;中分割变量的算法。&lt;/p&gt;
&lt;h3 id="performance"&gt;performance&lt;/h3&gt;
&lt;p&gt;Overloaded term with the following meanings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The traditional meaning within software engineering. Namely: How fast (or efficiently) does this piece of software run?&lt;/li&gt;
&lt;li&gt;The meaning within ML. Here, performance answers the following question: How correct is this &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt;? That is, how good are the model's predictions?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;多含义术语，具有以下含义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在软件工程中的传统含义。即：相应软件的运行速度有多快（或有多高效）？&lt;/li&gt;
&lt;li&gt;在机器学习中的含义。在机器学习领域，性能旨在回答以下问题：相应&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;的准确度有多高？即模型在预测方面的表现有多好？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="perplexity"&gt;perplexity&lt;/h3&gt;
&lt;p&gt;One measure of how well a &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; is accomplishing its task. For example, suppose your task is to read the first few letters of a word a user is typing on a smartphone keyboard, and to offer a list of possible completion words. Perplexity, P, for this task is approximately the number of guesses you need to offer in order for your list to contain the actual word the user is trying to type.&lt;/p&gt;
&lt;p&gt;Perplexity is related to &lt;a href="#cross-entropy"&gt;&lt;strong&gt;cross-entropy&lt;/strong&gt;&lt;/a&gt; as follows:&lt;/p&gt;
&lt;p&gt;一种衡量指标，用于衡量&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;能够多好地完成任务。例如，假设任务是读取用户使用智能手机键盘输入字词时输入的前几个字母，然后列出一组可能的完整字词。此任务的困惑度 (P) 是：为了使列出的字词中包含用户尝试输入的实际字词，您需要提供的猜测项的个数。&lt;/p&gt;
&lt;p&gt;困惑度与&lt;a href="#cross-entropy"&gt;&lt;strong&gt;交叉熵&lt;/strong&gt;&lt;/a&gt;的关系如下：&lt;/p&gt;
&lt;div class="math"&gt;$$P= 2^{-\text{cross entropy}}$$&lt;/div&gt;
&lt;h3 id="pipeline"&gt;pipeline&lt;/h3&gt;
&lt;p&gt;The infrastructure surrounding a machine learning algorithm. A pipeline includes gathering the data, putting the data into training data files, training one or more models, and exporting the models to production.&lt;/p&gt;
&lt;p&gt;机器学习算法的基础架构。流水线包括收集数据、将数据放入训练数据文件、训练一个或多个模型，以及将模型导出到生产环境。&lt;/p&gt;
&lt;h3 id="positive class"&gt;positive class&lt;/h3&gt;
&lt;p&gt;In &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt;, the two possible classes are labeled as positive and negative. The positive outcome is the thing we're testing for. (Admittedly, we're simultaneously testing for both outcomes, but play along.) For example, the positive class in a medical test might be "tumor." The positive class in an email classifier might be "spam."&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#negative class"&gt;&lt;strong&gt;negative class&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;中，两种可能的类别分别被标记为正类别和负类别。正类别结果是我们要测试的对象。（不可否认的是，我们会同时测试这两种结果，但只关注正类别结果。）例如，在医学检查中，正类别可以是&amp;ldquo;肿瘤&amp;rdquo;。在电子邮件分类器中，正类别可以是&amp;ldquo;垃圾邮件&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="precision"&gt;precision&lt;/h3&gt;
&lt;p&gt;A metric for &lt;a href="#classification model"&gt;&lt;strong&gt;classification models&lt;/strong&gt;&lt;/a&gt;. Precision identifies the frequency with which a model was correct when predicting the &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt;. That is:&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;指标。精确率指模型正确预测&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;的频率, 即：&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Precision} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Positives}}$$&lt;/div&gt;
&lt;h3 id="prediction"&gt;prediction&lt;/h3&gt;
&lt;p&gt;A model's output when provided with an input &lt;a href="#example"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;模型在收到输入的&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;后的输出。&lt;/p&gt;
&lt;h3 id="prediction bias"&gt;prediction bias&lt;/h3&gt;
&lt;p&gt;A value indicating how far apart the average of &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt; is from the average of &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt; in the data set.&lt;/p&gt;
&lt;p&gt;一个值，用于表明&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;平均值与数据集中&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的平均值相差有多大。&lt;/p&gt;
&lt;h3 id="pre-made estimator"&gt;pre-made Estimator&lt;/h3&gt;
&lt;p&gt;An &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt; that someone has already built. TensorFlow provides several pre-made Estimators, including &lt;code&gt;DNNClassifier&lt;/code&gt;, &lt;code&gt;DNNRegressor&lt;/code&gt;, and &lt;code&gt;LinearClassifier&lt;/code&gt;. You may build your own pre-made Estimators by following &lt;a href="https://www.tensorflow.org/extend/estimators"&gt;these instructions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;其他人已建好的 &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt;。TensorFlow 提供了一些预创建的 Estimator，包括 &lt;code&gt;DNNClassifier&lt;/code&gt;、&lt;code&gt;DNNRegressor&lt;/code&gt; 和 &lt;code&gt;LinearClassifier&lt;/code&gt;。您可以按照&lt;a href="https://www.tensorflow.org/extend/estimators"&gt;这些说明&lt;/a&gt;构建自己预创建的 Estimator。&lt;/p&gt;
&lt;h3 id="pre-trained model"&gt;pre-trained model&lt;/h3&gt;
&lt;p&gt;Models or model components (such as &lt;a href="#embeddings"&gt;&lt;strong&gt;embeddings&lt;/strong&gt;&lt;/a&gt;) that have been already been trained. Sometimes, you'll feed pre-trained embeddings into a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt;. Other times, your model will train the embeddings itself rather than rely on the pre-trained embeddings.&lt;/p&gt;
&lt;p&gt;已经过训练的模型或模型组件（例如&lt;a href="#embeddings"&gt;&lt;strong&gt;嵌套&lt;/strong&gt;&lt;/a&gt;）。有时，您需要将预训练的嵌套馈送到&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;。在其他时候，您的模型将自行训练嵌套，而不依赖于预训练的嵌套。&lt;/p&gt;
&lt;h3 id="prior belief"&gt;prior belief&lt;/h3&gt;
&lt;p&gt;What you believe about the data before you begin training on it. For example, &lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/a&gt; relies on a prior belief that &lt;a href="#weight"&gt;&lt;strong&gt;weights&lt;/strong&gt;&lt;/a&gt; should be small and normally distributed around zero.&lt;/p&gt;
&lt;p&gt;在开始采用相应数据进行训练之前，您对这些数据抱有的信念。例如，&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 正则化&lt;/strong&gt;&lt;/a&gt;依赖的先验信念是&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;应该很小且应以 0 为中心呈正态分布。&lt;/p&gt;
&lt;h2 id="q_1"&gt;Q&lt;/h2&gt;
&lt;h3 id="queue"&gt;queue&lt;/h3&gt;
&lt;p&gt;A TensorFlow &lt;a href="#operation (op)"&gt;&lt;strong&gt;Operation&lt;/strong&gt;&lt;/a&gt; that implements a queue data structure. Typically used in I/O.&lt;/p&gt;
&lt;p&gt;一种 TensorFlow &lt;a href="#operation (op)"&gt;&lt;strong&gt;操作&lt;/strong&gt;&lt;/a&gt;，用于实现队列数据结构。通常用于 I/O 中。&lt;/p&gt;
&lt;h2 id="r_1"&gt;R&lt;/h2&gt;
&lt;h3 id="rank"&gt;rank&lt;/h3&gt;
&lt;p&gt;Overloaded term in ML that can mean either of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of dimensions in a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt;. For instance, a scalar has rank 0, a vector has rank 1, and a matrix has rank 2.&lt;/li&gt;
&lt;li&gt;The ordinal position of a class in an ML problem that categorizes classes from highest to lowest. For example, a behavior ranking system could rank a dog's rewards from highest (a steak) to lowest (wilted kale).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;机器学习中的一个多含义术语，可以理解为下列含义之一：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;中的维度数量。例如，标量等级为 0，向量等级为 1，矩阵等级为 2。&lt;/li&gt;
&lt;li&gt;在将类别从最高到最低进行排序的机器学习问题中，类别的顺序位置。例如，行为排序系统可以将狗狗的奖励从最高（牛排）到最低（枯萎的羽衣甘蓝）进行排序。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rater"&gt;rater&lt;/h3&gt;
&lt;p&gt;A human who provides &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt; in &lt;a href="#example"&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt;. Sometimes called an "annotator."&lt;/p&gt;
&lt;p&gt;为&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;提供&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的人。有时称为&amp;ldquo;注释者&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="recall"&gt;recall&lt;/h3&gt;
&lt;p&gt;A metric for &lt;a href="#classification model"&gt;&lt;strong&gt;classification models&lt;/strong&gt;&lt;/a&gt; that answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is:&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Recall} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Negatives}} $$&lt;/div&gt;
&lt;p&gt;一种&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;指标，用于回答以下问题：在所有可能的正类别标签中，模型正确地识别出了多少个？即：&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{召回率} = \frac{\text{真正例数}} {\text{真正例数} + \text{假负例数}} $$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 id="rectified linear unit (relu)"&gt;Rectified Linear Unit (ReLU)&lt;/h3&gt;
&lt;p&gt;An &lt;a href="#activation function"&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/a&gt; with the following rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If input is negative or zero, output is 0.&lt;/li&gt;
&lt;li&gt;If input is positive, output is equal to input.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一种&lt;a href="#activation function"&gt;&lt;strong&gt;激活函数&lt;/strong&gt;&lt;/a&gt;，其规则如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果输入为负数或 0，则输出 0。&lt;/li&gt;
&lt;li&gt;如果输入为正数，则输出等于输入。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="regression model"&gt;regression model&lt;/h3&gt;
&lt;p&gt;A type of model that outputs continuous (typically, floating-point) values. Compare with &lt;a href="#classification model"&gt;&lt;strong&gt;classification models&lt;/strong&gt;&lt;/a&gt;, which output discrete values, such as "day lily" or "tiger lily."&lt;/p&gt;
&lt;p&gt;一种模型，能够输出连续的值（通常为浮点值）。请与&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;进行比较，分类模型输出离散值，例如&amp;ldquo;黄花菜&amp;rdquo;或&amp;ldquo;虎皮百合&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="regularization"&gt;regularization&lt;/h3&gt;
&lt;p&gt;The penalty on a model's complexity. Regularization helps prevent &lt;a href="#overfitting"&gt;&lt;strong&gt;overfitting&lt;/strong&gt;&lt;/a&gt;. Different kinds of regularization include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dropout regularization"&gt;&lt;strong&gt;dropout regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#early stopping"&gt;&lt;strong&gt;early stopping&lt;/strong&gt;&lt;/a&gt; (this is not a formal regularization method, but can effectively limit overfitting)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对模型复杂度的惩罚。正则化有助于防止出现&lt;a href="#overfitting"&gt;&lt;strong&gt;过拟合&lt;/strong&gt;&lt;/a&gt;，包含以下类型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dropout regularization"&gt;&lt;strong&gt;丢弃正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#early stopping"&gt;&lt;strong&gt;早停法&lt;/strong&gt;&lt;/a&gt;（这不是正式的正则化方法，但可以有效限制过拟合）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="regularization rate"&gt;regularization rate&lt;/h3&gt;
&lt;p&gt;A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; equation shows the regularization rate's influence:&lt;/p&gt;
&lt;p&gt;一种标量值，以 lambda 表示，用于指定正则化函数的相对重要性。从下面简化的&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;公式中可以看出正则化率的影响：&lt;/p&gt;
&lt;div class="math"&gt;$$\text{minimize(loss function + }\lambda\text{(regularization function))}$$&lt;/div&gt;
&lt;p&gt;Raising the regularization rate reduces &lt;a href="#overfitting"&gt;&lt;strong&gt;overfitting&lt;/strong&gt;&lt;/a&gt; but may make the model less &lt;a href="#accuracy"&gt;&lt;strong&gt;accurate&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;提高正则化率可以减少&lt;a href="#overfitting"&gt;&lt;strong&gt;过拟合&lt;/strong&gt;&lt;/a&gt;，但可能会使模型的&lt;a href="#accuracy"&gt;&lt;strong&gt;准确率&lt;/strong&gt;&lt;/a&gt;降低。&lt;/p&gt;
&lt;h3 id="representation"&gt;representation&lt;/h3&gt;
&lt;p&gt;The process of mapping data to useful &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;将数据映射到实用&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;的过程。&lt;/p&gt;
&lt;h3 id="roc (receiver operating characteristic) curve"&gt;ROC (receiver operating characteristic) Curve&lt;/h3&gt;
&lt;p&gt;A curve of &lt;a href="#true positive rate (tp rate)"&gt;&lt;strong&gt;true positive rate&lt;/strong&gt;&lt;/a&gt; vs. &lt;a href="#false positive rate (fp rate)"&gt;&lt;strong&gt;false positive rate&lt;/strong&gt;&lt;/a&gt; at different &lt;a href="#classification threshold"&gt;&lt;strong&gt;classification thresholds&lt;/strong&gt;&lt;/a&gt;. See also &lt;a href="#auc (area under the roc curve)"&gt;&lt;strong&gt;AUC&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;不同&lt;a href="#classification threshold"&gt;&lt;strong&gt;分类阈值&lt;/strong&gt;&lt;/a&gt;下的&lt;a href="#true positive rate (tp rate)"&gt;&lt;strong&gt;真正例率&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#false positive rate (fp rate)"&gt;&lt;strong&gt;假正例率&lt;/strong&gt;&lt;/a&gt;构成的曲线。另请参阅&lt;a href="#auc (area under the roc curve)"&gt;&lt;strong&gt;曲线下面积&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="root directory"&gt;root directory&lt;/h3&gt;
&lt;p&gt;The directory you specify for hosting subdirectories of the TensorFlow checkpoint and events files of multiple models.&lt;/p&gt;
&lt;p&gt;您指定的目录，用于托管多个模型的 TensorFlow 检查点和事件文件的子目录。&lt;/p&gt;
&lt;h3 id="root mean squared error (rmse)"&gt;Root Mean Squared Error (RMSE)&lt;/h3&gt;
&lt;p&gt;The square root of the &lt;a href="#mean squared error (mse)"&gt;&lt;strong&gt;Mean Squared Error&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#mean squared error (mse)"&gt;&lt;strong&gt;均方误差&lt;/strong&gt;&lt;/a&gt;的平方根。&lt;/p&gt;
&lt;h2 id="s_1"&gt;S&lt;/h2&gt;
&lt;h3 id="savedmodel"&gt;SavedModel&lt;/h3&gt;
&lt;p&gt;The recommended format for saving and recovering TensorFlow models. SavedModel is a language-neutral, recoverable serialization format, which enables higher-level systems and tools to produce, consume, and transform TensorFlow models.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://www.tensorflow.org/programmers_guide/saved_model"&gt;Saving and Restoring&lt;/a&gt; in the TensorFlow Programmer's Guide for complete details.&lt;/p&gt;
&lt;p&gt;保存和恢复 TensorFlow 模型时建议使用的格式。SavedModel 是一种独立于语言且可恢复的序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型。&lt;/p&gt;
&lt;p&gt;如需完整的详细信息，请参阅《TensorFlow 编程人员指南》中的&lt;a href="https://www.tensorflow.org/programmers_guide/saved_model"&gt;保存和恢复&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="saver"&gt;Saver&lt;/h3&gt;
&lt;p&gt;A &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver"&gt;TensorFlow object&lt;/a&gt; responsible for saving model checkpoints.&lt;/p&gt;
&lt;p&gt;一种 &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver"&gt;TensorFlow 对象&lt;/a&gt;，负责保存模型检查点。&lt;/p&gt;
&lt;h3 id="scaling"&gt;scaling&lt;/h3&gt;
&lt;p&gt;A commonly used practice in &lt;a href="#feature engineering"&gt;&lt;strong&gt;feature engineering&lt;/strong&gt;&lt;/a&gt; to tame a feature's range of values to match the range of other features in the data set. For example, suppose that you want all floating-point features in the data set to have a range of 0 to 1. Given a particular feature's range of 0 to 500, you could scale that feature by dividing each value by 500.&lt;/p&gt;
&lt;p&gt;See also &lt;a href="#normalization"&gt;&lt;strong&gt;normalization&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#feature engineering"&gt;&lt;strong&gt;特征工程&lt;/strong&gt;&lt;/a&gt;中的一种常用做法，是对某个特征的值区间进行调整，使之与数据集中其他特征的值区间一致。例如，假设您希望数据集中所有浮点特征的值都位于 0 到 1 区间内，如果某个特征的值位于 0 到 500 区间内，您就可以通过将每个值除以 500 来缩放该特征。&lt;/p&gt;
&lt;p&gt;另请参阅&lt;a href="#normalization"&gt;&lt;strong&gt;标准化&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="scikit-learn"&gt;scikit-learn&lt;/h3&gt;
&lt;p&gt;A popular open-source ML platform. See &lt;a href="http://www.scikit-learn.org/"&gt;www.scikit-learn.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一个热门的开放源代码机器学习平台。请访问 &lt;a href="http://www.scikit-learn.org/"&gt;www.scikit-learn.org&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="semi-supervised learning"&gt;semi-supervised learning&lt;/h3&gt;
&lt;p&gt;Training a model on data where some of the training examples have labels but others don&amp;rsquo;t. One technique for semi-supervised learning is to infer labels for the unlabeled examples, and then to train on the inferred labels to create a new model. Semi-supervised learning can be useful if labels are expensive to obtain but unlabeled examples are plentiful.&lt;/p&gt;
&lt;p&gt;训练模型时采用的数据中，某些训练样本有标签，而其他样本则没有标签。半监督式学习采用的一种技术是推断无标签样本的标签，然后使用推断出的标签进行训练，以创建新模型。如果获得有标签样本需要高昂的成本，而无标签样本则有很多，那么半监督式学习将非常有用。&lt;/p&gt;
&lt;h3 id="sequence model"&gt;sequence model&lt;/h3&gt;
&lt;p&gt;A model whose inputs have a sequential dependence. For example, predicting the next video watched from a sequence of previously watched videos.&lt;/p&gt;
&lt;p&gt;一种模型，其输入具有序列依赖性。例如，根据之前观看过的一系列视频对观看的下一个视频进行预测。&lt;/p&gt;
&lt;h3 id="session"&gt;session&lt;/h3&gt;
&lt;p&gt;Maintains state (for example, variables) within a TensorFlow program.&lt;/p&gt;
&lt;p&gt;维持 TensorFlow 程序中的状态（例如变量）。&lt;/p&gt;
&lt;h3 id="sigmoid function"&gt;sigmoid function&lt;/h3&gt;
&lt;p&gt;A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1. The sigmoid function has the following formula:&lt;/p&gt;
&lt;p&gt;一种函数，可将逻辑回归输出或多项回归输出（对数几率）映射到概率，以返回介于 0 到 1 之间的值。S 型函数的公式如下：&lt;/p&gt;
&lt;div class="math"&gt;$$y = \frac{1}{1 + e^{-\sigma}}$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; in &lt;a href="#logistic regression"&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt; problems is simply:&lt;/p&gt;
&lt;p&gt;在&lt;a href="#logistic regression"&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/a&gt;问题中，&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; 非常简单：&lt;/p&gt;
&lt;div class="math"&gt;$$\\sigma = b + w_1x_1 + w_2x_2 + &amp;hellip; w_nx_n$$&lt;/div&gt;
&lt;p&gt;In other words, the sigmoid function converts &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; into a probability between 0 and 1.&lt;/p&gt;
&lt;p&gt;换句话说，S 型函数可将 &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; 转换为介于 0 到 1 之间的概率。&lt;/p&gt;
&lt;p&gt;In some &lt;a href="#neural network"&gt;&lt;strong&gt;neural networks&lt;/strong&gt;&lt;/a&gt;, the sigmoid function acts as the &lt;a href="#activation function"&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在某些&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中，S 型函数可作为&lt;a href="#activation function"&gt;&lt;strong&gt;激活函数&lt;/strong&gt;&lt;/a&gt;使用。&lt;/p&gt;
&lt;h3 id="softmax"&gt;softmax&lt;/h3&gt;
&lt;p&gt;A function that provides probabilities for each possible class in a &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification model&lt;/strong&gt;&lt;/a&gt;. The probabilities add up to exactly 1.0. For example, softmax might determine that the probability of a particular image being a dog at 0.9, a cat at 0.08, and a horse at 0.02. (Also called &lt;strong&gt;full softmax&lt;/strong&gt;.)&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#candidate sampling"&gt;&lt;strong&gt;candidate sampling&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种函数，可提供&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类模型&lt;/strong&gt;&lt;/a&gt;中每个可能类别的概率。这些概率的总和正好为 1.0。例如，softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为&lt;strong&gt;完整 softmax&lt;/strong&gt;。）&lt;/p&gt;
&lt;p&gt;与&lt;a href="#candidate sampling"&gt;&lt;strong&gt;候选采样&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="sparse feature"&gt;sparse feature&lt;/h3&gt;
&lt;p&gt;&lt;a href="#feature"&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/a&gt; vector whose values are predominately zero or empty. For example, a vector containing a single 1 value and a million 0 values is sparse. As another example, words in a search query could also be a sparse feature&amp;mdash;there are many possible words in a given language, but only a few of them occur in a given query.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#dense feature"&gt;&lt;strong&gt;dense feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;向量，其中的大多数值都为 0 或为空。例如，某个向量包含一个为 1 的值和一百万个为 0 的值，则该向量就属于稀疏向量。再举一个例子，搜索查询中的单词也可能属于稀疏特征 - 在某种指定语言中有很多可能的单词，但在某个指定的查询中仅包含其中几个。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#dense feature"&gt;&lt;strong&gt;密集特征&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="squared hinge loss"&gt;squared hinge loss&lt;/h3&gt;
&lt;p&gt;The square of the &lt;a href="#hinge loss"&gt;&lt;strong&gt;hinge loss&lt;/strong&gt;&lt;/a&gt;. Squared hinge loss penalizes outliers more harshly than regular hinge loss.&lt;/p&gt;
&lt;p&gt;&lt;a href="#hinge loss"&gt;&lt;strong&gt;合页损失函数&lt;/strong&gt;&lt;/a&gt;的平方。与常规合页损失函数相比，平方合页损失函数对离群值的惩罚更严厉。&lt;/p&gt;
&lt;h3 id="squared loss"&gt;squared loss&lt;/h3&gt;
&lt;p&gt;The &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; function used in &lt;a href="#linear regression"&gt;&lt;strong&gt;linear regression&lt;/strong&gt;&lt;/a&gt;. (Also known as &lt;strong&gt;L2 Loss&lt;/strong&gt;.) This function calculates the squares of the difference between a model's predicted value for a labeled &lt;a href="#example"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt; and the actual value of the &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. Due to squaring, this loss function amplifies the influence of bad predictions. That is, squared loss reacts more strongly to outliers than &lt;a href="#l1 loss"&gt;&lt;strong&gt;L1 loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#linear regression"&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/a&gt;中使用的&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;函数（也称为 &lt;strong&gt;L2 损失函数&lt;/strong&gt;）。该函数可计算模型为有标签&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;预测的值和&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的实际值之差的平方。由于取平方值，因此该损失函数会放大不佳预测的影响。也就是说，与 &lt;a href="#l1 loss"&gt;&lt;strong&gt;L1 损失函数&lt;/strong&gt;&lt;/a&gt;相比，平方损失函数对离群值的反应更强烈。&lt;/p&gt;
&lt;h3 id="static model"&gt;static model&lt;/h3&gt;
&lt;p&gt;A model that is trained offline.&lt;/p&gt;
&lt;p&gt;离线训练的一种模型。&lt;/p&gt;
&lt;h3 id="stationarity"&gt;stationarity&lt;/h3&gt;
&lt;p&gt;A property of data in a data set, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December.&lt;/p&gt;
&lt;p&gt;数据集中数据的一种属性，表示数据分布在一个或多个维度保持不变。这种维度最常见的是时间，即表明平稳性的数据不随时间而变化。例如，从 9 月到 12 月，表明平稳性的数据没有发生变化。&lt;/p&gt;
&lt;h3 id="step"&gt;step&lt;/h3&gt;
&lt;p&gt;A forward and backward evaluation of one &lt;a href="#batch"&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;对一个&lt;a href="#batch"&gt;&lt;strong&gt;批次&lt;/strong&gt;&lt;/a&gt;的向前和向后评估。&lt;/p&gt;
&lt;h3 id="step size"&gt;step size&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#learning rate"&gt;&lt;strong&gt;learning rate&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#learning rate"&gt;&lt;strong&gt;学习速率&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="stochastic gradient descent (sgd)"&gt;stochastic gradient descent (SGD)&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; algorithm in which the batch size is one. In other words, SGD relies on a single example chosen uniformly at random from a data set to calculate an estimate of the gradient at each step.&lt;/p&gt;
&lt;p&gt;批次规模为 1 的一种&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;。换句话说，SGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。&lt;/p&gt;
&lt;h3 id="structural risk minimization (srm)"&gt;structural risk minimization (SRM)&lt;/h3&gt;
&lt;p&gt;An algorithm that balances two goals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The desire to build the most predictive model (for example, lowest loss).&lt;/li&gt;
&lt;li&gt;The desire to keep the model as simple as possible (for example, strong regularization).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, a model function that minimizes loss+regularization on the training set is a structural risk minimization algorithm.&lt;/p&gt;
&lt;p&gt;For more information, see &lt;a href="http://www.svms.org/srm/"&gt;http://www.svms.org/srm/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#empirical risk minimization (erm)"&gt;&lt;strong&gt;empirical risk minimization&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种算法，用于平衡以下两个目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;期望构建最具预测性的模型（例如损失最低）。&lt;/li&gt;
&lt;li&gt;期望使模型尽可能简单（例如强大的正则化）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如，旨在将基于训练集的损失和正则化降至最低的模型函数就是一种结构风险最小化算法。&lt;/p&gt;
&lt;p&gt;如需更多信息，请参阅 &lt;a href="http://www.svms.org/srm/"&gt;http://www.svms.org/srm/&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#empirical risk minimization (erm)"&gt;&lt;strong&gt;经验风险最小化&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="summary"&gt;summary&lt;/h3&gt;
&lt;p&gt;In TensorFlow, a value or set of values calculated at a particular &lt;a href="#step"&gt;&lt;strong&gt;step&lt;/strong&gt;&lt;/a&gt;, usually used for tracking model metrics during training.&lt;/p&gt;
&lt;p&gt;在 TensorFlow 中的某一&lt;a href="#step"&gt;&lt;strong&gt;步&lt;/strong&gt;&lt;/a&gt;计算出的一个值或一组值，通常用于在训练期间跟踪模型指标。&lt;/p&gt;
&lt;h3 id="supervised machine learning"&gt;supervised machine learning&lt;/h3&gt;
&lt;p&gt;Training a &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; from input data and its corresponding &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt;. Supervised machine learning is analogous to a student learning a subject by studying a set of questions and their corresponding answers. After mastering the mapping between questions and answers, the student can then provide answers to new (never-before-seen) questions on the same topic. Compare with &lt;a href="#unsupervised machine learning"&gt;&lt;strong&gt;unsupervised machine learning&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;根据输入数据及其对应的&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;来训练&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;。监督式机器学习类似于学生通过研究一系列问题及其对应的答案来学习某个主题。在掌握了问题和答案之间的对应关系后，学生便可以回答关于同一主题的新问题（以前从未见过的问题）。请与&lt;a href="#unsupervised machine learning"&gt;&lt;strong&gt;非监督式机器学习&lt;/strong&gt;&lt;/a&gt;进行比较。&lt;/p&gt;
&lt;h3 id="synthetic feature"&gt;synthetic feature&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#feature"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/a&gt; that is not present among the input features, but is derived from one or more of them. Kinds of synthetic features include the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiplying one feature by itself or by other feature(s). (These are termed &lt;a href="#feature cross"&gt;&lt;strong&gt;feature crosses&lt;/strong&gt;&lt;/a&gt;.)&lt;/li&gt;
&lt;li&gt;Dividing one feature by a second feature.&lt;/li&gt;
&lt;li&gt;&lt;a href="#bucketing"&gt;&lt;strong&gt;Bucketing&lt;/strong&gt;&lt;/a&gt; a continuous feature into range bins.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Features created by &lt;a href="#normalization"&gt;&lt;strong&gt;normalizing&lt;/strong&gt;&lt;/a&gt; or &lt;a href="#scaling"&gt;&lt;strong&gt;scaling&lt;/strong&gt;&lt;/a&gt; alone are not considered synthetic features.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，不在输入特征之列，而是从一个或多个输入特征衍生而来。合成特征包括以下类型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将一个特征与其本身或其他特征相乘（称为&lt;a href="#feature cross"&gt;&lt;strong&gt;特征组合&lt;/strong&gt;&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;两个特征相除。&lt;/li&gt;
&lt;li&gt;对连续特征进行&lt;a href="#bucketing"&gt;&lt;strong&gt;分桶&lt;/strong&gt;&lt;/a&gt;，以分为多个区间分箱。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过&lt;a href="#normalization"&gt;&lt;strong&gt;标准化&lt;/strong&gt;&lt;/a&gt;或&lt;a href="#scaling"&gt;&lt;strong&gt;缩放&lt;/strong&gt;&lt;/a&gt;单独创建的特征不属于合成特征。&lt;/p&gt;
&lt;h2 id="t_1"&gt;T&lt;/h2&gt;
&lt;h3 id="target"&gt;target&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="temporal data"&gt;temporal data&lt;/h3&gt;
&lt;p&gt;Data recorded at different points in time. For example, winter coat sales recorded for each day of the year would be temporal data.&lt;/p&gt;
&lt;p&gt;在不同时间点记录的数据。例如，记录的一年中每一天的冬外套销量就属于时态数据。&lt;/p&gt;
&lt;h3 id="tensor"&gt;Tensor&lt;/h3&gt;
&lt;p&gt;The primary data structure in TensorFlow programs. Tensors are N-dimensional (where N could be very large) data structures, most commonly scalars, vectors, or matrices. The elements of a Tensor can hold integer, floating-point, or string values.&lt;/p&gt;
&lt;p&gt;TensorFlow 程序中的主要数据结构。张量是 N 维（其中 N 可能非常大）数据结构，最常见的是标量、向量或矩阵。张量的元素可以包含整数值、浮点值或字符串值。&lt;/p&gt;
&lt;h3 id="tensor processing unit (tpu)"&gt;Tensor Processing Unit (TPU)&lt;/h3&gt;
&lt;p&gt;An ASIC (application-specific integrated circuit) that optimizes the performance of TensorFlow programs.&lt;/p&gt;
&lt;p&gt;一种 ASIC（应用专用集成电路），用于优化 TensorFlow 程序的性能。&lt;/p&gt;
&lt;h3 id="tensor rank"&gt;Tensor rank&lt;/h3&gt;
&lt;p&gt;See &lt;a href="#rank"&gt;&lt;strong&gt;rank&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="tensor shape"&gt;Tensor shape&lt;/h3&gt;
&lt;p&gt;The number of elements a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt; contains in various dimensions. For example, a [5, 10] Tensor has a shape of 5 in one dimension and 10 in another.&lt;/p&gt;
&lt;p&gt;&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;在各种维度中包含的元素数。例如，张量 [5, 10] 在一个维度中的形状为 5，在另一个维度中的形状为 10。&lt;/p&gt;
&lt;h3 id="tensor size"&gt;Tensor size&lt;/h3&gt;
&lt;p&gt;The total number of scalars a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt; contains. For example, a [5, 10] Tensor has a size of 50.&lt;/p&gt;
&lt;p&gt;&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;包含的标量总数。例如，张量 [5, 10] 的大小为 50。&lt;/p&gt;
&lt;h3 id="tensorboard"&gt;TensorBoard&lt;/h3&gt;
&lt;p&gt;The dashboard that displays the summaries saved during the execution of one or more TensorFlow programs.&lt;/p&gt;
&lt;p&gt;一个信息中心，用于显示在执行一个或多个 TensorFlow 程序期间保存的摘要信息。&lt;/p&gt;
&lt;h3 id="tensorflow"&gt;TensorFlow&lt;/h3&gt;
&lt;p&gt;A large-scale, distributed, machine learning platform. The term also refers to the base API layer in the TensorFlow stack, which supports general computation on dataflow graphs.&lt;/p&gt;
&lt;p&gt;Although TensorFlow is primarily used for machine learning, you may also use TensorFlow for non-ML tasks that require numerical computation using dataflow graphs.&lt;/p&gt;
&lt;p&gt;一个大型的分布式机器学习平台。该术语还指 TensorFlow 堆栈中的基本 API 层，该层支持对数据流图进行一般计算。&lt;/p&gt;
&lt;p&gt;虽然 TensorFlow 主要应用于机器学习领域，但也可用于需要使用数据流图进行数值计算的非机器学习任务。&lt;/p&gt;
&lt;h3 id="tensorflow playground"&gt;TensorFlow Playground&lt;/h3&gt;
&lt;p&gt;A program that visualizes how different &lt;a href="#hyperparameters"&gt;&lt;strong&gt;hyperparameters&lt;/strong&gt;&lt;/a&gt; influence model (primarily neural network) training. Go to &lt;a href="http://playground.tensorflow.org"&gt;http://playground.tensorflow.org&lt;/a&gt; to experiment with TensorFlow Playground.&lt;/p&gt;
&lt;p&gt;一款用于直观呈现不同的&lt;a href="#hyperparameters"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;对模型（主要是神经网络）训练的影响的程序。要试用 TensorFlow Playground，请前往 &lt;a href="http://playground.tensorflow.org"&gt;http://playground.tensorflow.org&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="tensorflow serving"&gt;TensorFlow Serving&lt;/h3&gt;
&lt;p&gt;A platform to deploy trained models in production.&lt;/p&gt;
&lt;p&gt;一个平台，用于将训练过的模型部署到生产环境。&lt;/p&gt;
&lt;h3 id="test set"&gt;test set&lt;/h3&gt;
&lt;p&gt;The subset of the data set that you use to test your &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; after the model has gone through initial vetting by the validation set.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#training set"&gt;&lt;strong&gt;training set&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#validation set"&gt;&lt;strong&gt;validation set&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;数据集的子集，用于在&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;经由验证集的初步验证之后测试模型。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#training set"&gt;&lt;strong&gt;训练集&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#validation set"&gt;&lt;strong&gt;验证集&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="tf.example"&gt;tf.Example&lt;/h3&gt;
&lt;p&gt;A standard &lt;a href="https://developers.google.com/protocol-buffers/"&gt;protocol buffer&lt;/a&gt; for describing input data for machine learning model training or inference.&lt;/p&gt;
&lt;p&gt;一种标准的 &lt;a href="https://developers.google.com/protocol-buffers/"&gt;proto buffer&lt;/a&gt;，旨在描述用于机器学习模型训练或推断的输入数据。&lt;/p&gt;
&lt;h3 id="time series analysis"&gt;time series analysis&lt;/h3&gt;
&lt;p&gt;A subfield of machine learning and statistics that analyzes &lt;a href="#temporal data"&gt;&lt;strong&gt;temporal data&lt;/strong&gt;&lt;/a&gt;. Many types of machine learning problems require time series analysis, including classification, clustering, forecasting, and anomaly detection. For example, you could use time series analysis to forecast the future sales of winter coats by month based on historical sales data.&lt;/p&gt;
&lt;p&gt;机器学习和统计学的一个子领域，旨在分析&lt;a href="#temporal data"&gt;&lt;strong&gt;时态数据&lt;/strong&gt;&lt;/a&gt;。很多类型的机器学习问题都需要时间序列分析，其中包括分类、聚类、预测和异常检测。例如，您可以利用时间序列分析根据历史销量数据预测未来每月的冬外套销量。&lt;/p&gt;
&lt;h3 id="training"&gt;training&lt;/h3&gt;
&lt;p&gt;The process of determining the ideal &lt;a href="#parameter"&gt;&lt;strong&gt;parameters&lt;/strong&gt;&lt;/a&gt; comprising a model.&lt;/p&gt;
&lt;p&gt;确定构成模型的理想&lt;a href="#parameter"&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/a&gt;的过程。&lt;/p&gt;
&lt;h3 id="training set"&gt;training set&lt;/h3&gt;
&lt;p&gt;The subset of the data set used to train a model.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#validation set"&gt;&lt;strong&gt;validation set&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#test set"&gt;&lt;strong&gt;test set&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;数据集的子集，用于训练模型。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#validation set"&gt;&lt;strong&gt;验证集&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#test set"&gt;&lt;strong&gt;测试集&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="transfer learning"&gt;transfer learning&lt;/h3&gt;
&lt;p&gt;Transferring information from one machine learning task to another. For example, in multi-task learning, a single model solves multiple tasks, such as a &lt;a href="#deep model"&gt;&lt;strong&gt;deep model&lt;/strong&gt;&lt;/a&gt; that has different output nodes for different tasks. Transfer learning might involve transferring knowledge from the solution of a simpler task to a more complex one, or involve transferring knowledge from a task where there is more data to one where there is less data.&lt;/p&gt;
&lt;p&gt;Most machine learning systems solve a &lt;em&gt;single&lt;/em&gt; task. Transfer learning is a baby step towards artificial intelligence in which a single program can solve &lt;em&gt;multiple&lt;/em&gt; tasks.&lt;/p&gt;
&lt;p&gt;将信息从一个机器学习任务转移到另一个机器学习任务。例如，在多任务学习中，一个模型可以完成多项任务，例如针对不同任务具有不同输出节点的&lt;a href="#deep model"&gt;&lt;strong&gt;深度模型&lt;/strong&gt;&lt;/a&gt;。转移学习可能涉及将知识从较简单任务的解决方案转移到较复杂的任务，或者将知识从数据较多的任务转移到数据较少的任务。&lt;/p&gt;
&lt;p&gt;大多数机器学习系统都只能完成一项任务。转移学习是迈向人工智能的一小步；在人工智能中，单个程序可以完成多项任务。&lt;/p&gt;
&lt;h3 id="true negative (tn)"&gt;true negative (TN)&lt;/h3&gt;
&lt;p&gt;An example in which the model &lt;em&gt;correctly&lt;/em&gt; predicted the &lt;a href="#negative class"&gt;&lt;strong&gt;negative class&lt;/strong&gt;&lt;/a&gt;. For example, the model inferred that a particular email message was not spam, and that email message really was not spam.&lt;/p&gt;
&lt;p&gt;被模型正确地预测为&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;的样本。例如，模型推断出某封电子邮件不是垃圾邮件，而该电子邮件确实不是垃圾邮件。&lt;/p&gt;
&lt;h3 id="true positive (tp)"&gt;true positive (TP)&lt;/h3&gt;
&lt;p&gt;An example in which the model &lt;em&gt;correctly&lt;/em&gt; predicted the &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt;. For example, the model inferred that a particular email message was spam, and that email message really was spam.&lt;/p&gt;
&lt;p&gt;被模型正确地预测为&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;的样本。例如，模型推断出某封电子邮件是垃圾邮件，而该电子邮件确实是垃圾邮件。&lt;/p&gt;
&lt;h3 id="true positive rate (tp rate)"&gt;true positive rate (TP rate)&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#recall"&gt;&lt;strong&gt;recall&lt;/strong&gt;&lt;/a&gt;. That is:&lt;/p&gt;
&lt;p&gt;是&lt;a href="#recall"&gt;&lt;strong&gt;召回率&lt;/strong&gt;&lt;/a&gt;的同义词，即：&lt;/p&gt;
&lt;div class="math"&gt;$$\text{True Positive Rate} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Negatives}}$$&lt;/div&gt;
&lt;p&gt;True positive rate is the y-axis in an &lt;a href="#roc (receiver operating characteristic) curve"&gt;&lt;strong&gt;ROC curve&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;真正例率是 &lt;a href="#roc (receiver operating characteristic) curve"&gt;&lt;strong&gt;ROC 曲线&lt;/strong&gt;&lt;/a&gt;的 y 轴。&lt;/p&gt;
&lt;h2 id="u_1"&gt;U&lt;/h2&gt;
&lt;h3 id="unlabeled example"&gt;unlabeled example&lt;/h3&gt;
&lt;p&gt;An example that contains &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; but no &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. Unlabeled examples are the input to &lt;a href="#inference"&gt;&lt;strong&gt;inference&lt;/strong&gt;&lt;/a&gt;. In &lt;a href="#semi-supervised learning"&gt;&lt;strong&gt;semi-supervised&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#unsupervised machine learning"&gt;&lt;strong&gt;unsupervised&lt;/strong&gt;&lt;/a&gt; learning, unlabeled examples are used during training.&lt;/p&gt;
&lt;p&gt;包含&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;但没有&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的样本。无标签样本是用于进行&lt;a href="#inference"&gt;&lt;strong&gt;推断&lt;/strong&gt;&lt;/a&gt;的输入内容。在&lt;a href="#semi-supervised learning"&gt;&lt;strong&gt;半监督式&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#unsupervised machine learning"&gt;&lt;strong&gt;非监督式&lt;/strong&gt;&lt;/a&gt;学习中，无标签样本在训练期间被使用。&lt;/p&gt;
&lt;h3 id="unsupervised machine learning"&gt;unsupervised machine learning&lt;/h3&gt;
&lt;p&gt;Training a &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; to find patterns in a data set, typically an unlabeled data set.&lt;/p&gt;
&lt;p&gt;The most common use of unsupervised machine learning is to cluster data into groups of similar examples. For example, an unsupervised machine learning algorithm can cluster songs together based on various properties of the music. The resulting clusters can become an input to other machine learning algorithms (for example, to a music recommendation service). Clustering can be helpful in domains where true labels are hard to obtain. For example, in domains such as anti-abuse and fraud, clusters can help humans better understand the data.&lt;/p&gt;
&lt;p&gt;Another example of unsupervised machine learning is &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;&lt;strong&gt;principal component analysis (PCA)&lt;/strong&gt;&lt;/a&gt;. For example, applying PCA on a data set containing the contents of millions of shopping carts might reveal that shopping carts containing lemons frequently also contain antacids.&lt;/p&gt;
&lt;p&gt;Compare with &lt;a href="#supervised machine learning"&gt;&lt;strong&gt;supervised machine learning&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;训练&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;，以找出数据集（通常是无标签数据集）中的模式。&lt;/p&gt;
&lt;p&gt;非监督式机器学习最常见的用途是将数据分为不同的聚类，使相似的样本位于同一组中。例如，非监督式机器学习算法可以根据音乐的各种属性将歌曲分为不同的聚类。所得聚类可以作为其他机器学习算法（例如音乐推荐服务）的输入。在很难获取真标签的领域，聚类可能会非常有用。例如，在反滥用和反欺诈等领域，聚类有助于人们更好地了解相关数据。&lt;/p&gt;
&lt;p&gt;非监督式机器学习的另一个例子是&lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;&lt;strong&gt;主成分分析 (PCA)&lt;/strong&gt;&lt;/a&gt;。例如，通过对包含数百万购物车中物品的数据集进行主成分分析，可能会发现有柠檬的购物车中往往也有抗酸药。&lt;/p&gt;
&lt;p&gt;请与&lt;a href="#supervised machine learning"&gt;&lt;strong&gt;监督式机器学习&lt;/strong&gt;&lt;/a&gt;进行比较。&lt;/p&gt;
&lt;h2 id="v_1"&gt;V&lt;/h2&gt;
&lt;h3 id="validation set"&gt;validation set&lt;/h3&gt;
&lt;p&gt;A subset of the data set&amp;mdash;disjunct from the training set&amp;mdash;that you use to adjust &lt;a href="#hyperparameter"&gt;&lt;strong&gt;hyperparameters&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#training set"&gt;&lt;strong&gt;training set&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#test set"&gt;&lt;strong&gt;test set&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;数据集的一个子集，从训练集分离而来，用于调整&lt;a href="#hyperparameter"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#training set"&gt;&lt;strong&gt;训练集&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#test set"&gt;&lt;strong&gt;测试集&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h2 id="w_1"&gt;W&lt;/h2&gt;
&lt;h3 id="weight"&gt;weight&lt;/h3&gt;
&lt;p&gt;A coefficient for a &lt;a href="#feature"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/a&gt; in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0, then its corresponding feature does not contribute to the model.&lt;/p&gt;
&lt;p&gt;线性模型中&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。&lt;/p&gt;
&lt;h3 id="wide model"&gt;wide model&lt;/h3&gt;
&lt;p&gt;A linear model that typically has many &lt;a href="#sparse feature"&gt;&lt;strong&gt;sparse input features&lt;/strong&gt;&lt;/a&gt;. We refer to it as "wide" since such a model is a special type of &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt; with a large number of inputs that connect directly to the output node. Wide models are often easier to debug and inspect than deep models. Although wide models cannot express nonlinearities through &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layers&lt;/strong&gt;&lt;/a&gt;, they can use transformations such as &lt;a href="#feature cross"&gt;&lt;strong&gt;feature crossing&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#bucketing"&gt;&lt;strong&gt;bucketization&lt;/strong&gt;&lt;/a&gt; to model nonlinearities in different ways.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#deep model"&gt;&lt;strong&gt;deep model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种线性模型，通常有很多&lt;a href="#sparse feature"&gt;&lt;strong&gt;稀疏输入特征&lt;/strong&gt;&lt;/a&gt;。我们之所以称之为&amp;ldquo;宽度模型&amp;rdquo;，是因为这是一种特殊类型的&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;，其大量输入均直接与输出节点相连。与深度模型相比，宽度模型通常更易于调试和检查。虽然宽度模型无法通过&lt;a href="#hidden-layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;来表示非线性关系，但可以利用&lt;a href="#feature cross"&gt;&lt;strong&gt;特征组合&lt;/strong&gt;&lt;/a&gt;、&lt;a href="#bucketing"&gt;&lt;strong&gt;分桶&lt;/strong&gt;&lt;/a&gt;等转换以不同的方式为非线性关系建模。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#deep model"&gt;&lt;strong&gt;深度模型&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;p&gt;上次更新日期：二月 27, 2018&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Python Numpy 教程</title><link href="https://freeopen.github.io/posts/python-numpy-jiao-cheng" rel="alternate"></link><published>2017-12-11T00:00:00+08:00</published><updated>2017-12-11T00:00:00+08:00</updated><author><name>Justin Johnson, 杜客(译), freeopen(编)</name></author><id>tag:freeopen.github.io,2017-12-11:/posts/python-numpy-jiao-cheng</id><summary type="html">&lt;p&gt;&lt;a href="http://cs231n.github.io/python-numpy-tutorial/"&gt;原文&lt;/a&gt; | &lt;a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit"&gt;原译文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 原译文没有目录导航，阅读起来不方便。补全少量漏翻的地方。虽然内容很基础，但需要时快速查阅一下还是很不错的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Python是一门伟大的通用编程语言，在一些常用库（numpy, scipy, matplotlib）的帮助下，它又会变成一个强大的科学计算环境。&lt;/p&gt;
&lt;p&gt;我们期望你们中大多数人对于Python语言和Numpy库比较熟悉，而对于没有Python经验的同学，这篇教程可以帮助你们快速了解Python编程环境和如何使用Python作为科学计算工具。&lt;/p&gt;
&lt;p&gt;一部分同学对于Matlab有一定经验。对于这部分同学，我们推荐阅读 &lt;a href="https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html"&gt;numpy for Matlab users 页面&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;本教程内容涵盖:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes&lt;/li&gt;
&lt;li&gt;Numpy: Arrays, Array indexing, Datatypes, Array math, Broadcasting&lt;/li&gt;
&lt;li&gt;SciPy: Image …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://cs231n.github.io/python-numpy-tutorial/"&gt;原文&lt;/a&gt; | &lt;a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit"&gt;原译文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 原译文没有目录导航，阅读起来不方便。补全少量漏翻的地方。虽然内容很基础，但需要时快速查阅一下还是很不错的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Python是一门伟大的通用编程语言，在一些常用库（numpy, scipy, matplotlib）的帮助下，它又会变成一个强大的科学计算环境。&lt;/p&gt;
&lt;p&gt;我们期望你们中大多数人对于Python语言和Numpy库比较熟悉，而对于没有Python经验的同学，这篇教程可以帮助你们快速了解Python编程环境和如何使用Python作为科学计算工具。&lt;/p&gt;
&lt;p&gt;一部分同学对于Matlab有一定经验。对于这部分同学，我们推荐阅读 &lt;a href="https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html"&gt;numpy for Matlab users 页面&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;本教程内容涵盖:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes&lt;/li&gt;
&lt;li&gt;Numpy: Arrays, Array indexing, Datatypes, Array math, Broadcasting&lt;/li&gt;
&lt;li&gt;SciPy: Image operations, MATLAB files, Distance between points,  &lt;/li&gt;
&lt;li&gt;Matplotlib: Plotting, Subplots, Images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="basics of python"&gt;Basics of Python&lt;/h2&gt;
&lt;p&gt;Python是一种高级的，动态类型的多范型编程语言。很多时候，大家会说Python看起来简直和伪代码一样，这是因为你能够通过很少行数的代码表达出很有力的思想。举个例子，下面是用Python实现的经典的quicksort算法例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;quicksort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;arr&lt;/span&gt;
    &lt;span class="n"&gt;pivot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arr&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;pivot&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;middle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arr&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;pivot&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arr&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;pivot&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;quicksort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;middle&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;quicksort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;quicksort&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="c1"&gt;# Prints "[1, 1, 2, 3, 6, 8, 10]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="python versions"&gt;Python versions&lt;/h3&gt;
&lt;p&gt;Python有两个支持的版本，分别是2.7和3.4。这有点让人迷惑，3.0向语言中引入了很多不向后兼容的变化，2.7下的代码有时候在3.4下是行不通的。在这个课程中，我们使用的是2.7版本。&lt;/p&gt;
&lt;p&gt;如何查看版本呢？使用&lt;code&gt;python --version&lt;/code&gt;命令。&lt;/p&gt;
&lt;h3 id="basic data types"&gt;Basic data types&lt;/h3&gt;
&lt;p&gt;和大多数编程语言一样，Python拥有一系列的基本数据类型，比如整型、浮点型、布尔型和字符串等。这些类型的使用方式和在其他语言中的使用方式是类似的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numbers&lt;/strong&gt;: 整型和浮点型的使用与其他语言类似。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# Prints "&amp;lt;class 'int'&amp;gt;"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# Prints "3"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Addition; prints "4"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Subtraction; prints "2"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Multiplication; prints "6"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Exponentiation; prints "9"&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "4"&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "8"&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# Prints "&amp;lt;class 'float'&amp;gt;"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Prints "2.5 3.5 5.0 6.25"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;需要注意的是，Python中没有 &lt;code&gt;x++&lt;/code&gt; 和 &lt;code&gt;x--&lt;/code&gt; 的操作符。&lt;/p&gt;
&lt;p&gt;Python也有内置的长整型和复杂数字类型，具体细节可以查看&lt;a href="https://docs.python.org/2/library/stdtypes.html#numeric-types-int-float-long-complex"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Booleans&lt;/strong&gt;: Python实现了所有的布尔逻辑，但用的是英语，而不是我们习惯的操作符（比如&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;和&lt;code&gt;||&lt;/code&gt;等）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# Prints "&amp;lt;class 'bool'&amp;gt;"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Logical AND; prints "False"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Logical OR; prints "True"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Logical NOT; prints "False"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Logical XOR; prints "True"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Strings&lt;/strong&gt;: Python对字符串的支持非常棒。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hello&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'hello'&lt;/span&gt;    &lt;span class="c1"&gt;# String literals can use single quotes&lt;/span&gt;
&lt;span class="n"&gt;world&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"world"&lt;/span&gt;    &lt;span class="c1"&gt;# or double quotes; it does not matter.&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# Prints "hello"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# String length; prints "5"&lt;/span&gt;
&lt;span class="n"&gt;hw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hello&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;' '&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;world&lt;/span&gt;  &lt;span class="c1"&gt;# String concatenation&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# prints "hello world"&lt;/span&gt;
&lt;span class="n"&gt;hw12&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt; &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt; &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;world&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# sprintf style string formatting&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hw12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# prints "hello world 12"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;字符串对象有一系列有用的方法，比如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"hello"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;capitalize&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  &lt;span class="c1"&gt;# Capitalize a string; prints "Hello"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;       &lt;span class="c1"&gt;# Convert a string to uppercase; prints "HELLO"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rjust&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;      &lt;span class="c1"&gt;# Right-justify a string, padding with spaces; prints "  hello"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;     &lt;span class="c1"&gt;# Center a string, padding with spaces; prints " hello "&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'l'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'(ell)'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# Replace all instances of one substring with another;&lt;/span&gt;
                                &lt;span class="c1"&gt;# prints "he(ell)(ell)o"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'  world '&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  &lt;span class="c1"&gt;# Strip leading and trailing whitespace; prints "world"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果想详细查看字符串方法，请看&lt;a href="https://docs.python.org/2/library/stdtypes.html#string-methods"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="containers"&gt;Containers&lt;/h3&gt;
&lt;p&gt;Python有以下几种容器类型：列表（lists）、字典（dictionaries）、集合（sets）和元组（tuples）。&lt;/p&gt;
&lt;h4&gt;Lists&lt;/h4&gt;
&lt;p&gt;列表就是Python中的数组，但是列表长度可变，且能包含不同类型元素。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    &lt;span class="c1"&gt;# Create a list&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[3, 1, 2] 2"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;     &lt;span class="c1"&gt;# Negative indices count from the end of the list; prints "2"&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'foo'&lt;/span&gt;     &lt;span class="c1"&gt;# Lists can contain elements of different types&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;# Prints "[3, 1, 'foo']"&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'bar'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Add a new element to the end of the list&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;# Prints "[3, 1, 'foo', 'bar']"&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;      &lt;span class="c1"&gt;# Remove and return the last element of the list&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;      &lt;span class="c1"&gt;# Prints "bar [3, 1, 'foo']"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;列表的细节，同样可以查阅&lt;a href="https://docs.python.org/2/tutorial/datastructures.html#more-on-lists"&gt;文档&lt;/a&gt;.。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slicing&lt;/strong&gt;: 为了一次性地获取列表中的元素，Python提供了一种简洁的语法，这就是切片。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;     &lt;span class="c1"&gt;# range is a built-in function that creates a list of integers&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;               &lt;span class="c1"&gt;# Prints "[0, 1, 2, 3, 4]"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;          &lt;span class="c1"&gt;# Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;           &lt;span class="c1"&gt;# Get a slice from index 2 to the end; prints "[2, 3, 4]"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;           &lt;span class="c1"&gt;# Get a slice from the start to index 2 (exclusive); prints "[0, 1]"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;[:])&lt;/span&gt;            &lt;span class="c1"&gt;# Get a slice of the whole list; prints "[0, 1, 2, 3, 4]"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;          &lt;span class="c1"&gt;# Slice indices can be negative; prints "[0, 1, 2, 3]"&lt;/span&gt;
&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;        &lt;span class="c1"&gt;# Assign a new sublist to a slice&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;               &lt;span class="c1"&gt;# Prints "[0, 1, 8, 9, 4]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Loops&lt;/strong&gt;: 我们可以这样遍历列表中的每一个元素：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dog'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'monkey'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Prints "cat", "dog", "monkey", each on its own line.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果想要在循环体内访问每个元素的指针，可以使用内置的&lt;code&gt;enumerate&lt;/code&gt;函数&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dog'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'monkey'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'#&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# Prints "#1: cat", "#2: dog", "#3: monkey", each on its own line&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;List comprehensions&lt;/strong&gt;: 在编程的时候，我们常常想要将一种数据类型转换为另一种。下面是一个简单例子，将列表中的每个元素变成它的平方。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;squares&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;squares&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;squares&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Prints [0, 1, 4, 9, 16]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;使用列表推导，你就可以让代码简化很多：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;squares&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;squares&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Prints [0, 1, 4, 9, 16]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;列表推导还可以包含条件：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;even_squares&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;even_squares&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[0, 4, 16]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Dictionaries&lt;/h4&gt;
&lt;p&gt;字典用来储存（键, 值）对，这和Java中的&lt;code&gt;Map&lt;/code&gt;差不多。你可以这样使用它：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'cute'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dog'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'furry'&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;  &lt;span class="c1"&gt;# Create a new dictionary with some data&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;       &lt;span class="c1"&gt;# Get an entry from a dictionary; prints "cute"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# Check if a dictionary has a given key; prints "True"&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'fish'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'wet'&lt;/span&gt;     &lt;span class="c1"&gt;# Set an entry in a dictionary&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'fish'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;      &lt;span class="c1"&gt;# Prints "wet"&lt;/span&gt;
&lt;span class="c1"&gt;# print(d['monkey'])  # KeyError: 'monkey' not a key of d&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'monkey'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'N/A'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# Get an element with a default; prints "N/A"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fish'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'N/A'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;    &lt;span class="c1"&gt;# Get an element with a default; prints "wet"&lt;/span&gt;
&lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'fish'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;         &lt;span class="c1"&gt;# Remove an element from a dictionary&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fish'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'N/A'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# "fish" is no longer a key; prints "N/A"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;想要知道字典的其他特性，请查阅&lt;a href="https://docs.python.org/2/library/stdtypes.html#dict"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loops&lt;/strong&gt;：在字典中，用键来迭代更加容易。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'person'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'spider'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;legs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'A &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt; has &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; legs'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;legs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# Prints "A person has 2 legs", "A cat has 4 legs", "A spider has 8 legs"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你想要访问键和对应的值，那就使用&lt;code&gt;iteritems&lt;/code&gt;方法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'person'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'spider'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;legs&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'A &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt; has &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; legs'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;legs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# Prints "A person has 2 legs", "A cat has 4 legs", "A spider has 8 legs"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Dictionary comprehensions&lt;/strong&gt;：和列表推导类似，但是允许你方便地构建字典。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;even_num_to_square&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;even_num_to_square&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "{0: 0, 2: 4, 4: 16}"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Sets&lt;/h4&gt;
&lt;p&gt;集合是独立不同个体的无序集合。示例如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dog'&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Check if an element is in a set; prints "True"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fish'&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# prints "False"&lt;/span&gt;
&lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fish'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# Add an element to a set&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fish'&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "True"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;       &lt;span class="c1"&gt;# Number of elements in a set; prints "3"&lt;/span&gt;
&lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="c1"&gt;# Adding an element that is already in the set does nothing&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;       &lt;span class="c1"&gt;# Prints "3"&lt;/span&gt;
&lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# Remove an element from a set&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;       &lt;span class="c1"&gt;# Prints "2"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;和前面一样，要知道更详细的，查看&lt;a href="https://docs.python.org/3.5/library/stdtypes.html#set"&gt;文档&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loops&lt;/strong&gt;：在集合中循环的语法和在列表中一样，但是集合是无序的，所以你在访问集合的元素的时候，不能做关于顺序的假设。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'cat'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dog'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'fish'&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animals&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'#&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# Prints "#1: fish", "#2: dog", "#3: cat"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Set comprehensions&lt;/strong&gt;：和字典推导一样，可以很方便地构建集合：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sqrt&lt;/span&gt;
&lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "{0, 1, 2, 3, 4, 5}"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Tuples&lt;/h4&gt;
&lt;p&gt;元组是一个值的有序列表（不可改变）。从很多方面来说，元组和列表都很相似。和列表最重要的不同在于，元组可以在字典中用作键，还可以作为集合的元素，而列表不行。例子如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;  &lt;span class="c1"&gt;# Create a dictionary with tuple keys&lt;/span&gt;
&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="c1"&gt;# Create a tuple&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;    &lt;span class="c1"&gt;# Prints "&amp;lt;class 'tuple'&amp;gt;"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;       &lt;span class="c1"&gt;# Prints "5"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "1"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3.5/tutorial/datastructures.html#tuples-and-sequences"&gt;文档&lt;/a&gt;有更多元组的信息。&lt;/p&gt;
&lt;h3 id="functions"&gt;Functions&lt;/h3&gt;
&lt;p&gt;Python函数使用&lt;code&gt;def&lt;/code&gt;来定义函数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;'positive'&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;'negative'&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;'zero'&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# Prints "negative", "zero", "positive"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;我们常常使用可选参数来定义函数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loud&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;loud&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'HELLO, &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;!'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Hello, &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Bob'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Prints "Hello, Bob"&lt;/span&gt;
&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Fred'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loud&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "HELLO, FRED!"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="classes"&gt;Classes&lt;/h3&gt;
&lt;p&gt;Python对于类的定义是简单直接的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Greeter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="c1"&gt;# Constructor&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;  &lt;span class="c1"&gt;# Create an instance variable&lt;/span&gt;

    &lt;span class="c1"&gt;# Instance method&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loud&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;loud&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'HELLO, &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;!'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Hello, &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Greeter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Fred'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Construct an instance of the Greeter class&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;greet&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;            &lt;span class="c1"&gt;# Call an instance method; prints "Hello, Fred"&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;greet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loud&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Call an instance method; prints "HELLO, FRED!"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;更多类的信息请查阅&lt;a href="https://docs.python.org/3.5/tutorial/classes.html"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="numpy_1"&gt;Numpy&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.numpy.org/"&gt;Numpy&lt;/a&gt;是Python中用于科学计算的核心库。它提供了高性能的多维数组对象，以及相关工具。如何熟悉MATLAB, 你可以从这份&lt;a href="http://wiki.scipy.org/NumPy_for_Matlab_Users"&gt;教程&lt;/a&gt;开始学习Numpy.&lt;/p&gt;
&lt;h3 id="arrays"&gt;Arrays&lt;/h3&gt;
&lt;p&gt;一个numpy数组是一个由不同数值组成的网格。网格中的数据都是同一种数据类型，可以通过非负整型数的元组来访问。维度的数量被称为数组的阶，数组的大小是一个由整型数构成的元组，可以描述数组不同维度上的大小。&lt;/p&gt;
&lt;p&gt;我们可以从列表创建数组，然后利用方括号访问其中的元素：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# Create a rank 1 array&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;            &lt;span class="c1"&gt;# Prints "&amp;lt;class 'numpy.ndarray'&amp;gt;"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;            &lt;span class="c1"&gt;# Prints "(3,)"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# Prints "1 2 3"&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;                  &lt;span class="c1"&gt;# Change an element of the array&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# Prints "[5, 2, 3]"&lt;/span&gt;

&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;    &lt;span class="c1"&gt;# Create a rank 2 array&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                     &lt;span class="c1"&gt;# Prints "(2, 3)"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# Prints "1 2 4"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Numpy还提供了很多其他创建数组的方法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;   &lt;span class="c1"&gt;# Create an array of all zeros&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;              &lt;span class="c1"&gt;# Prints "[[ 0.  0.]&lt;/span&gt;
                      &lt;span class="c1"&gt;#          [ 0.  0.]]"&lt;/span&gt;

&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;    &lt;span class="c1"&gt;# Create an array of all ones&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;              &lt;span class="c1"&gt;# Prints "[[ 1.  1.]]"&lt;/span&gt;

&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;full&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Create a constant array&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;               &lt;span class="c1"&gt;# Prints "[[ 7.  7.]&lt;/span&gt;
                       &lt;span class="c1"&gt;#          [ 7.  7.]]"&lt;/span&gt;

&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;# Create a 2x2 identity matrix&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;              &lt;span class="c1"&gt;# Prints "[[ 1.  0.]&lt;/span&gt;
                      &lt;span class="c1"&gt;#          [ 0.  1.]]"&lt;/span&gt;

&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# Create an array filled with random values&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                     &lt;span class="c1"&gt;# Might print "[[ 0.91940167  0.08143941]&lt;/span&gt;
                             &lt;span class="c1"&gt;#               [ 0.68744134  0.87236687]]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其他数组相关方法，请查看&lt;a href="http://docs.scipy.org/doc/numpy/user/basics.creation.html#arrays-creation"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="array indexing"&gt;Array indexing&lt;/h3&gt;
&lt;p&gt;Numpy提供了多种访问数组的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slicing&lt;/strong&gt;：和Python列表类似，numpy数组可以使用切片语法。因为数组可以是多维的，所以你必须为每个维度指定好切片。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# Create the following rank 2 array with shape (3, 4)&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 1  2  3  4]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 5  6  7  8]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 9 10 11 12]]&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="c1"&gt;# Use slicing to pull out the subarray consisting of the first 2 rows&lt;/span&gt;
&lt;span class="c1"&gt;# and columns 1 and 2; b is the following array of shape (2, 2):&lt;/span&gt;
&lt;span class="c1"&gt;# [[2 3]&lt;/span&gt;
&lt;span class="c1"&gt;#  [6 7]]&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# A slice of an array is a view into the same data, so modifying it&lt;/span&gt;
&lt;span class="c1"&gt;# will modify the original array.&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# Prints "2"&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;77&lt;/span&gt;     &lt;span class="c1"&gt;# b[0, 0] is the same piece of data as a[0, 1]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# Prints "77"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你可以同时使用整型和切片语法来访问数组。但是，这样做会产生一个比原数组低阶的新数组。需要注意的是，这里和MATLAB中的情况是不同的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# Create the following rank 2 array with shape (3, 4)&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 1  2  3  4]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 5  6  7  8]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 9 10 11 12]]&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="c1"&gt;# Two ways of accessing the data in the middle row of the array.&lt;/span&gt;
&lt;span class="c1"&gt;# Mixing integer indexing with slices yields an array of lower rank,&lt;/span&gt;
&lt;span class="c1"&gt;# while using only slices yields an array of the same rank as the&lt;/span&gt;
&lt;span class="c1"&gt;# original array:&lt;/span&gt;
&lt;span class="n"&gt;row_r1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;    &lt;span class="c1"&gt;# Rank 1 view of the second row of a&lt;/span&gt;
&lt;span class="n"&gt;row_r2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;  &lt;span class="c1"&gt;# Rank 2 view of the second row of a&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row_r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row_r1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[5 6 7 8] (4,)"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row_r2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row_r2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[[5 6 7 8]] (1, 4)"&lt;/span&gt;

&lt;span class="c1"&gt;# We can make the same distinction when accessing columns of an array:&lt;/span&gt;
&lt;span class="n"&gt;col_r1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;col_r2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col_r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col_r1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[ 2  6 10] (3,)"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col_r2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col_r2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[[ 2]&lt;/span&gt;
                             &lt;span class="c1"&gt;#          [ 6]&lt;/span&gt;
                             &lt;span class="c1"&gt;#          [10]] (3, 1)"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Integer array indexing&lt;/strong&gt;：当我们使用切片语法访问数组时，得到的总是原数组的一个子集。整型数组访问允许我们利用其它数组的数据构建一个新的数组：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="c1"&gt;# An example of integer array indexing.&lt;/span&gt;
&lt;span class="c1"&gt;# The returned array will have shape (3,) and&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[1 4 5]"&lt;/span&gt;

&lt;span class="c1"&gt;# The above example of integer array indexing is equivalent to this:&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]))&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[1 4 5]"&lt;/span&gt;

&lt;span class="c1"&gt;# When using integer array indexing, you can reuse the same&lt;/span&gt;
&lt;span class="c1"&gt;# element from the source array:&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[2 2]"&lt;/span&gt;

&lt;span class="c1"&gt;# Equivalent to the previous integer array indexing example&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]))&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[2 2]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;整型数组访问语法还有个有用的技巧，可以用来选择或者更改矩阵中每行中的一个元素：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# Create a new array from which we will select elements&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# prints "array([[ 1,  2,  3],&lt;/span&gt;
          &lt;span class="c1"&gt;#                [ 4,  5,  6],&lt;/span&gt;
          &lt;span class="c1"&gt;#                [ 7,  8,  9],&lt;/span&gt;
          &lt;span class="c1"&gt;#                [10, 11, 12]])"&lt;/span&gt;

&lt;span class="c1"&gt;# Create an array of indices&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Select one element from each row of a using the indices in b&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[ 1  6  7 11]"&lt;/span&gt;

&lt;span class="c1"&gt;# Mutate one element from each row of a using the indices in b&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# prints "array([[11,  2,  3],&lt;/span&gt;
          &lt;span class="c1"&gt;#                [ 4,  5, 16],&lt;/span&gt;
          &lt;span class="c1"&gt;#                [17,  8,  9],&lt;/span&gt;
          &lt;span class="c1"&gt;#                [10, 21, 12]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Boolean array indexing&lt;/strong&gt;：布尔型数组访问可以让你选择数组中任意元素。通常，这种访问方式用于选取数组中满足某些条件的元素，举例如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="n"&gt;bool_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Find the elements of a that are bigger than 2;&lt;/span&gt;
                     &lt;span class="c1"&gt;# this returns a numpy array of Booleans of the same&lt;/span&gt;
                     &lt;span class="c1"&gt;# shape as a, where each slot of bool_idx tells&lt;/span&gt;
                     &lt;span class="c1"&gt;# whether that element of a is &amp;gt; 2.&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bool_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;      &lt;span class="c1"&gt;# Prints "[[False False]&lt;/span&gt;
                     &lt;span class="c1"&gt;#          [ True  True]&lt;/span&gt;
                     &lt;span class="c1"&gt;#          [ True  True]]"&lt;/span&gt;

&lt;span class="c1"&gt;# We use boolean array indexing to construct a rank 1 array&lt;/span&gt;
&lt;span class="c1"&gt;# consisting of the elements of a corresponding to the True values&lt;/span&gt;
&lt;span class="c1"&gt;# of bool_idx&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bool_idx&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[3 4 5 6]"&lt;/span&gt;

&lt;span class="c1"&gt;# We can do all of the above in a single concise statement:&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;     &lt;span class="c1"&gt;# Prints "[3 4 5 6]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为了教程的简短，有很多数组访问的细节我们没有详细说明，可以查看&lt;a href="http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="datatypes"&gt;Datatypes&lt;/h3&gt;
&lt;p&gt;每个Numpy数组都是数据类型相同的元素组成的网格。Numpy提供了很多的数据类型用于创建数组。当你创建数组的时候，Numpy会尝试猜测数组的数据类型，你也可以通过参数直接指定数据类型，例子如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# Let numpy choose the datatype&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;# Prints "int64"&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# Let numpy choose the datatype&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;             &lt;span class="c1"&gt;# Prints "float64"&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Force a particular datatype&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                         &lt;span class="c1"&gt;# Prints "int64"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;更多细节查看&lt;a href="http://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="array math"&gt;Array math&lt;/h3&gt;
&lt;p&gt;基本数学计算函数会对数组中元素逐个进行计算，既可以利用操作符重载，也可以使用函数方式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Elementwise sum; both produce the array&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 6.0  8.0]&lt;/span&gt;
&lt;span class="c1"&gt;#  [10.0 12.0]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Elementwise difference; both produce the array&lt;/span&gt;
&lt;span class="c1"&gt;# [[-4.0 -4.0]&lt;/span&gt;
&lt;span class="c1"&gt;#  [-4.0 -4.0]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subtract&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Elementwise product; both produce the array&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 5.0 12.0]&lt;/span&gt;
&lt;span class="c1"&gt;#  [21.0 32.0]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Elementwise division; both produce the array&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 0.2         0.33333333]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 0.42857143  0.5       ]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;divide&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Elementwise square root; produces the array&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 1.          1.41421356]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 1.73205081  2.        ]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;和MATLAB不同，&lt;code&gt;*&lt;/code&gt;是元素逐个相乘，而不是矩阵乘法。在Numpy中使用&lt;code&gt;dot&lt;/code&gt;来进行矩阵乘法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Inner product of vectors; both produce 219&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Matrix / vector product; both produce the rank 1 array [29 67]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Matrix / matrix product; both produce the rank 2 array&lt;/span&gt;
&lt;span class="c1"&gt;# [[19 22]&lt;/span&gt;
&lt;span class="c1"&gt;#  [43 50]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Numpy提供了很多计算数组的函数，其中最常用的一个是'sum'：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# Compute sum of all elements; prints "10"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# Compute sum of each column; prints "[4 6]"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# Compute sum of each row; prints "[3 7]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;想要了解更多函数，可以查看&lt;a href="http://docs.scipy.org/doc/numpy/reference/routines.math.html"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;除了计算，我们还常常改变数组或者操作其中的元素。其中将矩阵转置是常用的一个，在Numpy中，使用&lt;code&gt;T&lt;/code&gt;来转置矩阵：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    &lt;span class="c1"&gt;# Prints "[[1 2]&lt;/span&gt;
            &lt;span class="c1"&gt;#          [3 4]]"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[[1 3]&lt;/span&gt;
            &lt;span class="c1"&gt;#          [2 4]]"&lt;/span&gt;

&lt;span class="c1"&gt;# Note that taking the transpose of a rank 1 array does nothing:&lt;/span&gt;
&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    &lt;span class="c1"&gt;# Prints "[1 2 3]"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[1 2 3]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Numpy还提供了更多操作数组的方法，请查看&lt;a href="http://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html"&gt;文档&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="broadcasting"&gt;Broadcasting&lt;/h3&gt;
&lt;p&gt;广播是一种强有力的机制，它让Numpy可以让不同大小的矩阵在一起进行数学计算。我们常常会有一个小的矩阵和一个大的矩阵，然后我们会需要用小的矩阵对大的矩阵做一些计算。&lt;/p&gt;
&lt;p&gt;举个例子，如果我们想要把一个向量加到矩阵的每一行，我们可以这样做：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# We will add the vector v to each row of the matrix x,&lt;/span&gt;
&lt;span class="c1"&gt;# storing the result in the matrix y&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# Create an empty matrix with the same shape as x&lt;/span&gt;

&lt;span class="c1"&gt;# Add the vector v to each row of the matrix x with an explicit loop&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;

&lt;span class="c1"&gt;# Now y is the following&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 2  2  4]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 5  5  7]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 8  8 10]&lt;/span&gt;
&lt;span class="c1"&gt;#  [11 11 13]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样是行得通的，但是当'x'矩阵非常大，利用循环来计算就会变得很慢很慢。我们可以换一种思路：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# We will add the vector v to each row of the matrix x,&lt;/span&gt;
&lt;span class="c1"&gt;# storing the result in the matrix y&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;vv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;   &lt;span class="c1"&gt;# Stack 4 copies of v on top of each other&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                 &lt;span class="c1"&gt;# Prints "[[1 0 1]&lt;/span&gt;
                          &lt;span class="c1"&gt;#          [1 0 1]&lt;/span&gt;
                          &lt;span class="c1"&gt;#          [1 0 1]&lt;/span&gt;
                          &lt;span class="c1"&gt;#          [1 0 1]]"&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;vv&lt;/span&gt;  &lt;span class="c1"&gt;# Add x and vv elementwise&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[[ 2  2  4&lt;/span&gt;
          &lt;span class="c1"&gt;#          [ 5  5  7]&lt;/span&gt;
          &lt;span class="c1"&gt;#          [ 8  8 10]&lt;/span&gt;
          &lt;span class="c1"&gt;#          [11 11 13]]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Numpy广播机制可以让我们不用创建&lt;code&gt;vv&lt;/code&gt;，就能直接运算，看看下面例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# We will add the vector v to each row of the matrix x,&lt;/span&gt;
&lt;span class="c1"&gt;# storing the result in the matrix y&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;  &lt;span class="c1"&gt;# Add v to each row of x using broadcasting&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "[[ 2  2  4]&lt;/span&gt;
          &lt;span class="c1"&gt;#          [ 5  5  7]&lt;/span&gt;
          &lt;span class="c1"&gt;#          [ 8  8 10]&lt;/span&gt;
          &lt;span class="c1"&gt;#          [11 11 13]]"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;y = x + v&lt;/code&gt;这一行，即使&lt;code&gt;x&lt;/code&gt;的维度为&lt;code&gt;(4, 3)&lt;/code&gt;, &lt;code&gt;v&lt;/code&gt;的维度为&lt;code&gt;(3,)&lt;/code&gt;，但由于广播机制，&lt;code&gt;v&lt;/code&gt;在计算时的实际维度为&lt;code&gt;(4, 3)&lt;/code&gt;，其中每行是&lt;code&gt;v&lt;/code&gt;的副本，然后再每个元素相加求得结果。&lt;/p&gt;
&lt;p&gt;对两个数组使用广播机制要遵守下列规则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果数组的秩不同，使用1来将秩较小的数组进行扩展，直到两个数组的尺寸的长度都一样。&lt;/li&gt;
&lt;li&gt;如果两个数组在某个维度上的长度是一样的，或者其中一个数组在该维度上长度为1，那么我们就说这两个数组在该维度上是相容的。 &lt;/li&gt;
&lt;li&gt;如果两个数组在所有维度上都是相容的，他们就能使用广播。 &lt;/li&gt;
&lt;li&gt;如果两个输入数组的尺寸不同，那么注意其中较大的那个尺寸。因为广播之后，两个数组的尺寸将和那个较大的尺寸一样。 &lt;/li&gt;
&lt;li&gt;在任何一个维度上，如果一个数组的长度为1，另一个数组长度大于1，那么在该维度上，就好像是对第一个数组进行了复制。 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果上述解释看不明白，可以读一读&lt;a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"&gt;文档&lt;/a&gt;和这个&lt;a href="http://wiki.scipy.org/EricsBroadcastingDoc"&gt;解释&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;支持广播机制的函数是全局函数。哪些是全局函数可以在&lt;a href="http://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs"&gt;文档&lt;/a&gt;中查找。&lt;/p&gt;
&lt;p&gt;下面是一些广播机制的使用：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# Compute outer product of vectors&lt;/span&gt;
&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# v has shape (3,)&lt;/span&gt;
&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;    &lt;span class="c1"&gt;# w has shape (2,)&lt;/span&gt;
&lt;span class="c1"&gt;# To compute an outer product, we first reshape v to be a column&lt;/span&gt;
&lt;span class="c1"&gt;# vector of shape (3, 1); we can then broadcast it against w to yield&lt;/span&gt;
&lt;span class="c1"&gt;# an output of shape (3, 2), which is the outer product of v and w:&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 4  5]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 8 10]&lt;/span&gt;
&lt;span class="c1"&gt;#  [12 15]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Add a vector to each row of a matrix&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="c1"&gt;# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),&lt;/span&gt;
&lt;span class="c1"&gt;# giving the following matrix:&lt;/span&gt;
&lt;span class="c1"&gt;# [[2 4 6]&lt;/span&gt;
&lt;span class="c1"&gt;#  [5 7 9]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Add a vector to each column of a matrix&lt;/span&gt;
&lt;span class="c1"&gt;# x has shape (2, 3) and w has shape (2,).&lt;/span&gt;
&lt;span class="c1"&gt;# If we transpose x then it has shape (3, 2) and can be broadcast&lt;/span&gt;
&lt;span class="c1"&gt;# against w to yield a result of shape (3, 2); transposing this result&lt;/span&gt;
&lt;span class="c1"&gt;# yields the final result of shape (2, 3) which is the matrix x with&lt;/span&gt;
&lt;span class="c1"&gt;# the vector w added to each column. Gives the following matrix:&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 5  6  7]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 9 10 11]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Another solution is to reshape w to be a column vector of shape (2, 1);&lt;/span&gt;
&lt;span class="c1"&gt;# we can then broadcast it directly against x to produce the same&lt;/span&gt;
&lt;span class="c1"&gt;# output.&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="c1"&gt;# Multiply a matrix by a constant:&lt;/span&gt;
&lt;span class="c1"&gt;# x has shape (2, 3). Numpy treats scalars as arrays of shape ();&lt;/span&gt;
&lt;span class="c1"&gt;# these can be broadcast together to shape (2, 3), producing the&lt;/span&gt;
&lt;span class="c1"&gt;# following array:&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 2  4  6]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 8 10 12]]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;广播机制能够让你的代码更简洁更迅速，能够用的时候请尽量使用！&lt;/p&gt;
&lt;h3 id="numpy wen dang"&gt;Numpy 文档&lt;/h3&gt;
&lt;p&gt;这篇教程涉及了你需要了解的numpy中的一些重要内容，但是numpy远不止如此。可以查阅&lt;a href="http://docs.scipy.org/doc/numpy/reference/"&gt;numpy文献&lt;/a&gt;来学习更多。&lt;/p&gt;
&lt;h2 id="scipy_1"&gt;SciPy&lt;/h2&gt;
&lt;p&gt;Numpy提供了高性能的多维数组，以及计算和操作数组的基本工具。SciPy基于Numpy，提供了大量的计算和操作数组的函数，这些函数对于不同类型的科学和工程计算非常有用。&lt;/p&gt;
&lt;p&gt;熟悉SciPy的最好方法就是阅读&lt;a href="http://docs.scipy.org/doc/scipy/reference/index.html"&gt;文档&lt;/a&gt;。我们会强调对于本课程有用的部分。&lt;/p&gt;
&lt;h3 id="tu xiang cao zuo"&gt;图像操作&lt;/h3&gt;
&lt;p&gt;SciPy提供了一些操作图像的基本函数。比如，它提供了将图像从硬盘读入到数组的函数，也提供了将数组中数据写入的硬盘成为图像的函数。下面是一个简单的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.misc&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imsave&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imresize&lt;/span&gt;

&lt;span class="c1"&gt;# Read an JPEG image into a numpy array&lt;/span&gt;
&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'assets/cat.jpg'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Prints "uint8 (400, 248, 3)"&lt;/span&gt;

&lt;span class="c1"&gt;# We can tint the image by scaling each of the color channels&lt;/span&gt;
&lt;span class="c1"&gt;# by a different scalar constant. The image has shape (400, 248, 3);&lt;/span&gt;
&lt;span class="c1"&gt;# we multiply it by the array [1, 0.95, 0.9] of shape (3,);&lt;/span&gt;
&lt;span class="c1"&gt;# numpy broadcasting means that this leaves the red channel unchanged,&lt;/span&gt;
&lt;span class="c1"&gt;# and multiplies the green and blue channels by 0.95 and 0.9&lt;/span&gt;
&lt;span class="c1"&gt;# respectively.&lt;/span&gt;
&lt;span class="n"&gt;img_tinted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Resize the tinted image to be 300 by 300 pixels.&lt;/span&gt;
&lt;span class="n"&gt;img_tinted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imresize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_tinted&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Write the tinted image back to disk&lt;/span&gt;
&lt;span class="n"&gt;imsave&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'assets/cat_tinted.jpg'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_tinted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;figure&gt;
&lt;img src="/images/cat.jpg" width="35%"/&gt;
&lt;img src="/images/cat_tinted.jpg" width="35%"/&gt;
&lt;br/&gt;
&lt;p style="font-size:14px;"&gt;_左边是原始图片，右边是变色和变形的图片_&lt;/p&gt;
&lt;/figure&gt;
&lt;h3 id="matlabwen jian"&gt;MATLAB文件&lt;/h3&gt;
&lt;p&gt;函数&lt;code&gt;scipy.io.loadmat&lt;/code&gt;和&lt;code&gt;scipy.io.savemat&lt;/code&gt;能够让你读和写MATLAB文件。具体请查看&lt;a href="http://docs.scipy.org/doc/scipy/reference/io.html"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="dian zhi jian de ju chi"&gt;点之间的距离&lt;/h3&gt;
&lt;p&gt;SciPy定义了一些有用的函数，可以计算集合中点之间的距离。&lt;/p&gt;
&lt;p&gt;函数&lt;code&gt;scipy.spatial.distance.pdist&lt;/code&gt;能够计算集合中所有两点之间的距离：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.spatial.distance&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pdist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;squareform&lt;/span&gt;

&lt;span class="c1"&gt;# Create the following array where each row is a point in 2D space:&lt;/span&gt;
&lt;span class="c1"&gt;# [[0 1]&lt;/span&gt;
&lt;span class="c1"&gt;#  [1 0]&lt;/span&gt;
&lt;span class="c1"&gt;#  [2 0]]&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Compute the Euclidean distance between all rows of x.&lt;/span&gt;
&lt;span class="c1"&gt;# d[i, j] is the Euclidean distance between x[i, :] and x[j, :],&lt;/span&gt;
&lt;span class="c1"&gt;# and d is the following array:&lt;/span&gt;
&lt;span class="c1"&gt;# [[ 0.          1.41421356  2.23606798]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 1.41421356  0.          1.        ]&lt;/span&gt;
&lt;span class="c1"&gt;#  [ 2.23606798  1.          0.        ]]&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;squareform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'euclidean'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;具体细节请阅读&lt;a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;函数&lt;code&gt;scipy.spatial.distance.cdist&lt;/code&gt;可以计算不同集合中点的距离，具体请查看&lt;a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="matplotlib_1"&gt;Matplotlib&lt;/h2&gt;
&lt;p&gt;Matplotlib是一个作图库。这里简要介绍'matplotlib.pyplot'模块，功能和MATLAB的作图功能类似。&lt;/p&gt;
&lt;h3 id="plotting"&gt;Plotting&lt;/h3&gt;
&lt;p&gt;matplotlib库中最重要的函数是&lt;code&gt;Plot&lt;/code&gt;。该函数允许你做出2D图形，如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="c1"&gt;# Compute the x and y coordinates for points on a sine curve&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Plot the points using matplotlib&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# You must call plt.show() to make graphics appear.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;figure&gt;
&lt;img src="/images/sine.png"/&gt;
&lt;/figure&gt;
&lt;p&gt;只需要少量工作，就可以一次画不同的线，加上标签，坐标轴标志等。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="c1"&gt;# Compute the x and y coordinates for points on sine and cosine curves&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_sin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_cos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Plot the points using matplotlib&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_sin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_cos&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'x axis label'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'y axis label'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Sine and Cosine'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;'Sine'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Cosine'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;figure&gt;
&lt;img src="/images/sine_cosine.png"/&gt;
&lt;/figure&gt;
&lt;h3 id="subplots"&gt;Subplots&lt;/h3&gt;
&lt;p&gt;可以使用&lt;code&gt;subplot&lt;/code&gt;函数来在一幅图中画不同的东西：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="c1"&gt;# Compute the x and y coordinates for points on sine and cosine curves&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_sin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_cos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Set up a subplot grid that has height 2 and width 1,&lt;/span&gt;
&lt;span class="c1"&gt;# and set the first such subplot as active.&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Make the first plot&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_sin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Sine'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Set the second subplot as active, and make the second plot.&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_cos&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Cosine'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Show the figure.&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;figure&gt;
&lt;img src="/images/sine_cosine_subplot.png"/&gt;
&lt;/figure&gt;
&lt;p&gt;关于&lt;code&gt;subplot&lt;/code&gt;的更多细节，可以阅读&lt;a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot"&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="images"&gt;Images&lt;/h3&gt;
&lt;p&gt;你可以使用&lt;code&gt;imshow&lt;/code&gt;函数来显示图像，如下所示：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.misc&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imresize&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'assets/cat.jpg'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;img_tinted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Show the original image&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Show the tinted image&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# A slight gotcha with imshow is that it might give strange results&lt;/span&gt;
&lt;span class="c1"&gt;# if presented with data that is not uint8. To work around this, we&lt;/span&gt;
&lt;span class="c1"&gt;# explicitly cast the image to uint8 before displaying it.&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_tinted&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;figure&gt;
&lt;img src="/images/cat_tinted_imshow.png"/&gt;
&lt;/figure&gt;</content></entry><entry><title>TensorLayer 教程</title><link href="https://freeopen.github.io/posts/tensorlayer-jiao-cheng" rel="alternate"></link><published>2017-09-09T00:00:00+08:00</published><updated>2017-09-09T00:00:00+08:00</updated><author><name>Xiaoping Fan(译), freeopen(修订)</name></author><id>tag:freeopen.github.io,2017-09-09:/posts/tensorlayer-jiao-cheng</id><summary type="html">&lt;p&gt;&lt;a href="https://github.com/zsdonghao/tensorlayer/blob/master/docs/user/tutorial.rst"&gt;原文&lt;/a&gt; | &lt;a href="https://github.com/shorxp/tensorlayer-chinese/blob/master/docs/user/tutorial.rst"&gt;原译文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 为什么研究 TensorLayer ? &lt;br/&gt;
在TF的编码中，常看到model被封装成class，进一步研究，发现为了简化模型的创建，有各种封装库，包括Keras, Tflearn, TFSlim, tf.contrib.learn(或称skflow) 等。
其中TFSlim和skflow属TensorFlow自带，考虑到TFSlim中有现成的模型，就修改了一个试试，发现坑不小。我需要首先把数据源转成TFRecord格式，然后用模型训练后报错（out of range), 然后我就首先检查数据源的格式是否有问题，我采用TFSlim的Data
provider来输出数据，可能我的数据维度太大，程序不报错，但一直死在那里不出结果，而同样我在使用numpy输出数据源数据时，没有任何问题，也没什么延时，瞬间我感觉到TFSlim的封装模式不仅把问题复杂化了，甚至还降低了程序的性能。Keras据说跑TF性能较慢，Tflearn感觉活跃度较低，考虑到TensorLayer是skflow上的再次封装，且不丧失灵活性，加上这篇最主要的教程写得真心不错，所以我发愿要好好研究一番。 &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;为什么我总喜欢修订一些长文翻译？&lt;br/&gt;
技术翻译文章经常在阅读时有不知所云的现象，我总结的原因主要有三：一是译者没有理解作者意图，甚至连文章都没读懂，就开始直译 …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://github.com/zsdonghao/tensorlayer/blob/master/docs/user/tutorial.rst"&gt;原文&lt;/a&gt; | &lt;a href="https://github.com/shorxp/tensorlayer-chinese/blob/master/docs/user/tutorial.rst"&gt;原译文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 为什么研究 TensorLayer ? &lt;br/&gt;
在TF的编码中，常看到model被封装成class，进一步研究，发现为了简化模型的创建，有各种封装库，包括Keras, Tflearn, TFSlim, tf.contrib.learn(或称skflow) 等。
其中TFSlim和skflow属TensorFlow自带，考虑到TFSlim中有现成的模型，就修改了一个试试，发现坑不小。我需要首先把数据源转成TFRecord格式，然后用模型训练后报错（out of range), 然后我就首先检查数据源的格式是否有问题，我采用TFSlim的Data
provider来输出数据，可能我的数据维度太大，程序不报错，但一直死在那里不出结果，而同样我在使用numpy输出数据源数据时，没有任何问题，也没什么延时，瞬间我感觉到TFSlim的封装模式不仅把问题复杂化了，甚至还降低了程序的性能。Keras据说跑TF性能较慢，Tflearn感觉活跃度较低，考虑到TensorLayer是skflow上的再次封装，且不丧失灵活性，加上这篇最主要的教程写得真心不错，所以我发愿要好好研究一番。 &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;为什么我总喜欢修订一些长文翻译？&lt;br/&gt;
技术翻译文章经常在阅读时有不知所云的现象，我总结的原因主要有三：一是译者没有理解作者意图，甚至连文章都没读懂，就开始直译，导致文理不通；
二是对技术术语不统一的称谓，本来英文是一个词，翻成中文后变成几个词，导致读者以为是不同的东西；三是过度翻译，把没必要或该省略的词也翻出来，反而影响理解。
比如vanilla RNN 中的vanilla, 并不代表&amp;ldquo;香草&amp;rdquo;，而是表示一个&amp;ldquo;普通的&amp;rdquo;或&amp;ldquo;原始的&amp;rdquo;意思，有时把这个词加进译文反而觉得多余，删掉最好。还有本文中&amp;ldquo;理解机器翻译－实现细节&amp;rdquo;部分的第6个段落，encoder_input_size 这个词，其实就是个代码变量，原译文居然也把它翻译出来，没有必要。我一般也是看到一点改一点，不保证能穷尽译文的所有地方，欢迎读者批评指正。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于深度学习，该教程会引导您使用MNIST数据集构建不同的手写数字的分类器，
这可以说是神经网络的 "Hello World" 。
对于强化学习，我们将让计算机根据屏幕输入来学习打乒乓球。
对于自然语言处理。我们从词嵌套（word embedding）开始，然后再实现语言建模和机器翻译。
此外，TensorLayer的Tutorial包含了所有TensorFlow官方深度学习教程的模块化实现，因此你可以对照TensorFlow深度学习教程&lt;a href="https://www.tensorflow.org/versions/master/tutorials/index.html"&gt;(英文&lt;/a&gt;&lt;a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/"&gt;|中文)&lt;/a&gt;来学习 。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;若你已经对TensorFlow非常熟悉，阅读 &lt;code&gt;InputLayer&lt;/code&gt; 和 &lt;code&gt;DenseLayer&lt;/code&gt; 的源代码可让您很好地理解 TensorLayer 是如何工作的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="zai wo men kai shi zhi qian"&gt;在我们开始之前&lt;/h2&gt;
&lt;p&gt;本教程假定您在神经网络和 TensorFlow (TensorLayer在它的基础上构建的)方面具有一定的基础。
您可以尝试从&lt;a href="http://deeplearning.stanford.edu/tutorial/"&gt;Deeplearning Tutorial&lt;/a&gt; 同时进行学习。&lt;/p&gt;
&lt;p&gt;对于人工神经网络更系统的介绍，我们推荐 Andrej Karpathy 等人所著的 &lt;a href="http://cs231n.github.io/"&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt;
和 Michael Nielsen 的 &lt;a href="http://neuralnetworksanddeeplearning.com/"&gt;Neural Networks and Deep Learning&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;要了解TensorFlow的更多内容，请阅读 &lt;a href="https://www.tensorflow.org/versions/r0.9/tutorials/index.html"&gt;TensorFlow tutorial&lt;/a&gt; 。
您不需要会它的全部，只要知道TensorFlow是如何工作的，就能够使用TensorLayer。
如果您是TensorFlow的新手，建议你阅读整个教程。&lt;/p&gt;
&lt;h2 id="tensorlayerhen jian dan"&gt;TensorLayer很简单&lt;/h2&gt;
&lt;p&gt;下面的代码是TensorLayer的一个简单例子，来自 &lt;code&gt;tutorial_mnist_simple.py&lt;/code&gt; 。
我们提供了很多方便的函数（如： &lt;code&gt;fit()&lt;/code&gt; ，&lt;code&gt;test()&lt;/code&gt; ），但如果你想了解更多实现细节，或想成为机器学习领域的专家，我们鼓励
您尽可能地直接使用TensorFlow原本的方法如 &lt;code&gt;sess.run()&lt;/code&gt; 来训练模型，请参考  &lt;code&gt;tutorial_mnist.py&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorlayer&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tl&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveSession&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# 准备数据&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                                &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_mnist_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# 定义 placeholder&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'x'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'y_'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 定义模型&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InputLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'input_layer'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DropoutLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'drop1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;act&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DropoutLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'drop2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;act&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DropoutLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'drop3'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;act&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;identity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'output_layer'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 定义损失函数和衡量指标&lt;/span&gt;
&lt;span class="c1"&gt;# tl.cost.cross_entropy 在内部使用 tf.nn.sparse_softmax_cross_entropy_with_logits() 实现 softmax&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;
&lt;span class="n"&gt;cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'cost'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;correct_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 定义 optimizer&lt;/span&gt;
&lt;span class="n"&gt;train_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_params&lt;/span&gt;
&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.999&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-08&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_locking&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 初始化 session 中的所有参数&lt;/span&gt;
&lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_global_variables&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 列出模型信息&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print_params&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print_layers&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# 训练模型&lt;/span&gt;
&lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_epoch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;print_freq&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eval_train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 评估模型&lt;/span&gt;
&lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 把模型保存成 .npz 文件&lt;/span&gt;
&lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_npz&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_params&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'model.npz'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="yun xing mnistli zi"&gt;运行MNIST例子&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;img src="/images/mnist.jpeg" width="80%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;在本教程的第一部分，我们仅仅运行TensorLayer内置的MNIST例子。
MNIST数据集包含了60000个28x28像素的手写数字图片，它通常用于训练各种图片识别系统。&lt;/p&gt;
&lt;p&gt;我们假设您已经按照 &lt;code&gt;installation&lt;/code&gt; 安装好了TensorLayer。如果您还没有，请复制一个TensorLayer的source目录到终端中，并进入该文件夹，
然后运行 &lt;code&gt;tutorial_mnist.py&lt;/code&gt; 这个例子脚本：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_mnist.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果所有设置都正确，您将得到下面的结果：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;GPU&lt;/span&gt; &lt;span class="n"&gt;MEM&lt;/span&gt; &lt;span class="n"&gt;Fraction&lt;/span&gt; &lt;span class="mf"&gt;0.300000&lt;/span&gt;
  &lt;span class="n"&gt;Downloading&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;idx3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ubyte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;
  &lt;span class="n"&gt;Downloading&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;idx1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ubyte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;
  &lt;span class="n"&gt;Downloading&lt;/span&gt; &lt;span class="n"&gt;t10k&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;idx3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ubyte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;
  &lt;span class="n"&gt;Downloading&lt;/span&gt; &lt;span class="n"&gt;t10k&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;idx1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ubyte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;

  &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
  &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
  &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
  &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="n"&gt;float32&lt;/span&gt;   &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;int64&lt;/span&gt;

  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;InputLayer&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DropoutLayer&lt;/span&gt; &lt;span class="n"&gt;drop1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.800000&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DenseLayer&lt;/span&gt; &lt;span class="n"&gt;relu1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DropoutLayer&lt;/span&gt; &lt;span class="n"&gt;drop2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.500000&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DenseLayer&lt;/span&gt; &lt;span class="n"&gt;relu2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DropoutLayer&lt;/span&gt; &lt;span class="n"&gt;drop3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.500000&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DenseLayer&lt;/span&gt; &lt;span class="n"&gt;output_layer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;identity&lt;/span&gt;

  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000053&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000043&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.035558&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000008&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000041&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.035371&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000469&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000432&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.049895&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1276810&lt;/span&gt;

  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"dropout/mul_1:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Relu:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"dropout_1/mul_1:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Relu_1:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"dropout_2/mul_1:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"add_2:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000100&lt;/span&gt;
  &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;

  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.342539&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.330111&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.298098&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.910700&lt;/span&gt;
  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.356471&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.085225&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.097082&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.971700&lt;/span&gt;
  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.352137&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.040741&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.070149&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.978600&lt;/span&gt;
  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.350814&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.022995&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.060471&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.982800&lt;/span&gt;
  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.350996&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.013713&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.055777&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.983700&lt;/span&gt;
  &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这个例子脚本允许您从 &lt;code&gt;if__name__=='__main__':&lt;/code&gt; 中选择不同的模型进行尝试，包括多层神经网络（Multi-Layer Perceptron），
退出（Dropout），退出连接（DropConnect），堆栈式降噪自编码器（Stacked Denoising Autoencoder）和卷积神经网络（CNN）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  main_test_layers(model='relu')
  main_test_denoise_AE(model='relu')
  main_test_stacked_denoise_AE(model='relu')
  main_test_cnn_layer()
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="li jie mnistli zi"&gt;理解MNIST例子&lt;/h2&gt;
&lt;p&gt;现在就让我们看看它是如何做到的！跟着下面的步骤，打开源代码。&lt;/p&gt;
&lt;h3 id="xu yan"&gt;序言&lt;/h3&gt;
&lt;p&gt;您可能会首先注意到，除TensorLayer之外，我们还导入了Numpy和TensorFlow：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorlayer&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tl&lt;/span&gt;
  &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorlayer.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;set_keep&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这是因为TensorLayer是建立在TensorFlow上的，TensorLayer设计的初衷是为了简化工作并提供帮助而不是取代TensorFlow。
所以您会需要一起使用TensorLayer和一些常见的TensorFlow代码。&lt;/p&gt;
&lt;p&gt;请注意，当使用降噪自编码器(Denoising Autoencoder)时，代码中的 &lt;code&gt;set_keep&lt;/code&gt; 被当作用来访问保持概率(Keeping Probabilities)的占位符。&lt;/p&gt;
&lt;h3 id="zai ru shu ju"&gt;载入数据&lt;/h3&gt;
&lt;p&gt;下面第一部分的代码首先定义了 &lt;code&gt;load_mnist_dataset()&lt;/code&gt; 函数。
其目的是为了下载MNIST数据集（如果还未下载），并且返回标准numpy数列通过numpy array的格式。
到这里还没有涉及TensorLayer，所以我们可以把它简单看作：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  X_train, y_train, X_val, y_val, X_test, y_test = \
                    tl.files.load_mnist_dataset(shape=(-1,784))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;X_train.shape&lt;/code&gt; 为 &lt;code&gt;(50000,784)&lt;/code&gt;，可以理解成共有50000张图片并且每张图片有784个像素点。
&lt;code&gt;Y_train.shape&lt;/code&gt; 为 &lt;code&gt;(50000,)&lt;/code&gt; ，它是一个和 &lt;code&gt;X_train&lt;/code&gt; 长度相同的向量，用于给出每幅图的数字标签，即这些图片所包含的位于0-9之间的数字（如果画这些数字的人没有想乱画别的东西）。&lt;/p&gt;
&lt;p&gt;另外对于卷积神经网络的例子，MNIST还可以按下面的4D版本来载入：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  X_train, y_train, X_val, y_val, X_test, y_test = \
              tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;X_train.shape&lt;/code&gt; 为 &lt;code&gt;(50000,28,28,1)&lt;/code&gt; ，这代表了50000张图片，每张图片使用一个通道（Channel），28行，28列。
通道为1是因为它是灰度图像，每个像素只能有一个值。&lt;/p&gt;
&lt;h3 id="jian li mo xing"&gt;建立模型&lt;/h3&gt;
&lt;p&gt;来到这里，就轮到TensorLayer来一显身手了！TensorLayer允许您通过创建，堆叠或者合并图层(Layers)来定义任意结构的神经网络。
每一层都知道它在网络中的直接输入层, 而每层的输出同时作为该层和整个网络的一个句柄，
通常是我们要传递给其余代码的唯一东西。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 上段的最后一句翻译，我认为原文中的output layer说法有问题，因为output layer通常指网络的最后一层, 或者表示当前层的下一层，这里应该表示为&amp;ldquo;每层的输出&amp;rdquo;才符合上下文，也符合代码逻辑。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;正如上文提到的， &lt;code&gt;tutorial_mnist.py&lt;/code&gt; 支持四类模型，我们通过同样的接口实现，只需简单的替换一下函数即可。
首先，我们将定义一个结构固定的多层次感知器（Multi-Layer Perceptron），所有的步骤都会详细的讲解。
然后，我们会实现一个去噪自编码器(Denosing Autoencoding)。
接着，我们要将所有去噪自编码器堆叠起来并对他们进行监督微调(Supervised Fine-tune)。
最后，我们将展示如何去创建一个卷积神经网络(Convolutional Neural Network)。&lt;/p&gt;
&lt;p&gt;此外，如果您有兴趣，我们还提供了一个简化版的MNIST例子在 &lt;code&gt;tutorial_mnist_simple.py&lt;/code&gt; 中，和一个
CIFAR-10数据集的卷积神经网络(CNN)的例子在 &lt;code&gt;tutorial_cifar10_tfrecord.py&lt;/code&gt; 中, 供你参考。&lt;/p&gt;
&lt;h3 id="duo ceng shen jing wang luo"&gt;多层神经网络&lt;/h3&gt;
&lt;p&gt;第一个脚本 &lt;code&gt;main_test_layers()&lt;/code&gt; ，创建了一个具有两个隐藏层，每层800个单元的多层次感知器，并且具有10个单元的SOFTMAX输出层紧随其后。
它对输入数据采用20%的退出率(dropout)并且对隐藏层应用50%的退出率(dropout)。&lt;/p&gt;
&lt;p&gt;为了提供数据给这个网络，TensorFlow占位符(placeholder)需要按如下定义。
在这里 &lt;code&gt;None&lt;/code&gt; 是指在编译之后，网络将接受任意批规模(batchsize)的数据
&lt;code&gt;x&lt;/code&gt; 是用来存放 &lt;code&gt;X_train&lt;/code&gt; 数据的并且 &lt;code&gt;y_&lt;/code&gt; 是用来存放 &lt;code&gt;y_train&lt;/code&gt; 数据的。
如果你已经知道批规模，那就不需要这种灵活性了。您可以在这里给出批规模，特别是对于卷积层，这样可以运用TensorFlow一些优化功能。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    x = tf.placeholder(tf.float32, shape=[None, 784], name='x')
    y_ = tf.placeholder(tf.int64, shape=[None, ], name='y_')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在TensorLayer中每个神经网络的基础是一个 &lt;code&gt;InputLayer&lt;/code&gt; 实例。它代表了将要提供(feed)给网络的输入数据。
值得注意的是 &lt;code&gt;InputLayer&lt;/code&gt; 并不依赖任何特定的数据。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.InputLayer(x, name='input_layer')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在添加第一层隐藏层之前，我们要对输入数据应用20%的退出率(dropout)。
这里我们是通过一个 &lt;code&gt;DropoutLayer&lt;/code&gt; 的实例来实现的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;请注意构造函数的第一个参数是输入层，第二个参数是激活值的保持概率(keeping probability for the activation value)
现在我们要继续构造第一个800个单位的全连接的隐藏层。
尤其是当要堆叠一个 &lt;code&gt;DenseLayer&lt;/code&gt; 时，要特别注意。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.DenseLayer(network, n_units=800, act = tf.nn.relu, name='relu1')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;同样，构造函数的第一个参数意味着这我们正在 &lt;code&gt;network&lt;/code&gt; 之上堆叠 &lt;code&gt;network&lt;/code&gt; 。
&lt;code&gt;n_units&lt;/code&gt; 简明得给出了全连接层的单位数。
&lt;code&gt;act&lt;/code&gt; 指定了一个激活函数，这里的激活函数有一部分已经被定义在了&lt;code&gt;tensorflow.nn&lt;/code&gt; 和  &lt;code&gt;tensorlayer.activation&lt;/code&gt; 中。
我们在这里选择了整流器(rectifier)，我们将得到ReLUs。
我们现在来添加50%的退出率，以及另外800个单位的稠密层(dense layer)，和50%的退出率：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')
    network = tl.layers.DenseLayer(network, n_units=800, act = tf.nn.relu, name='relu2')
    network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，我们加入&lt;code&gt;n_units&lt;/code&gt;等于分类个数的全连接的输出层。注意，&lt;code&gt;cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))&lt;/code&gt; 在内部实现 Softmax，以提高计算效率，因此最后一层的输出为 identity ，更多细节请参考 &lt;code&gt;tl.cost.cross_entropy()&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.DenseLayer(network,
                                  n_units=10,
                                  act = tl.activation.identity,
                                  name='output_layer')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如上所述，因为每一层都被链接到了它的输入层，所以我们只需要在TensorLayer中将输出层接入一个网络：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    y = network.outputs
    y_op = tf.argmax(tf.nn.softmax(y), 1)
    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在这里，&lt;code&gt;network.outputs&lt;/code&gt; 是网络的10个特征的输出(按照一个热门的格式)。
&lt;code&gt;y_op&lt;/code&gt; 是代表类索引的整数输出， &lt;code&gt;cost&lt;/code&gt; 是目标和预测标签的交叉熵。&lt;/p&gt;
&lt;h3 id="jiang zao zi bian ma qi"&gt;降噪自编码器&lt;/h3&gt;
&lt;p&gt;自编码器是一种无监督学习（Unsupervisered Learning）模型，可从数据中学习出更好的表达，
目前已经用于逐层贪婪的预训练（Greedy layer-wise pre-train）。
有关自编码器内容，请参考教程 &lt;a href="http://deeplearning.stanford.edu/tutorial/"&gt;Deeplearning Tutorial&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;脚本 &lt;code&gt;main_test_denoise_AE()&lt;/code&gt; 实现了有50%的腐蚀率(corrosion rate)的降噪自编码器。
这个自编码器可以按如下方式定义，这里的 &lt;code&gt;DenseLayer&lt;/code&gt; 代表了一个自编码器：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.InputLayer(x, name='input_layer')
    network = tl.layers.DropoutLayer(network, keep=0.5, name='denoising1')
    network = tl.layers.DenseLayer(network, n_units=200, act=tf.nn.sigmoid, name='sigmoid1')
    recon_layer1 = tl.layers.ReconLayer(network,
                                        x_recon=x,
                                        n_units=784,
                                        act=tf.nn.sigmoid,
                                        name='recon_layer1')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练 &lt;code&gt;DenseLayer&lt;/code&gt; ，只需要运行 &lt;code&gt;ReconLayer.Pretrain()&lt;/code&gt; 即可。
如果要使用降噪自编码器，腐蚀层(corrosion layer)(&lt;code&gt;DropoutLayer&lt;/code&gt;)的名字需要按后面说的指定。
如果要保存特征图像，设置 &lt;code&gt;save&lt;/code&gt; 为 True 。
根据不同的架构和应用这里可以设置许多预训练的度量(metric)&lt;/p&gt;
&lt;p&gt;对于 sigmoid型激活函数来说，自编码器可以用KL散度来实现。
而对于整流器(Rectifier)来说，对激活函数输出的L1正则化能使得输出变得稀疏。
所以 &lt;code&gt;ReconLayer&lt;/code&gt; 默认只对整流激活函数(ReLU)提供KLD和交叉熵这两种损失度量，而对sigmoid型激活函数提供均方误差以及激活输出的L1范数这两种损失度量。
我们建议您修改 &lt;code&gt;ReconLayer&lt;/code&gt; 来实现自己的预训练度量。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    recon_layer1.pretrain(sess,
                          x=x,
                          X_train=X_train,
                          X_val=X_val,
                          denoise_name='denoising1',
                          n_epoch=200,
                          batch_size=128,
                          print_freq=10,
                          save=True,
                          save_name='w1pre_')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;此外，脚本 &lt;code&gt;main_test_stacked_denoise_AE()&lt;/code&gt; 展示了如何将多个自编码器堆叠到一个网络，然后进行微调。&lt;/p&gt;
&lt;h3 id="juan ji shen jing wang luo"&gt;卷积神经网络&lt;/h3&gt;
&lt;p&gt;最后，&lt;code&gt;main_test_cnn_layer()&lt;/code&gt; 脚本创建了两个CNN层和最大汇流阶段(max pooling stages)，一个全连接的隐藏层和一个全连接的输出层。&lt;/p&gt;
&lt;p&gt;首先，我们用 &lt;code&gt;Conv2dLayer&lt;/code&gt;添加一个卷积层，它带有32个5x5的过滤器，紧接是2x2维度的最大池化。接着是64个5x5的过滤器的卷积层和同样的最大池化。之后，用｀FlattenLayer｀把4维输出转为1维向量，和50%的dropout在最后的隐层中。这里的&lt;code&gt;?&lt;/code&gt;表示每批数量(batch_size)。&lt;/p&gt;
&lt;p&gt;注，&lt;code&gt;tutorial_mnist.py&lt;/code&gt; 中介绍了针对初学者的简化版的 CNN API。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;network = tl.layers.InputLayer(x, name='input_layer')
network = tl.layers.Conv2dLayer(network,
                        act = tf.nn.relu,
                        shape = [5, 5, 1, 32],  # 32 features for each 5x5 patch
                        strides=[1, 1, 1, 1],
                        padding='SAME',
                        name ='cnn_layer1')     # output: (?, 28, 28, 32)
network = tl.layers.PoolLayer(network,
                        ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1],
                        padding='SAME',
                        pool = tf.nn.max_pool,
                        name ='pool_layer1',)   # output: (?, 14, 14, 32)
network = tl.layers.Conv2dLayer(network,
                        act = tf.nn.relu,
                        shape = [5, 5, 32, 64], # 64 features for each 5x5 patch
                        strides=[1, 1, 1, 1],
                        padding='SAME',
                        name ='cnn_layer2')     # output: (?, 14, 14, 64)
network = tl.layers.PoolLayer(network,
                        ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1],
                        padding='SAME',
                        pool = tf.nn.max_pool,
                        name ='pool_layer2',)   # output: (?, 7, 7, 64)
network = tl.layers.FlattenLayer(network, name='flatten_layer')
                                                # output: (?, 3136)
network = tl.layers.DropoutLayer(network, keep=0.5, name='drop1')
                                                # output: (?, 3136)
network = tl.layers.DenseLayer(network, n_units=256, act = tf.nn.relu, name='relu1')
                                                # output: (?, 256)
network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')
                                                # output: (?, 256)
network = tl.layers.DenseLayer(network, n_units=10, act = tl.identity, name='output_layer')
                                                # output: (?, 10)
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;对于专家们来说， &lt;code&gt;Conv2dLayer&lt;/code&gt; 将使用 &lt;code&gt;tensorflow.nn.conv2d&lt;/code&gt; ,TensorFlow默认的卷积方式来创建一个卷积层。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="xun lian mo xing"&gt;训练模型&lt;/h3&gt;
&lt;p&gt;在 &lt;code&gt;tutorial_mnist.py&lt;/code&gt; 脚本的其余部分，仅使用交叉熵代价函数来对MNIST数据集进行训练循环。&lt;/p&gt;
&lt;h4&gt;数据集迭代&lt;/h4&gt;
&lt;p&gt;迭代函数分别对inputs 和 targets 两个numpy数组，按照小批量的数量尺度进行迭代。
更多有关迭代函数的说明，可以在 &lt;code&gt;tensorlayer.iterate&lt;/code&gt; 中找到。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    tl.iterate.minibatches(inputs, targets, batchsize, shuffle=False)
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;损失和更新公式&lt;/h4&gt;
&lt;p&gt;我们继续创建一个在训练中被最小化的损失表达式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    y = network.outputs
    y_op = tf.argmax(tf.nn.softmax(y), 1)
    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;举 &lt;code&gt;main_test_layers()&lt;/code&gt; 这个例子来说，更多的成本或者正则化方法可以被应用在这里。
如果要在权重矩阵中应用最大模(max-norm)方法，你可以添加下列代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cost = cost + tl.cost.maxnorm_regularizer(1.0)(network.all_params[0]) +
                  tl.cost.maxnorm_regularizer(1.0)(network.all_params[2])
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;根据要解决的问题，您会需要使用不同的损失函数，更多有关损失函数的说明请见： &lt;code&gt;tensorlayer.cost&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;有了模型和定义的损失函数之后，我们就可以创建用于训练网络的更新公式。
接下去，我们将使用TensorFlow的优化器如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    train_params = network.all_params
    train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
        epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为了训练网络，我们需要提供数据和保持概率给 &lt;code&gt;feed_dict&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    feed_dict = {x: X_train_a, y_: y_train_a}
    feed_dict.update( network.all_drop )
    sess.run(train_op, feed_dict=feed_dict)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;同时为了进行验证和测试，我们这里用了略有不同的方法。
所有的Dropout，退连(DropConnect)，腐蚀层(Corrosion Layers)都将被禁用。
&lt;code&gt;tl.utils.dict_to_one&lt;/code&gt; 将会设置所有 &lt;code&gt;network.all_drop&lt;/code&gt; 值为1。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    dp_dict = tl.utils.dict_to_one( network.all_drop )
    feed_dict = {x: X_test_a, y_: y_test_a}
    feed_dict.update(dp_dict)
    err, ac = sess.run([cost, acc], feed_dict=feed_dict)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，作为一个额外的监测量，我们需要创建一个分类准确度的公式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    correct_prediction = tf.equal(tf.argmax(y, 1), y_)
    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;下一步？&lt;/h4&gt;
&lt;p&gt;在 &lt;code&gt;tutorial_cifar10_tfrecord.py&lt;/code&gt; 中我们还有更高级的图像分类的例子。
请阅读代码及注释，用以明白如何来生成更多的训练数据以及什么是局部响应正则化。
在这之后，您可以尝试着去实现 &lt;a href="http://doi.org/10.3389/fpsyg.2013.00124"&gt;残差网络(Residual Network)&lt;/a&gt;。
&lt;em&gt;小提示：您可能会用到Layer.outputs。&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="yun xing ping pang qiu li zi_1"&gt;运行乒乓球例子&lt;/h2&gt;
&lt;p&gt;在本教程的第二部分，我们将运行一个深度强化学习的例子，它在Karpathy的两篇博客 &lt;a href="http://karpathy.github.io/2016/05/31/rl/"&gt;Deep Reinforcement Learning:Pong from Pixels&lt;/a&gt; 有介绍。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_atari_pong.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在运行教程代码之前 你需要安装 &lt;a href="https://gym.openai.com/docs"&gt;OpenAI gym environment&lt;/a&gt; ,它是强化学习的一个标杆。
如果一切设置正确，您将得到一个类似以下的输出：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  [2016-07-12 09:31:59,760] Making new env: Pong-v0
    tensorlayer:Instantiate InputLayer input_layer (?, 6400)
    tensorlayer:Instantiate DenseLayer relu1: 200, relu
    tensorlayer:Instantiate DenseLayer output_layer: 3, identity
    param 0: (6400, 200) (mean: -0.000009, median: -0.000018 std: 0.017393)
    param 1: (200,) (mean: 0.000000, median: 0.000000 std: 0.000000)
    param 2: (200, 3) (mean: 0.002239, median: 0.003122 std: 0.096611)
    param 3: (3,) (mean: 0.000000, median: 0.000000 std: 0.000000)
    num of params: 1280803
    layer 0: Tensor("Relu:0", shape=(?, 200), dtype=float32)
    layer 1: Tensor("add_1:0", shape=(?, 3), dtype=float32)
  episode 0: game 0 took 0.17381s, reward: -1.000000
  episode 0: game 1 took 0.12629s, reward: 1.000000  !!!!!!!!
  episode 0: game 2 took 0.17082s, reward: -1.000000
  episode 0: game 3 took 0.08944s, reward: -1.000000
  episode 0: game 4 took 0.09446s, reward: -1.000000
  episode 0: game 5 took 0.09440s, reward: -1.000000
  episode 0: game 6 took 0.32798s, reward: -1.000000
  episode 0: game 7 took 0.74437s, reward: -1.000000
  episode 0: game 8 took 0.43013s, reward: -1.000000
  episode 0: game 9 took 0.42496s, reward: -1.000000
  episode 0: game 10 took 0.37128s, reward: -1.000000
  episode 0: game 11 took 0.08979s, reward: -1.000000
  episode 0: game 12 took 0.09138s, reward: -1.000000
  episode 0: game 13 took 0.09142s, reward: -1.000000
  episode 0: game 14 took 0.09639s, reward: -1.000000
  episode 0: game 15 took 0.09852s, reward: -1.000000
  episode 0: game 16 took 0.09984s, reward: -1.000000
  episode 0: game 17 took 0.09575s, reward: -1.000000
  episode 0: game 18 took 0.09416s, reward: -1.000000
  episode 0: game 19 took 0.08674s, reward: -1.000000
  episode 0: game 20 took 0.09628s, reward: -1.000000
  resetting env. episode reward total was -20.000000. running mean: -20.000000
  episode 1: game 0 took 0.09910s, reward: -1.000000
  episode 1: game 1 took 0.17056s, reward: -1.000000
  episode 1: game 2 took 0.09306s, reward: -1.000000
  episode 1: game 3 took 0.09556s, reward: -1.000000
  episode 1: game 4 took 0.12520s, reward: 1.000000  !!!!!!!!
  episode 1: game 5 took 0.17348s, reward: -1.000000
  episode 1: game 6 took 0.09415s, reward: -1.000000
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这个例子让电脑从屏幕输入来学习如何像人类一样打乒乓球。
在经过15000个序列的训练之后，计算机就可以赢得20%的比赛。
在20000个序列的训练之后，计算机可以赢得35%的比赛，
我们可以看到计算机学的越来越快，这是因为它有更多的胜利的数据来进行训练。
如果您用30000个序列来训练它，那么它会一直赢。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  render = False
  resume = False
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果您想显示游戏过程，那就设置 &lt;code&gt;render&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 。
当您再次运行该代码，您可以设置 &lt;code&gt;resume&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt;,那么代码将加载现有的模型并且会基于它进行训练。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/pong_game.jpeg" width="30%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;h2 id="li jie qiang hua xue xi"&gt;理解强化学习&lt;/h2&gt;
&lt;h3 id="ping pang qiu"&gt;乒乓球&lt;/h3&gt;
&lt;p&gt;要理解强化学习，我们要让电脑学习如何从原始的屏幕输入(像素输入)打乒乓球。
在我们开始之前，我们强烈建议您去浏览一个著名的博客叫做 &lt;a href="http://karpathy.github.io/2016/05/31/rl/"&gt;Deep Reinforcement Learning:pong from Pixels&lt;/a&gt; ,
这是使用python numpy库和OpenAI gym environment=来实现的一个深度强化学习的最简实现。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_atari_pong.py
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="ce lue wang luo (policy network)"&gt;策略网络(Policy Network)&lt;/h3&gt;
&lt;p&gt;在深度强化学习中，Policy Network 等同于 深度神经网络。
它是我们的选手(或者说&amp;ldquo;代理人(agent)&amp;rdquo;），它的输出告诉我们应该做什么(向上移动或向下移动)：
在Karpathy的代码中，他只定义了2个动作，向上移动和向下移动，并且仅使用单个simgoid输出：
为了使我们的教程更具有普遍性，我们使用3个 softmax 输出来定义向上移动，向下移动和停止(什么都不做)3个动作。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    # observation for training
    states_batch_pl = tf.placeholder(tf.float32, shape=[None, D])

    network = tl.layers.InputLayer(states_batch_pl, name='input_layer')
    network = tl.layers.DenseLayer(network, n_units=H,
                                    act = tf.nn.relu, name='relu1')
    network = tl.layers.DenseLayer(network, n_units=3,
                            act = tl.activation.identity, name='output_layer')
    probs = network.outputs
    sampling_prob = tf.nn.softmax(probs)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后我们的代理人就一直打乒乓球。它计算不同动作的概率，
并且之后会从这个均匀的分布中选取样本(动作)。
因为动作被1,2和3代表，但是softmax输出应该从0开始，所以我们从-1计算这个标签的价值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    prob = sess.run(
        sampling_prob,
        feed_dict={states_batch_pl: x}
    )
    # action. 1: STOP  2: UP  3: DOWN
    action = np.random.choice([1,2,3], p=prob.flatten())
    ...
    ys.append(action - 1)
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="ce lue bi jin (policy gradient)"&gt;策略逼近(Policy Gradient)&lt;/h3&gt;
&lt;p&gt;策略梯度下降法是一个end-to-end的算法，它直接学习从状态映射到动作的策略函数。
一个近似最优的策略可以通过最大化预期的奖励来直接学习。
策略函数的参数(例如，在乒乓球例子终使用的策略网络的参数)在预期奖励的近似值的引导下能够被训练和学习。
换句话说，我们可以通过更新它的参数来逐步调整策略函数，这样它能从给定的状态做出一系列行为来获得更高的奖励。&lt;/p&gt;
&lt;p&gt;策略迭代的一个替代算法就是深度Q-learning(DQN)。
他是基于Q-learning,学习一个映射状态和动作到一些值的价值函数的算法(叫Q函数)。
DQN采用了一个深度神经网络来作为Q函数的逼近来代表Q函数。
训练是通过最小化时序差分(temporal-difference)误差来实现。
一个名为&amp;ldquo;再体验(experience replay)&amp;rdquo;的神经生物学的启发式机制通常和DQN一起被使用来帮助提高非线性函数的逼近的稳定性&lt;/p&gt;
&lt;p&gt;您可以阅读以下文档，来得到对强化学习更好的理解：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html"&gt;Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf"&gt;Deep Reinforcement Learning. David Silver, Google DeepMind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;UCL Course on RL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;强化深度学习近些年来最成功的应用就是让模型去学习玩Atari的游戏。 AlphaGO同时也是使用类似的策略逼近方法来训练他们的策略网络而战胜了世界级的专业围棋选手。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"&gt;Atari - Playing Atari with Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html"&gt;Atari - Human-level control through deep reinforcement learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html"&gt;AlphaGO - Mastering the game of Go with deep neural networks and tree search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;数据集迭代&lt;/h4&gt;
&lt;p&gt;在强化学习中，我们把每场比赛所产生的所有决策来作为一个序列 (up,up,stop,...,down)。在乒乓球游戏中，比赛是在某一方达到21分后结束的，所以一个序列可能包含几十个决策。
然后我们可以设置一个批规模的大小，每一批包含一定数量的序列，基于这个批规模来更新我们的模型。
在本教程中，我们把每批规模设置成10个序列。使用RMSProp训练一个具有200个单元的隐藏层的2层策略网络&lt;/p&gt;
&lt;h4&gt;损失和更新公式&lt;/h4&gt;
&lt;p&gt;接着我们创建一个在训练中被最小化的损失公式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    actions_batch_pl = tf.placeholder(tf.int32, shape=[None])
    discount_rewards_batch_pl = tf.placeholder(tf.float32, shape=[None])
    loss = tl.rein.cross_entropy_reward_loss(probs, actions_batch_pl,
                                                  discount_rewards_batch_pl)
    ...
    ...
    sess.run(
        train_op,
        feed_dict={
            states_batch_pl: epx,
            actions_batch_pl: epy,
            discount_rewards_batch_pl: disR
        }
    )
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;一batch的损失和一个batch内的策略网络的所有输出，所有的我们做出的动作和相应的被打折的奖励有关
我们首先通过累加被打折的奖励和实际输出和真实动作的交叉熵计算每一个动作的损失。
最后的损失是所有动作的损失的和。&lt;/p&gt;
&lt;h3 id="xia yi bu ?"&gt;下一步?&lt;/h3&gt;
&lt;p&gt;上述教程展示了您如何去建立自己的代理人，end-to-end。
虽然它有很合理的品质，但它的默认参数不会给你最好的代理人模型。
这有一些您可以优化的内容。&lt;/p&gt;
&lt;p&gt;首先，与传统的MLP模型不同，比起 &lt;a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt; 更好的是我们可以使用CNNs来采集屏幕信息&lt;/p&gt;
&lt;p&gt;另外这个模型默认参数没有调整，您可以更改学习率，衰退率，或者用不同的方式来初始化您的模型的权重。&lt;/p&gt;
&lt;p&gt;最后，您可以尝试不同任务(游戏)的模型。&lt;/p&gt;
&lt;h2 id="yun xing word2vecli zi_1"&gt;运行Word2Vec例子&lt;/h2&gt;
&lt;p&gt;在教程的这一部分，我们训练一个词嵌套矩阵，每个词可以通过矩阵中唯一的行向量来表示。
在训练结束时，意思类似的单词会有相识的词向量。
在代码的最后，我们通过把单词放到一个平面上来可视化，我们可以看到相似的单词会被聚集在一起。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_word2vec_basic.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果一切设置正确，您最后会得到如下的可视化图。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/tsne.png" width="100%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;h2 id="li jie ci qian tao"&gt;理解词嵌套&lt;/h2&gt;
&lt;h3 id="ci qian tao (qian ru )"&gt;词嵌套（嵌入）&lt;/h3&gt;
&lt;p&gt;我们强烈建议您先阅读Colah的博客 &lt;a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"&gt;Word Representations&lt;/a&gt; &lt;a href="http://dataunion.org/9331.html"&gt;(中文翻译)&lt;/a&gt; ，
以理解为什么我们要使用一个向量来表示一个单词。更多Word2vec的细节可以在 &lt;a href="http://arxiv.org/abs/1411.2738"&gt;Word2vec Parameter Learning Explained&lt;/a&gt; 中找到。&lt;/p&gt;
&lt;p&gt;基本来说，训练一个嵌套矩阵是一个非监督学习的过程。一个单词使用唯一的ID来表示，而这个ID号就是嵌套矩阵的行号（row index），对应的行向量就是用来表示该单词的，使用向量来表示单词可以更好地表达单词的意思。比如，有4个单词的向量， &lt;code&gt;woman &amp;minus; man = queen - king&lt;/code&gt; ，这个例子中可以看到，嵌套矩阵中有一个纬度是用来表示性别的。&lt;/p&gt;
&lt;p&gt;定义一个Word2vec词嵌套矩阵如下。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # train_inputs is a row vector, a input is an integer id of single word.
  # train_labels is a column vector, a label is an integer id of single word.
  # valid_dataset is a column vector, a valid set is an integer id of single word.
  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

  # Look up embeddings for inputs.
  emb_net = tl.layers.Word2vecEmbeddingInputlayer(
          inputs = train_inputs,
          train_labels = train_labels,
          vocabulary_size = vocabulary_size,
          embedding_size = embedding_size,
          num_sampled = num_sampled,
          nce_loss_args = {},
          E_init = tf.random_uniform_initializer(minval=-1.0, maxval=1.0),
          E_init_args = {},
          nce_W_init = tf.truncated_normal_initializer(
                            stddev=float(1.0/np.sqrt(embedding_size))),
          nce_W_init_args = {},
          nce_b_init = tf.constant_initializer(value=0.0),
          nce_b_init_args = {},
          name ='word2vec_layer',
      )
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;数据迭代和损失函数&lt;/h4&gt;
&lt;p&gt;Word2vec使用负采样（Negative sampling）和Skip-gram模型进行训练。
噪音对比估计损失（NCE）会帮助减少损失函数的计算量，加快训练速度。
Skip-Gram 将文本（context）和目标（target）反转，尝试从目标单词预测目标文本单词。
我们使用 &lt;code&gt;tl.nlp.generate_skip_gram_batch&lt;/code&gt; 函数来生成训练数据，如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # NCE损失函数由 Word2vecEmbeddingInputlayer 提供
  cost = emb_net.nce_cost
  train_params = emb_net.all_params

  train_op = tf.train.AdagradOptimizer(learning_rate, initial_accumulator_value=0.1,
            use_locking=False).minimize(cost, var_list=train_params)

  data_index = 0
  while (step &amp;lt; num_steps):
    batch_inputs, batch_labels, data_index = tl.nlp.generate_skip_gram_batch(
                  data=data, batch_size=batch_size, num_skips=num_skips,
                  skip_window=skip_window, data_index=data_index)
    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}
    _, loss_val = sess.run([train_op, cost], feed_dict=feed_dict)
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;加载已训练好的的词嵌套矩阵&lt;/h4&gt;
&lt;p&gt;在训练嵌套矩阵的最后，我们保存矩阵及其词汇表、单词转ID字典、ID转单词字典。
然后，当下次做实际应用时，可以想下面的代码中那样加载这个已经训练好的矩阵和字典，
参考 &lt;code&gt;tutorial_generate_text.py&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  vocabulary_size = 50000
  embedding_size = 128
  model_file_name = "model_word2vec_50k_128"
  batch_size = None

  print("Load existing embedding matrix and dictionaries")
  all_var = tl.files.load_npy_to_any(name=model_file_name+'.npy')
  data = all_var['data']; count = all_var['count']
  dictionary = all_var['dictionary']
  reverse_dictionary = all_var['reverse_dictionary']

  tl.nlp.save_vocab(count, name='vocab_'+model_file_name+'.txt')

  del all_var, data, count

  load_params = tl.files.load_npz(name=model_file_name+'.npz')

  x = tf.placeholder(tf.int32, shape=[batch_size])
  y_ = tf.placeholder(tf.int32, shape=[batch_size, 1])

  emb_net = tl.layers.EmbeddingInputlayer(
                  inputs = x,
                  vocabulary_size = vocabulary_size,
                  embedding_size = embedding_size,
                  name ='embedding_layer')

  tl.layers.initialize_global_variables(sess)

  tl.files.assign_params(sess, [load_params[0]], emb_net)
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="yun xing ptbli zi_1"&gt;运行PTB例子&lt;/h2&gt;
&lt;p&gt;Penn TreeBank（PTB）数据集被用在很多语言建模（Language Modeling）的论文中，包括"Empirical Evaluation and Combination of Advanced Language Modeling Techniques"和
&amp;ldquo;Recurrent Neural Network Regularization&amp;rdquo;。该数据集的训练集有929k个单词，验证集有73K个单词，测试集有82k个单词。
在它的词汇表刚好有10k个单词。&lt;/p&gt;
&lt;p&gt;PTB例子是为了展示如何用递归神经网络（Recurrent Neural Network）来进行语言建模的。&lt;/p&gt;
&lt;p&gt;给一句话 "I am from Imperial College London", 这个模型可以从中学习出如何从&amp;ldquo;from Imperial College&amp;rdquo;来预测出&amp;ldquo;Imperial College London&amp;rdquo;。也就是说，它根据之前输入的单词序列来预测出下一步输出的单词序列，在刚才的例子中 &lt;code&gt;num_steps (序列长度，sequence length)&lt;/code&gt; 为 3。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_ptb_lstm.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;该脚本提供三种设置(小，中，大)，越大的模型有越好的建模性能，您可以修改下面的代码片段来选择不同的模型设置。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  flags.DEFINE_string(
      "model", "small",
      "A type of model. Possible options are: small, medium, large.")
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果您选择小设置，您将会看到：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Epoch: 1 Learning rate: 1.000
  0.004 perplexity: 5220.213 speed: 7635 wps
  0.104 perplexity: 828.871 speed: 8469 wps
  0.204 perplexity: 614.071 speed: 8839 wps
  0.304 perplexity: 495.485 speed: 8889 wps
  0.404 perplexity: 427.381 speed: 8940 wps
  0.504 perplexity: 383.063 speed: 8920 wps
  0.604 perplexity: 345.135 speed: 8920 wps
  0.703 perplexity: 319.263 speed: 8949 wps
  0.803 perplexity: 298.774 speed: 8975 wps
  0.903 perplexity: 279.817 speed: 8986 wps
  Epoch: 1 Train Perplexity: 265.558
  Epoch: 1 Valid Perplexity: 178.436
  ...
  Epoch: 13 Learning rate: 0.004
  0.004 perplexity: 56.122 speed: 8594 wps
  0.104 perplexity: 40.793 speed: 9186 wps
  0.204 perplexity: 44.527 speed: 9117 wps
  0.304 perplexity: 42.668 speed: 9214 wps
  0.404 perplexity: 41.943 speed: 9269 wps
  0.504 perplexity: 41.286 speed: 9271 wps
  0.604 perplexity: 39.989 speed: 9244 wps
  0.703 perplexity: 39.403 speed: 9236 wps
  0.803 perplexity: 38.742 speed: 9229 wps
  0.903 perplexity: 37.430 speed: 9240 wps
  Epoch: 13 Train Perplexity: 36.643
  Epoch: 13 Valid Perplexity: 121.475
  Test Perplexity: 116.716
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;PTB例子证明了递归神经网络能够实现语言建模，但是这个例子并没有做什么实际的事情。
在做具体应用之前，您应该浏览这个例子的代码和下一章 &amp;ldquo;理解 LSTM&amp;rdquo; 来学好递归神经网络的基础。
之后，您将学习如何用递归神经网络来生成文本，如何实现语言翻译和问题应答系统。&lt;/p&gt;
&lt;h2 id="li jie lstm"&gt;理解LSTM&lt;/h2&gt;
&lt;h3 id="di gui shen jing wang luo  (recurrent neural network)"&gt;递归神经网络 (Recurrent Neural Network)&lt;/h3&gt;
&lt;p&gt;我们认为Andrey Karpathy的博客 &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;Understand Recurrent Neural Network&lt;/a&gt; 是了解递归神经网络最好的材料。
读完这个博客后，Colah的博客 &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understand LSTM Network&lt;/a&gt; 能帮助你了解LSTM。
我们在这里不介绍更多关于递归神经网络的内容，所以在你继续下面的内容之前，请先阅读我们建议阅读的博客。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/karpathy_rnn.jpeg" width="100%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;图片由Andrey Karpathy提供&lt;/p&gt;
&lt;h3 id="tong bu shu ru yu shu chu xu lie  (synced sequence input and output)"&gt;同步输入与输出序列 (Synced sequence input and output)&lt;/h3&gt;
&lt;p&gt;PTB例子中的模型是一个典型的同步输入与输出，Karpathy 把它描述为
&amp;ldquo;(5) 同步序列输入与输出(例如视频分类中我们希望对每一帧进行标记)。&amp;ldquo;&lt;/p&gt;
&lt;p&gt;模型的构建如下，第一层是词嵌套层（嵌入），把每一个单词转换成对应的词向量，在该例子中没有使用预先训练好的
嵌套矩阵。第二，堆叠两层LSTM，使用Dropout来实现规则化，防止overfitting。
最后，使用全连接层输出一序列的softmax输出。&lt;/p&gt;
&lt;p&gt;第一层LSTM的输出形状是 [batch_size, num_steps, hidden_size]，这是为了让下一层LSTM可以堆叠在其上面。
第二层LSTM的输出形状是 [batch_size&lt;em&gt;num_steps, hidden_size]，这是为了让输出层（全连接层 Dense）可以堆叠在其上面。
然后计算每个样本的softmax输出，样本总数为 n_examples = batch_size&lt;/em&gt;num_steps。&lt;/p&gt;
&lt;p&gt;若想要更进一步理解该PTB教程，您也可以阅读 &lt;a href="https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html#recurrent-neural-networks"&gt;TensorFlow 官方的PTB教程&lt;/a&gt; ，中文翻译请见极客学院。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  network = tl.layers.EmbeddingInputlayer(
              inputs = x,
              vocabulary_size = vocab_size,
              embedding_size = hidden_size,
              E_init = tf.random_uniform_initializer(-init_scale, init_scale),
              name ='embedding_layer')
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop1')
  network = tl.layers.RNNLayer(network,
              cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
              cell_init_args={'forget_bias': 0.0},
              n_hidden=hidden_size,
              initializer=tf.random_uniform_initializer(-init_scale, init_scale),
              n_steps=num_steps,
              return_last=False,
              name='basic_lstm_layer1')
  lstm1 = network
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop2')
  network = tl.layers.RNNLayer(network,
              cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
              cell_init_args={'forget_bias': 0.0},
              n_hidden=hidden_size,
              initializer=tf.random_uniform_initializer(-init_scale, init_scale),
              n_steps=num_steps,
              return_last=False,
              return_seq_2d=True,
              name='basic_lstm_layer2')
  lstm2 = network
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop3')
  network = tl.layers.DenseLayer(network,
              n_units=vocab_size,
              W_init=tf.random_uniform_initializer(-init_scale, init_scale),
              b_init=tf.random_uniform_initializer(-init_scale, init_scale),
              act = tl.activation.identity, name='output_layer')
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;数据迭代&lt;/h4&gt;
&lt;p&gt;batch_size 数值可以被视为并行计算的数量。
如下面的例子所示，第一个 batch 使用 0 到 9 来学习序列信息。
第二个 batch 使用 10 到 19 来学习序列。
所以它忽略了 9 到 10 之间的信息。
只当我们 bath_size 设为 1，它才使用 0 到 20 之间所有的序列信息来学习。&lt;/p&gt;
&lt;p&gt;这里的 batch_size 的意思与 MNIST 例子略有不同。
在 MNIST 例子，batch_size 是每次迭代中我们使用的样本数量，
而在 PTB 的例子中，batch_size 是为加快训练速度的并行进程数。&lt;/p&gt;
&lt;p&gt;虽然当 batch_size &amp;gt; 1 时有些信息将会被忽略，
但是如果你的数据是足够长的（一个语料库通常有几十亿个字），被忽略的信息不会影响最终的结果。&lt;/p&gt;
&lt;p&gt;在PTB教程中，我们设置了 batch_size = 20，所以，我们将整个数据集拆分成 20 段（segment）。
在每一轮（epoch）的开始时，我们有 20 个初始化的 LSTM 状态（State），然后分别对 20 段数据进行迭代学习。&lt;/p&gt;
&lt;p&gt;训练数据迭代的例子如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  train_data = [i for i in range(20)]
  for batch in tl.iterate.ptb_iterator(train_data, batch_size=2, num_steps=3):
      x, y = batch
      print(x, '\n',y)
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  ... [[ 0  1  2] &amp;lt;---x                       1st subset/ iteration
  ...  [10 11 12]]
  ... [[ 1  2  3] &amp;lt;---y
  ...  [11 12 13]]
  ...
  ... [[ 3  4  5]  &amp;lt;--- 1st batch input       2nd subset/ iteration
  ...  [13 14 15]] &amp;lt;--- 2nd batch input
  ... [[ 4  5  6]  &amp;lt;--- 1st batch target
  ...  [14 15 16]] &amp;lt;--- 2nd batch target
  ...
  ... [[ 6  7  8]                             3rd subset/ iteration
  ...  [16 17 18]]
  ... [[ 7  8  9]
  ...  [17 18 19]]
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; 这个例子可以当作词嵌套矩阵的预训练。
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;损失和更新公式&lt;/h4&gt;
&lt;p&gt;损失函数是一系列输出cross entropy的均值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # 更多细节请见 tensorlayer.cost.cross_entropy_seq()
  def loss_fn(outputs, targets, batch_size, num_steps):
      # Returns the cost function of Cross-entropy of two sequences, implement
      # softmax internally.
      # outputs : 2D tensor [batch_size*num_steps, n_units of output layer]
      # targets : 2D tensor [batch_size, num_steps], need to be reshaped.
      # n_examples = batch_size * num_steps
      # so
      # cost is the averaged cost of each mini-batch (concurrent process).
      loss = tf.nn.seq2seq.sequence_loss_by_example(
          [outputs],
          [tf.reshape(targets, [-1])],
          [tf.ones([batch_size * num_steps])])
      cost = tf.reduce_sum(loss) / batch_size
      return cost

  # Cost for Training
  cost = loss_fn(network.outputs, targets, batch_size, num_steps)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在训练时，该例子在若干个epoch之后（由 &lt;code&gt;max_epoch&lt;/code&gt; 定义），才开始按比例下降学习率（learning rate），新学习率是前一个epoch的学习率乘以一个下降率（由 &lt;code&gt;lr_decay&lt;/code&gt; 定义）。
此外，截断反向传播（truncated backpropagation）截断了&lt;/p&gt;
&lt;p&gt;为使学习过程易于处理，通常的做法是将反向传播的梯度在（按时间）展开的步骤上照一个固定长度( &lt;code&gt;num_steps&lt;/code&gt; )截断。 通过在一次迭代中的每个时刻上提供长度为 &lt;code&gt;num_steps&lt;/code&gt; 的输入和每次迭代完成之后反向传导，这会很容易实现。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # 截断反响传播 Truncated Backpropagation for training
  with tf.variable_scope('learning_rate'):
      lr = tf.Variable(0.0, trainable=False)
  tvars = tf.trainable_variables()
  grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                    max_grad_norm)
  optimizer = tf.train.GradientDescentOptimizer(lr)
  train_op = optimizer.apply_gradients(zip(grads, tvars))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果当前epoch值大于 &lt;code&gt;max_epoch&lt;/code&gt; ，则把当前学习率乘以 &lt;code&gt;lr_decay&lt;/code&gt; 来降低学习率。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  new_lr_decay = lr_decay ** max(i - max_epoch, 0.0)
  sess.run(tf.assign(lr, learning_rate * new_lr_decay))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在每一个epoch的开始之前，LSTM的状态要被重置为零状态；在每一个迭代之后，LSTM状态都会被改变，所以要把最新的LSTM状态
作为下一个迭代的初始化状态。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # 在每一个epoch之前，把所有LSTM状态设为零状态
  state1 = tl.layers.initialize_rnn_state(lstm1.initial_state)
  state2 = tl.layers.initialize_rnn_state(lstm2.initial_state)
  for step, (x, y) in enumerate(tl.iterate.ptb_iterator(train_data,
                                              batch_size, num_steps)):
      feed_dict = {input_data: x, targets: y,
                  lstm1.initial_state: state1,
                  lstm2.initial_state: state2,
                  }
      # 启用dropout
      feed_dict.update( network.all_drop )
      # 把新的状态作为下一个迭代的初始状态
      _cost, state1, state2, _ = sess.run([cost,
                                      lstm1.final_state,
                                      lstm2.final_state,
                                      train_op],
                                      feed_dict=feed_dict
                                      )
      costs += _cost; iters += num_steps
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;预测&lt;/h4&gt;
&lt;p&gt;在训练完模型之后，当我们预测下一个输出时，我们不需要考虑序列长度了，因此 &lt;code&gt;batch_size&lt;/code&gt; 和 &lt;code&gt;num_steps&lt;/code&gt; 都设为 1 。
然后，我们可以一步一步地输出下一个单词，而不是通过一序列的单词来输出一序列的单词。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  input_data_test = tf.placeholder(tf.int32, [1, 1])
  targets_test = tf.placeholder(tf.int32, [1, 1])
  ...
  network_test, lstm1_test, lstm2_test = inference(input_data_test,
                        is_training=False, num_steps=1, reuse=True)
  ...
  cost_test = loss_fn(network_test.outputs, targets_test, 1, 1)
  ...
  print("Evaluation")
  # 测试
  # go through the test set step by step, it will take a while.
  start_time = time.time()
  costs = 0.0; iters = 0
  # 与训练时一样，设置所有LSTM状态为零状态
  state1 = tl.layers.initialize_rnn_state(lstm1_test.initial_state)
  state2 = tl.layers.initialize_rnn_state(lstm2_test.initial_state)
  for step, (x, y) in enumerate(tl.iterate.ptb_iterator(test_data,
                                          batch_size=1, num_steps=1)):
      feed_dict = {input_data_test: x, targets_test: y,
                  lstm1_test.initial_state: state1,
                  lstm2_test.initial_state: state2,
                  }
      _cost, state1, state2 = sess.run([cost_test,
                                      lstm1_test.final_state,
                                      lstm2_test.final_state],
                                      feed_dict=feed_dict
                                      )
      costs += _cost; iters += 1
  test_perplexity = np.exp(costs / iters)
  print("Test Perplexity: %.3f took %.2fs" % (test_perplexity, time.time() - start_time))
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="xia yi bu ?_1"&gt;下一步？&lt;/h3&gt;
&lt;p&gt;您已经明白了同步序列输入和序列输出（Synced sequence input and output）。
现在让我们思考下序列输入单一输出的情况（Sequence input and one output），
LSTM 也可以学会通过给定一序列输入如 &amp;ldquo;我来自北京，我会说.." 来输出
一个单词 "中文"。&lt;/p&gt;
&lt;p&gt;请仔细阅读并理解 &lt;code&gt;tutorial_generate_text.py&lt;/code&gt; 的代码，它讲了如何加载一个已经训练好的词嵌套矩阵，
以及如何给定机器一个文档，让它来学习文字自动生成。&lt;/p&gt;
&lt;p&gt;Karpathy的博客：
"(3) Sequence input (e.g. sentiment analysis where a given sentence is
classified as expressing positive or negative sentiment). "&lt;/p&gt;
&lt;h2 id="yun xing ji qi fan yi li zi_1"&gt;运行机器翻译例子&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_translate.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;该脚本将训练一个神经网络来把英文翻译成法文。
如果一切正常，您将看到：
- 下载WMT英文-法文翻译数据库，包括训练集和测试集。
- 通过训练集创建英文和法文的词汇表。
- 把训练集和测试集的单词转换成数字ID表示。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Prepare raw data
  Load or Download WMT English-to-French translation &amp;gt; wmt
  Training data : wmt/giga-fren.release2
  Testing data : wmt/newstest2013

  Create vocabularies
  Vocabulary of French : wmt/vocab40000.fr
  Vocabulary of English : wmt/vocab40000.en
  Creating vocabulary wmt/vocab40000.fr from data wmt/giga-fren.release2.fr
    processing line 100000
    processing line 200000
    processing line 300000
    processing line 400000
    processing line 500000
    processing line 600000
    processing line 700000
    processing line 800000
    processing line 900000
    processing line 1000000
    processing line 1100000
    processing line 1200000
    ...
    processing line 22500000
  Creating vocabulary wmt/vocab40000.en from data wmt/giga-fren.release2.en
    processing line 100000
    ...
    processing line 22500000

  ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;首先，我们从WMT'15网站上下载英语-法语翻译数据。训练数据和测试数据如下。
训练数据用于训练模型，测试数据用于评估该模型。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  wmt/training-giga-fren.tar  &amp;lt;-- 英文－法文训练集 (2.6GB)
                                  giga-fren.release2.* 从该文件解压出来
  wmt/dev-v2.tgz              &amp;lt;-- 多种语言的测试集 (21.4MB)
                                  newstest2013.* 从该文件解压出来

  wmt/giga-fren.release2.fr   &amp;lt;-- 法文训练集 (4.57GB)
  wmt/giga-fren.release2.en   &amp;lt;-- 英文训练集 (3.79GB)

  wmt/newstest2013.fr         &amp;lt;-- 法文测试集 (393KB)
  wmt/newstest2013.en         &amp;lt;-- 英文测试集 (333KB)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;所有 &lt;code&gt;giga-fren.release2.*&lt;/code&gt; 是训练数据， &lt;code&gt;giga-fren.release2.fr&lt;/code&gt; 内容如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Il a transform&amp;eacute; notre vie | Il a transform&amp;eacute; la soci&amp;eacute;t&amp;eacute; | Son fonctionnement | La technologie, moteur du changement Accueil | Concepts | Enseignants | Recherche | Aper&amp;ccedil;u | Collaborateurs | Web HHCC | Ressources | Commentaires Mus&amp;eacute;e virtuel du Canada
  Plan du site
  R&amp;eacute;troaction
  Cr&amp;eacute;dits
  English
  Qu&amp;rsquo;est-ce que la lumi&amp;egrave;re?
  La d&amp;eacute;couverte du spectre de la lumi&amp;egrave;re blanche Des codes dans la lumi&amp;egrave;re Le spectre &amp;eacute;lectromagn&amp;eacute;tique Les spectres d&amp;rsquo;&amp;eacute;mission Les spectres d&amp;rsquo;absorption Les ann&amp;eacute;es-lumi&amp;egrave;re La pollution lumineuse
  Le ciel des premiers habitants La vision contemporaine de l'Univers L&amp;rsquo;astronomie pour tous
  Bande dessin&amp;eacute;e
  Liens
  Glossaire
  Observatoires
  ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;giga-fren.release2.en&lt;/code&gt; 内容如下，我们可以看到单词或者句子用 &lt;code&gt;|&lt;/code&gt; 或 &lt;code&gt;\n&lt;/code&gt; 来分隔。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Changing Lives | Changing Society | How It Works | Technology Drives Change Home | Concepts | Teachers | Search | Overview | Credits | HHCC Web | Reference | Feedback Virtual Museum of Canada Home Page
  Site map
  Feedback
  Credits
  Fran&amp;ccedil;ais
  What is light ?
  The white light spectrum Codes in the light The electromagnetic spectrum Emission spectra Absorption spectra Light-years Light pollution
  The sky of the first inhabitants A contemporary vison of the Universe Astronomy for everyone
  Cartoon
  Links
  Glossary
  Observatories
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;测试数据 &lt;code&gt;newstest2013.en&lt;/code&gt; 和 &lt;code&gt;newstest2013.fr&lt;/code&gt; 如下所示：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  newstest2013.en :
  A Republican strategy to counter the re-election of Obama
  Republican leaders justified their policy by the need to combat electoral fraud.
  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.

  newstest2013.fr :
  Une strat&amp;eacute;gie r&amp;eacute;publicaine pour contrer la r&amp;eacute;&amp;eacute;lection d'Obama
  Les dirigeants r&amp;eacute;publicains justifi&amp;egrave;rent leur politique par la n&amp;eacute;cessit&amp;eacute; de lutter contre la fraude &amp;eacute;lectorale.
  Or, le Centre Brennan consid&amp;egrave;re cette derni&amp;egrave;re comme un mythe, affirmant que la fraude &amp;eacute;lectorale est plus rare aux &amp;Eacute;tats-Unis que le nombre de personnes tu&amp;eacute;es par la foudre.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;下载完数据之后，开始创建词汇表文件。
从训练数据 &lt;code&gt;giga-fren.release2.fr&lt;/code&gt; 和 &lt;code&gt;giga-fren.release2.en&lt;/code&gt;创建 &lt;code&gt;vocab40000.fr&lt;/code&gt; 和 &lt;code&gt;vocab40000.en&lt;/code&gt; 这个过程需要较长一段时间，数字 &lt;code&gt;40000&lt;/code&gt; 代表了词汇库的大小。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vocab40000.fr&lt;/code&gt; (381KB) 按下列所示地按每行一个单词的方式存储（one-item-per-line）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  _PAD
  _GO
  _EOS
  _UNK
  de
  ,
  .
  '
  la
  et
  des
  les
  &amp;agrave;
  le
  du
  l
  en
  )
  d
  0
  (
  00
  pour
  dans
  un
  que
  une
  sur
  au
  0000
  a
  par
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;vocab40000.en&lt;/code&gt; (344KB) 也是如此。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  _PAD
  _GO
  _EOS
  _UNK
  the
  .
  ,
  of
  and
  to
  in
  a
  )
  (
  0
  for
  00
  that
  is
  on
  The
  0000
  be
  by
  with
  or
  :
  as
  "
  000
  are
  ;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;接着我们开始创建英文和法文的数字化（ID）训练集和测试集。这也要较长一段时间。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Tokenize data
  Tokenizing data in wmt/giga-fren.release2.fr  &amp;lt;-- Training data of French
    tokenizing line 100000
    tokenizing line 200000
    tokenizing line 300000
    tokenizing line 400000
    ...
    tokenizing line 22500000
  Tokenizing data in wmt/giga-fren.release2.en  &amp;lt;-- Training data of English
    tokenizing line 100000
    tokenizing line 200000
    tokenizing line 300000
    tokenizing line 400000
    ...
    tokenizing line 22500000
  Tokenizing data in wmt/newstest2013.fr        &amp;lt;-- Testing data of French
  Tokenizing data in wmt/newstest2013.en        &amp;lt;-- Testing data of English
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，我们所有的文件如下所示：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  wmt/training-giga-fren.tar  &amp;lt;-- 英文－法文训练集 (2.6GB)
                                  giga-fren.release2.* 从该文件解压出来
  wmt/dev-v2.tgz              &amp;lt;-- 多种语言的测试集 (21.4MB)
                                  newstest2013.* 从该文件解压出来

  wmt/giga-fren.release2.fr   &amp;lt;-- 法文训练集 (4.57GB)
  wmt/giga-fren.release2.en   &amp;lt;-- 英文训练集 (3.79GB)

  wmt/newstest2013.fr         &amp;lt;-- 法文测试集 (393KB)
  wmt/newstest2013.en         &amp;lt;-- 英文测试集 (333KB)

  wmt/vocab40000.fr           &amp;lt;-- 法文词汇表 (381KB)
  wmt/vocab40000.en           &amp;lt;-- 英文词汇表 (344KB)

  wmt/giga-fren.release2.ids40000.fr   &amp;lt;-- 数字化法文训练集 (2.81GB)
  wmt/giga-fren.release2.ids40000.en   &amp;lt;-- 数字化英文训练集 (2.38GB)

  wmt/newstest2013.ids40000.fr         &amp;lt;-- 数字化法文训练集 (268KB)
  wmt/newstest2013.ids40000.en         &amp;lt;-- 数字化英文测试集 (232KB)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;现在，把数字化的数据读入buckets中，并计算不同buckets中数据样本的个数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Read development (test) data into buckets
  dev data: (5, 10) [[13388, 4, 949], [23113, 8, 910, 2]]
  en word_ids: [13388, 4, 949]
  en context: [b'Preventing', b'the', b'disease']
  fr word_ids: [23113, 8, 910, 2]
  fr context: [b'Pr\xc3\xa9venir', b'la', b'maladie', b'_EOS']

  Read training data into buckets (limit: 0)
    reading data line 100000
    reading data line 200000
    reading data line 300000
    reading data line 400000
    reading data line 500000
    reading data line 600000
    reading data line 700000
    reading data line 800000
    ...
    reading data line 22400000
    reading data line 22500000
  train_bucket_sizes: [239121, 1344322, 5239557, 10445326]
  train_total_size: 17268326.0
  train_buckets_scale: [0.013847375825543252, 0.09169638099257565, 0.3951164693091849, 1.0]
  train data: (5, 10) [[1368, 3344], [1089, 14, 261, 2]]
  en word_ids: [1368, 3344]
  en context: [b'Site', b'map']
  fr word_ids: [1089, 14, 261, 2]
  fr context: [b'Plan', b'du', b'site', b'_EOS']

  the num of training data in each buckets: [239121, 1344322, 5239557, 10445326]
  the num of training data: 17268326
  train_buckets_scale: [0.013847375825543252, 0.09169638099257565, 0.3951164693091849, 1.0]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后开始训练模型，当 &lt;code&gt;steps_per_checkpoint = 10&lt;/code&gt; 时，您将看到：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;steps_per_checkpoint = 10&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Create Embedding Attention Seq2seq Model

  global step 10 learning rate 0.5000 step-time 22.26 perplexity 12761.50
    eval: bucket 0 perplexity 5887.75
    eval: bucket 1 perplexity 3891.96
    eval: bucket 2 perplexity 3748.77
    eval: bucket 3 perplexity 4940.10
  global step 20 learning rate 0.5000 step-time 20.38 perplexity 28761.36
    eval: bucket 0 perplexity 10137.01
    eval: bucket 1 perplexity 12809.90
    eval: bucket 2 perplexity 15758.65
    eval: bucket 3 perplexity 26760.93
  global step 30 learning rate 0.5000 step-time 20.64 perplexity 6372.95
    eval: bucket 0 perplexity 1789.80
    eval: bucket 1 perplexity 1690.00
    eval: bucket 2 perplexity 2190.18
    eval: bucket 3 perplexity 3808.12
  global step 40 learning rate 0.5000 step-time 16.10 perplexity 3418.93
    eval: bucket 0 perplexity 4778.76
    eval: bucket 1 perplexity 3698.90
    eval: bucket 2 perplexity 3902.37
    eval: bucket 3 perplexity 22612.44
  global step 50 learning rate 0.5000 step-time 14.84 perplexity 1811.02
    eval: bucket 0 perplexity 644.72
    eval: bucket 1 perplexity 759.16
    eval: bucket 2 perplexity 984.18
    eval: bucket 3 perplexity 1585.68
  global step 60 learning rate 0.5000 step-time 19.76 perplexity 1580.55
    eval: bucket 0 perplexity 1724.84
    eval: bucket 1 perplexity 2292.24
    eval: bucket 2 perplexity 2698.52
    eval: bucket 3 perplexity 3189.30
  global step 70 learning rate 0.5000 step-time 17.16 perplexity 1250.57
    eval: bucket 0 perplexity 298.55
    eval: bucket 1 perplexity 502.04
    eval: bucket 2 perplexity 645.44
    eval: bucket 3 perplexity 604.29
  global step 80 learning rate 0.5000 step-time 18.50 perplexity 793.90
    eval: bucket 0 perplexity 2056.23
    eval: bucket 1 perplexity 1344.26
    eval: bucket 2 perplexity 767.82
    eval: bucket 3 perplexity 649.38
  global step 90 learning rate 0.5000 step-time 12.61 perplexity 541.57
    eval: bucket 0 perplexity 180.86
    eval: bucket 1 perplexity 350.99
    eval: bucket 2 perplexity 326.85
    eval: bucket 3 perplexity 383.22
  global step 100 learning rate 0.5000 step-time 18.42 perplexity 471.12
    eval: bucket 0 perplexity 216.63
    eval: bucket 1 perplexity 348.96
    eval: bucket 2 perplexity 318.20
    eval: bucket 3 perplexity 389.92
  global step 110 learning rate 0.5000 step-time 18.39 perplexity 474.89
    eval: bucket 0 perplexity 8049.85
    eval: bucket 1 perplexity 1677.24
    eval: bucket 2 perplexity 936.98
    eval: bucket 3 perplexity 657.46
  global step 120 learning rate 0.5000 step-time 18.81 perplexity 832.11
    eval: bucket 0 perplexity 189.22
    eval: bucket 1 perplexity 360.69
    eval: bucket 2 perplexity 410.57
    eval: bucket 3 perplexity 456.40
  global step 130 learning rate 0.5000 step-time 20.34 perplexity 452.27
    eval: bucket 0 perplexity 196.93
    eval: bucket 1 perplexity 655.18
    eval: bucket 2 perplexity 860.44
    eval: bucket 3 perplexity 1062.36
  global step 140 learning rate 0.5000 step-time 21.05 perplexity 847.11
    eval: bucket 0 perplexity 391.88
    eval: bucket 1 perplexity 339.09
    eval: bucket 2 perplexity 320.08
    eval: bucket 3 perplexity 376.44
  global step 150 learning rate 0.4950 step-time 15.53 perplexity 590.03
    eval: bucket 0 perplexity 269.16
    eval: bucket 1 perplexity 286.51
    eval: bucket 2 perplexity 391.78
    eval: bucket 3 perplexity 485.23
  global step 160 learning rate 0.4950 step-time 19.36 perplexity 400.80
    eval: bucket 0 perplexity 137.00
    eval: bucket 1 perplexity 198.85
    eval: bucket 2 perplexity 276.58
    eval: bucket 3 perplexity 357.78
  global step 170 learning rate 0.4950 step-time 17.50 perplexity 541.79
    eval: bucket 0 perplexity 1051.29
    eval: bucket 1 perplexity 626.64
    eval: bucket 2 perplexity 496.32
    eval: bucket 3 perplexity 458.85
  global step 180 learning rate 0.4950 step-time 16.69 perplexity 400.65
    eval: bucket 0 perplexity 178.12
    eval: bucket 1 perplexity 299.86
    eval: bucket 2 perplexity 294.84
    eval: bucket 3 perplexity 296.46
  global step 190 learning rate 0.4950 step-time 19.93 perplexity 886.73
    eval: bucket 0 perplexity 860.60
    eval: bucket 1 perplexity 910.16
    eval: bucket 2 perplexity 909.24
    eval: bucket 3 perplexity 786.04
  global step 200 learning rate 0.4901 step-time 18.75 perplexity 449.64
    eval: bucket 0 perplexity 152.13
    eval: bucket 1 perplexity 234.41
    eval: bucket 2 perplexity 249.66
    eval: bucket 3 perplexity 285.95
  ...
  global step 980 learning rate 0.4215 step-time 18.31 perplexity 208.74
    eval: bucket 0 perplexity 78.45
    eval: bucket 1 perplexity 108.40
    eval: bucket 2 perplexity 137.83
    eval: bucket 3 perplexity 173.53
  global step 990 learning rate 0.4173 step-time 17.31 perplexity 175.05
    eval: bucket 0 perplexity 78.37
    eval: bucket 1 perplexity 119.72
    eval: bucket 2 perplexity 169.11
    eval: bucket 3 perplexity 202.89
  global step 1000 learning rate 0.4173 step-time 15.85 perplexity 174.33
    eval: bucket 0 perplexity 76.52
    eval: bucket 1 perplexity 125.97
    eval: bucket 2 perplexity 150.13
    eval: bucket 3 perplexity 181.07
  ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;经过350000轮训练模型之后，您可以将代码中的 &lt;code&gt;main_train()&lt;/code&gt; 换为 &lt;code&gt;main_decode()&lt;/code&gt; 来使用训练好的翻译器，
您输入一个英文句子，程序将输出一个对应的法文句子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Reading model parameters from wmt/translate.ckpt-350000
  &amp;gt;  Who is the president of the United States?
  Qui est le pr&amp;eacute;sident des &amp;Eacute;tats-Unis ?
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="li jie ji qi fan yi"&gt;理解机器翻译&lt;/h2&gt;
&lt;h3 id="seq2seq"&gt;Seq2seq&lt;/h3&gt;
&lt;p&gt;序列到序列模型（Seq2seq）通常被用来转换一种语言到另一种语言。
但实际上它能用来做很多您可能无法想象的事情，比如我们可以将一个长的句子翻译成意思一样但短且简单的句子，
再比如，从莎士比亚的语言翻译成现代英语。若用上卷积神经网络(CNN)的话，我们能将视频翻译成句子，则自动看一段视频给出该视频的文字描述（Video captioning）。&lt;/p&gt;
&lt;p&gt;如果你只是想用 Seq2seq，你只需要考虑训练集的格式，比如如何切分单词、如何数字化单词等等。
所以，在本教程中，我们将讨论很多如何整理训练集。&lt;/p&gt;
&lt;h4&gt;基础&lt;/h4&gt;
&lt;p&gt;序列到序列模型是一种多对多（Many to many）的模型，但与PTB教程中的同步序列输入与输出(Synced sequence input and output）不一样，Seq2seq是在输入了整个序列之后，才开始输出新的序列（非同步）。
该教程用了下列两种最新的方法来提高准确度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把输入序列倒转输入（Reversing the inputs）&lt;/li&gt;
&lt;li&gt;注意机制（Attention mechanism）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了要加快训练速度，我们使用了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;softmax 抽样（Sampled softmax）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Karpathy的博客是这样描述Seq2seq的："(4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French)."&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/basic_seq2seq.png" width="80%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;如上图所示，编码器输入（encoder input），解码器输入（decoder input）以及输出目标（targets）如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   encoder_input =  A    B    C
   decoder_input =  &amp;lt;go&amp;gt; W    X    Y    Z
   targets       =  W    X    Y    Z    &amp;lt;eos&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Note：在代码实现中，targets的长度比decoder_input的长度小一，更多实现细节将在下文说明。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;文献&lt;/h4&gt;
&lt;p&gt;该英语-法语的机器翻译例子使用了多层递归神经网络以及注意机制。
该模型和如下论文中一样：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1412.7449"&gt;Grammar as a Foreign Language&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该例子采用了 softmax 抽样（sampled softmax）来解决当词汇表很大时计算量大的问题。
在该例子中，&lt;code&gt;target_vocab_size=4000&lt;/code&gt; ，若词汇量小于 &lt;code&gt;512&lt;/code&gt; 时用普通的softmax cross entropy即可。
Softmax 抽样在这篇论文的第三小节中描述:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1412.2007"&gt;On Using Very Large Target Vocabulary for Neural Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下文章讲述了把输入序列倒转（Reversing the inputs）和多层神递归神经网络用在Seq2seq的翻译应用非常成功：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1409.3215"&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下文章讲述了注意机制（Attention Mechanism）让解码器可以更直接地得到每一个输入的信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1409.0473"&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下文章讲述了另一种Seq2seq模型，则使用双向编码器（Bi-directional encoder）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1409.0473"&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="shi xian xi jie"&gt;实现细节&lt;/h3&gt;
&lt;h4&gt;Bucketing and Padding&lt;/h4&gt;
&lt;p&gt;Bucketing 是一种能有效处理不同句子长度的方法，为什么使用Bucketing，在 &lt;a href="https://www.zhihu.com/question/42057513"&gt;知乎&lt;/a&gt;上已经有很好的回答了。&lt;/p&gt;
&lt;p&gt;当将英文翻译成法文的时，我们有不同长度的英文句子输入（长度为 &lt;code&gt;L1&lt;/code&gt; ），以及不同长度的法文句子输出，（长度为 &lt;code&gt;L2&lt;/code&gt; ）。
我们原则上要建立每一种长度的可能性，则有很多个 &lt;code&gt;(L1, L2+1)&lt;/code&gt; ，其中 &lt;code&gt;L2&lt;/code&gt; 加一是因为有 GO 标志符。&lt;/p&gt;
&lt;p&gt;为了减少 bucket 的数量以及为句子找到最合适的 bucket，若 bucket 大于句子的长度，我们则使用 PAD 标志符填充之。&lt;/p&gt;
&lt;p&gt;为了提高效率，我们只使用几个 bucket，然后使用 padding 来让句子匹配到最相近的 bucket 中。
在该例子中，我们使用如下 4 个 buckets。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果输入的是一个有 &lt;code&gt;3&lt;/code&gt; 个单词的英文句子，对应的法文输出有 &lt;code&gt;6&lt;/code&gt; 个单词，
那么改数据将被放在第一个 bucket 中并且把 encoder inputs 和 decoder inputs 通过 padding 来让其长度变成 &lt;code&gt;5&lt;/code&gt; 和 &lt;code&gt;10&lt;/code&gt; 。
如果我们有 &lt;code&gt;8&lt;/code&gt; 个单词的英文句子，及 &lt;code&gt;18&lt;/code&gt; 个单词的法文句子，它们会被放到 &lt;code&gt;(20, 25)&lt;/code&gt; 的 bucket 中。&lt;/p&gt;
&lt;p&gt;换句话说，bucket &lt;code&gt;(I,O)&lt;/code&gt; 是 (encoder_input_size，decoder_inputs_size) 。&lt;/p&gt;
&lt;p&gt;给出一对数字化训练样本 &lt;code&gt;[["I", "go", "."], ["Je", "vais", "."]]&lt;/code&gt; ，我们把它转换为 &lt;code&gt;(5,10)&lt;/code&gt; 。
编码器输入（encoder inputs）的训练数据为  &lt;code&gt;[PAD PAD "." "go" "I"]&lt;/code&gt; ，而解码器的输入（decoder inputs）为 &lt;code&gt;[GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]&lt;/code&gt; 。
而输出目标（targets）是解码器输入（decoder inputs）平移一位。 &lt;code&gt;target_weights&lt;/code&gt; 是输出目标（targets）的掩码。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  bucket = (I, O) = (5, 10)
  encoder_inputs = [PAD PAD "." "go" "I"]                       &amp;lt;-- 5  x batch_size
  decoder_inputs = [GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD] &amp;lt;-- 10 x batch_size
  target_weights = [1   1     1     1   0 0 0 0 0 0 0]          &amp;lt;-- 10 x batch_size
  targets        = ["Je" "vais" "." EOS PAD PAD PAD PAD PAD]    &amp;lt;-- 9  x batch_size
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在该代码中，一个句子是由一个列向量表示，假设 &lt;code&gt;batch_size = 3&lt;/code&gt; ， &lt;code&gt;bucket = (5, 10)&lt;/code&gt; ，训练集如下所示。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  encoder_inputs    decoder_inputs    target_weights    targets
  0    0    0       1    1    1       1    1    1       87   71   16748
  0    0    0       87   71   16748   1    1    1       2    3    14195
  0    0    0       2    3    14195   0    1    1       0    2    2
  0    0    3233    0    2    2       0    0    0       0    0    0
  3    698  4061    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中 0 : _PAD    1 : _GO     2 : _EOS      3 : _UNK&lt;/p&gt;
&lt;p&gt;在训练过程中，解码器输入是目标，而在预测过程中，下一个解码器的输入是最后一个解码器的输出。&lt;/p&gt;
&lt;p&gt;在训练过程中，编码器输入（decoder inputs）就是目标输出（targets）；
当使用模型时，下一个编码器输入（decoder inputs）是上一个解码器输出（ decoder output）。&lt;/p&gt;
&lt;h4&gt;特殊标志符、标点符号与阿拉伯数字&lt;/h4&gt;
&lt;p&gt;该例子中的特殊标志符是：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="n"&gt;_PAD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;"_PAD"&lt;/span&gt;
  &lt;span class="n"&gt;_GO&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;"_GO"&lt;/span&gt;
  &lt;span class="n"&gt;_EOS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;"_EOS"&lt;/span&gt;
  &lt;span class="n"&gt;_UNK&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;"_UNK"&lt;/span&gt;
  &lt;span class="n"&gt;PAD_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;      &lt;span class="o"&gt;&amp;lt;--&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="n"&gt;number&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;
  &lt;span class="n"&gt;GO_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
  &lt;span class="n"&gt;EOS_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="n"&gt;UNK_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
  &lt;span class="n"&gt;_START_VOCAB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;_PAD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_GO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_EOS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_UNK&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;          ID号    意义
  _PAD    0       Padding, empty word
  _GO     1       decoder_inputs 的第一个元素
  _EOS    2       targets 的结束符
  _UNK    3       不明单词（Unknown word），没有在词汇表出现的单词被标记为3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;对于阿拉伯数字，建立词汇表时与数字化数据集时的 &lt;code&gt;normalize_digits&lt;/code&gt; 必须是一致的，若
&lt;code&gt;normalize_digits=True&lt;/code&gt; 所有阿拉伯数字都将被 &lt;code&gt;0&lt;/code&gt; 代替。比如 &lt;code&gt;123&lt;/code&gt; 被 &lt;code&gt;000&lt;/code&gt; 代替，&lt;code&gt;9&lt;/code&gt; 被 &lt;code&gt;0&lt;/code&gt;代替
，&lt;code&gt;1990-05&lt;/code&gt; 被 &lt;code&gt;0000-00` 代替，最后&lt;/code&gt;000&lt;code&gt;，&lt;/code&gt;0&lt;code&gt;，&lt;/code&gt;0000-00&lt;code&gt;等将在词汇库中(看&lt;/code&gt;vocab40000.en`` )。&lt;/p&gt;
&lt;p&gt;反之，如果 &lt;code&gt;normalize_digits=False&lt;/code&gt; ，不同的阿拉伯数字将会放入词汇表中，那么词汇表就变得十分大了。
本例子中寻找阿拉伯数字使用的正则表达式是 &lt;code&gt;_DIGIT_RE = re.compile(br"\d")&lt;/code&gt; 。(详见 &lt;code&gt;tl.nlp.create_vocabulary()&lt;/code&gt; 和 &lt;code&gt;`tl.nlp.data_to_token_ids()&lt;/code&gt; )&lt;/p&gt;
&lt;p&gt;对于分离句子成独立单词，本例子使用正则表达式 &lt;code&gt;_WORD_SPLIT = re.compile(b"([.,!?\"':;)(])")&lt;/code&gt; ，
这意味着使用这几个标点符号 &lt;code&gt;[ . , ! ? " ' : ; ) ( ]&lt;/code&gt; 以及空格来分割句子，详情请看 &lt;code&gt;tl.nlp.basic_tokenizer()&lt;/code&gt; 。这个分割方法是 &lt;code&gt;tl.nlp.create_vocabulary()&lt;/code&gt; 和  &lt;code&gt;tl.nlp.data_to_token_ids()&lt;/code&gt; 的默认方法。&lt;/p&gt;
&lt;p&gt;所有的标点符号，比如 &lt;code&gt;. , ) (&lt;/code&gt; 在英文和法文数据库中都会被全部保留下来。&lt;/p&gt;
&lt;h4&gt;Softmax 抽样 (Sampled softmax)&lt;/h4&gt;
&lt;p&gt;softmax抽样是一种词汇表很大（Softmax 输出很多）的时候用来降低损失（cost）计算量的方法。
与从所有输出中计算 cross-entropy 相比，这个方法只从 &lt;code&gt;num_samples&lt;/code&gt; 个输出中计算 cross-entropy。&lt;/p&gt;
&lt;h4&gt;损失和更新函数&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;EmbeddingAttentionSeq2seqWrapper&lt;/code&gt; 内部实现了 SGD optimizer。&lt;/p&gt;
&lt;h3 id="xia yi bu ?_2"&gt;下一步？&lt;/h3&gt;
&lt;p&gt;您可以尝试其他应用。&lt;/p&gt;
&lt;h2 id="fan yi dui zhao_1"&gt;翻译对照&lt;/h2&gt;
&lt;p&gt;Stacked Denosing Autoencoder 堆栈式降噪自编码器&lt;/p&gt;
&lt;p&gt;Word Embedding               词嵌套、词嵌入&lt;/p&gt;
&lt;p&gt;Iteration                    迭代&lt;/p&gt;
&lt;p&gt;Natural Language Processing  自然语言处理&lt;/p&gt;
&lt;p&gt;Sparse                       稀疏的&lt;/p&gt;
&lt;p&gt;Cost function                损失函数&lt;/p&gt;
&lt;p&gt;Regularization               规则化、正则化&lt;/p&gt;
&lt;p&gt;Tokenization                 数字化&lt;/p&gt;
&lt;p&gt;Truncated backpropagation    截断反向传播&lt;/p&gt;
&lt;h2 id="geng duo xin xi"&gt;更多信息&lt;/h2&gt;
&lt;p&gt;TensorLayer 还能做什么？请继续阅读本文档。&lt;/p&gt;
&lt;p&gt;最后，API 参考列表和说明如下：&lt;/p&gt;
&lt;p&gt;layers (&lt;code&gt;tensorlayer.layers&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;activation (&lt;code&gt;tensorlayer.activation&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;natural language processing (&lt;code&gt;tensorlayer.nlp&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;reinforcement learning (&lt;code&gt;tensorlayer.rein&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;cost expressions and regularizers (&lt;code&gt;tensorlayer.cost&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;load and save files (&lt;code&gt;tensorlayer.files&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;operating system (&lt;code&gt;tensorlayer.ops&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;helper functions (&lt;code&gt;tensorlayer.utils&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;visualization (&lt;code&gt;tensorlayer.visualize&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;iteration functions (&lt;code&gt;tensorlayer.iterate&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;preprocessing functions (&lt;code&gt;tensorlayer.prepro&lt;/code&gt;),&lt;/p&gt;</content></entry><entry><title>TensorFlow 的 MNIST 教程</title><link href="https://freeopen.github.io/posts/tensorflow-de-mnist-jiao-cheng" rel="alternate"></link><published>2017-08-02T00:00:00+08:00</published><updated>2017-08-02T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2017-08-02:/posts/tensorflow-de-mnist-jiao-cheng</id><summary type="html">&lt;p&gt;&lt;a href="https://www.tensorflow.org/get_started/mnist/pros"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这篇文章对初学tensorflow的朋友来说，有很好的参考作用。曾经在网上看过本教程的翻译稿，但版本偏老。本文翻译时，tensorflow的版本为1.3.0-rc1. 新版相较于老版，示例代码有变化，文字说明也有增补。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。
在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。&lt;/p&gt;
&lt;p&gt;这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看新手指南.&lt;/p&gt;
&lt;h2 id="guan yu ben jiao cheng"&gt;关于本教程&lt;/h2&gt;
&lt;p&gt;本教程第一部分讲解 &lt;a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_softmax.py"&gt;mnist_softmax.py&lt;/a&gt; 代码, 一个简单的tensorflow模型实现。第二部分介绍一些提高精度的方法。&lt;/p&gt;
&lt;p&gt;你可以从本教程拷贝和粘贴代码块到你的python环境，或者下载完整代码 &lt;a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_deep.py"&gt;mnist_deep.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;我们将完成如下目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于图片像素，创建一个softmax回归函数的模型来识别MNIST数字。&lt;/li&gt;
&lt;li&gt;用tensorflow训练模型识别数字。&lt;/li&gt;
&lt;li&gt;用测试数据检查模型精度&lt;/li&gt;
&lt;li&gt;构建、训练和测试一个多层卷积神经网络来提升模型精度。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="an zhuang"&gt;安装&lt;/h2&gt;
&lt;p&gt;在创建模型之前，我们会先加载MNIST数据集，然后启动一个TensorFlow的session。&lt;/p&gt;
&lt;h3 id="jia zai mnistshu ju"&gt;加载MNIST数据&lt;/h3&gt;
&lt;p&gt;为了方便起见，我们已经准备了一个脚本来自动下载和导入MNIST数据集。它会自动创建一个'MNIST_data'的目录来存储数据。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://www.tensorflow.org/get_started/mnist/pros"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这篇文章对初学tensorflow的朋友来说，有很好的参考作用。曾经在网上看过本教程的翻译稿，但版本偏老。本文翻译时，tensorflow的版本为1.3.0-rc1. 新版相较于老版，示例代码有变化，文字说明也有增补。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。
在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。&lt;/p&gt;
&lt;p&gt;这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看新手指南.&lt;/p&gt;
&lt;h2 id="guan yu ben jiao cheng"&gt;关于本教程&lt;/h2&gt;
&lt;p&gt;本教程第一部分讲解 &lt;a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_softmax.py"&gt;mnist_softmax.py&lt;/a&gt; 代码, 一个简单的tensorflow模型实现。第二部分介绍一些提高精度的方法。&lt;/p&gt;
&lt;p&gt;你可以从本教程拷贝和粘贴代码块到你的python环境，或者下载完整代码 &lt;a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_deep.py"&gt;mnist_deep.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;我们将完成如下目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于图片像素，创建一个softmax回归函数的模型来识别MNIST数字。&lt;/li&gt;
&lt;li&gt;用tensorflow训练模型识别数字。&lt;/li&gt;
&lt;li&gt;用测试数据检查模型精度&lt;/li&gt;
&lt;li&gt;构建、训练和测试一个多层卷积神经网络来提升模型精度。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="an zhuang"&gt;安装&lt;/h2&gt;
&lt;p&gt;在创建模型之前，我们会先加载MNIST数据集，然后启动一个TensorFlow的session。&lt;/p&gt;
&lt;h3 id="jia zai mnistshu ju"&gt;加载MNIST数据&lt;/h3&gt;
&lt;p&gt;为了方便起见，我们已经准备了一个脚本来自动下载和导入MNIST数据集。它会自动创建一个'MNIST_data'的目录来存储数据。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'MNIST_data'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里，&lt;code&gt;mnist&lt;/code&gt;是一个轻量级的类。它以Numpy数组的形式存储着训练、校验和测试数据集。同时提供了一个函数，用于在迭代中获得minibatch，后面我们将会用到。&lt;/p&gt;
&lt;h3 id="yun xing tensorflowde interactivesession"&gt;运行TensorFlow的InteractiveSession&lt;/h3&gt;
&lt;p&gt;Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。&lt;/p&gt;
&lt;p&gt;这里，我们使用更加方便的InteractiveSession类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些计算图，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用InteractiveSession，那么你需要在启动session之前构建整个计算图，然后启动该计算图。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveSession&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="ji suan tu"&gt;计算图&lt;/h3&gt;
&lt;p&gt;为了在Python中进行高效的数值计算，我们通常会使用像NumPy一类的库，将一些诸如矩阵乘法的耗时操作在Python环境的外部来计算，这些计算通常会通过其它语言并用更为高效的代码来实现。&lt;/p&gt;
&lt;p&gt;但遗憾的是，每一个操作切换回Python环境时仍需要不小的开销。如果你想在GPU或者分布式环境中计算时，这一开销更加可怖，这一开销主要可能是用来进行数据迁移。&lt;/p&gt;
&lt;p&gt;TensorFlow也是在Python外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在Python外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在Python外部。这与Theano或Torch的做法类似。&lt;/p&gt;
&lt;p&gt;因此Python代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。详情请查看基本用法中的计算图表一节。&lt;/p&gt;
&lt;h2 id="gou jian  softmax hui gui mo xing_1"&gt;构建 Softmax 回归模型&lt;/h2&gt;
&lt;p&gt;在这一节中我们将建立一个拥有一个线性层的softmax回归模型。在下一节，我们会将其扩展为一个拥有多层卷积网络的softmax回归模型。&lt;/p&gt;
&lt;h3 id="zhan wei fu (placeholders)"&gt;占位符(placeholders)&lt;/h3&gt;
&lt;p&gt;我们通过为输入图像和目标输出类别创建节点，来构建计算图。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里的&lt;code&gt;x&lt;/code&gt;和&lt;code&gt;y_&lt;/code&gt;并不是特定的值，相反，他们都只是一个&lt;code&gt;占位符&lt;/code&gt;，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。&lt;/p&gt;
&lt;p&gt;输入图片&lt;code&gt;x&lt;/code&gt;是一个2维的浮点数张量。这里，分配给它的shape为[None, 784]，其中784是一张展平的MNIST图片的维度。None表示其值大小不定，在这里作为第一个维度值，用以指代batch的大小，意即x的数量不定。输出类别值y_也是一个2维张量，其中每一行为一个10维的one-hot向量,用于代表对应某一MNIST图片的类别。&lt;/p&gt;
&lt;p&gt;虽然placeholder的shape参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。&lt;/p&gt;
&lt;h3 id="bian liang"&gt;变量&lt;/h3&gt;
&lt;p&gt;我们现在为模型定义权重W和偏置b。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：变量。一个变量代表着TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用Variable来表示。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;我们在调用&lt;code&gt;tf.Variable&lt;/code&gt;的时候传入初始值。
在这个例子里，我们把&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;都初始化为零向量。&lt;code&gt;W&lt;/code&gt;是一个784x10的矩阵（因为我们有784个特征和10个输出值）。&lt;code&gt;b&lt;/code&gt;是一个10维的向量（因为我们有10个分类）。&lt;/p&gt;
&lt;p&gt;变量需要通过session初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个变量,可以一次性为所有变量完成此操作。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_all_variables&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="lei bie yu ce yu sun shi han shu_1"&gt;类别预测与损失函数&lt;/h2&gt;
&lt;p&gt;现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片&lt;code&gt;x&lt;/code&gt;和权重矩阵&lt;code&gt;W&lt;/code&gt;相乘，加上偏置&lt;code&gt;b&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以很容易的为训练过程指定最小化误差用的损失函数，损失表示在一个样本上模型预测有多差; 我们试图在所有的样本上最小化这个损失。这里， 我们的损失函数是目标类别和softmax激活函数之间的交叉熵。在教程开始，我们使用如下公式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，&lt;code&gt;tf.nn.softmax_cross_entropy_with_logits&lt;/code&gt;在内部用softmax规范化模型，
并对所有分类求和，而&lt;code&gt;tf.reduce_mean&lt;/code&gt;对这些和求平均 。&lt;/p&gt;
&lt;h2 id="xun lian mo xing"&gt;训练模型&lt;/h2&gt;
&lt;p&gt;我们已经定义好模型和训练用的损失函数，那么用TensorFlow进行训练就很简单了。因为TensorFlow知道整个计算图，它可以使用自动微分法找到对于各个变量的损失的梯度值。TensorFlow有大量内置的优化算法 这个例子中，我们用最速下降法让交叉熵下降，步长为0.5.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。&lt;/p&gt;
&lt;p&gt;返回的&lt;code&gt;train_step&lt;/code&gt;操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行&lt;code&gt;train_step&lt;/code&gt;来完成。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;每一步迭代，我们都会加载100个训练样本，然后执行一次&lt;code&gt;train_step&lt;/code&gt;，并通过&lt;code&gt;feed_dict&lt;/code&gt;将&lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y_&lt;/code&gt; 张量占位符用训练训练数据替代。
注意，在计算图中，你可以用&lt;code&gt;feed_dict&lt;/code&gt;来替代任何张量，并不仅限于替换占位符。&lt;/p&gt;
&lt;h3 id="ping gu mo xing"&gt;评估模型&lt;/h3&gt;
&lt;p&gt;那么我们的模型性能如何呢？&lt;/p&gt;
&lt;p&gt;首先让我们找出那些预测正确的标签。&lt;code&gt;tf.argmax&lt;/code&gt; 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如&lt;code&gt;tf.argmax(y,1)&lt;/code&gt;返回的是模型对于任一输入&lt;code&gt;x&lt;/code&gt;预测到的标签值，而&lt;code&gt;tf.argmax(y_,1)&lt;/code&gt; 代表正确的标签，我们可以用 &lt;code&gt;tf.equal&lt;/code&gt; 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：[True, False, True, True]变为[1,0,1,1]，计算出平均值为0.75。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，我们可以计算出在测试数据上的准确率，大概是92%。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="gou jian duo ceng juan ji wang luo_1"&gt;构建多层卷积网络&lt;/h2&gt;
&lt;p&gt;在MNIST上只有92% 的准确率，实在太糟糕。在这节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。&lt;/p&gt;
&lt;h3 id="quan zhong chu shi hua"&gt;权重初始化&lt;/h3&gt;
&lt;p&gt;为了创建这个模型，我们需要创建大量的权重和偏置项。
这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。
由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;initial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;initial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="juan ji he chi hua"&gt;卷积和池化&lt;/h3&gt;
&lt;p align="center"&gt;
&lt;img src="/images/convgaus.gif"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：卷积，从数学角度说，指计算一个函数通过另一个函数时，有多少重叠的积分。也可以视为通过相乘的方式将两个函数进行混合。详见上图,
红色曲线（函数f）下的区域是积分，蓝色曲线(函数g)从左向右缓慢移动，绿色曲线下的区域是红色曲线和蓝色曲线的卷积，灰色阴影表示在绿色垂直线位置时，红色曲线和蓝色曲线的卷积, 其值为f(a)*g(x-a)。 &lt;br&gt; &lt;br/&gt;
下面再给个我非常喜欢的说明，帮助理解这个公式，写得非常有意思。  &lt;br&gt; &lt;br/&gt;
比如说你的老板命令你干活，你却到楼下打台球去了，后来被老板发现，他非常气愤，扇了你一巴掌（注意，这就是输入信号，脉冲），于是你的脸上会渐渐地（贱贱地）鼓起来一个包，你的脸就是一个系统，而鼓起来的包就是你的脸对巴掌的响应，好，这样就和信号系统建立起来意义对应的联系。下面还需要一些假设来保证论证的严谨：假定你的脸是线性时不变系统，也就是说，无论什么时候老板打你一巴掌，打在你脸的同一位置（这似乎要求你的脸足够光滑，如果你说你长了很多青春痘，甚至整个脸皮处处连续处处不可导，那难度太大了，我就无话可说了哈哈），你的脸上总是会在相同的时间间隔内鼓起来一个相同高度的包来，并且假定以鼓起来的包的大小作为系统输出。好了，那么，下面可以进入核心内容&amp;mdash;&amp;mdash;卷积了！  &lt;br&gt; &lt;br/&gt;
如果你每天都到楼下去打台球，那么老板每天都要扇你一巴掌，不过当老板打你一巴掌后，你5分钟就消肿了，所以时间长了，你甚至就适应这种生活了&amp;hellip;&amp;hellip;如果有一天，老板忍无可忍，以0.5秒的间隔开始不间断的扇你的过程，这样问题就来了，第一次扇你鼓起来的包还没消肿，第二个巴掌就来了，你脸上的包就可能鼓起来两倍高，老板不断扇你，脉冲不断作用在你脸上，效果不断叠加了，这样这些效果就可以求和了，结果就是你脸上的包的高度随时间变化的一个函数了（注意理解）；如果老板再狠一点，频率越来越高，以至于你都辨别不清时间间隔了，那么，求和就变成积分了。可以这样理解，在这个过程中的某一固定的时刻，你的脸上的包的鼓起程度和什么有关呢？和之前每次打你都有关！但是各次的贡献是不一样的，越早打的巴掌，贡献越小，所以这就是说，某一时刻的输出是之前很多次输入乘以各自的衰减系数之后的叠加而形成某一点的输出，然后再把不同时刻的输出点放在一起，形成一个函数，这就是卷积，卷积之后的函数就是你脸上的包的大小随时间变化的函数。本来你的包几分钟就可以消肿，可是如果连续打，几个小时也消不了肿了，这难道不是一种平滑过程么？反映到剑桥大学的公式上，f(a)就是第a个巴掌，g(x-a)就是第a个巴掌在x时刻的作用程度，乘起来再叠加就ok了，大家说是不是这个道理呢？&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？
在这个实例里，我们会一直使用普通版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。
我们的池化用简单传统的2x2大小的模板做最大池化。为了代码更简洁，我们把这部分抽象成一个函数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                        &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="di yi ceng juan ji"&gt;第一层卷积&lt;/h3&gt;
&lt;p&gt;现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为了用这一层，我们把&lt;code&gt;x&lt;/code&gt;变成一个4维向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，
如果是rgb彩色图，则为3)。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这里&lt;code&gt;x&lt;/code&gt;是2维矩阵（m, 784), 其中m表示样本数量，所以下面的reshape中的 &lt;code&gt;-1&lt;/code&gt;的位置实质表示的是m值，此处的&lt;code&gt;-1&lt;/code&gt;可以推断出m值，在具体执行时，将用m值替换.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;我们把&lt;code&gt;x_image&lt;/code&gt;和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行最大池化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;h_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_pool1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="di er ceng juan ji"&gt;第二层卷积&lt;/h3&gt;
&lt;p&gt;为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;h_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="quan lian jie ceng"&gt;全连接层&lt;/h3&gt;
&lt;p&gt;现在，图片尺寸缩小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;h_pool2_flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;h_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool2_flat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_fc1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_fc1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="dropout"&gt;Dropout&lt;/h3&gt;
&lt;p&gt;为了减少过拟合，我们在输出层之前加入&lt;a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"&gt;dropout&lt;/a&gt;。我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。
这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 
TensorFlow的&lt;code&gt;tf.nn.dropout&lt;/code&gt;操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;keep_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_fc1_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="shu chu ceng"&gt;输出层&lt;/h3&gt;
&lt;p&gt;最后，我们添加一个softmax层，就像前面的单层 softmax 回归一样。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W_fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;y_conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_fc1_drop&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_fc2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_fc2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="xun lian he ping gu mo xing"&gt;训练和评估模型&lt;/h3&gt;
&lt;p&gt;这个模型的效果如何呢？&lt;/p&gt;
&lt;p&gt;为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，
这个模型和上个模型的不同之处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只是我们会用更加复杂的&lt;a href="http://arxiv.org/abs/1412.6980"&gt;ADAM优化器&lt;/a&gt;来做梯度最速下降，&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;feed_dict&lt;/code&gt;中加入额外的参数&lt;code&gt;keep_prob&lt;/code&gt;来控制dropout比例。&lt;/li&gt;
&lt;li&gt;然后每100次迭代输出一次日志。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们使用了 &lt;code&gt;tf.Session&lt;/code&gt; 而不是 &lt;code&gt;tf.InteractiveSession&lt;/code&gt;. 将创建图(模型规格)和评估图(模型匹配)的处理分开, 这样便产生更清晰的代码。tf.Session被with代码块创建，以至于一旦块退出时它将自动销毁。&lt;/p&gt;
&lt;p&gt;随时运行此代码。请注意, 它会进行2万次训练迭代, 并且需要一段时间 (可能长达半小时), 这取决于你的处理器。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_conv&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;correct_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_conv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;train_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'step &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;, training accuracy &lt;/span&gt;&lt;span class="si"&gt;%g&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_accuracy&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'test accuracy &lt;/span&gt;&lt;span class="si"&gt;%g&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;以上代码，在最终测试集上的准确率大概是99.2%。&lt;/p&gt;
&lt;p&gt;目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。&lt;/p&gt;
&lt;p&gt;如果你想用力鼓励一下，欢迎&lt;a href="../../pay.html"&gt;打赏&lt;/a&gt;，官方建议零售价 &amp;yen;2 。&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;在小型卷积网络中，有没有dropout对性能影响不大。Dropout 通常可以降低过拟合，它常常被用于大型的神经网络中。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content></entry><entry><title>Neural Machine Translation (seq2seq) 教程</title><link href="https://freeopen.github.io/posts/neural-machine-translation-seq2seq-jiao-cheng" rel="alternate"></link><published>2017-07-18T00:00:00+08:00</published><updated>2017-07-18T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2017-07-18:/posts/neural-machine-translation-seq2seq-jiao-cheng</id><summary type="html">&lt;p&gt;2018-01-31 第一次修订&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/nmt"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="jie shao"&gt;介绍&lt;/h2&gt;
&lt;p&gt;序列到序列(seq2seq)模型 (&lt;a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"&gt;Sutskever et al., 2014&lt;/a&gt;, &lt;a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf"&gt;Cho et al., 2014&lt;/a&gt;) 在诸如机器翻译、语音识别和文本概括等任务中取得了巨大成功. 本教程为读者提供对 seq2seq 模型的全面理解，并展示如何从头构建一个有竞争力的 seq2seq 模型. 我们专注于神经机器翻译（NMT）任务，这是一个很好的、已获得广泛&lt;a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;成功&lt;/a&gt;的 seq2seq 模型的试验台. 所含的代码轻量、高质、实用，并整合了最新的研究思路。我们通过以下方式达成此目标 :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用最新的 解码器 / attention wrapper &lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops"&gt;API&lt;/a&gt;, TensorFlow 1.2 数据迭代器&lt;/li&gt;
&lt;li&gt;结合我们在建立循环神经网络和序列到序列模型方面的强大专长&lt;/li&gt;
&lt;li&gt;提供一些巧思来构建最好的 NMT 模型 …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;2018-01-31 第一次修订&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/nmt"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="jie shao"&gt;介绍&lt;/h2&gt;
&lt;p&gt;序列到序列(seq2seq)模型 (&lt;a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"&gt;Sutskever et al., 2014&lt;/a&gt;, &lt;a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf"&gt;Cho et al., 2014&lt;/a&gt;) 在诸如机器翻译、语音识别和文本概括等任务中取得了巨大成功. 本教程为读者提供对 seq2seq 模型的全面理解，并展示如何从头构建一个有竞争力的 seq2seq 模型. 我们专注于神经机器翻译（NMT）任务，这是一个很好的、已获得广泛&lt;a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;成功&lt;/a&gt;的 seq2seq 模型的试验台. 所含的代码轻量、高质、实用，并整合了最新的研究思路。我们通过以下方式达成此目标 :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用最新的 解码器 / attention wrapper &lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops"&gt;API&lt;/a&gt;, TensorFlow 1.2 数据迭代器&lt;/li&gt;
&lt;li&gt;结合我们在建立循环神经网络和序列到序列模型方面的强大专长&lt;/li&gt;
&lt;li&gt;提供一些巧思来构建最好的 NMT 模型，并复制一个谷歌神经机器翻译系统 &lt;a href="https://research.google.com/pubs/pub45610.html"&gt;Google&amp;rsquo;s NMT (GNMT) system&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们认为，重要的是提供人们可以轻松复制的基准. 因此，我们提供了完整的实验结果，并对以下公开的数据集进行了预训练 :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;小规模&lt;/em&gt;: 英语-越南语平行语料库(133K 句子对,TED 对话), 由 &lt;a href="https://sites.google.com/site/iwsltevaluation2015/"&gt;IWSLT Evaluation Campaign&lt;/a&gt; 提供.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;大规模&lt;/em&gt;: 德语-英语平行语料库(4.5M 句子对) , 由 &lt;a href="http://www.statmt.org/wmt16/translation-task.html"&gt;WMT Evaluation Campaign&lt;/a&gt; 提供.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们首先建立关于 seq2seq 模型的一些基本知识, 说明如何构建和训练一个普通的 NMT 模型. 第二部分将详细介绍采用注意力机制（attention mechanism) 建立一个较好的 NMT 模型. 然后，我们将讨论构建更好 NMT 模型（包括速度和翻译质量）的技巧，比如 TensorFlow 的最佳实践（batching, bucketing）, 双向 RNNs 和 定向搜索.&lt;/p&gt;
&lt;h2 id="ji chu"&gt;基础&lt;/h2&gt;
&lt;h3 id="shen jing ji qi fan yi de bei jing"&gt;神经机器翻译的背景&lt;/h3&gt;
&lt;p&gt;回到过去，传统的基于短语的翻译系统通过将语句拆成多个小块，然后再一小块一小块的翻译。这导致不流畅的翻译结果，并不十分像我们人类的翻译。我们是先读懂整个句子，再翻译出来。神经机器翻译(NMT)就是在模仿这种方式！&lt;/p&gt;
&lt;p&gt;具体来说,  NMT 系统首先使用 &lt;em&gt;编码器&lt;/em&gt; 读取源句来构建一个 &lt;a href="https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence"&gt;"thought" 向量&lt;/a&gt; , 一个表示句子意义的数字序列; 然后，&lt;em&gt;解码器&lt;/em&gt; 处理这个向量输出翻译结果
, 如图 1 . 这通常被称为 &lt;em&gt;编码器 - 解码器结构&lt;/em&gt;. 以这种方式, NMT 解决了传统的基于短语翻译的遗留问题: 它可以捕获语言的 &lt;em&gt;远程依赖性&lt;/em&gt; , 比如，词性、语法结构等，并生成顺畅的翻译，如 &lt;a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;Google Neural Machine Translation systems&lt;/a&gt; 所示.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：上文的 &lt;em&gt;"thought"&lt;/em&gt; 只是个比喻，不要当真. 机器学习建立在统计学方法的基础上，跟&amp;ldquo;思考&amp;rdquo;没有半毛钱关系，至于理解人类语言中表达的意义,那更是遥远得离谱的事情.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/encdec.jpg" width="90%"/&gt;
&lt;br/&gt;
图1. &lt;b&gt;编码器-解码器结构&lt;/b&gt; &amp;ndash; NMT 的通用示例. 编码器转换源语句为&amp;ldquo;含义&amp;rdquo;向量，再由&lt;i&gt;解码器&lt;/i&gt;产生翻译结果.
&lt;/p&gt;
&lt;p&gt;NMT 模型的具体结构有所不同. 对顺序数据而言，大多数 NMT 模型的一个自然选择是采用循环神经网络 (RNN).
通常，RNN 同时使用编码器和解码器. 然而，RNN 模型在以下方面有所不同: (a) &lt;em&gt;方向性&lt;/em&gt; &amp;ndash;  单向或双向; (b) &lt;em&gt;深度&lt;/em&gt; &amp;ndash; 单层或多层;  (c) &lt;em&gt;类型&lt;/em&gt; &amp;ndash; 常见的有 RNN,  Long Short-term Memory (LSTM),  或 gated recurrent unit
(GRU). 有兴趣的读者可以在这篇&lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;博文&lt;/a&gt;上找到有关 RNNs 和 LSTM 的更多信息 .&lt;/p&gt;
&lt;p&gt;在本教程中, 我们将考察一个 &lt;em&gt;深度多层 RNN&lt;/em&gt; 的例子，它是单向的，并使用 LSTM 作为循环单元. 如图 2. 在这个例子中，我们将 "I am a student"  翻译成 "Je suis &amp;eacute;tudiant". 在高层上, 这个 NMT 模型由两个循环神经网络组成: &lt;em&gt;编码器&lt;/em&gt;
RNN 简单的吃进输入文字，不做任何预测; 
另一方面，&lt;em&gt;解码器&lt;/em&gt;在预测下一个单词时处理目标句子.&lt;/p&gt;
&lt;p&gt;更多信息, 请参阅 &lt;a href="https://github.com/lmthang/thesis"&gt;Luong (2016)&lt;/a&gt; .&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/seq2seq.jpg" width="58%"/&gt;
&lt;br/&gt;
图2. &lt;b&gt;神经机器翻译&lt;/b&gt; &amp;ndash; 一个深度循环网络的例子，把源语句 "I am a student" 翻译成目标语句 
 "Je suis &amp;eacute;tudiant". 这里, "&amp;lt;s&amp;gt;" 表示解码处理的开始,
而 "&amp;lt;/s&amp;gt;" 表示解码结束.
&lt;/p&gt;
&lt;h3 id="an zhuang jiao cheng"&gt;安装教程&lt;/h3&gt;
&lt;p&gt;要安装本教程, 你需要在系统上安装 TensorFlow. 本教程撰写时 TensorFlow 的版本为 &lt;strong&gt;1.2.1&lt;/strong&gt; .&lt;br/&gt;
安装 TensorFlow 请参阅 &lt;a href="https://www.tensorflow.org/install/"&gt;installation instructions here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一旦安装了 TensorFlow, 你就可以运行以下脚本下载本教程的源码了:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/tensorflow/nmt/
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="xun lian  - ru he jian li wo men de di yi ge  nmt xi tong"&gt;训练 &amp;ndash; 如何建立我们的第一个 NMT 系统&lt;/h3&gt;
&lt;p&gt;让我们先走进构建 NMT 模型的核心代码，一会儿我们将详细解释图 2 . 我们晚点再来看完整代码及数据准备部分. 这部分的代码文件为
&lt;em&gt;&lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py"&gt;model.py&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;如图 2 的底层, 编码器和解码器的循环神经网络接收下列输入: 首先, 是待翻译的句子, 接着是一个边界标记 "&amp;lt;s&amp;gt;&amp;rdquo;，表示从编码到解码模式的转换, 最后是翻译好的句子.  为了&lt;em&gt;训练&lt;/em&gt;, 我们将为系统提供以下张量,
它们包含词汇索引和时序格式（time-major format）:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;encoder_inputs&lt;/strong&gt; [max_encoder_time, batch_size]: 原始输入字.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;decoder_inputs&lt;/strong&gt; [max_decoder_time, batch_size]: 目标输入字.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;decoder_outputs&lt;/strong&gt; [max_decoder_time, batch_size]: 目标输出字. 
这里目标输入字 &lt;em&gt;decoder_inputs&lt;/em&gt; 向左移动一个时间步长，并在右边附加一个句末标记.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了效率，我们一次训练多个句子 (batch_size). 测试时略有不同，我们稍后再行讨论.&lt;/p&gt;
&lt;h4&gt;Embedding&lt;/h4&gt;
&lt;p&gt;根据词汇的含义，模型必须首先找出源词和目标词对应的词向量表达。为使 &lt;em&gt;embedding layer&lt;/em&gt; 工作，首先为每种语言选择一个词表。
通常, 把词表的长度设为 V (即不重复的词汇数量). 而其它词则设为&amp;ldquo;unknown&amp;rdquo;标记，并赋予同样的词向量值。一种语言一套词向量, 一般通过训练学到。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Embedding&lt;/span&gt;
&lt;span class="n"&gt;embedding_encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;"embedding_encoder"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;src_vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Look up embedding:&lt;/span&gt;
&lt;span class="c1"&gt;#   encoder_inputs: [max_time, batch_size]&lt;/span&gt;
&lt;span class="c1"&gt;#   encoder_emp_inp: [max_time, batch_size, embedding_size]&lt;/span&gt;
&lt;span class="n"&gt;encoder_emb_inp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_ops&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;embedding_encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;类似的，我们可以构建 &lt;em&gt;embedding_decoder&lt;/em&gt; 和 &lt;em&gt;decoder_emb_inp&lt;/em&gt; 。注意，可以使用已训练好的词向量, 如 word2vec 或  Glove vectors, 来初始化我们的词向量. 通常，如果有大量的训练数据，我们也可以从头开始训练这些词向量。&lt;/p&gt;
&lt;h4&gt;编码器&lt;/h4&gt;
&lt;p&gt;一旦检索到这个词，就将其对应的词向量作为输入发送到主网络，该网络由两个多层 RNN 组成 ，一个针对源语言的编码器和一个针对目标语言的解码器。
这两个 RNN 原则上可以共享相同的权重；然而，实践中，我们经常使用不同的参数（这样的模型在拟合大规模训练数据集时做得更好）. 
这个 RNN &lt;em&gt;编码器&lt;/em&gt;使用 0 向量作为初始状态，建立如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Build RNN cell&lt;/span&gt;
&lt;span class="n"&gt;encoder_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicLSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Run Dynamic RNN&lt;/span&gt;
&lt;span class="c1"&gt;#   encoder_outpus: [max_time, batch_size, num_units]&lt;/span&gt;
&lt;span class="c1"&gt;#   encoder_state: [batch_size, num_units]&lt;/span&gt;
&lt;span class="n"&gt;encoder_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dynamic_rnn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;encoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_emb_inp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;sequence_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;source_seqence_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time_major&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意, 句子有不同的长度，我们通过 &lt;em&gt;source_sequence_length&lt;/em&gt; 来告诉 &lt;em&gt;dynamic_rnn&lt;/em&gt; 确切的源句长度，以避免计算上的浪费. 由于我们的输入有时序，我们设置 &lt;em&gt;time_major=True&lt;/em&gt; . 这里，我们仅建立一个单层的 LSTM &lt;em&gt;编码器单元&lt;/em&gt;. 我们将在后面的章节说明如何构建多层LSTM，增加 dropout, 和使用 attention.&lt;/p&gt;
&lt;h4&gt;解码器&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;decoder&lt;/em&gt; 也需要访问源信息，一个简单的方法是用编码器最后的隐藏状态来初始化它。如图2，我们把源语言的&amp;ldquo;student&amp;rdquo;的隐藏状态传递到解码器端。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Build RNN cell&lt;/span&gt;
&lt;span class="n"&gt;decoder_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicLSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Helper&lt;/span&gt;
&lt;span class="n"&gt;helper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TrainingHelper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder_emb_inp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;decoder_lengths&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time_major&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Decoder&lt;/span&gt;
&lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicDecoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;helper&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output_layer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;projection_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Dynamic decoding&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dynamic_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里，这个代码的核心部分是 &lt;em&gt;BasicDecoder&lt;/em&gt;，它接收 &lt;em&gt;decoder_cell&lt;/em&gt; (类似 encoder_cell)、&lt;em&gt;helper&lt;/em&gt;、前一个 &lt;em&gt;encoder_state&lt;/em&gt; 作为输入输出到 &lt;em&gt;decoder&lt;/em&gt; 对象。通过分开 decoder 和 helper，我们可以在不同代码中重用。例如，&lt;em&gt;TrainingHelper&lt;/em&gt; 可以被 &lt;em&gt;GreedyEmbeddingHelper&lt;/em&gt; 取代做贪心解码。更多内容请看&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py"&gt;helper.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;最后，我们没提到的 &lt;em&gt;projection_layer&lt;/em&gt; 是个密集矩阵，它将顶部的隐藏状态转为 V 个维度的 logit 向量。我们在图 2 的顶部展示了这个过程。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;projection_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layers_core&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tgt_vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;误差&lt;/h4&gt;
&lt;p&gt;根据上面给定的 &lt;em&gt;logits&lt;/em&gt; ，我们现在准备计算我们的训练误差:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;crossent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparse_softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;decoder_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;crossent&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;target_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里，&lt;em&gt;target_weights&lt;/em&gt; 是一个和 &lt;em&gt;decoder_outputs&lt;/em&gt; 维度一样的 0-1 矩阵.它把超出目标序列长度之外的地方填充为0。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;重要注意事项&lt;/em&gt;&lt;/strong&gt;: 值得指出的是，我们将误差除以 &lt;em&gt;batch_size&lt;/em&gt;, 所以我们的超参数对 batch_size 是不变的. 有些人将误差除以(&lt;em&gt;batch_size&lt;/em&gt; * &lt;em&gt;num_time_steps&lt;/em&gt;)，它可以降低短句的错误. 更微妙的是，我们的超参数（用于前一种方法）不能被用于后一种方法.  例如，如果两个方法都使用 1.0 为学习率的 SGD（随机梯度下降算法），后一种方法会更有效, 因为它采用了更小的 1 / &lt;em&gt;num_time_steps&lt;/em&gt; 作为学习率。&lt;/p&gt;
&lt;h4&gt;梯度计算 &amp;amp; 优化&lt;/h4&gt;
&lt;p&gt;我们现在已经定义了正向传播的 NMT 模型。计算反向传播只是几行代码的问题:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Calculate and clip gradients&lt;/span&gt;
&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;trainable_variables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;gradients&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clipped_gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip_by_global_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_gradient_norm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练 RNN 的一个重要步骤是梯度调整。这里，我们按照惯例来调整。&lt;em&gt;max_gradient_norm&lt;/em&gt; 的最大值, 通常设为 5 或 1. 最后一步是选择优化器. Adam 优化器是常用的选择.  我们也选择一个学习率，这个值常在 0.0001 到 0.001 之间; 并且可以随训练进度而减小.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Optimization&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;update_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clipped_gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在我们自己的实验中，我们采用可自动降低学习率的标准 SGD 优化器（tf.train.GradientDescentOptimizer），从而产生更好的性能。参见 &lt;a href="#评测"&gt;评测&lt;/a&gt;. &lt;/p&gt;
&lt;h3 id="dong shou  - rang wo men xun lian yi ge nmtmo xing"&gt;动手 - 让我们训练一个NMT模型&lt;/h3&gt;
&lt;p&gt;让我们训练我们的第一个 NMT 模型，把越南语翻译成英语！我们代码的入口是
&lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/nmt.py"&gt;&lt;em&gt;nmt.py&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;我们将使用 &lt;em&gt;small-scale parallel corpus of TED talks&lt;/em&gt; (133K training
examples) 进行此练习. 所有数据可在:
&lt;a href="https://nlp.stanford.edu/projects/nmt/"&gt;https://nlp.stanford.edu/projects/nmt/&lt;/a&gt;找到. 我们将使用 tst2012 作为训练数据集,  tst2013 作为测试数据集.&lt;/p&gt;
&lt;p&gt;运行下列命令下载训练 NMT 模型的数据:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;nmt/scripts/download_iwslt15.sh /tmp/nmt_data&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;运行如下命令开始训练:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir /tmp/nmt_model
python -m nmt.nmt &lt;span class="se"&gt;\&lt;/span&gt;
    --src&lt;span class="o"&gt;=&lt;/span&gt;vi --tgt&lt;span class="o"&gt;=&lt;/span&gt;en &lt;span class="se"&gt;\&lt;/span&gt;
    --vocab_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/vocab  &lt;span class="se"&gt;\&lt;/span&gt;
    --train_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/train &lt;span class="se"&gt;\&lt;/span&gt;
    --dev_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/tst2012  &lt;span class="se"&gt;\&lt;/span&gt;
    --test_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/tst2013 &lt;span class="se"&gt;\&lt;/span&gt;
    --out_dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_model &lt;span class="se"&gt;\&lt;/span&gt;
    --num_train_steps&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;12000&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --steps_per_stats&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --num_layers&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --num_units&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;128&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --dropout&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.2 &lt;span class="se"&gt;\&lt;/span&gt;
    --metrics&lt;span class="o"&gt;=&lt;/span&gt;bleu
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上述命令训练一个 2 层 LSTM seq2seq 模型，含 128 个隐藏单元和 12 轮的 embedding 操作。我们使用的 dropout 值为 0.2（维持概率在0.8 ）。如果没有错误，我们应该在我们训练时看到类似于下面的日志。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# First evaluation, global step 0
  eval dev: perplexity 17193.66
  eval test: perplexity 17193.27
# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017
  sample train data:
    src_reverse: &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt; Điều đ&amp;oacute; , dĩ nhi&amp;ecirc;n , l&amp;agrave; c&amp;acirc;u chuyện tr&amp;iacute;ch ra từ học thuyết của Karl Marx .
    ref: That , of course , was the &lt;span class="nt"&gt;&amp;lt;unk&amp;gt;&lt;/span&gt; distilled from the theories of Karl Marx . &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt;
  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00
  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00
  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00
  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00
  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;有关详细信息，请参阅 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/train.py"&gt;&lt;em&gt;train.py&lt;/em&gt;&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;我们可以在训练期间启动 Tensorboard 查看模型的统计：:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tensorboard --port &lt;span class="m"&gt;22222&lt;/span&gt; --logdir /tmp/nmt_model/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从英语到越南语的训练可以简单地改变:
&lt;code&gt;--src=en --tgt=vi&lt;/code&gt;&lt;/p&gt;
&lt;h3 id="tui li  - ru he chan sheng fan yi"&gt;推理 &amp;ndash; 如何产生翻译&lt;/h3&gt;
&lt;p&gt;当你训练你的 NMT 模型（一旦你已训练好模型），你可以得到以前未见过的源语句的翻译。这个过程称为推理。训练和推理（测试）之间有明确的区别：在推理时，我们只能访问源语句，即 encoder_inputs。解码有很多种方法, 包括贪心、采样和定向搜索几种。在这里，我们将讨论贪心解码法。&lt;/p&gt;
&lt;p&gt;这个想法很简单，我们在图 3 中说明:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们仍然以与训练期间相同的方式对源语句进行编码以获得 encoder_state，并使用该 encoder_state 来初始化解码器。&lt;/li&gt;
&lt;li&gt;一旦解码器接收到起始符号&amp;ldquo;&amp;lt;s&amp;gt;&amp;rdquo;（参见我们代码中的 tgt_sos_id ），解码（翻译）处理就开始&lt;/li&gt;
&lt;li&gt;对于解码器侧的每个时间步长，我们将 RNN 的输出视为一组 logit。我们选择最可能的字，与最大 logit 值相关联的 id 作为译出的字（这是&amp;ldquo;贪婪&amp;rdquo;行为）。例如在图 3 中，在第一个解码步骤中，词&amp;ldquo;moi&amp;rdquo;具有最高的翻译概率。然后，我们将这个词作为输入提供给下一个时间步。(译者注：由于输出的logit向量维度为词表长度V，当词表很大时计算量很大)&lt;/li&gt;
&lt;li&gt;继续第3步直到遇到句子的结尾标记&amp;ldquo;&amp;lt;/s&amp;gt;&amp;rdquo;（参见我们的代码中的 tgt_eos_id ）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/greedy_dec.jpg" width="50%"/&gt;
&lt;br/&gt;
图3. &lt;b&gt;贪心解码&lt;/b&gt; &amp;ndash; 如何用贪心搜索法训练 NMT 模型, 使源语句产生"Je suis &amp;eacute;tudiant"的翻译
&lt;/p&gt;
&lt;p&gt;第 3 步的推理与训练不同。不是总是将正确的目标词作为输入，推理使用模型预测的单词。以下是实现贪心解码的代码。它与训练解码器非常相似。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Helper&lt;/span&gt;
&lt;span class="n"&gt;helper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GreedyEmbeddingHelper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;embedding_decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;tgt_sos_id&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tgt_eos_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Decoder&lt;/span&gt;
&lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicDecoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;helper&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output_layer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;projection_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Dynamic decoding&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dynamic_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maximum_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;maximum_iterations&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;translations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_id&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里，我们使用 &lt;em&gt;GreedyEmbeddingHelper&lt;/em&gt; 代替 &lt;em&gt;TrainingHelper&lt;/em&gt;。由于我们预先不知道目标序列的长度，所以我们使用 &lt;em&gt;maximum_iterations&lt;/em&gt; 来限制翻译的长度。一个启发式的用法是采用源语句长度的两倍来解码。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;maximum_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source_sequence_length&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练好一个模型后，我们现在可以创建一个推理文件并翻译一些句子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cat &amp;gt; /tmp/my_infer_file.vi
&lt;span class="c1"&gt;# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)&lt;/span&gt;

python -m nmt.nmt &lt;span class="se"&gt;\&lt;/span&gt;
    --model_dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_model &lt;span class="se"&gt;\&lt;/span&gt;
    --inference_input_file&lt;span class="o"&gt;=&lt;/span&gt;/tmp/my_infer_file.vi &lt;span class="se"&gt;\&lt;/span&gt;
    --inference_output_file&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_model/output_infer

cat /tmp/nmt_model/output_infer &lt;span class="c1"&gt;# To view the inference as output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，只要存在训练检查点，即使模型仍在训练中，也可以运行上述命令。详见 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/inference.py"&gt;&lt;em&gt;inference.py&lt;/em&gt;&lt;/a&gt; 。&lt;/p&gt;
&lt;h2 id="jin jie_1"&gt;进阶&lt;/h2&gt;
&lt;p&gt;经历了最基本的 seq2seq 模型，让我们进一步完善它！为了建立最先进的神经机器翻译系统，我们还需要更多的&amp;ldquo;内功心法&amp;rdquo;：
&lt;em&gt;注意力机制(attention mechanism)&lt;/em&gt;，这是由 &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanau等人2015年&lt;/a&gt;首次引入，然后由 &lt;a href="https://arxiv.org/abs/1508.04025"&gt;Luong等人2015年&lt;/a&gt;完善。&lt;em&gt;注意力机制&lt;/em&gt;的关键在于，通过在翻译过程中对相关的源内容进行&amp;ldquo;关注&amp;rdquo;，建立目标和源之间的直接连接。注意力机制 的一个很好的副产品是在源和目标句子之间生成一个易于查看的对齐矩阵（如图4所示）&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/attention_vis.jpg" width="50%"/&gt;
&lt;br/&gt;
图4. &lt;b&gt;Attention 可视化&lt;/b&gt; &amp;ndash; 源语句与目标语句的词汇对齐矩阵示例， 图片来自 (Bahdanau et al., 2015).
&lt;/p&gt;
&lt;p&gt;请记住，在 seq2seq 模型中，当开始解码时，我们将最后的源状态从编码器传递到解码器。这对中短句的效果很好; 而对于长句，单个固定大小的隐藏状态会成为信息瓶颈。
注意力机制不是放弃在源 RNN 中计算的所有隐藏状态，而是提供了一种允许解码器窥视它们的方法（将它们视为源信息的动态存储器）。
通过这样做，注意力机制改善了较长句子的翻译质量。现在，注意力机制已成为事实上的标准，并已成功应用于许多其他任务（包括图像字幕生成，语音识别和文本摘要等）。&lt;/p&gt;
&lt;h3 id="zhu yi li ji zhi de bei jing"&gt;注意力机制的背景&lt;/h3&gt;
&lt;p&gt;我们现在讲讲（Luong等人，2015年）中提出的注意力机制(attention mechanism)的一个实例，该实例已被用于包括 &lt;a href="http://opennmt.net/about/"&gt;OpenNMT&lt;/a&gt; 等开源工具包在内的多个最先进的系统，以及本教程的 TF seq2seq API 中。我们还将提供注意力机制其他变种的连接。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/attention_mechanism.jpg" width="58%"/&gt;
&lt;br/&gt;
图5. &lt;b&gt;Attention mechanism&lt;/b&gt; &amp;ndash; (Luong et al., 2015)中提到的基于 attention NMT 系统的示例. 我们重点突出 attention 计算的第一个步骤. 为了清晰，我们没像图2那样显示 embedding 层和 projection 层.
&lt;/p&gt;
&lt;p&gt;如图 5 所示，attention 计算发生在每个解码器的时间步长。它包括以下步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将当前目标的隐藏状态与所有源的状态进行比较以获得 &lt;em&gt;attention weights&lt;/em&gt;（如图4所示）。&lt;/li&gt;
&lt;li&gt;基于 attention weights ，我们计算&lt;em&gt;上下文矢量&lt;/em&gt;（&lt;em&gt;context vector&lt;/em&gt;） 作为源状态的加权平均值。&lt;/li&gt;
&lt;li&gt;将上下文矢量与当前目标的隐藏状态组合以产生最终的 &lt;em&gt;attention vector&lt;/em&gt;。&lt;/li&gt;
&lt;li&gt;attention vector 作为输入发送到下一个时间步（&lt;em&gt;input feeding&lt;/em&gt;）。前三个步骤通过以下等式来总结：&lt;/li&gt;
&lt;/ol&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/attention_equation_0.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;这里，函数 &lt;code&gt;score&lt;/code&gt; 用于将目标隐藏状态 &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; 与每个源隐藏状态 &lt;span class="math"&gt;\(\overline{h}_s\)&lt;/span&gt; 进行比较，并将结果归一化以产生 attention weights（一个基于源语言的位置分布）。这里的 score 函数有多种选择; 流行的 score 函数包括等式（4）列出的乘法和加法形式。一旦完成计算，Attention vector &lt;span class="math"&gt;\(a_t\)&lt;/span&gt; 被用来导出softmax logit 和 loss。这类似于 seq2seq 模型顶层的目标隐藏状态。这个函数 &lt;code&gt;f&lt;/code&gt; 也可以采取其他形式。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/attention_equation_1.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;attention mechanisms 的各种实现可以在 &lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py"&gt;attention_wrapper.py&lt;/a&gt; 中找到.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;注意力机制有多重要?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如上述方程式所示，有许多不同的 attention 变体。这些变体取决于 score 函数和 attention 函数的形式，以及在 score 函数中是否使用上一个时间步的状态 &lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt; 而不是 &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; （源自 Bahdanau et al.,2015 的建议）。经验上，我们发现只有某些选择很重要。首先，是  attention 的基本形式，即目标语言和源语言之间的直接联系。第二，重要的是把 attention vector 送到下一个时间步，以通报网络关于过去的 attention 决定（源自Luong et al., 2015 中的示范）。最后，score 函数的选择常常会导致不同的表现。更多内容请看&lt;a href="#评测"&gt;评测结果&lt;/a&gt;部分。&lt;/p&gt;
&lt;h3 id="attention wrapper api"&gt;Attention Wrapper API&lt;/h3&gt;
&lt;p&gt;在实现 &lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py"&gt;AttentionWrapper&lt;/a&gt; 时，我们借鉴了 &lt;a href="https://arxiv.org/abs/1409.0473"&gt;(Weston et al., 2015)&lt;/a&gt; 在 &lt;em&gt;memory networks&lt;/em&gt; 方面的一些术语。本教程介绍的 attention mechanism 是只读 memory，而不是可读写的 memory。具体来说，一组源语隐藏状态（或其转换版本，如：Luong 的评分中的  &lt;span class="math"&gt;\(W\overline{h}_s\)&lt;/span&gt;，或 Bahong 的评分中的 &lt;span class="math"&gt;\(W_2\overline{h}_s\)&lt;/span&gt;）被作为 &lt;em&gt;&amp;ldquo;memory&amp;rdquo;&lt;/em&gt; 。在每个时间步骤中，我们使用当前的目标隐藏状态作为 &lt;em&gt;&amp;ldquo;query&amp;rdquo;&lt;/em&gt; 来决定要读取 memory 的哪个部分。通常，查询需要与对应于各个 memory 插槽的键值进行比较。在上述 attention mechanism 的介绍中，我们恰好将源语隐藏状态（或其转换版本，例如，Bahdanau 的评分中的 &lt;span class="math"&gt;\(W_1h_t\)&lt;/span&gt;）作为&amp;ldquo;键&amp;rdquo;值。可以通过这种记忆网络术语来启发其他形式的  attention！&lt;/p&gt;
&lt;p&gt;多亏了对 attention 的封装，使得我们扩展原始 seq2seq 代码时很方便。这部分文件参见 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/attention_model.py"&gt;&lt;em&gt;attention_model.py&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;首先，我们需要定义一个attention mechanism，例如(Luong et al., 2015):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# attention_states: [batch_size, max_time, num_units]&lt;/span&gt;
&lt;span class="n"&gt;attention_states&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Create an attention mechanism&lt;/span&gt;
&lt;span class="n"&gt;attention_mechanism&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LuongAttention&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attention_states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;memory_sequence_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;source_sequence_length&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在前面的&lt;a href="#编码器"&gt;编码器&lt;/a&gt; 部分，&lt;em&gt;encoder_outputs&lt;/em&gt; 是顶层所有源语隐藏状态的集合，其形状为 &lt;em&gt;[max_time，batch_size，num_units]&lt;/em&gt; （因为我们使用 &lt;em&gt;dynamic_rnn&lt;/em&gt;，&lt;em&gt;time_major&lt;/em&gt; 设置为 &lt;em&gt;True&lt;/em&gt; 以获得效率）。对于 attention mechanism，我们需要确保传递的&amp;ldquo;memory&amp;rdquo;是批处理的，所以我们需要转置 &lt;em&gt;attention_states&lt;/em&gt;。我们将 &lt;em&gt;source_sequence_length&lt;/em&gt; 传递给 attention machanism，以确保 attention weight 适当归一化（仅在非填充位置上）。&lt;/p&gt;
&lt;p&gt;定义了 attention mechanism 后，我们使用 &lt;em&gt;AttentionWrapper&lt;/em&gt; 来包装 decoding_cell：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;decoder_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AttentionWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attention_mechanism&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;attention_layer_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;代码的其余部分与&lt;a href="#解码器"&gt;解码器&lt;/a&gt;部分几乎相同！&lt;/p&gt;
&lt;h3 id="dong shou  - jian li ji yu  attention de  nmt mo xing"&gt;动手 &amp;ndash; 建立基于 attention 的 NMT 模型&lt;/h3&gt;
&lt;p&gt;要启用的 attention ，在训练时我们需要使用 &lt;code&gt;luong&lt;/code&gt;、&lt;code&gt;scaled_luong&lt;/code&gt;、&lt;code&gt;bahdanau&lt;/code&gt;  或 &lt;code&gt;normed_bahdanau&lt;/code&gt; 中的一个作为 &lt;code&gt;attention&lt;/code&gt; 标志的值。该标志指定了我们将要使用的 attention mechanism。此外，我们需要为 attention 模型创建一个新的目录，所以我们不能重复使用前面训练过的简单 NMT 模型。&lt;/p&gt;
&lt;p&gt;运行以下命令开始训练:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir /tmp/nmt_attention_model

python -m nmt.nmt &lt;span class="se"&gt;\&lt;/span&gt;
    --attention&lt;span class="o"&gt;=&lt;/span&gt;scaled_luong &lt;span class="se"&gt;\&lt;/span&gt;
    --src&lt;span class="o"&gt;=&lt;/span&gt;vi --tgt&lt;span class="o"&gt;=&lt;/span&gt;en &lt;span class="se"&gt;\&lt;/span&gt;
    --vocab_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/vocab  &lt;span class="se"&gt;\&lt;/span&gt;
    --train_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/train &lt;span class="se"&gt;\&lt;/span&gt;
    --dev_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/tst2012  &lt;span class="se"&gt;\&lt;/span&gt;
    --test_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/tst2013 &lt;span class="se"&gt;\&lt;/span&gt;
    --out_dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_attention_model &lt;span class="se"&gt;\&lt;/span&gt;
    --num_train_steps&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;12000&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --steps_per_stats&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --num_layers&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --num_units&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;128&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --dropout&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.2 &lt;span class="se"&gt;\&lt;/span&gt;
    --metrics&lt;span class="o"&gt;=&lt;/span&gt;bleu
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练后，我们可以使用相同的推理命令与新的 model_dir 进行推理：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python -m nmt.nmt &lt;span class="se"&gt;\&lt;/span&gt;
    --model_dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_attention_model &lt;span class="se"&gt;\&lt;/span&gt;
    --inference_input_file&lt;span class="o"&gt;=&lt;/span&gt;/tmp/my_infer_file.vi &lt;span class="se"&gt;\&lt;/span&gt;
    --inference_output_file&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_attention_model/output_infer
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="ti shi  &amp;amp; ji qiao_1"&gt;提示 &amp;amp; 技巧&lt;/h2&gt;
&lt;h3 id="xun lian , ping gu , he tui li tu"&gt;训练, 评估, 和推理图&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：&amp;ldquo;图&amp;rdquo;的定义原文，A Graph contains a set of Operation objects, which represent units of computation; and Tensor objects, which represent the units of data that flow between operations. 简单说, &amp;ldquo;图&amp;rdquo;相当于一个数学公式，其中的 tensor 相当于变量 x  ，而定义的 op 相当于连接变量的运算符号.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 TensorFlow 中构建机器学习模型时，最好建立三个独立的图：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;训练图, 其中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把来自文件或外部导入的数据批次化（即分成数量相同的很多组，每次处理一组），作为输入数据&lt;/li&gt;
&lt;li&gt;包含正向和反向传播操作.&lt;/li&gt;
&lt;li&gt;构建优化器，并加到训练中.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;评估图, 其中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;批次化输入数据.&lt;/li&gt;
&lt;li&gt;包含在训练图中用过的正向传播操作，增加了一个没在训练图中用过的评估操作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;推理图, 其中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可能不批量输入数据.&lt;/li&gt;
&lt;li&gt;不对输入数据进行子采样或批次化.&lt;/li&gt;
&lt;li&gt;从占位符读取输入数据（可以通过 &lt;em&gt;feed_dict&lt;/em&gt; 或 C ++ TensorFlow serving binary 将数据直接提供给推理图）.&lt;/li&gt;
&lt;li&gt;包括模型正向传播操作的一个子集，和一个额外的在 session.run 调用之间存储状态的特殊输入/输出（或有）.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分别构建图有几个好处:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推理图通常与其他两个很不同，因此分开构建是有意义的.&lt;/li&gt;
&lt;li&gt;由于没有反向转播操作，评估图变得更简单.&lt;/li&gt;
&lt;li&gt;数据分别提供给每个图.&lt;/li&gt;
&lt;li&gt;复用简单得多.  比如, 在评估图中，不需要用 &lt;em&gt;reuse=True&lt;/em&gt; 来打开变量的作用域，因为训练模型已经创建了这些变量.  所以相同的代码不需要加上 &lt;em&gt;reuse=&lt;/em&gt; 参数就能被复用.&lt;/li&gt;
&lt;li&gt;在分布式训练中，把训练、评估和推理的工作分开是很平常的。这就需要它们各自建立自己的图。因此，以这种方式构建的系统将为你提供分布式训练的准备。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;复杂性的主要来源是如何在单个计算机设置中跨三个图共享变量. 可以为每个图使用单独的会话来解决. 训练会话定期的保存检查点，评估会话和推断会话从检查点导入参数. 下面的例子显示了两种方法的主要区别.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;之前: 三个模型在一个图中并共享一个会话&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'root'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;train_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildTrainModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;initializer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'root'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reuse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;eval_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;eval_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildEvalModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'root'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reuse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;infer_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;inference_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildInferenceModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="n"&gt;train_input_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;train_inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;train_input_data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;EVAL_STEPS&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;data_to_eval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;eval_input_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
      &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;eval_loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;eval_inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;eval_input_data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;INFER_STEPS&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inference_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;infer_inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;infer_input_data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;之后: 三个模型在三个图中，有三个会话并共享同样的变量&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;eval_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;infer_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;train_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="n"&gt;train_iterator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;train_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildTrainModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_iterator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;initializer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;eval_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="n"&gt;eval_iterator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;eval_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildEvalModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_iterator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;infer_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="n"&gt;infer_iterator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;infer_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;infer_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildInferenceModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_iterator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;checkpoints_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"/tmp/model/checkpoints"&lt;/span&gt;

&lt;span class="n"&gt;train_sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;eval_sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eval_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;infer_sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;infer_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;

  &lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;EVAL_STEPS&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;checkpoint_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoints_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;data_to_eval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;eval_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_sess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;INFER_STEPS&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;checkpoint_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoints_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;infer_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;infer_sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;infer_inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;infer_input_data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;data_to_infer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;infer_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;infer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_sess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意后一种方法是如何转换为分布式版本的。&lt;/p&gt;
&lt;p&gt;新方法的另一个区别在于，在每个 &lt;em&gt;session.run&lt;/em&gt; 调用时，我们使用有状态的迭代器对象来代替 &lt;em&gt;feed_dicts&lt;/em&gt; 提供数据（从而我们能自己批处理、切分和操作数据）。这些迭代器使"输入管道"在设置单机和分布式时，都容易很多。我们将在下一节中介绍新的输入数据管道（ 限TensorFlow 1.2 ）。&lt;/p&gt;
&lt;h3 id="shu ju shu ru guan dao"&gt;数据输入管道&lt;/h3&gt;
&lt;p&gt;在 TensorFlow 1.2 之前, 用户有三种方式把数据提供给 TensorFlow 进行训练和评估:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在每次调用 &lt;em&gt;session.run&lt;/em&gt; 时，用 &lt;em&gt;feed_dict&lt;/em&gt; 提供数据.&lt;/li&gt;
&lt;li&gt;在 &lt;em&gt;tf.train&lt;/em&gt; (e.g. &lt;em&gt;tf.train.batch&lt;/em&gt;) 和 &lt;em&gt;tf.contrib.train&lt;/em&gt; 中使用排队机制 .&lt;/li&gt;
&lt;li&gt;使用象 &lt;em&gt;tf.contrib.learn&lt;/em&gt; 或 &lt;em&gt;tf.contrib.slim&lt;/em&gt; 等高级框架中的 helper (用 #2 效率更高).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一种方法对于不熟悉 TensorFlow 的用户或只能在 Python 中需要做个性化输入（如：自己的小批次队列）的用户更容易。第二和第三种方法更标准，但灵活性稍差一些；他们还需要启动多个 python 线程（queue runners）。此外，如果使用错误的队列可能会导致死锁或不可知的错误。然而，队列比使用 &lt;em&gt;feed_dict&lt;/em&gt; 更高效，也是单机和分布式训练的标准方式。&lt;/p&gt;
&lt;p&gt;从 TensorFlow 1.2 开始，有一个新的方法可用于将数据读入 TensorFlow 模型：数据集迭代器（dataset iterators），可在 &lt;strong&gt;tf.contrib.data&lt;/strong&gt;  模块中找到。数据迭代器是灵活的、易理解的和可定制的，并能依赖 TensorFlow C ++ 运行库提供高效和多线程的读入操作。&lt;/p&gt;
&lt;p&gt;一个&lt;strong&gt;数据集&lt;/strong&gt;可以从一批数据张量、一个文件名，或包含多个文件名的一个张量来创建。举例如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Training dataset consists of multiple files.&lt;/span&gt;
&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextLineDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_files&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Evaluation dataset uses a single file, but we may&lt;/span&gt;
&lt;span class="c1"&gt;# point to a different file for each evaluation round.&lt;/span&gt;
&lt;span class="n"&gt;eval_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;eval_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextLineDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# For inference, feed input data to the dataset directly via feed_dict.&lt;/span&gt;
&lt;span class="n"&gt;infer_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_infer_examples&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;infer_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_tensor_slices&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;所有数据集的输入处理都是类似的。包括数据的读取和清理，数据的切分（在训练和评估时）、过滤及批处理等。&lt;/p&gt;
&lt;p&gt;把每个句子转换成单词的字符串向量，例如，我们对数据集做 map 转换：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string_split&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后，我们把每个句子向量切换成一个包含向量及其动态长度的元组：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，我们对每个句子执行词汇查找。根据给定的查找表，把元组中的元素从字符串向量转换为整数向量。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;拼接两个数据集也很容易。如果有两个彼此逐行互译的文件，且每个文件都有自己的数据集，则可按以下方式创建一个新数据集来合并这两个数据集：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;source_target_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;source_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_dataset&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;不定长句子的批处理是简单明了的。接下来对 &lt;em&gt;source_target_dataset&lt;/em&gt; 数据集中的元素按 &lt;em&gt;batch_size&lt;/em&gt; 的大小规格做批次转换， 同时在每批中，把源向量和目标向量进行填充，使其长度与它们当中最长的向量长度一致。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这句话不是人翻的，不信你去看原文，我是看了半天下面的 python 源码才搞出来，我在怀疑是我的语文不好还是作者的语文不好，哎！  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;batched_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;source_target_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;padded_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;padded_shapes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TensorShape&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;  &lt;span class="c1"&gt;# source vectors of unknown size&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TensorShape&lt;/span&gt;&lt;span class="p"&gt;([])),&lt;/span&gt;     &lt;span class="c1"&gt;# size(source)&lt;/span&gt;
                   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TensorShape&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;  &lt;span class="c1"&gt;# target vectors of unknown size&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TensorShape&lt;/span&gt;&lt;span class="p"&gt;([]))),&lt;/span&gt;    &lt;span class="c1"&gt;# size(target)&lt;/span&gt;
    &lt;span class="n"&gt;padding_values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;src_eos_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# source vectors padded on the right with src_eos_id&lt;/span&gt;
                     &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;          &lt;span class="c1"&gt;# size(source) -- unused&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tgt_eos_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# target vectors padded on the right with tgt_eos_id&lt;/span&gt;
                     &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;         &lt;span class="c1"&gt;# size(target) -- unused&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从这个数据集传出的值是一个嵌套元组，其张量最左边的维度就是 &lt;em&gt;batch_size&lt;/em&gt; 的大小。该结构为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;iterator[0][0] 是分好批和填充好的源句子矩阵.&lt;/li&gt;
&lt;li&gt;iterator[0][1] 是分好批的源句子长度向量.&lt;/li&gt;
&lt;li&gt;iterator[1][0] 是分好批和填充好的目标句子矩阵.&lt;/li&gt;
&lt;li&gt;iterator[1][1] 是分好批的目标句子长度向量.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后, 尽量把这些长度相似的源句子打包在一起。更多内容及完整实现请参阅 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py"&gt;utils/iterator_utils.py&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;从数据集读取数据仅需三行代码：创建迭代器，获取其值，和初始化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;batched_iterator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batched_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_initializable_iterator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;source_lengths&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_lenghts&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batched_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# At initialization time.&lt;/span&gt;
&lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batched_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;一旦迭代器被初始化，每个 &lt;em&gt;session.run&lt;/em&gt; 调用（访问源或目标张量）将从底层数据集请求下一批数据。&lt;/p&gt;
&lt;h3 id="guan yu zeng qiang  nmt mo xing de qi ta xi jie"&gt;关于增强 NMT 模型的其它细节&lt;/h3&gt;
&lt;h4&gt;双向 RNNs&lt;/h4&gt;
&lt;p&gt;在编码器端双向化通常会带来更好的性能(随着层数的增加速度会有所降低). 这里, 我们给个简单的例子，建立一个双向单层的编码器：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Construct forward and backward cells&lt;/span&gt;
&lt;span class="n"&gt;forward_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicLSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;backward_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicLSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;bi_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bidirectional_dynamic_rnn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;forward_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;backward_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_emb_inp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;sequence_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;source_sequence_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time_major&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;encoder_outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bi_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;变量 &lt;em&gt;encoder_outputs&lt;/em&gt; 和 &lt;em&gt;encoder_state&lt;/em&gt; 的用法与编码器那节中说的一样. 注意, 对多个双向层, 我们需要对 encoder_state 做些修改, 详见 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py"&gt;model.py&lt;/a&gt;,中的 &lt;em&gt;_build_bidirectional_rnn()&lt;/em&gt; 方法 .&lt;/p&gt;
&lt;h4&gt;定向搜索&lt;/h4&gt;
&lt;p&gt;虽然用贪心法解码能带给我们较满意的翻译质量，但用定向搜索解码能进一步提高性能。定向搜索的想法是，在翻译的同时，保留一个小小的最佳候选集以便更容易检索。这个定向的范围称为 &lt;em&gt;beam width&lt;/em&gt;; 一般这个值设为 10 就够了. 更多信息请参阅 &lt;a href="https://arxiv.org/abs/1703.01619"&gt;Neubig, (2017)&lt;/a&gt; 第 7.2.3 节。举例如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Replicate encoder infos beam_width times&lt;/span&gt;
&lt;span class="n"&gt;decoder_initial_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tile_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;encoder_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;multiplier&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;beam_width&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Define a beam-search decoder&lt;/span&gt;
&lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BeamSearchDecoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;cell&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;decoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;embedding_decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;start_tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;start_tokens&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;end_token&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;end_token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;initial_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;decoder_initial_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;beam_width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;beam_width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;output_layer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;projection_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;length_penalty_weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Dynamic decoding&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dynamic_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意这里对 &lt;em&gt;dynamic_decode()&lt;/em&gt; API 的调用和&lt;a href="#解码器"&gt;解码器&lt;/a&gt;那节一样。解码后，我们就能向下面那样获取翻译结果了：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;translations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predicted_ids&lt;/span&gt;
&lt;span class="c1"&gt;# Make sure translations shape is [batch_size, beam_width, time]&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time_major&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
   &lt;span class="n"&gt;translations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;translations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;关于 &lt;em&gt;_build_decoder()&lt;/em&gt; 方法的更多信息，详见 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py"&gt;model.py&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;超参数&lt;/h4&gt;
&lt;p&gt;有几个超参数可以提高性能。这里，我们根据自己的经验列出一些［免责声明：其它人可能不同意我说的］。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;优化&lt;/em&gt;&lt;/strong&gt;: Adam 优化器有些不太寻常的结构, 如果你用 SGD （随机梯度下降算法）训练，它一般会带来更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Attention&lt;/em&gt;&lt;/strong&gt;: 在编码器端，Bahdanau-style attention 通常需要双向 RNN 才运行良好; 而 Luong-style attention 适用于不同的设置. 在本教程中, 我们推荐使用 Luong 和 Bahdanau-style attentions 的改进版本： &lt;em&gt;scaled_luong&lt;/em&gt; 和 &lt;em&gt;normed bahdanau&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;多 GPU 训练&lt;/h4&gt;
&lt;p&gt;训练一个 NMT 模型可能要好几天。把不同的 RNN 层放到不同的 GPU 上，能提高训练速度。下面是在多 GPU 上创建 RNN 层的例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DeviceWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
      &lt;span class="s2"&gt;"/gpu:&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;num_gpus&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MultiRNNCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;另外，我们要在&lt;code&gt;tf.gradients&lt;/code&gt; 中启用 &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt;选项，才可以对梯度进行并行计算。 &lt;/p&gt;
&lt;p&gt;你可能注意到即使增加GPU，对基于 attention 的 NMT 模型的速度提升也很小。attention 架构的一个主要缺点是，在每一个时间步，采用顶层（即最后一层）输出来查询 attention。这就意味着每次解码时必须等前面的所以步骤完成才行；因此，我们不能简单的把 RNN 层放在多个 GPU 上来并行解码。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1609.08144.pdf"&gt;GNMT attention architecture&lt;/a&gt; 提出，通过使用底层（即第一层）输出来查询 attention 来并行解码运算。这样，每次解码就能在前面的第一层完成后开始。我们在 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/gnmt_model.py"&gt;GNMTAttentionMultiCell&lt;/a&gt; 中的子类 &lt;em&gt;tf.contrib.rnn.MultiRNNCell&lt;/em&gt;上实现此架构，下面是用 &lt;em&gt;GNMTAttentionMultiCell&lt;/em&gt; 创建一个解码单元的示例。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DeviceWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
      &lt;span class="s2"&gt;"/gpu:&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;num_gpus&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;attention_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;attention_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AttentionWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;attention_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;attention_mechanism&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;attention_layer_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# don't add an additional dense layer.&lt;/span&gt;
    &lt;span class="n"&gt;output_attention&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;span class="n"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GNMTAttentionMultiCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attention_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="ping ce_1"&gt;评测&lt;/h2&gt;
&lt;h3 id="iwslt ying yu -yue nan yu"&gt;IWSLT 英语-越南语&lt;/h3&gt;
&lt;p&gt;样本集: 133K examples, 训练集=tst2012, 测试集=tst2013,
&lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/scripts/download_iwslt15.sh"&gt;下载脚本&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;训练细节&lt;/em&gt;&lt;/strong&gt;. 我们用双向编码器（编码器有一个双向层）训练 512 单元的 2 层 LSTM，embedding 维度为 512。LuongAttention (scale=True) 与 keep_prob 为 0.8 的 dropout 一起使用。所有参数均匀。我们采用学习率为 1.0 的 SGD 算法：训练12K 步（12 轮）；8K 步后，我们开始每 1K 步减半学习率。 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;结果&lt;/em&gt;&lt;/strong&gt;.
TODO(rzhao): 添加英语-越南语模型的 URL。&lt;/p&gt;
&lt;p&gt;以下是两个模型的平均结果
(&lt;a href="http://download.tensorflow.org/models/nmt/envi_model_1.zip"&gt;model 1&lt;/a&gt;, &lt;a href="http://download.tensorflow.org/models/nmt/envi_model_2.zip"&gt;model 2&lt;/a&gt;).
我们用 BLEU 评分来评估翻译质量 &lt;a href="http://www.aclweb.org/anthology/P02-1040.pdf"&gt;(Papineni et al., 2002)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;tst2012 (dev)&lt;/th&gt;
&lt;th align="center"&gt;test2013 (test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NMT (greedy)&lt;/td&gt;
&lt;td align="center"&gt;23.2&lt;/td&gt;
&lt;td align="center"&gt;25.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT (beam=10)&lt;/td&gt;
&lt;td align="center"&gt;23.8&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;26.1&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://stanford.edu/~lmthang/data/papers/iwslt15.pdf"&gt;(Luong &amp;amp; Manning, 2015)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;23.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;训练速度&lt;/strong&gt;: 在 &lt;em&gt;K40m&lt;/em&gt; 上 (0.37 秒每步 , 15.3K 字每秒)  &amp;amp; 在 &lt;em&gt;TitanX&lt;/em&gt; 上 (0.17 秒每步, 32.2K 字每秒) .
这里，每步时间指运行一个小批量（128个）所需的时间。对于字每秒，我们统计的是源和目标上的单词。&lt;/p&gt;
&lt;h3 id="wmt de yu -ying yu"&gt;WMT 德语-英语&lt;/h3&gt;
&lt;p&gt;样本集: 4.5M examples, 训练集=newstest2013, 测试集=newstest2015
&lt;a href="nmt/scripts/wmt16_en_de.sh"&gt;下载脚本&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;训练细节&lt;/em&gt;&lt;/strong&gt;. 我们训练的超参数与英语-越南语的实验类似，除了以下细节. 采用 &lt;a href="https://github.com/rsennrich/subword-nmt"&gt;BPE&lt;/a&gt;(32K 操作)将数据分成子字单元. 我们用双向编码器 1024（编码器有2个双向层）训练 1024 单元的 4 层 LSTM , embedding 维度为 1024. 我们训练了 359K 步 (10轮); 170K 步后, 我们开始每 17K 步减半学习率.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;结果&lt;/em&gt;&lt;/strong&gt;.
TODO(rzhao): 添加德语-英语模型的 URL.&lt;/p&gt;
&lt;p&gt;前 2 行是模型 1、2 的评价结果(&lt;a href="http://download.tensorflow.org/models/nmt/deen_model_1.zip"&gt;model 1&lt;/a&gt;,&lt;a href="http://download.tensorflow.org/models/nmt/deen_model_2.zip"&gt;model 2&lt;/a&gt;).
第 4 行是运行在 4 个 GPU 上的 GNMT attention &lt;a href="http://download.tensorflow.org/models/nmt/deen_gnmt_model_4_layer.zip"&gt;模型&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;newstest2013 (dev)&lt;/th&gt;
&lt;th align="center"&gt;newstest2015&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NMT (greedy)&lt;/td&gt;
&lt;td align="center"&gt;27.1&lt;/td&gt;
&lt;td align="center"&gt;27.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT (beam=10)&lt;/td&gt;
&lt;td align="center"&gt;28.0&lt;/td&gt;
&lt;td align="center"&gt;28.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT + GNMT attention (beam=10)&lt;/td&gt;
&lt;td align="center"&gt;29.0&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;29.9&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://matrix.statmt.org/"&gt;WMT SOTA&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;29.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这些结果表明，我们的代码为 NMT 建立了强大的基线系统。
(请注意，WMT 系统通常会使用大量的单种语料数据，我们目前没有。)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;训练速度&lt;/strong&gt;: 在 &lt;em&gt;Nvidia K40m&lt;/em&gt; 上 (2.1 秒每步, 3.4K 字每秒)  &amp;amp; 在 &lt;em&gt;Nvidia TitanX&lt;/em&gt; 上(0.7 秒每步, 8.7K 字每秒) 。
为看 GNMT attention 的速度提升效果, 我们仅在 &lt;em&gt;K40m&lt;/em&gt; 上做测试:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;1 gpu&lt;/th&gt;
&lt;th align="center"&gt;4 gpus&lt;/th&gt;
&lt;th align="center"&gt;8 gpus&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NMT (4 layers)&lt;/td&gt;
&lt;td align="center"&gt;2.2s, 3.4K&lt;/td&gt;
&lt;td align="center"&gt;1.9s, 3.9K&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT (8 layers)&lt;/td&gt;
&lt;td align="center"&gt;3.5s, 2.0K&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;2.9s, 2.4K&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT + GNMT attention (4 layers)&lt;/td&gt;
&lt;td align="center"&gt;2.6s, 2.8K&lt;/td&gt;
&lt;td align="center"&gt;1.7s, 4.3K&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT + GNMT attention (8 layers)&lt;/td&gt;
&lt;td align="center"&gt;4.2s, 1.7K&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;1.9s, 3.8K&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这些结果表明，没有 GNMT attention，使用多 GPU 获得的效果很小。而使用 GNMT attention，我们从多 GPU 上获得了 50%-100% 的速度提升。&lt;/p&gt;
&lt;h3 id="wmt ying yu -de yu  -- wan quan bi jiao"&gt;WMT 英语-德语 &amp;mdash; 完全比较&lt;/h3&gt;
&lt;p&gt;前 2 行是 GNMT attention 模型: &lt;a href="http://download.tensorflow.org/models/nmt/ende_gnmt_model_4_layer.zip"&gt;model 1 (4 层)&lt;/a&gt;,&lt;a href="http://download.tensorflow.org/models/nmt/ende_gnmt_model_8_layer.zip"&gt;model 2 (8 层)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;newstest2014&lt;/th&gt;
&lt;th align="center"&gt;newstest2015&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Ours&lt;/em&gt; &amp;mdash; NMT + GNMT attention (4 layers)&lt;/td&gt;
&lt;td align="center"&gt;23.7&lt;/td&gt;
&lt;td align="center"&gt;26.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Ours&lt;/em&gt; &amp;mdash; NMT + GNMT attention (8 layers)&lt;/td&gt;
&lt;td align="center"&gt;24.4&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;27.6&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://matrix.statmt.org/"&gt;WMT SOTA&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;20.6&lt;/td&gt;
&lt;td align="center"&gt;24.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenNMT &lt;a href="https://arxiv.org/abs/1701.02810"&gt;(Klein et al., 2017)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;19.3&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tf-seq2seq &lt;a href="https://arxiv.org/abs/1703.03906"&gt;(Britz et al., 2017)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;22.2&lt;/td&gt;
&lt;td align="center"&gt;25.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GNMT &lt;a href="https://research.google.com/pubs/pub45610.html"&gt;(Wu et al., 2016)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;24.6&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;上面的结果表明，我们的模型在类似架构中有很强的竞争力。
注意，OpenNMT 使用较小的模型，而目前在 Transformer network &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Vaswani et al., 2017&lt;/a&gt;中获得的最佳结果为 28.4，不过这是明显不同的架构.&lt;/p&gt;
&lt;h2 id="qi ta zi yuan_1"&gt;其它资源&lt;/h2&gt;
&lt;p&gt;为深入了解神经机器翻译和序列到序列模型，我们强烈推荐下面的材料
&lt;a href="https://sites.google.com/site/acl16nmt/"&gt;Luong, Cho, Manning, (2016)&lt;/a&gt;;
&lt;a href="https://github.com/lmthang/thesis"&gt;Luong, (2016)&lt;/a&gt;;
and &lt;a href="https://arxiv.org/abs/1703.01619"&gt;Neubig, (2017)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;可用不同的工具构建 seq2seq 模型，我们每样选了一种: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stanford NMT &lt;a href="https://nlp.stanford.edu/projects/nmt/"&gt;https://nlp.stanford.edu/projects/nmt/&lt;/a&gt;&lt;em&gt;[Matlab]&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;tf-seq2seq &lt;a href="https://github.com/google/seq2seq"&gt;https://github.com/google/seq2seq&lt;/a&gt;&lt;em&gt;[TensorFlow]&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;Nemantus &lt;a href="https://github.com/rsennrich/nematus"&gt;https://github.com/rsennrich/nematus&lt;/a&gt;&lt;em&gt;[Theano]&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;OpenNMT &lt;a href="http://opennmt.net/"&gt;http://opennmt.net/&lt;/a&gt; &lt;em&gt;[Torch]&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zhi xie"&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们要感谢 Denny Britz, Anna Goldie, Derek Murray, 和 Cinjon Resnick 为 TensorFlow 和 seq2seq 库带来的新特性. 还要感谢 Lukasz Kaiser 在 seq2seq 代码库上最初的帮助; Quoc Le 提议复现一个 GNMT; Yonghui Wu 和 Zhifeng Chen 负责 GNMT 系统的细节; 同时还要感谢谷歌大脑 （Google Brain ）团队的支持和反馈!&lt;/p&gt;
&lt;h2 id="can kao"&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015.&lt;a href="https://arxiv.org/pdf/1409.0473.pdf"&gt; Neural machine translation by jointly learning to align and translate&lt;/a&gt;. ICLR.&lt;/li&gt;
&lt;li&gt;Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015.&lt;a href="https://arxiv.org/pdf/1508.04025.pdf"&gt; Effective approaches to attention-based neural machine translation&lt;/a&gt;. EMNLP.&lt;/li&gt;
&lt;li&gt;Ilya Sutskever, Oriol Vinyals, and Quoc
V. Le. 2014.&lt;a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"&gt; Sequence to sequence learning with neural networks&lt;/a&gt;. NIPS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="yi hou yu"&gt;译后语&lt;/h2&gt;
&lt;p&gt;这是我发表的第一篇译文，本来是自己想弄明白这份教程，进而把代码吃透，再顺便弄个自己的翻译系统当玩具。结果把原文读了几遍，发现仍然头晕脑胀，不知所云，毕竟专业术语太多，需要的背景知识也不少，索性一发狠就把它译了出来。人笨，大约花了5天时间。&lt;/p&gt;
&lt;p&gt;翻译是直接在原文上编辑，所以原文里的排版、链接、跳转都得以保留，而且还修改了原文中几个显示不正确的数学符号。文中的标点符号半角、全角混用，其实我倾向于全部用半角符号，因为如果有英文，采用全角的标点符号很丑。但大家知道的，中文输入法自动就出中文标点符号，于是索性不管了，遇到什么就是什么。发现很多术语如果翻出来，特别影响理解，比如本文中的&amp;ldquo;attention&amp;rdquo;、&amp;ldquo;embedding&amp;rdquo;，不管译成什么，放到句中一读，立即变成天书。思来想去，不如保留原文，你就把它当作一个标识，反而更容易理解。原文中还有些地方写得比较晦涩，我为了说清楚（其实是保证自己弄明白），要么意译，要么直接把人家一句扩成三句。哎，个中滋味，一把心酸泪。&lt;/p&gt;
&lt;p&gt;文中错误，再所难免，欢迎大家批评指正，我知错就改。我给本 blog 加了评论功能，方便大家讨论交流。该评论系统来自海外，如果你是天朝子民，可能需要翻墙。
(18.1.31补：把需要翻墙的评论关了，因为发现如果在墙内，评论代码会影响右侧目录的功能，而方便阅读才是我最想要的）&lt;/p&gt;
&lt;p&gt;最后应个景，如果有读者想用力鼓励我一下，欢迎&lt;a href="/pages/da-shang.html"&gt;打赏&lt;/a&gt; 。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>艾波寧捎信</title><link href="https://freeopen.github.io/posts/ai-bo-zhu-shao-xin" rel="alternate"></link><published>2017-06-11T00:00:00+08:00</published><updated>2017-06-11T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2017-06-11:/posts/ai-bo-zhu-shao-xin</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;由台大葉丙成老師的学生创作的概率问题，文章本身比问题更有趣。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;巴黎, 1832 年 6 月, 一個飄著毛雨的濕熱夜晚。&lt;/p&gt;
&lt;p&gt;一間破舊的旅社, 歪一邊的招牌寫著「德納第客棧」(Thénardier&amp;rsquo;s)。
一樓的大廳裡, &amp;mdash;&amp;mdash; 說大廳, 其實只是兩張沙發跟一個小圓桌組成的狹窄空間 &amp;mdash;&amp;mdash; 只有坐著的兩位老友,
他們多年不見, 正在暢談。&lt;/p&gt;
&lt;p&gt;留著滿臉花白大鬍子跟八字鬚,滔滔不絕的是作家維克托.雨果(Victor Hugo)。
旁邊靜靜聽著的那人是雨果的朋友, 一位正直的英國詩人。
他年輕許多,留著捲髮,有深 邃的眼神跟鷹鉤鼻。&lt;/p&gt;
&lt;p&gt;「最近交稿期限又要到了,」雨果說,
「我的靈感卻像這個酒杯一樣枯竭呀。老闆娘! 來一瓶上好的波本威士忌,再來一盤肉乾下酒。」&lt;/p&gt;
&lt;p&gt;一直低頭擦盤子的老闆娘放下工作, 來招呼她的顧客。&lt;/p&gt;
&lt;p&gt;「我也何嘗不是入不敷出。」英國詩人說。「畢竟作詩這種東西又不能當飯吃。」&lt;/p&gt;
&lt;p&gt;這時有人推開嘎吱作響的木門, 匆忙走進旅社, 靠近沙發上的兩人。
他的臉有點圓而結實, 額上的皺紋大概是常年經過思考的結果。
「維克托! 」他喊道。&lt;/p&gt;
&lt;p&gt;「大數學家西米翁 …&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;由台大葉丙成老師的学生创作的概率问题，文章本身比问题更有趣。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;巴黎, 1832 年 6 月, 一個飄著毛雨的濕熱夜晚。&lt;/p&gt;
&lt;p&gt;一間破舊的旅社, 歪一邊的招牌寫著「德納第客棧」(Thénardier&amp;rsquo;s)。
一樓的大廳裡, &amp;mdash;&amp;mdash; 說大廳, 其實只是兩張沙發跟一個小圓桌組成的狹窄空間 &amp;mdash;&amp;mdash; 只有坐著的兩位老友,
他們多年不見, 正在暢談。&lt;/p&gt;
&lt;p&gt;留著滿臉花白大鬍子跟八字鬚,滔滔不絕的是作家維克托.雨果(Victor Hugo)。
旁邊靜靜聽著的那人是雨果的朋友, 一位正直的英國詩人。
他年輕許多,留著捲髮,有深 邃的眼神跟鷹鉤鼻。&lt;/p&gt;
&lt;p&gt;「最近交稿期限又要到了,」雨果說,
「我的靈感卻像這個酒杯一樣枯竭呀。老闆娘! 來一瓶上好的波本威士忌,再來一盤肉乾下酒。」&lt;/p&gt;
&lt;p&gt;一直低頭擦盤子的老闆娘放下工作, 來招呼她的顧客。&lt;/p&gt;
&lt;p&gt;「我也何嘗不是入不敷出。」英國詩人說。「畢竟作詩這種東西又不能當飯吃。」&lt;/p&gt;
&lt;p&gt;這時有人推開嘎吱作響的木門, 匆忙走進旅社, 靠近沙發上的兩人。
他的臉有點圓而結實, 額上的皺紋大概是常年經過思考的結果。
「維克托! 」他喊道。&lt;/p&gt;
&lt;p&gt;「大數學家西米翁.泊松(Siméon Poisson)駕到啦! 」雨果調侃道。
「幾年沒見了? 怎麼氣喘吁吁的? 」&lt;/p&gt;
&lt;p&gt;「呼...呼...都半百年紀了,一把老骨頭了。」泊松說道, 揉了揉背。&lt;/p&gt;
&lt;p&gt;「下禮拜就是你 51 歲生日了吧? 順便慶祝一下? 」雨果真是那壺不開提那壺。&lt;/p&gt;
&lt;p&gt;「別提了....」泊松說。「咦? 我剛才要說什麼? 對了, 警察把學生叛軍逼到絕路,
剩下一個據點了。附近的街道都封起來, 說經過的格殺勿論! 大家要小心。」 &lt;/p&gt;
&lt;p&gt;「學生就是這樣, 因為一點小事動不動抗議。」雨果說。「哎,這個社會最近真的
很亂。聽說有個逃犯叫尚萬強(Jean-Valjean), 非常危險, 本以為被抓了, 原來是弄錯人。」&lt;/p&gt;
&lt;p&gt;「這種人最好關到死。」泊松道。「大革命之後已經亂了三十幾年了, 不用嚴刑峻法怎麼治國? 」他舉杯。「乾啦! 」&lt;/p&gt;
&lt;p&gt;門外的街道上,一個棕髮女子戴著毛氈帽,穿土黃色大衣,正快步經過。
「如果我 們之間從來都不可能,」她喃喃自語道,「告白只會讓他困擾吧? 
我想著他,跟他想著 她,有什麼不同? 也許我只是個好人,」她踢了踢年久失修的鋪路的石版。
「那就讓我 再作一次好人吧。假如我是他&amp;mdash;&amp;mdash;寬宏大量的革命領袖,馬里歐斯(Marius) &amp;mdash;&amp;mdash;」
她呼了一口氣,不知是因為喘還是憤怒。
「又何嘗不想把這封訣別的情書交到柯賽特 (Cosette)手裡?」她的手一直埋在大衣口袋裡, 這時終於伸出。
月色下,隱約見到她原來是捏著一封信,汗都快把封蠟融化了。
她繼續沒入巷弄陰影中。&lt;/p&gt;
&lt;p&gt;砰! 老闆娘手裡的盤子摔到地上, 引起三人的注意。「艾波寧(Éponine)! 艾波寧!」 老闆娘頭探出窗外叫道,「你要去哪?」&lt;/p&gt;
&lt;p&gt;幾陣槍響讓室內一片寂靜, 只剩下老掛鐘的滴答聲渾然不覺於此。英國詩人坐得離
窗戶最遠,也把頭湊近。可是那女孩早已隱沒在黑夜中。&lt;/p&gt;
&lt;p&gt;「我看她凶多吉少。」雨果道, 又啜了一口威士忌。&lt;/p&gt;
&lt;p&gt;「不會吧? 」詩人說, 也喝了口咖啡。「政府財務拮据,那槍也不知道放幾年了, 又在黑夜當中,我懷疑 20 秒能不能打中一發! 」
他想開個玩笑,兩位朋友卻沒有反應。&lt;/p&gt;
&lt;p&gt;「你估的差不多。」泊松說,「其實我剛才一直在注意士兵的命中率,」他敲了敲懷錶的錶面, 眼睛亮了起來。
「讓我們來作個假設: 士兵是否擊中她,是隨時間的均 勻泊松事件(Poisson process),參數也不隨距離變化。」&lt;/p&gt;
&lt;p&gt;「又在炫耀以他為名的分布,」詩人咕噥著。&lt;/p&gt;
&lt;p&gt;「她被擊中之後,」雨果也被引起興趣了,「應該還可以走一段路。那就假設她每中一槍, 速度就減半,
直到中 4 槍時當場斃命。那她成功穿過巷子的機率是多少,你算得出來嗎? 」&lt;/p&gt;
&lt;p&gt;「小意思! 」 泊松答道,「為了方便計算,令 &amp;mu; = 100 / ln( 50 ) = 25.5622,再令這條巷子長 400 m。
她以 4 m/s 的初速出發, 而且一出巷子就不會被擊中, 好嗎? 」 雨果點頭。&lt;/p&gt;
&lt;p&gt;「你有沒有在聽啊? 」泊松問詩人, 後者一直低頭查字典。&lt;/p&gt;
&lt;p&gt;「感謝那個狙擊手! 我又有靈感了。」詩人抬起頭說,「你們要聽聽嗎? 」他等不及兩人回應, 便拿出羽毛筆, 順手沾了咖啡, 拿起泛黃的報紙,邊寫邊念。他秀麗的字 跡活像中世紀手稿。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Neath lamplights as &amp;lsquo;Ponine roved, on her own        當艾波宁形單影隻地漫步,沿著街燈

Professing to thin air (her absent crush)            向空氣(她不 在場的心上人)吐露

What each heart-broken maid, e&amp;rsquo;en lad, had known,    所有心碎女孩(甚至男孩)都懂的心聲,

The rain her tears wiped; Seine, weeping a gush,     雨水拭去她眼泪;塞納河洪流如注, 

Too called the freedom fighter who did fix           一起哭著呼喚自由鬥士: 他鎖定目光,

His eyes on noosing curly locks &amp;mdash; not wars           向柯賽特能絞死人的捲髮,

He led &amp;mdash; of blonde Cosette with rosy cheeks          而非戰爭。弟兄的血塊與刀傷,

Bloodier than brethren&amp;rsquo;s gore ignored and sores.     血紅色比不上金髮女粉紅雙頰。

O singing swan! Whose speed, once shot, would halve; 歌唱的天鵝! (一被擊中速度就減半) 

Must you this letter send to Hades black?            為何定要送這信到黑暗的冥府?

With Poisson bein&amp;rsquo; the butcher&amp;rsquo;s PMF,                助紂為虐的子彈不會收回刀尖,

Abetting bullets take no blade aback!                因為泊松分布是屠夫的機率質量函数 !

Cruel witnesses! Ask Probability,                    無情的目擊者! 去請求&amp;ldquo;機率&amp;rdquo; 

Still speechless, to relate the tragedy.             (他仍啞口無言)訴说這場悲劇。
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;「講法文啦,我聽不懂。」泊松還在低頭計算。&lt;/p&gt;
&lt;p&gt;「寫的真好! 」雨果言不由衷地附和著。「我知道! 這是...」他說, 想要裝懂。 
「義大利八行體(octave)? 法國十九行體(villanelle)? 」&lt;/p&gt;
&lt;p&gt;「你連幾行都數不出來嗎? 你根本沒在聽吧? 」詩人激動地站起來,把咖啡弄倒 了。&lt;/p&gt;
&lt;p&gt;「你如果真的關心社會,」雨果說, 生氣了。「為什麼不像拜倫(Byron)一樣參與希臘革命, 卻在這裡大放厥詞? 」&lt;/p&gt;
&lt;p&gt;「那你呢? 」詩人回嘴,「自以為在揭露陰暗面,還不是坐在這裡喝咖啡! 」&lt;/p&gt;
&lt;p&gt;「誰跟你社會寫實了? 」雨果說,「只要把情節弄聳動一點,能賣錢就好啦! 」他趁老闆娘不注意, 別過頭,啐了一口煙草到地上。&lt;/p&gt;
&lt;p&gt;「我算出來了! 」泊松趕緊轉移話題。「我跟你講,先算恰中 n 槍的機率,再遞迴 地算中第 n + 1 槍的機率。我真是天才! 哈哈哈!」&lt;/p&gt;
&lt;p&gt;「可是這個積分好難算喔。」雨果湊過去看。&lt;/p&gt;
&lt;p&gt;「你笨耶! 先作個變數變換,然後這樣...這樣...」他指著紙上潦草的鬼畫符說。 
「嗯...嗯...」詩人應道,但是根本沒在看。
「這個肉乾到底是貓肉還是馬肉呀? 我突然覺得肚子不太舒服,先去休息了。」
看到兩位朋友對他的詩興趣索然,詩人有點不悅,找了藉口。&lt;/p&gt;
&lt;p&gt;「老鼠肉。」老闆娘低聲道, 小聲到三人沒有聽見。她早已恢復平靜,繼續撿地上的碎片。&lt;/p&gt;
&lt;p&gt;「其實我也覺得胃有點痛。」泊松道,看起來不像是裝的。瞪著天花板沈思許久的雨果起身。
「都多虧你的詩,」他說,「又讓我有一個長篇 小說的題材了! 我得趁忘記以前熬夜把大綱寫下來。」
他起身。「明天見囉! 到時再帶 你去參觀奧賽美術館(Musée d'Orsay)。」
他跟詩人說。「只是要繞過抗議的學生就是。」&lt;/p&gt;
&lt;p&gt;「他們真的很煩。」泊松附和道。&lt;/p&gt;
&lt;p&gt;「對呀。」雨果說。「我們的詩人朋友難得來拜訪, 當然要帶他好好參觀這個古都呀! 偏偏碰到這種事。」&lt;/p&gt;
&lt;p&gt;互道晚安後,一行人各自上樓回房了。當旅社最後一盞燈暗下,花都巴黎又恢復了
平日的寂靜。除了遠處群眾隱約的喧鬧聲,還有槍枝的星火與夜空中的星光相映。&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;如上, 巷子呈直線, 長 L0 = 400 m, 艾波宁以 v0 = 4 m/s 初速等速穿越。
士兵時時刻刻瞄準她; 第 t 秒時是否擊中她,是隨時間 t 的均勻的泊松事件(Poisson process), 且與距离無關。
其中, 平均每 &amp;mu; 秒能擊中一次, &amp;mu; = 100 / ln( 50 ) 約為 25.5622。
士兵無法 擊中巷子以外的區域; 另外,只要她處於巷中, &amp;mu; 就是常数 。&lt;/p&gt;
&lt;p&gt;當她每被擊中一槍, 速度就會減半; 直到她恰中 4 槍時, 會當場死亡。
亦即, 中 n 槍時速依序為 4、2、1、0.5 m/s,其中 n 依序為 0、 1、 2、 3。&lt;/p&gt;
&lt;p&gt;請問艾波宁成功捎信的機率為何? 亦即, 在她處於巷子之中時, 被射中低於四槍的機率為何?&lt;/p&gt;</content></entry><entry><title>编程的智慧</title><link href="https://freeopen.github.io/posts/bian-cheng-de-zhi-hui" rel="alternate"></link><published>2017-05-16T00:00:00+08:00</published><updated>2017-05-16T00:00:00+08:00</updated><author><name>王垠</name></author><id>tag:freeopen.github.io,2017-05-16:/posts/bian-cheng-de-zhi-hui</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;转自&lt;a href="http://www.yinwang.org/blog-cn/2015/11/21/programming-philosophy"&gt;王垠博客&lt;/a&gt; 2015-11-21 的旧文，作为程序语言专家，此君的建议很值得借鉴和参考。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;编程是一种创造性的工作，是一门艺术。精通任何一门艺术，都需要很多的练习和领悟，所
以这里提出的&amp;ldquo;智慧&amp;rdquo;，并不是号称一天瘦十斤的减肥药，它并不能代替你自己的勤奋。然而
由于软件行业喜欢标新立异，喜欢把简单的事情搞复杂，我希望这些文字能给迷惑中的人们
指出一些正确的方向，让他们少走一些弯路，基本做到一分耕耘一分收获。&lt;/p&gt;
&lt;h2 id="fan fu tui qiao dai ma"&gt;反复推敲代码&lt;/h2&gt;
&lt;p&gt;既然&amp;ldquo;天才是百分之一的灵感，百分之九十九的汗水&amp;rdquo;，那我先来谈谈这汗水的部分吧。有人
问我，提高编程水平最有效的办法是什么？我想了很久，终于发现最有效的办法，其实是反
反复复地修改和推敲代码。&lt;/p&gt;
&lt;p&gt;在IU的时候，由于Dan Friedman的严格教导，我们以写出冗长复杂的代码为耻。如果你代码
多写了几行，这老顽童就会大笑，说：&amp;ldquo;当年我解决这个问题，只写了5行代码，你回去再想
想吧&amp;hellip;&amp;hellip;&amp;rdquo; 当然，有时候他只是夸张一下，故意刺激你的，其实没有人能只用5行代码完成 …&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;转自&lt;a href="http://www.yinwang.org/blog-cn/2015/11/21/programming-philosophy"&gt;王垠博客&lt;/a&gt; 2015-11-21 的旧文，作为程序语言专家，此君的建议很值得借鉴和参考。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;编程是一种创造性的工作，是一门艺术。精通任何一门艺术，都需要很多的练习和领悟，所
以这里提出的&amp;ldquo;智慧&amp;rdquo;，并不是号称一天瘦十斤的减肥药，它并不能代替你自己的勤奋。然而
由于软件行业喜欢标新立异，喜欢把简单的事情搞复杂，我希望这些文字能给迷惑中的人们
指出一些正确的方向，让他们少走一些弯路，基本做到一分耕耘一分收获。&lt;/p&gt;
&lt;h2 id="fan fu tui qiao dai ma"&gt;反复推敲代码&lt;/h2&gt;
&lt;p&gt;既然&amp;ldquo;天才是百分之一的灵感，百分之九十九的汗水&amp;rdquo;，那我先来谈谈这汗水的部分吧。有人
问我，提高编程水平最有效的办法是什么？我想了很久，终于发现最有效的办法，其实是反
反复复地修改和推敲代码。&lt;/p&gt;
&lt;p&gt;在IU的时候，由于Dan Friedman的严格教导，我们以写出冗长复杂的代码为耻。如果你代码
多写了几行，这老顽童就会大笑，说：&amp;ldquo;当年我解决这个问题，只写了5行代码，你回去再想
想吧&amp;hellip;&amp;hellip;&amp;rdquo; 当然，有时候他只是夸张一下，故意刺激你的，其实没有人能只用5行代码完成。
然而这种提炼代码，减少冗余的习惯，却由此深入了我的骨髓。&lt;/p&gt;
&lt;p&gt;有些人喜欢炫耀自己写了多少多少万行的代码，仿佛代码的数量是衡量编程水平的标准。然
而，如果你总是匆匆写出代码，却从来不回头去推敲，修改和提炼，其实是不可能提高编程
水平的。你会制造出越来越多平庸甚至糟糕的代码。在这种意义上，很多人所谓的&amp;ldquo;工作经验&amp;rdquo;，
跟他代码的质量，其实不一定成正比。如果有几十年的工作经验，却从来不回头去提炼和反
思自己的代码，那么他也许还不如一个只有一两年经验，却喜欢反复推敲，仔细领悟的人。&lt;/p&gt;
&lt;p&gt;有位文豪说得好：&amp;ldquo;看一个作家的水平，不是看他发表了多少文字，而要看他的废纸篓里扔掉
了多少。&amp;rdquo; 我觉得同样的理论适用于编程。好的程序员，他们删掉的代码，比留下来的还要多很多。
如果你看见一个人写了很多代码，却没有删掉多少，那他的代码一定有很多垃圾。&lt;/p&gt;
&lt;p&gt;就像文学作品一样，代码是不可能一蹴而就的。灵感似乎总是零零星星，陆陆续续到来的。
任何人都不可能一笔呵成，就算再厉害的程序员，也需要经过一段时间，才能发现最简单优雅
的写法。有时候你反复提炼一段代码，觉得到了顶峰，没法再改进了，可是过了几个月再回头
来看，又发现好多可以改进和简化的地方。这跟写文章一模一样，回头看几个月或者几年前写
的东西，你总能发现一些改进。&lt;/p&gt;
&lt;p&gt;所以如果反复提炼代码已经不再有进展，那么你可以暂时把它放下。过几个星期或者几个月
再回头来看，也许就有焕然一新的灵感。这样反反复复很多次之后，你就积累起了灵感和智
慧，从而能够在遇到新问题的时候直接朝正确，或者接近正确的方向前进。&lt;/p&gt;
&lt;h2 id="xie you ya de dai ma"&gt;写优雅的代码&lt;/h2&gt;
&lt;p&gt;人们都讨厌&amp;ldquo;面条代码&amp;rdquo;（spaghetti code），因为它就像面条一样绕来绕去，没法理清头绪。
那么优雅的代码一般是什么形状的呢？经过多年的观察，我发现优雅的代码，在形状上有一些明显的特征。&lt;/p&gt;
&lt;p&gt;如果我们忽略具体的内容，从大体结构上来看，优雅的代码看起来就像是一些整整齐齐，套
在一起的盒子。如果跟整理房间做一个类比，就很容易理解。如果你把所有物品都丢在一个
很大的抽屉里，那么它们就会全都混在一起。你就很难整理，很难迅速的找到需要的东西。
但是如果你在抽屉里再放几个小盒子，把物品分门别类放进去，那么它们就不会到处乱跑，
你就可以比较容易的找到和管理它们。&lt;/p&gt;
&lt;p&gt;优雅的代码的另一个特征是，它的逻辑大体上看起来，是枝丫分明的树状结构（tree）。这
是因为程序所做的几乎一切事情，都是信息的传递和分支。你可以把代码看成是一个电路，
电流经过导线，分流或者汇合。如果你是这样思考的，你的代码里就会比较少出现只有一个
分支的 if 语句，它看起来就会像这个样子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意到了吗？在我的代码里面，if 语句几乎总是有两个分支。它们有可能嵌套，有多层的缩进，
而且else分支里面有可能出现少量重复的代码。然而这样的结构，逻辑却非常严密和清晰。
在后面我会告诉你为什么 if 语句最好有两个分支。&lt;/p&gt;
&lt;h2 id="xie mo kuai hua de dai ma"&gt;写模块化的代码&lt;/h2&gt;
&lt;p&gt;有些人吵着闹着要让程序&amp;ldquo;模块化&amp;rdquo;，结果他们的做法是把代码分部到多个文件和目录里面，
然后把这些目录或者文件叫做&amp;ldquo;module&amp;rdquo;。他们甚至把这些目录分放在不同的VCS repo里面。
结果这样的作法并没有带来合作的流畅，而是带来了许多的麻烦。
这是因为他们其实并不理解什么叫做&amp;ldquo;模块&amp;rdquo;，肤浅的把代码切割开来，分放在不同的位置，
其实非但不能达到模块化的目的，而且制造了不必要的麻烦。&lt;/p&gt;
&lt;p&gt;真正的模块化，并不是文本意义上的，而是逻辑意义上的。一个模块应该像一个电路芯片，
它有定义良好的输入和输出。实际上一种很好的模块化方法早已经存在，它的名字叫做&amp;ldquo;函数&amp;rdquo;。
每一个函数都有明确的输入（参数）和输出（返回值），同一个文件里可以包含多个函数，
所以你其实根本不需要把代码分开在多个文件或者目录里面，同样可以完成代码的模块化。
我可以把代码全都写在同一个文件里，却仍然是非常模块化的代码。&lt;/p&gt;
&lt;p&gt;想要达到很好的模块化，你需要做到以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;避免写太长的函数。如果发现函数太大了，就应该把它拆分成几个更小的。
  通常我写的函数长度都不超过40行。对比一下，一般笔记本电脑屏幕所能容纳的代码行数是50行。
  我可以一目了然的看见一个40行的函数，而不需要滚屏。只有40行而不是50行的原因是，
  我的眼球不转的话，最大的视角只看得到40行代码。&lt;/p&gt;
&lt;p&gt;如果我看代码不转眼球的话，我就能把整片代码完整的映射到我的视觉神经里，这样就算忽然闭上眼睛，
我也能看得见这段代码。我发现闭上眼睛的时候，大脑能够更加有效地处理代码，
你能想象这段代码可以变成什么其它的形状。40行并不是一个很大的限制，因为函数里面比较复杂的部分，
往往早就被我提取出去，做成了更小的函数，然后从原来的函数里面调用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;制造小的工具函数。如果你仔细观察代码，就会发现其实里面有很多的重复。
  这些常用的代码，不管它有多短，提取出去做成函数，都可能是会有好处的。
  有些帮助函数也许就只有两行，然而它们却能大大简化主要函数里面的逻辑。&lt;/p&gt;
&lt;p&gt;有些人不喜欢使用小的函数，因为他们想避免函数调用的开销，结果他们写出几百行之大的函数。
这是一种过时的观念。现代的编译器都能自动的把小的函数内联（inline）到调用它的地方，
所以根本不产生函数调用，也就不会产生任何多余的开销。&lt;/p&gt;
&lt;p&gt;同样的一些人，也爱使用宏（macro）来代替小函数，这也是一种过时的观念。
在早期的C语言编译器里，只有宏是静态&amp;ldquo;内联&amp;rdquo;的，所以他们使用宏，其实是为了达到内联的目的。
然而能否内联，其实并不是宏与函数的根本区别。宏与函数有着巨大的区别（这个我以后再讲），
应该尽量避免使用宏。为了内联而使用宏，其实是滥用了宏，这会引起各种各样的麻烦，比如使程
序难以理解，难以调试，容易出错等等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个函数只做一件简单的事情。有些人喜欢制造一些&amp;ldquo;通用&amp;rdquo;的函数，既可以做这个又可以做那个，
  它的内部依据某些变量和条件，来&amp;ldquo;选择&amp;rdquo;这个函数所要做的事情。比如，你也许写出这样的函数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;getOS&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;equals&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"MacOS"&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;getOS&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;equals&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"MacOS"&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;写这个函数的人，根据系统是否为&amp;ldquo;MacOS&amp;rdquo;来做不同的事情。你可以看出这个函数里，
其实只有&lt;code&gt;c()&lt;/code&gt;是两种系统共有的，而其它的&lt;code&gt;a(), b(), d(), e()&lt;/code&gt;都属于不同的分支。&lt;/p&gt;
&lt;p&gt;这种&amp;ldquo;复用&amp;rdquo;其实是有害的。如果一个函数可能做两种事情，它们之间共同点少于它们的不同点，
那你最好就写两个不同的函数，否则这个函数的逻辑就不会很清晰，容易出现错误。其实，上面这个函数可以改写成两个函数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;fooMacOS&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;和&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;fooOther&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你发现两件事情大部分内容相同，只有少数不同，多半时候你可以把相同的部分提取出去，做成一个辅助函数。比如，如果你有个函数是这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;getOS&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;equals&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"MacOS"&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中&lt;code&gt;a()，b()，c()&lt;/code&gt;都是一样的，只有&lt;code&gt;d()&lt;/code&gt;和&lt;code&gt;e()&lt;/code&gt;根据系统有所不同。那么你可以把&lt;code&gt;a()，b()，c()&lt;/code&gt;提取出去：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;preFoo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后制造两个函数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;fooMacOS&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;preFoo&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;和&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;fooOther&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;preFoo&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样一来，我们既共享了代码，又做到了每个函数只做一件简单的事情。这样的代码，逻辑就更加清晰。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;避免使用全局变量和类成员（class member）来传递信息，尽量使用局部变量和参数。有些人写代码，经常用类成员来传递信息，就像这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;A&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;findX&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;findX&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;首先，他使用&lt;code&gt;findX()&lt;/code&gt;，把一个值写入成员&lt;code&gt;x&lt;/code&gt;。然后，使用&lt;code&gt;x&lt;/code&gt;的值。这样，&lt;code&gt;x&lt;/code&gt;就变
成了&lt;code&gt;findX&lt;/code&gt;和&lt;code&gt;print&lt;/code&gt;之间的数据通道。由于&lt;code&gt;x&lt;/code&gt;属于&lt;code&gt;class A&lt;/code&gt;，这样程序就失去了模块化的结构。
由于这两个函数依赖于成员&lt;code&gt;x&lt;/code&gt;，它们不再有明确的输入和输出，而是依赖全局的数据。
&lt;code&gt;findX&lt;/code&gt;和&lt;code&gt;foo&lt;/code&gt;不再能够离开&lt;code&gt;class A&lt;/code&gt;而存在，而且由于类成员还有可能被其他代码改
变，代码变得难以理解，难以确保正确性。&lt;/p&gt;
&lt;p&gt;如果你使用局部变量而不是类成员来传递信息，那么这两个函数就不需要依赖于某一个
&lt;code&gt;class&lt;/code&gt;，而且更加容易理解，不易出错：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;findX&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;findX&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="xie ke du de dai ma"&gt;写可读的代码&lt;/h2&gt;
&lt;p&gt;有些人以为写很多注释就可以让代码更加可读，然而却发现事与愿违。注释不但没能让代码
变得可读，反而由于大量的注释充斥在代码中间，让程序变得障眼难读。而且代码的逻辑一
旦修改，就会有很多的注释变得过时，需要更新。修改注释是相当大的负担，所以大量的注
释，反而成为了妨碍改进代码的绊脚石。&lt;/p&gt;
&lt;p&gt;实际上，真正优雅可读的代码，是几乎不需要注释的。如果你发现需要写很多注释，那么你
的代码肯定是含混晦涩，逻辑不清晰的。其实，程序语言相比自然语言，是更加强大而严谨
的，它其实具有自然语言最主要的元素：主语，谓语，宾语，名词，动词，如果，那么，否
则，是，不是，&amp;hellip;&amp;hellip; 所以如果你充分利用了程序语言的表达能力，你完全可以用程序本身来表
达它到底在干什么，而不需要自然语言的辅助。&lt;/p&gt;
&lt;p&gt;有少数的时候，你也许会为了绕过其他一些代码的设计问题，采用一些违反直觉的作法。这
时候你可以使用很短注释，说明为什么要写成那奇怪的样子。这样的情况应该少出现，否则
这意味着整个代码的设计都有问题。&lt;/p&gt;
&lt;p&gt;如果没能合理利用程序语言提供的优势，你会发现程序还是很难懂，以至于需要写注释。所
以我现在告诉你一些要点，也许可以帮助你大大减少写注释的必要：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用有意义的函数和变量名字。如果你的函数和变量的名字，能够切实的描述它们的逻
  辑，那么你就不需要写注释来解释它在干什么。比如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;// put elephant1 into fridge2&lt;/span&gt;
&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elephant1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fridge2&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;由于我的函数名&lt;code&gt;put&lt;/code&gt;，加上两个有意义的变量名&lt;code&gt;elephant1&lt;/code&gt;和&lt;code&gt;fridge2&lt;/code&gt;，
已经说明了这是在干什么（把大象放进冰箱），所以上面那句注释完全没有必要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;局部变量应该尽量接近使用它的地方。有些人喜欢在函数最开头定义很多局部变量，然
  后在下面很远的地方使用它，就像这个样子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...;&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;由于这中间都没有使用过&lt;code&gt;index&lt;/code&gt;，也没有改变过它所依赖的数据，所以这个变量定义，其实可以挪到接近使用它的地方：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...;&lt;/span&gt;
    &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你看透了局部变量的本质&amp;mdash;&amp;mdash;它们就是电路里的导线，那你就能更好的理解近距离的好处。
变量定义离用的地方越近，导线的长度就越短。你不需要摸着一根导线，绕来绕去找很远，就能发现接收它的端口，这样的电路就更容易理解。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;局部变量名字应该简短。这貌似跟第一点相冲突，简短的变量名怎么可能有意义呢？注意我这
  里说的是局部变量，因为它们处于局部，再加上第2点已经把它放到离使用位置尽量近的地方，所
  以根据上下文你就会容易知道它的意思：&lt;/p&gt;
&lt;p&gt;比如，你有一个局部变量，表示一个操作是否成功：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;successInDeleteFile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deleteFile&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"foo.txt"&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;successInDeleteFile&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这个局部变量&lt;code&gt;successInDeleteFile&lt;/code&gt;大可不必这么啰嗦。因为它只用过一次，而且用它的地方就
在下面一行，所以读者可以轻松发现它是&lt;code&gt;deleteFile&lt;/code&gt;返回的结果。如果你把它改名为&lt;code&gt;success&lt;/code&gt;，
其实读者根据一点上下文，也知道它表示"success in deleteFile"。所以你可以把它改成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;success&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deleteFile&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"foo.txt"&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;success&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样的写法不但没漏掉任何有用的语义信息，而且更加易读。&lt;code&gt;successInDeleteFile&lt;/code&gt;这种
&amp;ldquo;camelCase&amp;rdquo;，如果超过了三个单词连在一起，其实是很碍眼的东西。所以如果你能用一个单词表示同样的意义，那当然更好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不要重用局部变量。很多人写代码不喜欢定义新的局部变量，而喜欢&amp;ldquo;重用&amp;rdquo;同一个局部变量，
  通过反复对它们进行赋值，来表示完全不同意思。比如这样写：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"succeed"&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;info&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"failed"&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;info&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;虽然这样在逻辑上是没有问题的，然而却不易理解，容易混淆。变量&lt;code&gt;msg&lt;/code&gt;两次被赋值，
表示完全不同的两个值。它们立即被&lt;code&gt;log.info&lt;/code&gt;使用，没有传递到其它地方去。这种赋值的做法，
把局部变量的作用域不必要的增大，让人以为它可能在将来改变，也许会在其它地方被使用。
更好的做法，其实是定义两个变量：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"succeed"&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;info&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"failed"&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;info&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;由于这两个&lt;code&gt;msg&lt;/code&gt;变量的作用域仅限于它们所处的&lt;code&gt;if&lt;/code&gt;语句分支，你可以很清楚的看到这两个&lt;code&gt;msg&lt;/code&gt;被使用的范围，而且知道它们之间没有任何关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把复杂的逻辑提取出去，做成&amp;ldquo;帮助函数&amp;rdquo;。有些人写的函数很长，以至于看不清楚里面的语句在干什么，
  所以他们误以为需要写注释。如果你仔细观察这些代码，就会发现不清晰的那片代码，往往可以被提取出去，
  做成一个函数，然后在原来的地方调用。由于函数有一个名字，这样你就可以使用有意义的函数名来代替注释。举一个例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="c1"&gt;// put elephant1 into fridge2&lt;/span&gt;
&lt;span class="n"&gt;openDoor&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fridge2&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt; &lt;span class="n"&gt;elephant1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;alive&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;closeDoor&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fridge2&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你把这片代码提出去定义成一个函数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;put&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Elephant&lt;/span&gt; &lt;span class="n"&gt;elephant&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Fridge&lt;/span&gt; &lt;span class="n"&gt;fridge&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;openDoor&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fridge&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elephant&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;alive&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;closeDoor&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fridge&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样原来的代码就可以改成：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elephant1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fridge2&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;更加清晰，而且注释也没必要了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把复杂的表达式提取出去，做成中间变量。有些人听说&amp;ldquo;函数式编程&amp;rdquo;是个好东西，
  也不理解它的真正含义，就在代码里大量使用嵌套的函数。像这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Pizza&lt;/span&gt; &lt;span class="n"&gt;pizza&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;makePizza&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;crust&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;salt&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;butter&lt;/span&gt;&lt;span class="o"&gt;()),&lt;/span&gt;
        &lt;span class="n"&gt;topping&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;onion&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;tomato&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;sausage&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样的代码一行太长，而且嵌套太多，不容易看清楚。其实训练有素的函数式程序员，
都知道中间变量的好处，不会盲目的使用嵌套的函数。他们会把这代码变成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Crust&lt;/span&gt; &lt;span class="n"&gt;crust&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;crust&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;salt&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;butter&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
&lt;span class="n"&gt;Topping&lt;/span&gt; &lt;span class="n"&gt;topping&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;topping&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;onion&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;tomato&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;sausage&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
&lt;span class="n"&gt;Pizza&lt;/span&gt; &lt;span class="n"&gt;pizza&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;makePizza&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;crust&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;topping&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样写，不但有效地控制了单行代码的长度，而且由于引入的中间变量具有&amp;ldquo;意义&amp;rdquo;，步骤清晰，变得很容易理解。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在合理的地方换行。对于绝大部分的程序语言，代码的逻辑是和空白字符无关的，
  所以你可以在几乎任何地方换行，你也可以不换行。这样的语言设计是个好东西，
  因为它给了程序员自由控制自己代码格式的能力。然而，它也引起了一些问题，因为很多人不知道如何合理的换行。&lt;/p&gt;
&lt;p&gt;有些人喜欢利用IDE的自动换行机制，编辑之后用一个热键把整个代码重新格式化一遍，
IDE就会把超过行宽限制的代码自动折行。可是这种自动这行，往往没有根据代码的逻辑来进行，
不能帮助理解代码。自动换行之后可能产生这样的代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;someLongCondition1&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;someLongCondition2&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;someLongCondition3&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;someLongCondition4&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;由于&lt;code&gt;someLongCondition4()&lt;/code&gt;超过了行宽限制，被编辑器自动换到了下面一行。虽然满足了行宽限制，
换行的位置却是相当任意的，它并不能帮助人理解这代码的逻辑。这几个&lt;code&gt;boolean&lt;/code&gt;表达式，全都用
&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;连接，所以它们其实处于平等的地位。为了表达这一点，当需要折行的时候，你应该把每一个表
达式都放到新的一行，就像这个样子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;someLongCondition1&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;someLongCondition2&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;someLongCondition3&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;someLongCondition4&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样每一个条件都对齐，里面的逻辑就很清楚了。再举个例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;info&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"failed to find file {} for command {}, with exception {}"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;exception&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这行因为太长，被自动折行成这个样子。&lt;code&gt;file，command&lt;/code&gt;和&lt;code&gt;exception&lt;/code&gt;本来是同一类东西，
却有两个留在了第一行，最后一个被折到第二行。它就不如手动换行成这个样子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;info&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"failed to find file {} for command {}, with exception {}"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exception&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;把格式字符串单独放在一行，而把它的参数一并放在另外一行，这样逻辑就更加清晰。&lt;/p&gt;
&lt;p&gt;为了避免IDE把这些手动调整好的换行弄乱，很多IDE（比如IntelliJ）的自动格式化设定里都有
&amp;ldquo;保留原来的换行符&amp;rdquo;的设定。如果你发现IDE的换行不符合逻辑，你可以修改这些设定，然后在某些地方保留你自己的手动换行。&lt;/p&gt;
&lt;p&gt;说到这里，我必须警告你，这里所说的&amp;ldquo;不需注释，让代码自己解释自己&amp;rdquo;，并不是说要让代码看起来像某种自然语言。
有个叫 Chai 的 JavaScript 测试工具，可以让你这样写代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;to&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;be&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'string'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nx"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;to&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'bar'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nx"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;to&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;have&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nx"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tea&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;to&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;have&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;property&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'flavors'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="kd"&gt;with&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这种做法是极其错误的。程序语言本来就比自然语言简单清晰，这种写法让它看起来像自然语言的样子，反而变得复杂难懂了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="xie jian dan de dai ma"&gt;写简单的代码&lt;/h2&gt;
&lt;p&gt;程序语言都喜欢标新立异，提供这样那样的&amp;ldquo;特性&amp;rdquo;，然而有些特性其实并不是什么好东西。
很多特性都经不起时间的考验，最后带来的麻烦，比解决的问题还多。很多人盲目的追求
&amp;ldquo;短小&amp;rdquo;和&amp;ldquo;精悍&amp;rdquo;，或者为了显示自己头脑聪明，学得快，所以喜欢利用语言里的一些特殊
构造，写出过于&amp;ldquo;聪明&amp;rdquo;，难以理解的代码。&lt;/p&gt;
&lt;p&gt;并不是语言提供什么，你就一定要把它用上的。实际上你只需要其中很小的一部分功能，
就能写出优秀的代码。我一向反对&amp;ldquo;充分利用&amp;rdquo;程序语言里的所有特性。实际上，我心目
中有一套最好的构造。不管语言提供了多么&amp;ldquo;神奇&amp;rdquo;的，&amp;ldquo;新&amp;rdquo;的特性，我基本都只用经过
千锤百炼，我觉得值得信赖的那一套。&lt;/p&gt;
&lt;p&gt;现在针对一些有问题的语言特性，我介绍一些我自己使用的代码规范，并且讲解一下为什么它们能让代码更简单。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;避免使用自增减表达式&lt;code&gt;（i++，++i，i&amp;ndash;，&amp;ndash;i）&lt;/code&gt;。这种自增减操作表达式其实是历史遗留的设计失误。它们含义蹊跷，非常容易弄错。
  它们把读和写这两种完全不同的操作，混淆缠绕在一起，把语义搞得乌七八糟。含有它们的表达式，
  结果可能取决于求值顺序，所以它可能在某种编译器下能正确运行，换一个编译器就出现离奇的错误。&lt;/p&gt;
&lt;p&gt;其实这两个表达式完全可以分解成两步，把读和写分开：一步更新i的值，另外一步使用i的值。比如，如果你想写&lt;code&gt;foo(i++)&lt;/code&gt;，你完全可以把它拆成&lt;code&gt;int t = i; i += 1; foo(t);&lt;/code&gt;。如果你想写&lt;code&gt;foo(++i)&lt;/code&gt;，可以拆成&lt;code&gt;i += 1; foo(i)&lt;/code&gt;; 拆开之后的代码，含义完全一致，却清晰很多。到底更新是在取值之前还是之后，一目了然。&lt;/p&gt;
&lt;p&gt;有人也许以为&lt;code&gt;i++&lt;/code&gt;或者&lt;code&gt;++i&lt;/code&gt;的效率比拆开之后要高，这只是一种错觉。这些代码经过基本的编译器优化之后，生成的机器代码是完全没有区别的。自增减表达式只有在两种情况下才可以安全的使用。一种是在for循环的update部分，比如&lt;code&gt;for(int i = 0; i &amp;lt; 5; i++)&lt;/code&gt;。另一种情况是写成单独的一行，比如&lt;code&gt;i++;&lt;/code&gt;。这两种情况是完全没有歧义的。你需要避免其它的情况，比如用在复杂的表达式里面，比如&lt;code&gt;foo(i++)，foo(++i) + foo(i)，&amp;hellip;&amp;hellip;&lt;/code&gt; 没有人应该知道，或者去追究这些是什么意思。&lt;/p&gt;
&lt;p&gt;永远不要省略花括号。很多语言允许你在某种情况下省略掉花括号，比如 C，Java 都允许你在if语句里面只有一句话的时候省略掉花括号：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt;
    &lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;咋一看少打了两个字，多好。可是这其实经常引起奇怪的问题。比如，你后来想要加一句话&lt;code&gt;action2()&lt;/code&gt;到这个if里面，于是你就把代码改成：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt;
    &lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;action2&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为了美观，你很小心的使用了&lt;code&gt;action1()&lt;/code&gt;的缩进。咋一看它们是在一起的，所以你下意识里以为它们只会在if的条件为真的时候执行，然而&lt;code&gt;action2()&lt;/code&gt;却其实在if外面，它会被无条件的执行。我把这种现象叫做&amp;ldquo;光学幻觉&amp;rdquo;（optical illusion），理论上每个程序员都应该发现这个错误，然而实际上却容易被忽视。&lt;/p&gt;
&lt;p&gt;那么你问，谁会这么傻，我在加入&lt;code&gt;action2()&lt;/code&gt;的时候加上花括号不就行了？可是从设计的角度来看，这样其实并不是合理的作法。
首先，也许你以后又想把&lt;code&gt;action2()&lt;/code&gt;去掉，这样你为了样式一致，又得把花括号拿掉，烦不烦啊？其次，这使得代码样式不一致，
有的if有花括号，有的又没有。况且，你为什么需要记住这个规则？如果你不问三七二十一，只要是&lt;code&gt;if-else&lt;/code&gt;语句，把花括号全都打上，
就可以想都不用想了，就当 C 和 Java 没提供给你这个特殊写法。这样就可以保持完全的一致性，减少不必要的思考。&lt;/p&gt;
&lt;p&gt;有人可能会说，全都打上花括号，只有一句话也打上，多碍眼啊？然而经过实行这种编码规范几年之后，我并没有发现这种写法更加碍眼，反而由于花括号的存在，使得代码界限明确，让我的眼睛负担更小了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;合理使用括号，不要盲目依赖操作符优先级。利用操作符的优先级来减少括号，对于&lt;code&gt;1 + 2 * 3&lt;/code&gt;这样常见的算数表达式，是没问题的。然而有些人如此的仇恨括号，以至于他们会写出&lt;code&gt;2 &amp;lt;&amp;lt; 7 - 2 * 3&lt;/code&gt;这样的表达式，而完全不用括号。&lt;/p&gt;
&lt;p&gt;这里的问题，在于移位操作&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt;的优先级，是很多人不熟悉，而且是违反常理的。由于&lt;code&gt;x &amp;lt;&amp;lt; 1&lt;/code&gt;相当于把&lt;code&gt;x乘以2&lt;/code&gt;，很多人误以为这个表达式相当于&lt;code&gt;(2 &amp;lt;&amp;lt; 7) - (2 * 3)&lt;/code&gt;，所以等于250。然而实际上&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt;的优先级比加法&lt;code&gt;+&lt;/code&gt;还要低，所以这表达式其实相当于&lt;code&gt;2 &amp;lt;&amp;lt; (7 - 2 * 3)&lt;/code&gt;，所以等于4！&lt;/p&gt;
&lt;p&gt;解决这个问题的办法，不是要每个人去把操作符优先级表给硬背下来，而是合理的加入括号。比如上面的例子，最好直接加上括号写成&lt;code&gt;2 &amp;lt;&amp;lt; (7 - 2 * 3)&lt;/code&gt;。虽然没有括号也表示同样的意思，但是加上括号就更加清晰，读者不再需要死记&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt;的优先级就能理解代码。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;避免使用&lt;code&gt;continue&lt;/code&gt;和&lt;code&gt;break&lt;/code&gt;。循环语句&lt;code&gt;（for，while）&lt;/code&gt;里面出现&lt;code&gt;return&lt;/code&gt;是没问题的，然而如果你使用了&lt;code&gt;continue&lt;/code&gt;或者&lt;code&gt;break&lt;/code&gt;，就会让循环的逻辑和终止条件变得复杂，难以确保正确。&lt;/p&gt;
&lt;p&gt;出现&lt;code&gt;continue&lt;/code&gt;或者&lt;code&gt;break&lt;/code&gt;的原因，往往是对循环的逻辑没有想清楚。如果你考虑周全了，应该是几乎不需要&lt;code&gt;continue&lt;/code&gt;或者&lt;code&gt;break&lt;/code&gt;的。如果你的循环里出现了&lt;code&gt;continue&lt;/code&gt;或者&lt;code&gt;break&lt;/code&gt;，你就应该考虑改写这个循环。改写循环的办法有多种：&lt;/p&gt;
&lt;p&gt;如果出现了&lt;code&gt;continue&lt;/code&gt;，你往往只需要把&lt;code&gt;continue&lt;/code&gt;的条件反向，就可以消除&lt;code&gt;continue&lt;/code&gt;。
如果出现了&lt;code&gt;break&lt;/code&gt;，你往往可以把&lt;code&gt;break&lt;/code&gt;的条件，合并到循环头部的终止条件里，从而去掉&lt;code&gt;break&lt;/code&gt;。
有时候你可以把&lt;code&gt;break&lt;/code&gt;替换成&lt;code&gt;return&lt;/code&gt;，从而去掉&lt;code&gt;break&lt;/code&gt;。
如果以上都失败了，你也许可以把循环里面复杂的部分提取出来，做成函数调用，之后&lt;code&gt;continue&lt;/code&gt;或者&lt;code&gt;break&lt;/code&gt;就可以去掉了。
下面我对这些情况举一些例子。&lt;/p&gt;
&lt;p&gt;情况1：下面这段代码里面有一个&lt;code&gt;continue&lt;/code&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;goodNames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;ArrayList&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;();&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;contains&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bad"&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;goodNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;add&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;它说：&amp;ldquo;如果name含有&amp;rsquo;bad&amp;rsquo;这个词，跳过后面的循环代码&amp;hellip;&amp;hellip;&amp;rdquo; 注意，这是一种&amp;ldquo;负面&amp;rdquo;的描述，它不是在告诉你什么时候&amp;ldquo;做&amp;rdquo;一件事，而是在告诉你什么时候&amp;ldquo;不做&amp;rdquo;一件事。为了知道它到底在干什么，你必须搞清楚&lt;code&gt;continue&lt;/code&gt;会导致哪些语句被跳过了，然后脑子里把逻辑反个向，你才能知道它到底想做什么。这就是为什么含有&lt;code&gt;continue&lt;/code&gt;和&lt;code&gt;break&lt;/code&gt;的循环不容易理解，它们依靠&amp;ldquo;控制流&amp;rdquo;来描述&amp;ldquo;不做什么&amp;rdquo;，&amp;ldquo;跳过什么&amp;rdquo;，结果到最后你也没搞清楚它到底&amp;ldquo;要做什么&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;其实，我们只需要把&lt;code&gt;continue&lt;/code&gt;的条件反向，这段代码就可以很容易的被转换成等价的，不含&lt;code&gt;continue&lt;/code&gt;的代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;goodNames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;ArrayList&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;();&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(!&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;contains&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bad"&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;goodNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;add&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;goodNames.add(name);&lt;/code&gt;和它之后的代码全部被放到了&lt;code&gt;if&lt;/code&gt;里面，多了一层缩进，然而&lt;code&gt;continue&lt;/code&gt;却没有了。你再读这段代码，就会发现更加清晰。
因为它是一种更加&amp;ldquo;正面&amp;rdquo;地描述。它说：&amp;ldquo;在name不含有&amp;rsquo;bad&amp;rsquo;这个词的时候，把它加到&lt;code&gt;goodNames&lt;/code&gt;的链表里面&amp;hellip;&amp;hellip;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;情况2：&lt;code&gt;for&lt;/code&gt;和&lt;code&gt;while&lt;/code&gt;头部都有一个循环的&amp;ldquo;终止条件&amp;rdquo;，那本来应该是这个循环唯一的退出条件。如果你在循环中间有&lt;code&gt;break&lt;/code&gt;，它其实给这个循环增加了一个退出条件。你往往只需要把这个条件合并到循环头部，就可以去掉&lt;code&gt;break&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;比如下面这段代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;condition1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;condition2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;当&lt;code&gt;condition&lt;/code&gt;成立的时候，&lt;code&gt;break&lt;/code&gt;会退出循环。其实你只需要把&lt;code&gt;condition2&lt;/code&gt;反转之后，放到&lt;code&gt;while&lt;/code&gt;头部的终止条件，就可以去掉这种&lt;code&gt;break&lt;/code&gt;语句。改写后的代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;condition1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;condition2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这种情况表面上貌似只适用于&lt;code&gt;break&lt;/code&gt;出现在循环开头或者末尾的时候，然而其实大部分时候，&lt;code&gt;break&lt;/code&gt;都可以通过某种方式，移动到循环的开头或者末尾。具体的例子我暂时没有，等出现的时候再加进来。&lt;/p&gt;
&lt;p&gt;情况3：很多&lt;code&gt;break&lt;/code&gt;退出循环之后，其实接下来就是一个&lt;code&gt;return&lt;/code&gt;。这种&lt;code&gt;break&lt;/code&gt;往往可以直接换成&lt;code&gt;return&lt;/code&gt;。比如下面这个例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;boolean&lt;/span&gt; &lt;span class="nf"&gt;hasBadName&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;contains&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bad"&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这个函数检查&lt;code&gt;names&lt;/code&gt;链表里是否存在一个名字，包含&lt;code&gt;&amp;ldquo;bad&amp;rdquo;&lt;/code&gt;这个词。它的循环里包含一个&lt;code&gt;break&lt;/code&gt;语句。这个函数可以被改写成：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;boolean&lt;/span&gt; &lt;span class="nf"&gt;hasBadName&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;contains&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bad"&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;改进后的代码，在&lt;code&gt;name&lt;/code&gt;里面含有&lt;code&gt;&amp;ldquo;bad&amp;rdquo;&lt;/code&gt;的时候，直接用&lt;code&gt;return true&lt;/code&gt;返回，而不是对&lt;code&gt;result&lt;/code&gt;变量赋值，&lt;code&gt;break&lt;/code&gt;出去，最后才返回。
如果循环结束了还没有&lt;code&gt;return&lt;/code&gt;，那就返回&lt;code&gt;false&lt;/code&gt;，表示没有找到这样的名字。使用&lt;code&gt;return&lt;/code&gt;来代替&lt;code&gt;break&lt;/code&gt;，这样&lt;code&gt;break&lt;/code&gt;语句和&lt;code&gt;result&lt;/code&gt;这个变量，都一并被消除掉了。
我曾经见过很多其他使用&lt;code&gt;continue&lt;/code&gt;和&lt;code&gt;break&lt;/code&gt;的例子，几乎无一例外的可以被消除掉，变换后的代码变得清晰很多。我的经验是，99%的&lt;code&gt;break&lt;/code&gt;和&lt;code&gt;continue&lt;/code&gt;，都可以通过替换成&lt;code&gt;return语句&lt;/code&gt;，或者翻转if条件的方式来消除掉。剩下的1%含有复杂的逻辑，但也可以通过提取一个帮助函数来消除掉。修改之后的代码变得容易理解，容易确保正确。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;


&lt;h2 id="xie zhi guan de dai ma"&gt;写直观的代码&lt;/h2&gt;
&lt;p&gt;我写代码有一条重要的原则：如果有更加直接，更加清晰的写法，就选择它，即使它看起来更长，更笨，也一样选择它。比如，Unix命令行有一种&amp;ldquo;巧妙&amp;rdquo;的写法是这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;command1 &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;amp&lt;span class="p"&gt;;&amp;amp;&lt;/span&gt;amp&lt;span class="p"&gt;;&lt;/span&gt; command2 &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;amp&lt;span class="p"&gt;;&amp;amp;&lt;/span&gt;amp&lt;span class="p"&gt;;&lt;/span&gt; command3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;由于Shell语言的逻辑操作&lt;code&gt;a &amp;amp;&amp;amp; b&lt;/code&gt;具有&amp;ldquo;短路&amp;rdquo;的特性，如果&lt;code&gt;a&lt;/code&gt;等于&lt;code&gt;false&lt;/code&gt;，那么&lt;code&gt;b&lt;/code&gt;就没必要执行了。这就是为什么当&lt;code&gt;command1&lt;/code&gt;成功，才会执行&lt;code&gt;command2&lt;/code&gt;，当&lt;code&gt;command2&lt;/code&gt;成功，才会执行&lt;code&gt;command3&lt;/code&gt;。同样，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;command1 &lt;span class="o"&gt;||&lt;/span&gt; command2 &lt;span class="o"&gt;||&lt;/span&gt; command3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;操作符&lt;code&gt;||&lt;/code&gt;也有类似的特性。上面这个命令行，如果&lt;code&gt;command1&lt;/code&gt;成功，那么&lt;code&gt;command2&lt;/code&gt;和&lt;code&gt;command3&lt;/code&gt;都不会被执行。如果&lt;code&gt;command1&lt;/code&gt;失败，&lt;code&gt;command2&lt;/code&gt;成功，那么&lt;code&gt;command3&lt;/code&gt;就不会被执行。&lt;/p&gt;
&lt;p&gt;这比起用&lt;code&gt;if&lt;/code&gt;语句来判断失败，似乎更加巧妙和简洁，所以有人就借鉴了这种方式，在程序的代码里也使用这种方式。比如他们可能会写这样的代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;action2&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;action3&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你看得出来这代码是想干什么吗？&lt;code&gt;action2&lt;/code&gt;和&lt;code&gt;action3&lt;/code&gt;什么条件下执行，什么条件下不执行？也许稍微想一下，你知道它在干什么：&amp;ldquo;如果&lt;code&gt;action1&lt;/code&gt;失败了，执行&lt;code&gt;action2&lt;/code&gt;，如果&lt;code&gt;action2&lt;/code&gt;成功了，执行&lt;code&gt;action3&lt;/code&gt;&amp;rdquo;。然而那种语义，并不是直接的&amp;ldquo;映射&amp;rdquo;在这代码上面的。比如&amp;ldquo;失败&amp;rdquo;这个词，对应了代码里的哪一个字呢？你找不出来，因为它包含在了&lt;code&gt;||&lt;/code&gt;的语义里面，你需要知道&lt;code&gt;||&lt;/code&gt;的短路特性，以及逻辑或的语义才能知道这里面在说&amp;ldquo;如果&lt;code&gt;action1&lt;/code&gt;失败&amp;hellip;&amp;hellip;&amp;rdquo;。每一次看到这行代码，你都需要思考一下，这样积累起来的负荷，就会让人很累。&lt;/p&gt;
&lt;p&gt;其实，这种写法是滥用了逻辑操作&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;和&lt;code&gt;||&lt;/code&gt;的短路特性。这两个操作符可能不执行右边的表达式，原因是为了机器的执行效率，而不是为了给人提供这种&amp;ldquo;巧妙&amp;rdquo;的用法。这两个操作符的本意，只是作为逻辑操作，它们并不是拿来给你代替 &lt;code&gt;if&lt;/code&gt; 语句的。也就是说，它们只是碰巧可以达到某些 &lt;code&gt;if&lt;/code&gt; 语句的效果，但你不应该因此就用它来代替 &lt;code&gt;if&lt;/code&gt; 语句。如果你这样做了，就会让代码晦涩难懂。&lt;/p&gt;
&lt;p&gt;上面的代码写成笨一点的办法，就会清晰很多：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(!&lt;/span&gt;&lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action2&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;action3&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里我很明显的看出这代码在说什么，想都不用想：如果&lt;code&gt;action1()&lt;/code&gt;失败了，那么执行&lt;code&gt;action2()&lt;/code&gt;，如果&lt;code&gt;action2()&lt;/code&gt;成功了，执行&lt;code&gt;action3()&lt;/code&gt;。你发现这里面的一一对应关系吗？&lt;code&gt;if&lt;/code&gt;=如果，&lt;code&gt;!&lt;/code&gt;=失败，&amp;hellip;&amp;hellip; 你不需要利用逻辑学知识，就知道它在说什么。&lt;/p&gt;
&lt;h2 id="xie wu xie ke ji de dai ma"&gt;写无懈可击的代码&lt;/h2&gt;
&lt;p&gt;在之前一节里，我提到了自己写的代码里面很少出现只有一个分支的&lt;code&gt;if&lt;/code&gt;语句。我写出的&lt;code&gt;if&lt;/code&gt;语句，大部分都有两个分支，所以我的代码很多看起来是这个样子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;使用这种方式，其实是为了无懈可击的处理所有可能出现的情况，避免漏掉corner case。每个if语句都有两个分支的理由是：如果&lt;code&gt;if&lt;/code&gt;的条件成立，你做某件事情；但是如果&lt;code&gt;if&lt;/code&gt;的条件不成立，你应该知道要做什么另外的事情。不管你的&lt;code&gt;if&lt;/code&gt;有没有&lt;code&gt;else&lt;/code&gt;，你终究是逃不掉，必须得思考这个问题的。&lt;/p&gt;
&lt;p&gt;很多人写&lt;code&gt;if&lt;/code&gt;语句喜欢省略&lt;code&gt;else&lt;/code&gt;的分支，因为他们觉得有些&lt;code&gt;else分支&lt;/code&gt;的代码重复了。比如我的代码里，两个&lt;code&gt;else分支&lt;/code&gt;都是&lt;code&gt;return true&lt;/code&gt;。为了避免重复，他们省略掉那两个&lt;code&gt;else分支&lt;/code&gt;，只在最后使用一个&lt;code&gt;return true&lt;/code&gt;。这样，缺了&lt;code&gt;else分支&lt;/code&gt;的&lt;code&gt;if&lt;/code&gt;语句，控制流自动&amp;ldquo;掉下去&amp;rdquo;，到达最后的&lt;code&gt;return true&lt;/code&gt;。他们的代码看起来像这个样子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这种写法看似更加简洁，避免了重复，然而却很容易出现疏忽和漏洞。嵌套的if语句省略了一些&lt;code&gt;else&lt;/code&gt;，依靠语句的&amp;ldquo;控制流&amp;rdquo;来处理&lt;code&gt;else&lt;/code&gt;的情况，是很难正确的分析和推理的。如果你的&lt;code&gt;if条件&lt;/code&gt;里使用了&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;和&lt;code&gt;||&lt;/code&gt;之类的逻辑运算，就更难看出是否涵盖了所有的情况。&lt;/p&gt;
&lt;p&gt;由于疏忽而漏掉的分支，全都会自动&amp;ldquo;掉下去&amp;rdquo;，最后返回意想不到的结果。即使你看一遍之后确信是正确的，每次读这段代码，你都不能确信它照顾了所有的情况，又得重新推理一遍。这简洁的写法，带来的是反复的，沉重的头脑开销。这就是所谓&amp;ldquo;面条代码&amp;rdquo;，因为程序的逻辑分支，不是像一棵枝叶分明的树，而是像面条一样绕来绕去。&lt;/p&gt;
&lt;p&gt;另外一种省略&lt;code&gt;else分支&lt;/code&gt;的情况是这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"ok"&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;写这段代码的人，脑子里喜欢使用一种&amp;ldquo;缺省值&amp;rdquo;的做法。&lt;code&gt;s&lt;/code&gt;缺省为&lt;code&gt;null&lt;/code&gt;，如果&lt;code&gt;x&amp;lt;5&lt;/code&gt;，那么把它改变（mutate）成&amp;ldquo;ok&amp;rdquo;。这种写法的缺点是，当&lt;code&gt;x&amp;lt;5&lt;/code&gt;不成立的时候，你需要往上面看，才能知道&lt;code&gt;s&lt;/code&gt;的值是什么。这还是你运气好的时候，因为s就在上面不远。很多人写这种代码的时候，&lt;code&gt;s&lt;/code&gt;的初始值离判断语句有一定的距离，中间还有可能插入一些其它的逻辑和赋值操作。这样的代码，把变量改来改去的，看得人眼花，就容易出错。&lt;/p&gt;
&lt;p&gt;现在比较一下我的写法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"ok"&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这种写法貌似多打了一两个字，然而它却更加清晰。这是因为我们明确的指出了&lt;code&gt;x&amp;lt;5&lt;/code&gt;不成立的时候，&lt;code&gt;s&lt;/code&gt;的值是什么。它就摆在那里，它是&lt;code&gt;""&lt;/code&gt;（空字符串）。注意，虽然我也使用了赋值操作，然而我并没有&amp;ldquo;改变&amp;rdquo;&lt;code&gt;s&lt;/code&gt;的值。&lt;code&gt;s&lt;/code&gt;一开始的时候没有值，被赋值之后就再也没有变过。我的这种写法，通常被叫做更加&amp;ldquo;函数式&amp;rdquo;，因为我只赋值一次。&lt;/p&gt;
&lt;p&gt;如果我漏写了&lt;code&gt;else&lt;/code&gt;分支，Java 编译器是不会放过我的。它会抱怨：&amp;ldquo;在某个分支，&lt;code&gt;s&lt;/code&gt;没有被初始化。&amp;rdquo;这就强迫我清清楚楚的设定各种条件下&lt;code&gt;s&lt;/code&gt;的值，不漏掉任何一种情况。&lt;/p&gt;
&lt;p&gt;当然，由于这个情况比较简单，你还可以把它写成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="s"&gt;"ok"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;对于更加复杂的情况，我建议还是写成&lt;code&gt;if&lt;/code&gt;语句为好。&lt;/p&gt;
&lt;h2 id="zheng que chu li cuo wu"&gt;正确处理错误&lt;/h2&gt;
&lt;p&gt;使用有两个分支的if语句，只是我的代码可以达到无懈可击的其中一个原因。这样写if语句的思路，其实包含了使代码可靠的一种通用思想：穷举所有的情况，不漏掉任何一个。&lt;/p&gt;
&lt;p&gt;程序的绝大部分功能，是进行信息处理。从一堆纷繁复杂，模棱两可的信息中，排除掉绝大部分&amp;ldquo;干扰信息&amp;rdquo;，找到自己需要的那一个。正确地对所有的&amp;ldquo;可能性&amp;rdquo;进行推理，就是写出无懈可击代码的核心思想。这一节我来讲一讲，如何把这种思想用在错误处理上。&lt;/p&gt;
&lt;p&gt;错误处理是一个古老的问题，可是经过了几十年，还是很多人没搞明白。Unix的系统API手册，一般都会告诉你可能出现的返回值和错误信息。比如，Linux的&lt;code&gt;read&lt;/code&gt;系统调用手册里面有如下内容：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;RETURN VALUE 
On success, the number of bytes read is returned...

On error, -1 is returned, and errno is set appropriately.

ERRORS

EAGAIN, EBADF, EFAULT, EINTR, EINVAL, ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;很多初学者，都会忘记检查&lt;code&gt;read&lt;/code&gt;的返回值是否为&lt;code&gt;-1&lt;/code&gt;，觉得每次调用&lt;code&gt;read&lt;/code&gt;都得检查返回值真繁琐，不检查貌似也相安无事。这种想法其实是很危险的。如果函数的返回值告诉你，要么返回一个正数，表示读到的数据长度，要么返回&lt;code&gt;-1&lt;/code&gt;，那么你就必须要对这个&lt;code&gt;-1&lt;/code&gt;作出相应的，有意义的处理。千万不要以为你可以忽视这个特殊的返回值，因为它是一种&amp;ldquo;可能性&amp;rdquo;。代码漏掉任何一种可能出现的情况，都可能产生意想不到的灾难性结果。&lt;/p&gt;
&lt;p&gt;对于 Java 来说，这相对方便一些。Java 的函数如果出现问题，一般通过异常（exception）来表示。你可以把异常加上函数本来的返回值，看成是一个&amp;ldquo;&lt;code&gt;union类型&lt;/code&gt;&amp;rdquo;。比如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;MyException&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里&lt;code&gt;MyException&lt;/code&gt;是一个错误返回。你可以认为这个函数返回一个&lt;code&gt;union类型&lt;/code&gt;：&lt;code&gt;{String, MyException}&lt;/code&gt;。任何调用&lt;code&gt;foo&lt;/code&gt;的代码，必须对&lt;code&gt;MyException&lt;/code&gt;作出合理的处理，才有可能确保程序的正确运行。&lt;code&gt;Union类型&lt;/code&gt;是一种相当先进的类型，目前只有极少数语言（比如Typed Racket）具有这种类型，我在这里提到它，只是为了方便解释概念。掌握了概念之后，你其实可以在头脑里实现一个&lt;code&gt;union类型&lt;/code&gt;系统，这样使用普通的语言也能写出可靠的代码。&lt;/p&gt;
&lt;p&gt;由于 Java 的类型系统强制要求函数在类型里面声明可能出现的异常，而且强制调用者处理可能出现的异常，所以基本上不可能出现由于疏忽而漏掉的情况。但有些 Java 程序员有一种恶习，使得这种安全机制几乎完全失效。每当编译器报错，说&amp;ldquo;你没有&lt;code&gt;catch&lt;/code&gt;这个&lt;code&gt;foo&lt;/code&gt;函数可能出现的异常&amp;rdquo;时，有些人想都不想，直接把代码改成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;或者最多在里面放个&lt;code&gt;log&lt;/code&gt;，或者干脆把自己的函数类型上加上&lt;code&gt;throws Exception&lt;/code&gt;，这样编译器就不再抱怨。这些做法貌似很省事，然而都是错误的，你终究会为此付出代价。&lt;/p&gt;
&lt;p&gt;如果你把异常&lt;code&gt;catch&lt;/code&gt;了，忽略掉，那么你就不知道&lt;code&gt;foo&lt;/code&gt;其实失败了。这就像开车时看到路口写着&amp;ldquo;前方施工，道路关闭&amp;rdquo;，还继续往前开。这当然迟早会出问题，因为你根本不知道自己在干什么。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;catch&lt;/code&gt;异常的时候，你不应该使用&lt;code&gt;Exception&lt;/code&gt;这么宽泛的类型。你应该正好&lt;code&gt;catch&lt;/code&gt;可能发生的那种异常A。使用宽泛的异常类型有很大的问题，因为它会不经意的  &lt;code&gt;catch&lt;/code&gt;住另外的异常（比如B）。你的代码逻辑是基于判断A是否出现，可你却&lt;code&gt;catch&lt;/code&gt;所有的异常（Exception类），所以当其它的异常B出现的时候，你的代码就会出现莫名其妙的问题，因为你以为A出现了，而其实它没有。这种bug，有时候甚至使用debugger都难以发现。&lt;/p&gt;
&lt;p&gt;如果你在自己函数的类型加上&lt;code&gt;throws Exception&lt;/code&gt;，那么你就不可避免的需要在调用它的地方处理这个异常，如果调用它的函数也写着&lt;code&gt;throws Exception&lt;/code&gt;，这毛病就传得更远。我的经验是，尽量在异常出现的当时就作出处理。否则如果你把它返回给你的调用者，它也许根本不知道该怎么办了。&lt;/p&gt;
&lt;p&gt;另外，&lt;code&gt;try { &amp;hellip; } catch&lt;/code&gt;里面，应该包含尽量少的代码。比如，如果&lt;code&gt;foo&lt;/code&gt;和&lt;code&gt;bar&lt;/code&gt;都可能产生异常A，你的代码应该尽可能写成：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{...}&lt;/span&gt;

&lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{...}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;而不是&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{...}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;第一种写法能明确的分辨是哪一个函数出了问题，而第二种写法全都混在一起。明确的分辨是哪一个函数出了问题，有很多的好处。比如，如果你的&lt;code&gt;catch&lt;/code&gt;代码里面包含&lt;code&gt;log&lt;/code&gt;，它可以提供给你更加精确的错误信息，这样会大大地加速你的调试过程。&lt;/p&gt;
&lt;h2 id="zheng que chu li  null zhi zhen"&gt;正确处理 null 指针&lt;/h2&gt;
&lt;p&gt;穷举的思想是如此的有用，依据这个原理，我们可以推出一些基本原则，它们可以让你无懈可击的处理&lt;code&gt;null指针&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;首先你应该知道，许多语言（C，C++，Java，C#，&amp;hellip;&amp;hellip;）的类型系统对于null的处理，其实是完全错误的。这个错误源自于Tony Hoare最早的设计，Hoare把这个错误称为自己的&amp;ldquo;billion dollar mistake&amp;rdquo;，因为由于它所产生的财产和人力损失，远远超过十亿美元。&lt;/p&gt;
&lt;p&gt;这些语言的类型系统允许&lt;code&gt;null&lt;/code&gt;出现在任何对象（指针）类型可以出现的地方，然而&lt;code&gt;null&lt;/code&gt;其实根本不是一个合法的对象。它不是一个&lt;code&gt;String&lt;/code&gt;，不是一个&lt;code&gt;Integer&lt;/code&gt;，也不是一个自定义的类。&lt;code&gt;null&lt;/code&gt;的类型本来应该是&lt;code&gt;NULL&lt;/code&gt;，也就是&lt;code&gt;null&lt;/code&gt;自己。根据这个基本观点，我们推导出以下原则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;尽量不要产生&lt;code&gt;null指针&lt;/code&gt;。尽量不要用&lt;code&gt;null&lt;/code&gt;来初始化变量，函数尽量不要返回&lt;code&gt;null&lt;/code&gt;。如果你的函数要返回&amp;ldquo;没有&amp;rdquo;，&amp;ldquo;出错了&amp;rdquo;之类的结果，尽量使用 Java 的异常机制。虽然写法上有点别扭，然而 Java 的异常，和函数的返回值合并在一起，基本上可以当成&lt;code&gt;union&lt;/code&gt;类型来用。&lt;/p&gt;
&lt;p&gt;比如，如果你有一个函数&lt;code&gt;find&lt;/code&gt;，可以帮你找到一个&lt;code&gt;String&lt;/code&gt;，也有可能什么也找不到，你可以这样写：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;find&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;NotFoundException&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(...)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;...;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;NotFoundException&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Java的类型系统会强制你&lt;code&gt;catch&lt;/code&gt;这个&lt;code&gt;NotFoundException&lt;/code&gt;，所以你不可能像漏掉检查&lt;code&gt;null&lt;/code&gt;一样，漏掉这种情况。&lt;/p&gt;
&lt;p&gt;Java 的异常也是一个比较容易滥用的东西，不过我已经在上一节告诉你如何正确的使用异常。    &lt;/p&gt;
&lt;p&gt;Java 的&lt;code&gt;try&amp;hellip;catch&lt;/code&gt;语法相当的繁琐和蹩脚，所以如果你足够小心的话，像&lt;code&gt;find&lt;/code&gt;这类函数，也可以返回&lt;code&gt;null&lt;/code&gt;来表示&amp;ldquo;没找到&amp;rdquo;。
这样稍微好看一些，因为你调用的时候不必用&lt;code&gt;try&amp;hellip;catch&lt;/code&gt;。很多人写的函数，返回&lt;code&gt;null&lt;/code&gt;来表示&amp;ldquo;出错了&amp;rdquo;，这其实是对&lt;code&gt;null&lt;/code&gt;的误用。&amp;ldquo;出错了&amp;rdquo;和&amp;ldquo;没有&amp;rdquo;，其实完全是两码事。&amp;ldquo;没有&amp;rdquo;是一种很常见，正常的情况，比如查哈希表没找到，很正常。&amp;ldquo;出错了&amp;rdquo;则表示罕见的情况，本来正常情况下都应该存在有意义的值，偶然出了问题。如果你的函数要表示&amp;ldquo;出错了&amp;rdquo;，应该使用异常，而不是&lt;code&gt;null&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不要&lt;code&gt;catch NullPointerException&lt;/code&gt;。有些人写代码很nice，他们喜欢&amp;ldquo;容错&amp;rdquo;。首先他们写一些函数，这些函数里面不大小心，没检查&lt;code&gt;null&lt;/code&gt;指针：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;length&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;当&lt;code&gt;foo&lt;/code&gt;调用产生了异常，他们不管三七二十一，就把调用的地方改成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样当&lt;code&gt;found&lt;/code&gt;是&lt;code&gt;null&lt;/code&gt;的时候，&lt;code&gt;NullPointerException&lt;/code&gt;就会被捕获并且得到处理。这其实是很错误的作法。首先，上一节已经提到了，&lt;code&gt;catch (Exception e)&lt;/code&gt;这种写法是要绝对避免的，因为它捕获所有的异常，包括&lt;code&gt;NullPointerException&lt;/code&gt;。这会让你意外地捕获&lt;code&gt;try&lt;/code&gt;语句里面出现的&lt;code&gt;NullPointerException&lt;/code&gt;，从而把代码的逻辑搅得一塌糊涂。&lt;/p&gt;
&lt;p&gt;另外就算你写成&lt;code&gt;catch (NullPointerException e)&lt;/code&gt;也是不可以的。由于&lt;code&gt;foo&lt;/code&gt;的内部缺少了&lt;code&gt;null&lt;/code&gt;检查，才出现了&lt;code&gt;NullPointerException&lt;/code&gt;。现在你不对症下药，倒把每个调用它的地方加上&lt;code&gt;catch&lt;/code&gt;，以后你的生活就会越来越苦。正确的做法应该是改动&lt;code&gt;foo&lt;/code&gt;，而不改调用它的代码。&lt;code&gt;foo&lt;/code&gt;应该被改成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;length&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在&lt;code&gt;null&lt;/code&gt;可能出现的当时就检查它是否是&lt;code&gt;null&lt;/code&gt;，然后进行相应的处理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不要把&lt;code&gt;null&lt;/code&gt;放进&amp;ldquo;容器数据结构&amp;rdquo;里面。所谓容器（collection），是指一些对象以某种方式集合在一起，所以&lt;code&gt;null&lt;/code&gt;不应该被放进&lt;code&gt;Array，List，Set&lt;/code&gt;等结构，不应该出现在&lt;code&gt;Map&lt;/code&gt;的&lt;code&gt;key&lt;/code&gt;或者&lt;code&gt;value&lt;/code&gt;里面。把&lt;code&gt;null&lt;/code&gt;放进容器里面，是一些莫名其妙错误的来源。因为对象在容器里的位置一般是动态决定的，所以一旦&lt;code&gt;null&lt;/code&gt;从某个入口跑进去了，你就很难再搞明白它去了哪里，你就得被迫在所有从这个容器里取值的位置检查&lt;code&gt;null&lt;/code&gt;。你也很难知道到底是谁把它放进去的，代码多了就导致调试极其困难。&lt;/p&gt;
&lt;p&gt;解决方案是：如果你真要表示&amp;ldquo;没有&amp;rdquo;，那你就干脆不要把它放进去（&lt;code&gt;Array，List，Set&lt;/code&gt;没有元素，&lt;code&gt;Map&lt;/code&gt;根本没那个&lt;code&gt;entry&lt;/code&gt;），或者你可以指定一个特殊的，真正合法的对象，用来表示&amp;ldquo;没有&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;需要指出的是，类对象并不属于容器。所以&lt;code&gt;null&lt;/code&gt;在必要的时候，可以作为对象成员的值，表示它不存在。比如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;A&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;之所以可以这样，是因为&lt;code&gt;null&lt;/code&gt;只可能在&lt;code&gt;A对象&lt;/code&gt;的&lt;code&gt;name成员&lt;/code&gt;里出现，你不用怀疑其它的成员因此成为&lt;code&gt;null&lt;/code&gt;。所以你每次访问&lt;code&gt;name成员&lt;/code&gt;时，检查它是否是&lt;code&gt;null&lt;/code&gt;就可以了，不需要对其他成员也做同样的检查。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;函数调用者：明确理解&lt;code&gt;null&lt;/code&gt;所表示的意义，尽早检查和处理&lt;code&gt;null&lt;/code&gt;返回值，减少它的传播。&lt;code&gt;null&lt;/code&gt;很讨厌的一个地方，在于它在不同的地方可能表示不同的意义。有时候它表示&amp;ldquo;没有&amp;rdquo;，&amp;ldquo;没找到&amp;rdquo;。有时候它表示&amp;ldquo;出错了&amp;rdquo;，&amp;ldquo;失败了&amp;rdquo;。有时候它甚至可以表示&amp;ldquo;成功了&amp;rdquo;，&amp;hellip;&amp;hellip; 这其中有很多误用之处，不过无论如何，你必须理解每一个&lt;code&gt;null&lt;/code&gt;的意义，不能给混淆起来。&lt;/p&gt;
&lt;p&gt;如果你调用的函数有可能返回&lt;code&gt;null&lt;/code&gt;，那么你应该在第一时间对&lt;code&gt;null&lt;/code&gt;做出&amp;ldquo;有意义&amp;rdquo;的处理。比如，上述的函数&lt;code&gt;find&lt;/code&gt;，返回&lt;code&gt;null&lt;/code&gt;表示&amp;ldquo;没找到&amp;rdquo;，那么调用&lt;code&gt;find&lt;/code&gt;的代码就应该在它返回的第一时间，检查返回值是否是&lt;code&gt;null&lt;/code&gt;，并且对&amp;ldquo;没找到&amp;rdquo;这种情况，作出有意义的处理。&lt;/p&gt;
&lt;p&gt;&amp;ldquo;有意义&amp;rdquo;是什么意思呢？我的意思是，使用这函数的人，应该明确的知道在拿到&lt;code&gt;null&lt;/code&gt;的情况下该怎么做，承担起责任来。他不应该只是&amp;ldquo;向上级汇报&amp;rdquo;，把责任踢给自己的调用者。如果你违反了这一点，就有可能采用一种不负责任，危险的写法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;当看到&lt;code&gt;find()&lt;/code&gt;返回了&lt;code&gt;null&lt;/code&gt;，&lt;code&gt;foo&lt;/code&gt;自己也返回&lt;code&gt;null&lt;/code&gt;。这样&lt;code&gt;null&lt;/code&gt;就从一个地方，游走到了另一个地方，而且它表示另外一个意思。如果你不假思索就写出这样的代码，最后的结果就是代码里面随时随地都可能出现&lt;code&gt;null&lt;/code&gt;。到后来为了保护自己，你的每个函数都会写成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;函数作者：明确声明不接受&lt;code&gt;null&lt;/code&gt;参数，当参数是&lt;code&gt;null&lt;/code&gt;时立即崩溃。不要试图对&lt;code&gt;null&lt;/code&gt;进行&amp;ldquo;容错&amp;rdquo;，不要让程序继续往下执行。如果调用者使用了&lt;code&gt;null&lt;/code&gt;作为参数，那么调用者（而不是函数作者）应该对程序的崩溃负全责。&lt;/p&gt;
&lt;p&gt;上面的例子之所以成为问题，就在于人们对于&lt;code&gt;null&lt;/code&gt;的&amp;ldquo;容忍态度&amp;rdquo;。这种&amp;ldquo;保护式&amp;rdquo;的写法，试图&amp;ldquo;容错&amp;rdquo;，试图&amp;ldquo;优雅的处理&lt;code&gt;null&lt;/code&gt;&amp;rdquo;，其结果是让调用者更加肆无忌惮的传递&lt;code&gt;null&lt;/code&gt;给你的函数。到后来，你的代码里出现一堆堆&lt;code&gt;nonsense&lt;/code&gt;的情况，&lt;code&gt;null&lt;/code&gt;可以在任何地方出现，都不知道到底是哪里产生出来的。谁也不知道出现了&lt;code&gt;null&lt;/code&gt;是什么意思，该做什么，所有人都把&lt;code&gt;null&lt;/code&gt;踢给其他人。最后这&lt;code&gt;null&lt;/code&gt;像瘟疫一样蔓延开来，到处都是，成为一场噩梦。&lt;/p&gt;
&lt;p&gt;正确的做法，其实是强硬的态度。你要告诉函数的使用者，我的参数全都不能是&lt;code&gt;null&lt;/code&gt;，如果你给我&lt;code&gt;null&lt;/code&gt;，程序崩溃了该你自己负责。至于调用者代码里有&lt;code&gt;null&lt;/code&gt;怎么办，他自己该知道怎么处理（参考以上几条），不应该由函数作者来操心。&lt;/p&gt;
&lt;p&gt;采用强硬态度一个很简单的做法是使用&lt;code&gt;Objects.requireNonNull()&lt;/code&gt;。它的定义很简单：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="nf"&gt;requireNonNull&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;obj&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obj&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;NullPointerException&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;obj&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你可以用这个函数来检查不想接受&lt;code&gt;null&lt;/code&gt;的每一个参数，只要传进来的参数是&lt;code&gt;null&lt;/code&gt;，就会立即触发&lt;code&gt;NullPointerException&lt;/code&gt;崩溃掉，这样你就可以有效地防止&lt;code&gt;null&lt;/code&gt;指针不知不觉传递到其它地方去。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用&lt;code&gt;@NotNull&lt;/code&gt;和&lt;code&gt;@Nullable&lt;/code&gt;标记。IntelliJ提供了&lt;code&gt;@NotNull&lt;/code&gt;和&lt;code&gt;@Nullable&lt;/code&gt;两种标记，加在类型前面，这样可以比较简洁可靠地防止&lt;code&gt;null&lt;/code&gt;指针的出现。IntelliJ本身会对含有这种标记的代码进行静态分析，指出运行时可能出现&lt;code&gt;NullPointerException&lt;/code&gt;的地方。在运行时，会在null指针不该出现的地方产生&lt;code&gt;IllegalArgumentException&lt;/code&gt;，即使那个&lt;code&gt;null&lt;/code&gt;指针你从来没有 deference 。这样你可以在尽量早期发现并且防止&lt;code&gt;null&lt;/code&gt;指针的出现。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用&lt;code&gt;Optional&lt;/code&gt;类型。Java 8 和 Swift 之类的语言，提供了一种叫&lt;code&gt;Optional&lt;/code&gt;的类型。正确的使用这种类型，可以在很大程度上避免&lt;code&gt;null&lt;/code&gt;的问题。&lt;code&gt;null&lt;/code&gt;指针的问题之所以存在，是因为你可以在没有&amp;ldquo;检查&amp;rdquo;&lt;code&gt;null&lt;/code&gt;的情况下，&amp;ldquo;访问&amp;rdquo;对象的成员。
    &lt;code&gt;Optional&lt;/code&gt;类型的设计原理，就是把&amp;ldquo;检查&amp;rdquo;和&amp;ldquo;访问&amp;rdquo;这两个操作合二为一，成为一个&amp;ldquo;原子操作&amp;rdquo;。这样你没法只访问，而不进行检查。这种做法其实是ML，Haskell等语言里的模式匹配（pattern matching）的一个特例。模式匹配使得类型判断和访问成员这两种操作合二为一，所以你没法犯错。&lt;/p&gt;
&lt;p&gt;比如，在Swift里面，你可以这样写：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nv"&gt;found&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;find&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nv"&gt;content&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="bp"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"found: "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你从&lt;code&gt;find()&lt;/code&gt;函数得到一个&lt;code&gt;Optional&lt;/code&gt;类型的值&lt;code&gt;found&lt;/code&gt;。假设它的类型是&lt;code&gt;String?&lt;/code&gt;，那个问号表示它可能包含一个&lt;code&gt;String&lt;/code&gt;，也可能是&lt;code&gt;nil&lt;/code&gt;。然后你就可以用一种特殊的&lt;code&gt;if&lt;/code&gt;语句，同时进行&lt;code&gt;null&lt;/code&gt;检查和访问其中的内容。这个if语句跟普通的&lt;code&gt;if&lt;/code&gt;语句不一样，它的条件不是一个&lt;code&gt;Bool&lt;/code&gt;，而是一个变量绑定&lt;code&gt;let content = found&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;我不是很喜欢这语法，不过这整个语句的含义是：如果&lt;code&gt;found&lt;/code&gt;是&lt;code&gt;nil&lt;/code&gt;，那么整个&lt;code&gt;if&lt;/code&gt;语句被略过。如果它不是&lt;code&gt;nil&lt;/code&gt;，那么变量&lt;code&gt;content&lt;/code&gt;被绑定到&lt;code&gt;found&lt;/code&gt;里面的值（unwrap操作），然后执行&lt;code&gt;print("found: " + content)&lt;/code&gt;。由于这种写法把检查和访问合并在了一起，你没法只进行访问而不检查。&lt;/p&gt;
&lt;p&gt;Java 8 的做法比较蹩脚一些。如果你得到一个&lt;code&gt;Optional&lt;/code&gt;类型的值&lt;code&gt;found&lt;/code&gt;，你必须使用&amp;ldquo;函数式编程&amp;rdquo;的方式，来写这之后的代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;ifPresent&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;-&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"found: "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这段Java代码跟上面的Swift代码等价，它包含一个&amp;ldquo;判断&amp;rdquo;和一个&amp;ldquo;取值&amp;rdquo;操作。&lt;code&gt;ifPresent&lt;/code&gt;先判断&lt;code&gt;found&lt;/code&gt;是否有值（相当于判断是不是&lt;code&gt;null&lt;/code&gt;）。如果有，那么将其内容&amp;ldquo;绑定&amp;rdquo;到&lt;code&gt;lambda&lt;/code&gt;表达式的 &lt;code&gt;content&lt;/code&gt; 参数（unwrap操作），然后执行&lt;code&gt;lambda&lt;/code&gt;里面的内容，否则如果&lt;code&gt;found&lt;/code&gt;没有内容，那么&lt;code&gt;ifPresent&lt;/code&gt;里面的&lt;code&gt;lambda&lt;/code&gt;不执行。&lt;/p&gt;
&lt;p&gt;Java的这种设计有个问题。判断&lt;code&gt;null&lt;/code&gt;之后分支里的内容，全都得写在&lt;code&gt;lambda&lt;/code&gt;里面。在函数式编程里，这个&lt;code&gt;lambda&lt;/code&gt;叫做&amp;ldquo;continuation&amp;rdquo;，Java把它叫做 &amp;ldquo;Consumer&amp;rdquo;，它表示&amp;ldquo;如果&lt;code&gt;found&lt;/code&gt;不是&lt;code&gt;null&lt;/code&gt;，拿到它的值，然后应该做什么&amp;rdquo;。由于&lt;code&gt;lambda&lt;/code&gt;是个函数，你不能在里面写&lt;code&gt;return&lt;/code&gt;语句返回出外层的函数。比如，如果你要改写下面这个函数（含有&lt;code&gt;null&lt;/code&gt;）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;就会比较麻烦。因为如果你写成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;ifPresent&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;-&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;    &lt;span class="c1"&gt;// can't return from foo here&lt;/span&gt;
            &lt;span class="o"&gt;});&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;里面的&lt;code&gt;return content&lt;/code&gt;，并不能从函数&lt;code&gt;foo&lt;/code&gt;返回出去。它只会从&lt;code&gt;lambda&lt;/code&gt;返回，而且由于那个&lt;code&gt;lambda（Consumer.accept）&lt;/code&gt;的返回类型必须是&lt;code&gt;void&lt;/code&gt;，编译器会报错，说你返回了&lt;code&gt;String&lt;/code&gt;。由于Java里&lt;code&gt;closure&lt;/code&gt;的自由变量是只读的，你没法对&lt;code&gt;lambda&lt;/code&gt;外面的变量进行赋值，所以你也不能采用这种写法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;ifPresent&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;-&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;    &lt;span class="c1"&gt;// can't assign to result&lt;/span&gt;
            &lt;span class="o"&gt;});&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;所以，虽然你在&lt;code&gt;lambda&lt;/code&gt;里面得到了&lt;code&gt;found&lt;/code&gt;的内容，如何使用这个值，如何返回一个值，却让人摸不着头脑。
你平时的那些Java编程手法，在这里几乎完全废掉了。
实际上，判断&lt;code&gt;null&lt;/code&gt;之后，你必须使用 Java 8 提供的一系列古怪的函数式编程操作：&lt;code&gt;map&lt;/code&gt;, &lt;code&gt;flatMap&lt;/code&gt;, &lt;code&gt;orElse&lt;/code&gt;之类，想法把它们组合起来，才能表达出原来代码的意思。比如之前的代码，只能改写成这样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;orElse&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这简单的情况还好。复杂一点的代码，我还真不知道怎么表达，我怀疑Java 8
的&lt;code&gt;Optional&lt;/code&gt;类型的方法，到底有没有提供足够的表达力。那里面少数几个东西表达能力不咋的，
论工作原理，却可以扯到&lt;code&gt;functor&lt;/code&gt;，&lt;code&gt;continuation&lt;/code&gt;，甚至&lt;code&gt;monad&lt;/code&gt;等高深的理论&amp;hellip;&amp;hellip; 
仿佛用了&lt;code&gt;Optional&lt;/code&gt;之后，这语言就不再是Java了一样。&lt;/p&gt;
&lt;p&gt;所以Java虽然提供了&lt;code&gt;Optional&lt;/code&gt;，但我觉得可用性其实比较低，难以被人接受。相比之下，
Swift的设计更加简单直观，接近普通的过程式编程。你只需要记住一个特殊的语法
&lt;code&gt;if let content = found {...}&lt;/code&gt;，里面的代码写法，跟普通的过程式语言没有任何差别。&lt;/p&gt;
&lt;p&gt;总之你只要记住，使用&lt;code&gt;Optional&lt;/code&gt;类型，要点在于&amp;ldquo;原子操作&amp;rdquo;，使得&lt;code&gt;null&lt;/code&gt;检查与取值合二为一。
这要求你必须使用我刚才介绍的特殊写法。如果你违反了这一原则，把检查和取值分成两步做，
还是有可能犯错误。比如在 Java 8 里面，你可以使用&lt;code&gt;found.get()&lt;/code&gt;这样的方式直接访问&lt;code&gt;found&lt;/code&gt;
里面的内容。在Swift里你也可以使用&lt;code&gt;found!&lt;/code&gt;来直接访问而不进行检查。&lt;/p&gt;
&lt;p&gt;你可以写这样的Java代码来使用Optional类型：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Option&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;isPresent&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"found: "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你使用这种方式，把检查和取值分成两步做，就可能会出现运行时错误。&lt;code&gt;if (found.isPresent())&lt;/code&gt;本质上跟普通的&lt;code&gt;null&lt;/code&gt;检查，
其实没什么两样。如果你忘记判断&lt;code&gt;found.isPresent()&lt;/code&gt;，直接进行&lt;code&gt;found.get()&lt;/code&gt;，就会出现&lt;code&gt;NoSuchElementException&lt;/code&gt;。
这跟&lt;code&gt;NullPointerException&lt;/code&gt;本质上是一回事。所以这种写法，比起普通的&lt;code&gt;null&lt;/code&gt;的用法，
其实换汤不换药。如果你要用&lt;code&gt;Optional&lt;/code&gt;类型而得到它的益处，请务必遵循我之前介绍的&amp;ldquo;原子操作&amp;rdquo;写法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="fang zhi guo du gong cheng"&gt;防止过度工程&lt;/h2&gt;
&lt;p&gt;人的脑子真是奇妙的东西。虽然大家都知道过度工程（over-engineering）不好，在实际的工程中却经常不由自主的出现过度工程。
我自己也犯过好多次这种错误，所以觉得有必要分析一下，过度工程出现的信号和兆头，这样可以在初期的时候就及时发现并且避免。&lt;/p&gt;
&lt;p&gt;过度工程即将出现的一个重要信号，就是当你过度的思考&amp;ldquo;将来&amp;rdquo;，考虑一些还没有发生的事情，还没有出现的需求。
比如，&amp;ldquo;如果我们将来有了上百万行代码，有了几千号人，这样的工具就支持不了了&amp;rdquo;，&amp;ldquo;将来我可能需要这个功能，
所以我现在就把代码写来放在那里&amp;rdquo;，&amp;ldquo;将来很多人要扩充这片代码，所以现在我们就让它变得可重用&amp;rdquo;&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;这就是为什么很多软件项目如此复杂。实际上没做多少事情，却为了所谓的&amp;ldquo;将来&amp;rdquo;，加入了很多不必要的复杂性。
眼前的问题还没解决呢，就被&amp;ldquo;将来&amp;rdquo;给拖垮了。人们都不喜欢目光短浅的人，然而在现实的工程中，有时候你就
是得看近一点，把手头的问题先搞定了，再谈以后扩展的问题。&lt;/p&gt;
&lt;p&gt;另外一种过度工程的来源，是过度的关心&amp;ldquo;代码重用&amp;rdquo;。很多人&amp;ldquo;可用&amp;rdquo;的代码还没写出来呢，就在关心&amp;ldquo;重用&amp;rdquo;。
为了让代码可以重用，最后被自己搞出来的各种框架捆住手脚，最后连可用的代码就没写好。如果可用的代
码都写不好，又何谈重用呢？很多一开头就考虑太多重用的工程，到后来被人完全抛弃，没人用了，因为别
人发现这些代码太难懂了，自己从头开始写一个，反而省好多事。&lt;/p&gt;
&lt;p&gt;过度地关心&amp;ldquo;测试&amp;rdquo;，也会引起过度工程。有些人为了测试，把本来很简单的代码改成&amp;ldquo;方便测试&amp;rdquo;的形式，结
果引入很多复杂性，以至于本来一下就能写对的代码，最后复杂不堪，出现很多bug。&lt;/p&gt;
&lt;p&gt;世界上有两种&amp;ldquo;没有bug&amp;rdquo;的代码。一种是&amp;ldquo;没有明显的bug的代码&amp;rdquo;，另一种是&amp;ldquo;明显没有bug的代码&amp;rdquo;。
第一种情况，由于代码复杂不堪，加上很多测试，各种coverage，貌似测试都通过了，所以就认为代码是正确的。
第二种情况，由于代码简单直接，就算没写很多测试，你一眼看去就知道它不可能有bug。你喜欢哪一种&amp;ldquo;没有bug&amp;rdquo;的代码呢？&lt;/p&gt;
&lt;p&gt;根据这些，我总结出来的防止过度工程的原则如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先把眼前的问题解决掉，解决好，再考虑将来的扩展问题。&lt;/li&gt;
&lt;li&gt;先写出可用的代码，反复推敲，再考虑是否需要重用的问题。&lt;/li&gt;
&lt;li&gt;先写出可用，简单，明显没有bug的代码，再考虑测试的问题。&lt;/li&gt;
&lt;/ol&gt;</content></entry><entry><title>Markdown 数学公式指导手册</title><link href="https://freeopen.github.io/posts/markdown-shu-xue-gong-shi-zhi-dao-shou-ce" rel="alternate"></link><published>2017-05-11T00:00:00+08:00</published><updated>2017-05-11T00:00:00+08:00</updated><author><name>潘嘉豪(编译), freeopen(修订)</name></author><id>tag:freeopen.github.io,2017-05-11:/posts/markdown-shu-xue-gong-shi-zhi-dao-shou-ce</id><summary type="html">&lt;p&gt;本手册的主要内容来自 &lt;a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference"&gt;mathjax-basic-tutorial-and-quick-reference&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: 鼠标右键单击公式表达式, 选择 "Show Math As &amp;gt; TeX Commands", 即可查看公式的LaTex代码。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="yi , gong shi shi yong can kao"&gt;一、公式使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he cha ru gong shi"&gt;1．如何插入公式&lt;/h3&gt;
&lt;p&gt;LaTex 的数学公式有两种：行中公式和独立公式。行中公式放在文中与其它文字混编，独立公式单独成行。&lt;/p&gt;
&lt;p&gt;行中公式示例: &lt;code&gt;$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$&lt;/code&gt; &lt;span class="math"&gt;\(\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;独立公式示例：&lt;code&gt;$$\sum_{i …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;本手册的主要内容来自 &lt;a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference"&gt;mathjax-basic-tutorial-and-quick-reference&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: 鼠标右键单击公式表达式, 选择 "Show Math As &amp;gt; TeX Commands", 即可查看公式的LaTex代码。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="yi , gong shi shi yong can kao"&gt;一、公式使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he cha ru gong shi"&gt;1．如何插入公式&lt;/h3&gt;
&lt;p&gt;LaTex 的数学公式有两种：行中公式和独立公式。行中公式放在文中与其它文字混编，独立公式单独成行。&lt;/p&gt;
&lt;p&gt;行中公式示例: &lt;code&gt;$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$&lt;/code&gt; &lt;span class="math"&gt;\(\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;独立公式示例：&lt;code&gt;$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$&lt;/code&gt; &lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: 以下代码示例，为排版方便，将省略公式插入符号 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;自动编号的公式可以用如下方法表示：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若需要手动编号，参见 &lt;a href="#大括号和行标的使用"&gt;大括号和行标的使用&lt;/a&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{equation}
数学公式
\label{eq:当前公式名}
\end{equation}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;自动编号后的公式可在全文任意处使用&lt;code&gt;\eqref{eq:公式名}&lt;/code&gt; 语句引用。&lt;/p&gt;
&lt;h3 id="2.ru he shu ru shang xia biao"&gt;2．如何输入上下标&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;^&lt;/code&gt; 表示上标,&lt;code&gt;_&lt;/code&gt; 表示下标。如果上下标的内容多于一个字符，需要用 &lt;code&gt;{}&lt;/code&gt; 将这些内容括成一个整体。上下标可以嵌套，也可以同时使用。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x^{y^z}=(1+{\rm e}^x)^{-2xy^w} 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ x^{y^z}=(1+{\rm e}^x)^{-2xy^w} $$&lt;/div&gt;
&lt;p&gt;另外，如果要在左右两边都有上下标，可以用 &lt;code&gt;\sideset&lt;/code&gt; 命令。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\sideset{^1_2}{^3_4}\bigotimes 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \sideset{^1_2}{^3_4}\bigotimes $$&lt;/div&gt;
&lt;h3 id="3.ru he shu ru gua hao he fen ge fu"&gt;3．如何输入括号和分隔符&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;()&lt;/code&gt;、&lt;code&gt;[]&lt;/code&gt; 和 &lt;code&gt;|&lt;/code&gt; 表示符号本身，使用 &lt;code&gt;\{\}&lt;/code&gt; 来表示 &lt;code&gt;{}&lt;/code&gt; 。当要显示大号的括号或分隔符时，要用 &lt;code&gt;\left&lt;/code&gt; 和 &lt;code&gt;\right&lt;/code&gt; 命令。&lt;/p&gt;
&lt;p&gt;一些特殊的括号：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示}  \\ 
\hline
\text{\langle} &amp;amp; \langle &amp;amp; \text{\rangle} &amp;amp; \rangle \\  
\text{\lceil } &amp;amp; \lceil  &amp;amp; \text{\rceil} &amp;amp;  \rceil  \\  
\text{\lfloor} &amp;amp; \lfloor &amp;amp; \text{\rfloor} &amp;amp; \rfloor \\
\text{\lbrace} &amp;amp; \lbrace &amp;amp; \text{\rbrace} &amp;amp; \rbrace \\
\hline 
\end{array}
$$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right) 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right) $$&lt;/div&gt;
&lt;p&gt;有时候要用 &lt;code&gt;\left.&lt;/code&gt; 或 &lt;code&gt;\right.&lt;/code&gt; 进行匹配而不显示本身。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\left. \frac{{\rm d}u}{{\rm d}x} \right| _{x=0} 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \left. \frac{{\rm d}u}{{\rm d}x} \right| _{x=0} $$&lt;/div&gt;
&lt;h3 id="4.ru he shu ru fen shu"&gt;4．如何输入分数&lt;/h3&gt;
&lt;p&gt;通常使用 &lt;code&gt;\frac {分子} {分母}&lt;/code&gt; 命令产生一个分数，分数可嵌套。 
便捷情况可直接输入 \frac ab 来快速生成一个  。 
如果分式很复杂，亦可使用 &lt;code&gt;分子 \over 分母&lt;/code&gt; 命令，此时分数仅有一层。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\frac{a-1}{b-1} \quad and \quad {a+1\over b+1}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\frac{a-1}{b-1} \quad and \quad {a+1\over b+1}$$&lt;/div&gt;
&lt;h3 id="5.ru he shu ru kai fang"&gt;5．如何输入开方&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\sqrt [根指数，省略时为2] {被开方数}&lt;/code&gt; 命令输入开方。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\sqrt{2} \quad and \quad \sqrt[n]{3}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\sqrt{2} \quad and \quad \sqrt[n]{3}$$&lt;/div&gt;
&lt;h3 id="6.ru he shu ru sheng lue hao"&gt;6．如何输入省略号&lt;/h3&gt;
&lt;p&gt;数学公式中常见的省略号有两种，&lt;code&gt;\ldots&lt;/code&gt; 表示与文本底线对齐的省略号，&lt;code&gt;\cdots&lt;/code&gt; 表示与文本中线对齐的省略号。&lt;/p&gt;
&lt;p&gt;```f(x_1,x_2,\underbrace{\ldots}&lt;em _rm="\rm" cdots=""&gt;{\rm ldots} ,x_n) = x_1^2 + x_2^2 + \underbrace{\cdots}&lt;/em&gt; + x_n^2&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$$f(x_1,x_2,\underbrace{\ldots}_{\rm ldots} ,x_n) = x_1^2 + x_2^2 + \underbrace{\cdots}_{\rm cdots} + x_n^2$$

### 7．如何输入矢量

使用 `\vec{矢量}` 来自动产生一个矢量。也可以使用 `\overrightarrow` 等命令自定义字母上方的符号。


```\vec{a} \cdot \vec{b}=0
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\vec{a} \cdot \vec{b}=0$$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\overleftarrow{xy} \quad and \quad \overleftrightarrow{xy} \quad and \quad \overrightarrow{xy}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\overleftarrow{xy} \quad and \quad \overleftrightarrow{xy} \quad and \quad \overrightarrow{xy}$$&lt;/div&gt;
&lt;h3 id="8.ru he shu ru ji fen"&gt;8．如何输入积分&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\int_积分下限^积分上限 {被积表达式}&lt;/code&gt; 来输入一个积分。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\int_0^1 {x^2} \,{\rm d}x
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\int_0^1 {x^2} \,{\rm d}x$$&lt;/div&gt;
&lt;p&gt;本例中 &lt;code&gt;\,&lt;/code&gt; 和 &lt;code&gt;{\rm d}&lt;/code&gt; 部分可省略，但建议加入，能使式子更美观。&lt;/p&gt;
&lt;h3 id="9.ru he shu ru ji xian yun suan"&gt;9．如何输入极限运算&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\lim_{变量 \to 表达式}&lt;/code&gt; 表达式 来输入一个极限。如有需求，可以更改 &lt;code&gt;\to&lt;/code&gt; 符号至任意符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\lim_{n \to +\infty} \frac{1}{n(n+1)} \quad and \quad \lim_{x\leftarrow{示例}} \frac{1}{n(n+1)} 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \lim_{n \to +\infty} \frac{1}{n(n+1)} \quad and \quad \lim_{x\leftarrow{示例}} \frac{1}{n(n+1)} $$&lt;/div&gt;
&lt;h3 id="10.ru he shu ru lei jia , lei cheng yun suan"&gt;10．如何输入累加、累乘运算&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\sum_{下标表达式}^{上标表达式} {累加表达式}&lt;/code&gt; 来输入一个累加。 
与之类似，使用 &lt;code&gt;\prod \bigcup \bigcap&lt;/code&gt; 来分别输入累乘、并集和交集。 
此类符号在行内显示时上下标表达式将会移至右上角和右下角。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sum_{i=1}^n \frac{1}{i^2} \quad and \quad \prod_{i=1}^n \frac{1}{i^2} \quad and \quad \bigcup_{i=1}^{2} R
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^n \frac{1}{i^2} \quad and \quad \prod_{i=1}^n \frac{1}{i^2} \quad and \quad \bigcup_{i=1}^{2} R$$&lt;/div&gt;
&lt;h3 id="11.ru he shu ru xi la zi mu"&gt;11．如何输入希腊字母&lt;/h3&gt;
&lt;p&gt;输入 &lt;code&gt;\小写希腊字母英文全称&lt;/code&gt; 和 &lt;code&gt;\首字母大写希腊字母英文全称&lt;/code&gt; 来分别输入小写和大写希腊字母。 
对于大写希腊字母与现有字母相同的，直接输入大写字母即可。&lt;/p&gt;
&lt;div class="math"&gt;\begin{array}{cccc|cccc|cccc}
\hline
\text{大写} &amp;amp; \text{小写} &amp;amp; \text{名称} &amp;amp; \text{音标} &amp;amp; \text{大写} &amp;amp; \text{小写} &amp;amp; \text{名称} &amp;amp; \text{音标} \\ 
\hline
A &amp;amp; \alpha &amp;amp; alpha &amp;amp; \text{/ˈ&amp;aelig;lfə/} &amp;amp; N   &amp;amp; \nu &amp;amp; nu &amp;amp; \text{/nju:/}  \\
B &amp;amp; \beta  &amp;amp; beta  &amp;amp; \text{/ˈbeɪtə/} &amp;amp; \Xi &amp;amp; \xi &amp;amp; xi &amp;amp; \text{/ˈzaɪ/ or /ˈksaɪ/}  \\
\Gamma &amp;amp; \gamma &amp;amp; gamma &amp;amp; \text{/ˈg&amp;aelig;mə/} &amp;amp; O   &amp;amp; o &amp;amp; omicron &amp;amp; \text{/əuˈmaikrən/ or /ˈɑmɪˌkrɑn/} \\
\Delta &amp;amp; \delta &amp;amp; delta &amp;amp; \text{/ˈdeltə/} &amp;amp; \Pi &amp;amp; \pi &amp;amp; pi &amp;amp; \text{/paɪ/}  \\
E &amp;amp; \epsilon &amp;amp; epsilon &amp;amp; \text{/ˈepsɪlɒn/} &amp;amp; P      &amp;amp; \rho  &amp;amp; rho &amp;amp; \text{/rəʊ/}  \\
Z &amp;amp; \zeta    &amp;amp; zeta &amp;amp; \text{/ˈzi:tə/}  &amp;amp; \Sigma &amp;amp; \sigma &amp;amp; sigma &amp;amp; \text{/ˈsɪɡmə/}   \\
H &amp;amp; \eta     &amp;amp; eta  &amp;amp; \text{/ˈi:tə/}   &amp;amp; T &amp;amp; \tau &amp;amp; tau &amp;amp; \text{/tɔ:/ or /taʊ/}   \\
\Theta &amp;amp; \theta &amp;amp; theta &amp;amp; \text{/ˈ&amp;theta;i:tə/} &amp;amp; \Upsilon &amp;amp; \upsilon &amp;amp; upsilon  &amp;amp; \text{/ˈipsilon/ or /ˈʌpsɨlɒn/} \\
I &amp;amp; \iota  &amp;amp; iota &amp;amp; \text{/aɪˈəʊtə/} &amp;amp; \Phi &amp;amp; \phi &amp;amp; phi &amp;amp; \text{/faɪ/} \\
K &amp;amp; \kappa &amp;amp; kappa &amp;amp; \text{/ˈk&amp;aelig;pə/} &amp;amp; X    &amp;amp; \chi &amp;amp; chi &amp;amp; \text{/kaɪ/} \\
\Lambda &amp;amp; \lambda &amp;amp; lambda &amp;amp; \text{/ˈl&amp;aelig;mdə/} &amp;amp; \Psi &amp;amp; \psi &amp;amp; psi &amp;amp; \text{/psaɪ/} \\
M  &amp;amp; \mu &amp;amp; mu &amp;amp; \text{/mju:/} &amp;amp; \Omega &amp;amp; \omega &amp;amp; omega &amp;amp; \text{/ˈəʊmɪɡə/ or /oʊˈmeɡə/} \\
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;输入法对照表&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{cc|cc|cc}
\hline
\text{大写输入} &amp;amp; \text{小写输入}  &amp;amp; \text{大写输入} &amp;amp; \text{小写输入} &amp;amp;  \text{大写输入} &amp;amp; \text{小写输入}  \\ 
\hline
\text{A} &amp;amp; \text{\alpha} &amp;amp;  \text{I} &amp;amp; \text{\iota}  &amp;amp;  \text{P}      &amp;amp; \text{\rho}  \\
\text{B} &amp;amp; \text{\beta}  &amp;amp; \text{K} &amp;amp; \text{\kappa} &amp;amp; \text{\Sigma} &amp;amp; \text{\sigma} \\
\text{\Gamma} &amp;amp; \text{\gamma}  &amp;amp; \text{\Lambda} &amp;amp; \text{\lambda} &amp;amp; \text{T} &amp;amp; \text{\tau}  \\
\text{\Delta} &amp;amp; \text{\delta}  &amp;amp; \text{M}  &amp;amp; \text{\mu}  &amp;amp; \text{\Upsilon} &amp;amp; \text{\upsilon}  \\
\text{E} &amp;amp; \text{\epsilon} &amp;amp; \text{N}   &amp;amp; \text{\nu} &amp;amp;  \text{\Phi} &amp;amp; \text{\phi}  \\
\text{Z} &amp;amp; \text{\zeta}    &amp;amp; \text{\Xi} &amp;amp; \text{\xi} &amp;amp; \text{X}    &amp;amp; \text{\chi}  \\
\text{H} &amp;amp; \text{\eta}     &amp;amp; \text{O}   &amp;amp; \text{o} &amp;amp; \text{\Psi} &amp;amp; \text{\psi}  \\
\text{\Theta} &amp;amp; \text{\theta}  &amp;amp; \text{\Pi} &amp;amp; \text{\pi} &amp;amp; \text{\Omega} &amp;amp; \text{\omega}  \\
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;部分字母有变量专用形式，以 &lt;code&gt;\var&lt;/code&gt; 开头。 &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{c|c|c|l}
\hline
\text{小写} &amp;amp; \text{大写}  &amp;amp; \text{变量形式} &amp;amp; \text{输入}  \\ 
\hline
\epsilon &amp;amp;  E &amp;amp; \varepsilon &amp;amp; \text{\varepsilon} \\ 
\theta &amp;amp;    \Theta &amp;amp;    \vartheta &amp;amp;  \text{\vartheta} \\    
\rho &amp;amp;  P &amp;amp; \varrho &amp;amp;  \text{\varrho} \\    
\sigma &amp;amp;    \Sigma &amp;amp;    \varsigma &amp;amp; \text{\varsigma} \\     
\phi &amp;amp;  \Phi &amp;amp;  \varphi &amp;amp;  \text{\varphi} \\
\hline
\end{array}&lt;/div&gt;
&lt;h3 id="12.ru he shu ru qi ta te shu zi fu"&gt;12．如何输入其它特殊字符&lt;/h3&gt;
&lt;p&gt;若需要显示更大或更小的字符，在符号前插入 &lt;code&gt;\large&lt;/code&gt; 或 &lt;code&gt;\small&lt;/code&gt; 命令。&lt;/p&gt;
&lt;p&gt;若找不到需要的符号，使用 &lt;a href="http://detexify.kirelabs.org/classify.html"&gt;Detexify&lt;/a&gt; 来画出想要的符号。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1)．关系运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{\pm} &amp;amp; \pm &amp;amp; \text{\times} &amp;amp; \times &amp;amp; \text{\div} &amp;amp; \div &amp;amp; \text{\mid} &amp;amp; \mid \\  
\text{\nmid} &amp;amp; \nmid &amp;amp; \text{\cdot} &amp;amp; \cdot &amp;amp; \text{\circ} &amp;amp; \circ &amp;amp; \text{\ast} &amp;amp; \ast \\  
\text{\bigodot} &amp;amp; \bigodot &amp;amp; \text{\bigotimes} &amp;amp; \bigotimes &amp;amp; \text{\bigoplus} &amp;amp; \bigoplus &amp;amp; \text{\leq} &amp;amp; \leq \\  
\text{\geq} &amp;amp; \geq &amp;amp; \text{\neq} &amp;amp; \neq &amp;amp; \text{\approx} &amp;amp; \approx &amp;amp; \text{\equiv} &amp;amp; \equiv \\  
\text{\sum} &amp;amp; \sum &amp;amp; \text{\prod} &amp;amp; \prod &amp;amp; \text{\coprod} &amp;amp; \coprod &amp;amp; \text{\backslash} &amp;amp; \backslash \\    
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(2)．集合运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{\emptyset} &amp;amp; \emptyset &amp;amp; \text{\in} &amp;amp; \in &amp;amp; \text{\notin} &amp;amp; \notin \\ 
\text{\subset} &amp;amp; \subset &amp;amp; \text{\supset} &amp;amp; \supset &amp;amp; \text{\subseteq} &amp;amp; \subseteq \\   
\text{\supseteq} &amp;amp; \supseteq &amp;amp; \text{\bigcap} &amp;amp; \bigcap &amp;amp; \text{\bigcup} &amp;amp; \bigcup \\   
\text{\bigvee} &amp;amp; \bigvee &amp;amp; \text{\bigwedge} &amp;amp; \bigwedge &amp;amp; \text{\biguplus} &amp;amp; \biguplus \\   
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(3)．对数运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{\log} &amp;amp; \log &amp;amp; \text{\lg} &amp;amp; \lg &amp;amp; \text{\ln} &amp;amp; \ln \\ 
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(4)．三角运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{30^\circ} &amp;amp; 30^\circ    &amp;amp; \text{\bot} &amp;amp; \bot &amp;amp; \text{\angle A} &amp;amp; \angle A \\  
\text{\sin} &amp;amp; \sin    &amp;amp; \text{\cos} &amp;amp; \cos &amp;amp; \text{\tan} &amp;amp; \tan \\  
\text{\csc} &amp;amp; \csc  &amp;amp; \text{\sec} &amp;amp; \sec  &amp;amp; \text{\cot} &amp;amp; \cot \\   
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(5)．微积分运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{\int} &amp;amp; \int    &amp;amp; \text{\iint} &amp;amp; \iint &amp;amp; \text{\iiint} &amp;amp; \iiint \\    
\text{\iiiint} &amp;amp; \iiiint  &amp;amp; \text{\oint} &amp;amp; \oint &amp;amp; \text{\prime} &amp;amp; \prime \\    
\text{\lim} &amp;amp; \lim  &amp;amp; \text{\infty} &amp;amp; \infty  &amp;amp; \text{\nabla} &amp;amp; \nabla \\   
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(6)．逻辑运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示}  \\ 
\hline
\text{\because} &amp;amp; \because &amp;amp; \text{\therefore} &amp;amp; \therefore \\  
\text{\forall} &amp;amp; \forall &amp;amp; \text{\exists} &amp;amp; \exists \\  
\text{\not\subset} &amp;amp; \not\subset &amp;amp; \text{\not&amp;lt;} &amp;amp; \not&amp;lt;  \\ 
\text{\not&amp;gt;} &amp;amp; \not&amp;gt; &amp;amp;  \text{\not=} &amp;amp; \not=  \\    
\hline
\end{array}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(7)．戴帽符号&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示}  \\ 
\hline
\text{\hat{xy}} &amp;amp; \hat{xy}    &amp;amp; \text{\widehat{xyz}} &amp;amp; \widehat{xyz} \\ 
\text{\tilde{xy}} &amp;amp; \tilde{xy}    &amp;amp; \text{\widetilde{xyz}} &amp;amp; \widetilde{xyz} \\ 
\text{\check{x}} &amp;amp; \check{x}    &amp;amp; \text{\breve{y}} &amp;amp; \breve{y}  \\  
\text{\grave{x}} &amp;amp; \grave{x} &amp;amp;  \text{\acute{y}} &amp;amp; \acute{y}   \\   
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(8)．连线符号&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c}
\hline
\text{输入} &amp;amp; \text{显示}    \\ 
\hline
\text{\fbox{a+b+c+d}} &amp;amp; \fbox{a+b+c+d}  \\     
\hline
\text{\overleftarrow{a+b+c+d}} &amp;amp; \overleftarrow{a+b+c+d} \\ 
\hline
\text{\overrightarrow{a+b+c+d}} &amp;amp; \overrightarrow{a+b+c+d}  \\  
\hline
\text{\overleftrightarrow{a+b+c+d}} &amp;amp; \overleftrightarrow{a+b+c+d} \\   
\hline
\text{\underleftarrow{a+b+c+d}} &amp;amp; \underleftarrow{a+b+c+d}  \\ 
\hline
\text{\underrightarrow{a+b+c+d}} &amp;amp; \underrightarrow{a+b+c+d}  \\    
\hline
\text{\underleftrightarrow{a+b+c+d}} &amp;amp; \underleftrightarrow{a+b+c+d} \\ 
\hline
\text{\overline{a+b+c+d}} &amp;amp; \overline{a+b+c+d}   \\ 
\hline
\text{\underline{a+b+c+d}} &amp;amp; \underline{a+b+c+d}    \\ 
\hline
\text{\overbrace{a+b+c+d}^{Sample}} &amp;amp; \overbrace{a+b+c+d}^{Sample}  \\  
\hline
\text{\underbrace{a+b+c+d}_{Sample}} &amp;amp; \underbrace{a+b+c+d}_{Sample} \\  
\hline
\text{\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}} &amp;amp; \overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}  \\  
\hline
\text{\underbrace{a\cdot a\cdots a}_{b\text{ times}}} &amp;amp; \underbrace{a\cdot a\cdots a}_{b\text{ times}}  \\ 
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(9)．箭头符号&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示}  \\ 
\hline
\text{\uparrow} &amp;amp; \uparrow    &amp;amp; \text{\Uparrow} &amp;amp; \Uparrow \\   
\text{\downarrow} &amp;amp; \downarrow    &amp;amp; \text{\Downarrow} &amp;amp; \Downarrow \\   
\text{\leftarrow} &amp;amp; \leftarrow  &amp;amp; \text{\Leftarrow} &amp;amp; \Leftarrow  \\    
\text{\rightarrow or \to} &amp;amp; \rightarrow &amp;amp;   \text{\Rightarrow} &amp;amp; \Rightarrow   \\   
\text{\leftrightarrow} &amp;amp; \leftrightarrow    &amp;amp; \text{\Leftrightarrow} &amp;amp; \Leftrightarrow  \\  
\text{\longleftarrow} &amp;amp; \longleftarrow  &amp;amp; \text{\Longleftarrow or \impliedby} &amp;amp; \Longleftarrow  \\  
\text{\longrightarrow} &amp;amp; \longrightarrow    &amp;amp; \text{\Longrightarrow or \implies} &amp;amp; \Longrightarrow  \\  
\text{\longleftrightarrow} &amp;amp; \longleftrightarrow    &amp;amp; \text{\Longleftrightarrow or \iff} &amp;amp; \Longleftrightarrow  \\  
\text{\mapsto} &amp;amp; \mapsto  &amp;amp; \text{ } &amp;amp; \text{ } \\  
\hline
\end{array}&lt;/div&gt;
&lt;h3 id="13.ru he jin xing zi ti zhuan huan"&gt;13．如何进行字体转换&lt;/h3&gt;
&lt;p&gt;若要对公式的某一部分字符进行字体转换，可以用 &lt;code&gt;{\字体 {需转换的部分字符}}&lt;/code&gt; 命令，其中 &lt;code&gt;\字体&lt;/code&gt; 部分可以参照下表选择合适的字体。一般情况下，公式默认为意大利体  。&lt;/p&gt;
&lt;p&gt;示例中 &lt;strong&gt;全部大写&lt;/strong&gt; 的字体仅大写可用。&lt;/p&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{说明} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{说明} &amp;amp; \text{显示}  \\ 
\hline
\text{\rm} &amp;amp; \text{罗马体} &amp;amp; \rm{Sample} &amp;amp; \text{\cal}  &amp;amp; \text{花体} &amp;amp; \cal{SAMPLE} \\  
\text{\it} &amp;amp; \text{意大利体} &amp;amp; \it{Sample} &amp;amp; \text{\Bbb}  &amp;amp; \text{黑板粗体} &amp;amp; \Bbb{SAMPLE} \\   
\text{\bf} &amp;amp; \text{粗体} &amp;amp; \rm{Sample} &amp;amp; \text{\mit}  &amp;amp; \text{数字斜体} &amp;amp; \mit{SAMPLE} \\ 
\text{\sf} &amp;amp; \text{等线体} &amp;amp; \sf{Sample} &amp;amp; \text{\scr}  &amp;amp; \text{手写体} &amp;amp; \scr{SAMPLE} \\ 
\text{\tt} &amp;amp; \text{打字机体} &amp;amp; \tt{Sample} &amp;amp; \text{ }  &amp;amp; \text{ } &amp;amp;  \\ 
\text{\frak} &amp;amp; \text{旧德式体} &amp;amp; \frak{Sample} &amp;amp; \text{ }  &amp;amp; \text{ } &amp;amp;  \\ 
\hline
\end{array}&lt;/div&gt;
&lt;p&gt;转换字体十分常用，例如在积分中：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c | c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\int_0^1 x^2 dx &amp;amp; \int_0^1 x^2 \,{\rm d}x
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c | c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\int_0^1 x^2 dx &amp;amp; \int_0^1 x^2 \,{\rm d}x
\end{array}
$$&lt;/div&gt;
&lt;p&gt;注意比较两个式子间 &lt;code&gt;dx&lt;/code&gt; 的不同。 
使用 &lt;code&gt;\operatorname&lt;/code&gt; 命令也可以达到相同的效果，详见 [定义新的符号 &lt;code&gt;\operatorname&lt;/code&gt;] 。&lt;/p&gt;
&lt;h3 id="14.da gua hao he xing biao de shi yong"&gt;14．大括号和行标的使用&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\left&lt;/code&gt; 和 &lt;code&gt;\right&lt;/code&gt; 来创建自动匹配高度的 (圆括号)，[方括号] 和 {花括号} 。 
在每个公式末尾前使用 &lt;code&gt;\tag{行标}&lt;/code&gt; 来实现行标。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f\left(
   \left[ 
     \frac{
       1+\left\{x,y\right\}
     }{
       \left(
          \frac{x}{y}+\frac{y}{x}
       \right)
       \left(u+1\right)
     }+a
   \right]^{3/2}
\right)
\tag{行标}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
f\left(
   \left[ 
     \frac{
       1+\left\{x,y\right\}
     }{
       \left(
          \frac{x}{y}+\frac{y}{x}
       \right)
       \left(u+1\right)
     }+a
   \right]^{3/2}
\right)
\tag{行标}
$$&lt;/div&gt;
&lt;p&gt;如果你需要在不同的行显示对应括号，可以在每一行对应处使用 &lt;code&gt;\left.&lt;/code&gt; 或 &lt;code&gt;\right.&lt;/code&gt; 来放一个"影子"括号：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{aligned}
a=&amp;amp;\left(1+2+3+  \cdots \right. \\
&amp;amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)
\end{aligned}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
a=&amp;amp;\left(1+2+3+  \cdots \right. \\
&amp;amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;如果你需要将行内显示的分隔符也变大，可以使用 &lt;code&gt;\middle&lt;/code&gt; 命令：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\left\langle  
  q
\middle\|
  \frac{\frac{x}{y}}{\frac{u}{v}}
\middle| 
   p 
\right\rangle
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\left\langle  
  q
\middle\|
  \frac{\frac{x}{y}}{\frac{u}{v}}
\middle| 
   p 
\right\rangle
$$&lt;/div&gt;
&lt;h3 id="15.qi ta ming ling"&gt;15．其它命令&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;(1)．定义新的符号 &lt;code&gt;\operatorname&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;查询 &lt;a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference/15077#15077"&gt;关于此命令的定义&lt;/a&gt; 和 &lt;a href="http://meta.math.stackexchange.com/search?q=operatorname"&gt;关于此命令的讨论&lt;/a&gt; 来进一步了解此命令。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\operatorname{Symbol} A 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \operatorname{Symbol} A $$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(2)．添加注释文字 &lt;code&gt;\text&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 &lt;code&gt;\text {文字}&lt;/code&gt; 中仍可以使用 &lt;code&gt;$公式$&lt;/code&gt; 插入其它公式。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(n)= \begin{cases} n/2, &amp;amp; \text {if $n$ is even} \\ 3n+1, &amp;amp; \text{if $n$ is odd} \end{cases} 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ f(n)= \begin{cases} n/2, &amp;amp; \text {if $n$ is even} \\ 3n+1, &amp;amp; \text{if $n$ is odd} \end{cases} $$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(3)．在字符间加入空格&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有四种宽度的空格可以使用： &lt;code&gt;\,&lt;/code&gt;、&lt;code&gt;\;&lt;/code&gt;、&lt;code&gt;\quad&lt;/code&gt; 和 &lt;code&gt;\qquad&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;a \, b \mid a \; b \mid a \quad b \mid a \qquad b 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ a \, b \mid a \; b \mid a \quad b \mid a \qquad b $$&lt;/div&gt;
&lt;p&gt;当然，使用 &lt;code&gt;\text {n个空格}&lt;/code&gt; 也可以达到同样效果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(4)．更改文字颜色&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用 &lt;code&gt;\color{颜色}{文字}&lt;/code&gt; 来更改特定的文字颜色。 
更改文字颜色 &lt;em&gt;需要浏览器支持&lt;/em&gt; ，如果浏览器不知道你所需的颜色，那么文字将被渲染为黑色。&lt;/p&gt;
&lt;p&gt;对于较旧的浏览器（HTML4与CSS2），支持的颜色较少;
对于较新的浏览器（HTML5与CSS3），额外的124种颜色将被支持
输入 &lt;code&gt;\color {#rgb} {text}&lt;/code&gt; 来自定义更多的颜色，其中 &lt;code&gt;#rgb&lt;/code&gt; 的 &lt;code&gt;r g b&lt;/code&gt; 可输入 &lt;code&gt;0-9&lt;/code&gt; 和 &lt;code&gt;a-f&lt;/code&gt; 来表示红色、绿色和蓝色的纯度（饱和度）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{|rrrrrrrr|}
\hline
\verb+#000+ &amp;amp; \color{#000}{text} &amp;amp; \verb+#005+ &amp;amp; \color{#005}{text} &amp;amp; \verb+#00A+ &amp;amp; \color{#00A}{text} &amp;amp; \verb+#00F+ &amp;amp; \color{#00F}{text}  \\
\verb+#500+ &amp;amp; \color{#500}{text} &amp;amp; \verb+#505+ &amp;amp; \color{#505}{text} &amp;amp; \verb+#50A+ &amp;amp; \color{#50A}{text} &amp;amp; \verb+#50F+ &amp;amp; \color{#50F}{text}  \\
\verb+#A00+ &amp;amp; \color{#A00}{text} &amp;amp; \verb+#A05+ &amp;amp; \color{#A05}{text} &amp;amp; \verb+#A0A+ &amp;amp; \color{#A0A}{text} &amp;amp; \verb+#A0F+ &amp;amp; \color{#A0F}{text}  \\
\verb+#F00+ &amp;amp; \color{#F00}{text} &amp;amp; \verb+#F05+ &amp;amp; \color{#F05}{text} &amp;amp; \verb+#F0A+ &amp;amp; \color{#F0A}{text} &amp;amp; \verb+#F0F+ &amp;amp; \color{#F0F}{text}  \\
\hline
\verb+#080+ &amp;amp; \color{#080}{text} &amp;amp; \verb+#085+ &amp;amp; \color{#085}{text} &amp;amp; \verb+#08A+ &amp;amp; \color{#08A}{text} &amp;amp; \verb+#08F+ &amp;amp; \color{#08F}{text}  \\
\verb+#580+ &amp;amp; \color{#580}{text} &amp;amp; \verb+#585+ &amp;amp; \color{#585}{text} &amp;amp; \verb+#58A+ &amp;amp; \color{#58A}{text} &amp;amp; \verb+#58F+ &amp;amp; \color{#58F}{text}  \\
\verb+#A80+ &amp;amp; \color{#A80}{text} &amp;amp; \verb+#A85+ &amp;amp; \color{#A85}{text} &amp;amp; \verb+#A8A+ &amp;amp; \color{#A8A}{text} &amp;amp; \verb+#A8F+ &amp;amp; \color{#A8F}{text}  \\
\verb+#F80+ &amp;amp; \color{#F80}{text} &amp;amp; \verb+#F85+ &amp;amp; \color{#F85}{text} &amp;amp; \verb+#F8A+ &amp;amp; \color{#F8A}{text} &amp;amp; \verb+#F8F+ &amp;amp; \color{#F8F}{text}  \\
\hline
\verb+#0F0+ &amp;amp; \color{#0F0}{text} &amp;amp; \verb+#0F5+ &amp;amp; \color{#0F5}{text} &amp;amp; \verb+#0FA+ &amp;amp; \color{#0FA}{text} &amp;amp; \verb+#0FF+ &amp;amp; \color{#0FF}{text}  \\
\verb+#5F0+ &amp;amp; \color{#5F0}{text} &amp;amp; \verb+#5F5+ &amp;amp; \color{#5F5}{text} &amp;amp; \verb+#5FA+ &amp;amp; \color{#5FA}{text} &amp;amp; \verb+#5FF+ &amp;amp; \color{#5FF}{text}  \\
\verb+#AF0+ &amp;amp; \color{#AF0}{text} &amp;amp; \verb+#AF5+ &amp;amp; \color{#AF5}{text} &amp;amp; \verb+#AFA+ &amp;amp; \color{#AFA}{text} &amp;amp; \verb+#AFF+ &amp;amp; \color{#AFF}{text}  \\
\verb+#FF0+ &amp;amp; \color{#FF0}{text} &amp;amp; \verb+#FF5+ &amp;amp; \color{#FF5}{text} &amp;amp; \verb+#FFA+ &amp;amp; \color{#FFA}{text} &amp;amp; \verb+#FFF+ &amp;amp; \color{#FFF}{text}  \\
\hline
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{|rrrrrrrr|}
\hline
\verb+#000+ &amp;amp; \color{#000}{text} &amp;amp; \verb+#005+ &amp;amp; \color{#005}{text} &amp;amp; \verb+#00A+ &amp;amp; \color{#00A}{text} &amp;amp; \verb+#00F+ &amp;amp; \color{#00F}{text}  \\
\verb+#500+ &amp;amp; \color{#500}{text} &amp;amp; \verb+#505+ &amp;amp; \color{#505}{text} &amp;amp; \verb+#50A+ &amp;amp; \color{#50A}{text} &amp;amp; \verb+#50F+ &amp;amp; \color{#50F}{text}  \\
\verb+#A00+ &amp;amp; \color{#A00}{text} &amp;amp; \verb+#A05+ &amp;amp; \color{#A05}{text} &amp;amp; \verb+#A0A+ &amp;amp; \color{#A0A}{text} &amp;amp; \verb+#A0F+ &amp;amp; \color{#A0F}{text}  \\
\verb+#F00+ &amp;amp; \color{#F00}{text} &amp;amp; \verb+#F05+ &amp;amp; \color{#F05}{text} &amp;amp; \verb+#F0A+ &amp;amp; \color{#F0A}{text} &amp;amp; \verb+#F0F+ &amp;amp; \color{#F0F}{text}  \\
\hline
\verb+#080+ &amp;amp; \color{#080}{text} &amp;amp; \verb+#085+ &amp;amp; \color{#085}{text} &amp;amp; \verb+#08A+ &amp;amp; \color{#08A}{text} &amp;amp; \verb+#08F+ &amp;amp; \color{#08F}{text}  \\
\verb+#580+ &amp;amp; \color{#580}{text} &amp;amp; \verb+#585+ &amp;amp; \color{#585}{text} &amp;amp; \verb+#58A+ &amp;amp; \color{#58A}{text} &amp;amp; \verb+#58F+ &amp;amp; \color{#58F}{text}  \\
\verb+#A80+ &amp;amp; \color{#A80}{text} &amp;amp; \verb+#A85+ &amp;amp; \color{#A85}{text} &amp;amp; \verb+#A8A+ &amp;amp; \color{#A8A}{text} &amp;amp; \verb+#A8F+ &amp;amp; \color{#A8F}{text}  \\
\verb+#F80+ &amp;amp; \color{#F80}{text} &amp;amp; \verb+#F85+ &amp;amp; \color{#F85}{text} &amp;amp; \verb+#F8A+ &amp;amp; \color{#F8A}{text} &amp;amp; \verb+#F8F+ &amp;amp; \color{#F8F}{text}  \\
\hline
\verb+#0F0+ &amp;amp; \color{#0F0}{text} &amp;amp; \verb+#0F5+ &amp;amp; \color{#0F5}{text} &amp;amp; \verb+#0FA+ &amp;amp; \color{#0FA}{text} &amp;amp; \verb+#0FF+ &amp;amp; \color{#0FF}{text}  \\
\verb+#5F0+ &amp;amp; \color{#5F0}{text} &amp;amp; \verb+#5F5+ &amp;amp; \color{#5F5}{text} &amp;amp; \verb+#5FA+ &amp;amp; \color{#5FA}{text} &amp;amp; \verb+#5FF+ &amp;amp; \color{#5FF}{text}  \\
\verb+#AF0+ &amp;amp; \color{#AF0}{text} &amp;amp; \verb+#AF5+ &amp;amp; \color{#AF5}{text} &amp;amp; \verb+#AFA+ &amp;amp; \color{#AFA}{text} &amp;amp; \verb+#AFF+ &amp;amp; \color{#AFF}{text}  \\
\verb+#FF0+ &amp;amp; \color{#FF0}{text} &amp;amp; \verb+#FF5+ &amp;amp; \color{#FF5}{text} &amp;amp; \verb+#FFA+ &amp;amp; \color{#FFA}{text} &amp;amp; \verb+#FFF+ &amp;amp; \color{#FFF}{text}  \\
\hline
\end{array}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(5)．添加删除线&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用删除线功能必须声明 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;p&gt;在公式内使用 &lt;code&gt;\require{cancel}&lt;/code&gt; 来允许 &lt;em&gt;片段删除线&lt;/em&gt; 的显示。 
声明片段删除线后，使用 &lt;code&gt;\cancel{字符}&lt;/code&gt;、&lt;code&gt;\bcancel{字符}&lt;/code&gt;、&lt;code&gt;\xcancel{字符}&lt;/code&gt; 和 &lt;code&gt;\cancelto{字符}&lt;/code&gt; 来实现各种片段删除线效果。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\require{cancel}\begin{array}{rl}
\verb|y+\cancel{x}| &amp;amp; y+\cancel{x}\\
\verb|\cancel{y+x}| &amp;amp; \cancel{y+x}\\
\verb|y+\bcancel{x}| &amp;amp; y+\bcancel{x}\\
\verb|y+\xcancel{x}| &amp;amp; y+\xcancel{x}\\
\verb|y+\cancelto{0}{x}| &amp;amp; y+\cancelto{0}{x}\\
\verb+\frac{1\cancel9}{\cancel95} = \frac15+&amp;amp; \frac{1\cancel9}{\cancel95} = \frac15 \\
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\require{cancel}\begin{array}{rl}
\verb|y+\cancel{x}| &amp;amp; y+\cancel{x}\\
\verb|\cancel{y+x}| &amp;amp; \cancel{y+x}\\
\verb|y+\bcancel{x}| &amp;amp; y+\bcancel{x}\\
\verb|y+\xcancel{x}| &amp;amp; y+\xcancel{x}\\
\verb|y+\cancelto{0}{x}| &amp;amp; y+\cancelto{0}{x}\\
\verb+\frac{1\cancel9}{\cancel95} = \frac15+&amp;amp; \frac{1\cancel9}{\cancel95} = \frac15 \\
\end{array}
$$&lt;/div&gt;
&lt;p&gt;使用 &lt;code&gt;\require{enclose}&lt;/code&gt; 来允许 整段删除线 的显示。 
声明整段删除线后，使用 &lt;code&gt;\enclose{删除线效果}{字符}&lt;/code&gt; 来实现各种整段删除线效果。 
其中，删除线效果有 &lt;code&gt;horizontalstrike&lt;/code&gt;、&lt;code&gt;verticalstrike&lt;/code&gt;、&lt;code&gt;updiagonalstrike&lt;/code&gt; 和 &lt;code&gt;downdiagonalstrike&lt;/code&gt;，可叠加使用。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\require{enclose}\begin{array}{rl}
\verb|\enclose{horizontalstrike}{x+y}| &amp;amp; \enclose{horizontalstrike}{x+y}\\
\verb|\enclose{verticalstrike}{\frac xy}| &amp;amp; \enclose{verticalstrike}{\frac xy}\\
\verb|\enclose{updiagonalstrike}{x+y}| &amp;amp; \enclose{updiagonalstrike}{x+y}\\
\verb|\enclose{downdiagonalstrike}{x+y}| &amp;amp; \enclose{downdiagonalstrike}{x+y}\\
\verb|\enclose{horizontalstrike,updiagonalstrike}{x+y}| &amp;amp; \enclose{horizontalstrike,updiagonalstrike}{x+y}\\
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\require{enclose}\begin{array}{rl}
\verb|\enclose{horizontalstrike}{x+y}| &amp;amp; \enclose{horizontalstrike}{x+y}\\
\verb|\enclose{verticalstrike}{\frac xy}| &amp;amp; \enclose{verticalstrike}{\frac xy}\\
\verb|\enclose{updiagonalstrike}{x+y}| &amp;amp; \enclose{updiagonalstrike}{x+y}\\
\verb|\enclose{downdiagonalstrike}{x+y}| &amp;amp; \enclose{downdiagonalstrike}{x+y}\\
\verb|\enclose{horizontalstrike,updiagonalstrike}{x+y}| &amp;amp; \enclose{horizontalstrike,updiagonalstrike}{x+y}\\
\end{array}
$$&lt;/div&gt;
&lt;p&gt;此外，&lt;code&gt;\enclose&lt;/code&gt; 命令还可以产生包围的边框和圆等，参见 &lt;a href="https://developer.mozilla.org/en-US/docs/Web/MathML/Element/menclose"&gt;MathML Menclose Documentation&lt;/a&gt; 以查看更多效果。&lt;/p&gt;
&lt;h2 id="er , ju zhen shi yong can kao_1"&gt;二、矩阵使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru wu kuang ju zhen"&gt;1．如何输入无框矩阵&lt;/h3&gt;
&lt;p&gt;在开头使用 &lt;code&gt;begin{matrix}&lt;/code&gt;，在结尾使用 &lt;code&gt;end{matrix}&lt;/code&gt;，在中间插入矩阵元素，每个元素之间插入 &lt;code&gt;&amp;amp;&lt;/code&gt; ，并在每行结尾处使用 \ 。 
使用矩阵时必须声明 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        \begin{matrix}
        1 &amp;amp; x &amp;amp; x^2 \\
        1 &amp;amp; y &amp;amp; y^2 \\
        1 &amp;amp; z &amp;amp; z^2 \\
        \end{matrix}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
        \begin{matrix}
        1 &amp;amp; x &amp;amp; x^2 \\
        1 &amp;amp; y &amp;amp; y^2 \\
        1 &amp;amp; z &amp;amp; z^2 \\
        \end{matrix}
$$&lt;/div&gt;
&lt;h3 id="2.ru he shu ru dai bian kuang de ju zhen"&gt;2．如何输入带边框的矩阵&lt;/h3&gt;
&lt;p&gt;在开头将 &lt;code&gt;matrix&lt;/code&gt; 替换为 &lt;code&gt;pmatrix bmatrix Bmatrix vmatrix Vmatrix&lt;/code&gt; 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;pmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{pmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{pmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;bmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{bmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{bmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Bmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{Bmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{Bmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;vmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{vmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{vmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Vmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{Vmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{Vmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="3.ru he shu ru dai sheng lue fu hao de ju zhen"&gt;3．如何输入带省略符号的矩阵&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\cdots&lt;/code&gt;  , &lt;code&gt;\ddots&lt;/code&gt;  , &lt;code&gt;\vdots&lt;/code&gt;  来输入省略符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        \begin{pmatrix}
        1 &amp;amp; a_1 &amp;amp; a_1^2 &amp;amp; \cdots &amp;amp; a_1^n \\
        1 &amp;amp; a_2 &amp;amp; a_2^2 &amp;amp; \cdots &amp;amp; a_2^n \\
        \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
        1 &amp;amp; a_m &amp;amp; a_m^2 &amp;amp; \cdots &amp;amp; a_m^n \\
        \end{pmatrix}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
        \begin{pmatrix}
        1 &amp;amp; a_1 &amp;amp; a_1^2 &amp;amp; \cdots &amp;amp; a_1^n \\
        1 &amp;amp; a_2 &amp;amp; a_2^2 &amp;amp; \cdots &amp;amp; a_2^n \\
        \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
        1 &amp;amp; a_m &amp;amp; a_m^2 &amp;amp; \cdots &amp;amp; a_m^n \\
        \end{pmatrix}
$$&lt;/div&gt;
&lt;h3 id="4.ru he shu ru dai fen ge fu hao de ju zhen"&gt;4．如何输入带分割符号的矩阵&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        \left[
            \begin{array}{cc|c}
              1&amp;amp;2&amp;amp;3\\
              4&amp;amp;5&amp;amp;6
            \end{array}
        \right]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中 &lt;code&gt;cc|c&lt;/code&gt; 代表在一个三列矩阵中的第二和第三列之间插入分割线。&lt;/p&gt;
&lt;div class="math"&gt;$$
\left[
    \begin{array}{cc|c}
      1&amp;amp;2&amp;amp;3\\
      4&amp;amp;5&amp;amp;6
    \end{array}
\right]
$$&lt;/div&gt;
&lt;h3 id="5.ru he shu ru xing zhong ju zhen"&gt;5．如何输入行中矩阵&lt;/h3&gt;
&lt;p&gt;若想在一行内显示矩阵，使用&lt;code&gt;\bigl(\begin{smallmatrix} ... \end{smallmatrix}\bigr)&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;这是一个行中矩阵的示例 $\bigl( \begin{smallmatrix} a &amp;amp; b \\ c &amp;amp; d \end{smallmatrix} \bigr)$ 。
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这是一个行中矩阵的示例&lt;/p&gt;
&lt;div class="math"&gt;$$\bigl(\begin{smallmatrix} a &amp;amp; b \\ c &amp;amp; d \end{smallmatrix}\bigr)$$&lt;/div&gt;
&lt;p&gt; 。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 这里的页面生成有bug, 左右两边均为两个$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="san , fang cheng shi xu lie shi yong can kao_1"&gt;三、方程式序列使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge fang cheng shi xu lie"&gt;1．如何输入一个方程式序列&lt;/h3&gt;
&lt;p&gt;人们经常想要一列整齐且居中的方程式序列。使用 &lt;code&gt;\begin{align}&amp;hellip;\end{align}&lt;/code&gt; 来创造一列方程式，其中在每行结尾处使用 &lt;code&gt;\\&lt;/code&gt; 。 
使用方程式序列无需声明公式符号 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;请注意 &lt;code&gt;{align}&lt;/code&gt; 语句是 &lt;strong&gt;自动编号&lt;/strong&gt; 的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{align}
\sqrt{37} &amp;amp; = \sqrt{\frac{73^2-1}{12^2}}  \\
 &amp;amp; = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}}  \\ 
 &amp;amp; = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}}  \\
 &amp;amp; = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}}  \\ 
 &amp;amp; \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right)
\end{align}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;\begin{align}
\sqrt{37} &amp;amp; = \sqrt{\frac{73^2-1}{12^2}} \tag{1} \\
 &amp;amp; = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \tag{2} \\ 
 &amp;amp; = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \tag{3} \\
 &amp;amp; = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \tag{4} \\ 
 &amp;amp; \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right) \tag{5}
\end{align}&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: 本blog不能自动编号, 示例效果采用手动编号&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="2.zai yi ge fang cheng shi xu lie de mei yi xing zhong zhu ming yuan yin"&gt;2．在一个方程式序列的每一行中注明原因&lt;/h3&gt;
&lt;p&gt;在 &lt;code&gt;{align}&lt;/code&gt; 中灵活组合 &lt;code&gt;\text&lt;/code&gt; 和 &lt;code&gt;\tag&lt;/code&gt; 语句。&lt;code&gt;\tag&lt;/code&gt; 语句编号优先级高于自动编号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{align}
   v + w &amp;amp; = 0  &amp;amp;\text{Given} \tag 1\\
   -w &amp;amp; = -w + 0 &amp;amp; \text{additive identity} \tag 2\\
   -w + 0 &amp;amp; = -w + (v + w) &amp;amp; \text{equations $(1)$ and $(2)$}
\end{align}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align}
   v + w &amp;amp; = 0  &amp;amp;\text{Given} \tag 1\\
   -w &amp;amp; = -w + 0 &amp;amp; \text{additive identity} \tag 2\\
   -w + 0 &amp;amp; = -w + (v + w) &amp;amp; \text{equations $(1)$ and $(2)$}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;本例中第一、第二行的自动编号被 &lt;code&gt;\tag&lt;/code&gt; 语句覆盖，第三行的编号为自动编号。&lt;/p&gt;
&lt;h2 id="si , tiao jian biao da shi shi yong can kao_1"&gt;四、条件表达式使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge tiao jian biao da shi"&gt;1．如何输入一个条件表达式&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;begin{cases}&lt;/code&gt; 来创造一组条件表达式，在每一行条件中插入 &lt;code&gt;&amp;amp;&lt;/code&gt; 来指定需要对齐的内容，并在每一行结尾处使用 &lt;code&gt;\\&lt;/code&gt;，以 &lt;code&gt;end{cases}&lt;/code&gt; 结束。 
条件表达式无需声明 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        f(n) =
        \begin{cases}
        n/2,  &amp;amp; \text{if $n$ is even} \\
        3n+1, &amp;amp; \text{if $n$ is odd}
        \end{cases}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
        f(n) =
        \begin{cases}
        n/2,  &amp;amp; \text{if $n$ is even} \\
        3n+1, &amp;amp; \text{if $n$ is odd}
        \end{cases}
$$&lt;/div&gt;
&lt;h3 id="2.ru he shu ru yi ge zuo ce dui qi de tiao jian biao da shi"&gt;2．如何输入一个左侧对齐的条件表达式&lt;/h3&gt;
&lt;p&gt;若想让文字在 左侧对齐显示 ，则有如下方式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        \left.
        \begin{array}{l}
        \text{if $n$ is even:}&amp;amp;n/2\\
        \text{if $n$ is odd:}&amp;amp;3n+1
        \end{array}
        \right\}
        =f(n)
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
        \left.
        \begin{array}{l}
        \text{if $n$ is even:}&amp;amp;n/2\\
        \text{if $n$ is odd:}&amp;amp;3n+1
        \end{array}
        \right\}
        =f(n)
$$&lt;/div&gt;
&lt;h3 id="3.ru he shi tiao jian biao da shi gua pei xing gao"&gt;3．如何使条件表达式适配行高&lt;/h3&gt;
&lt;p&gt;在一些情况下，条件表达式中某些行的行高为非标准高度，此时使用 &lt;code&gt;\\[2ex]&lt;/code&gt; 语句代替该行末尾的 &lt;code&gt;\\&lt;/code&gt; 来让编辑器适配。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不适配[2ex]&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(n) = 
\begin{cases}
\frac{n}{2},  &amp;amp; \text{if $n$ is even} \\
3n+1, &amp;amp; \text{if $n$ is odd}
\end{cases}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
f(n) = 
\begin{cases}
\frac{n}{2},  &amp;amp; \text{if $n$ is even} \\
3n+1, &amp;amp; \text{if $n$ is odd}
\end{cases}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;适配[2ex]&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(n) = 
\begin{cases}
\frac{n}{2},  &amp;amp; \text{if $n$ is even} \\[2ex]
3n+1, &amp;amp; \text{if $n$ is odd}
\end{cases}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
f(n) = 
\begin{cases}
\frac{n}{2},  &amp;amp; \text{if $n$ is even} \\[2ex]
3n+1, &amp;amp; \text{if $n$ is odd}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;一个 &lt;code&gt;[ex]&lt;/code&gt; 指一个 "X-Height"，即&lt;code&gt;x&lt;/code&gt;字母高度。可以根据情况指定多个 &lt;code&gt;[ex]&lt;/code&gt;，如 &lt;code&gt;[3ex]&lt;/code&gt;、&lt;code&gt;[4ex]&lt;/code&gt; 等。 
其实可以在任何地方使用 &lt;code&gt;\\[2ex]&lt;/code&gt; 语句，只要你觉得合适。&lt;/p&gt;
&lt;h2 id="wu , shu zu yu biao ge shi yong can kao_1"&gt;五、数组与表格使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge shu zu huo biao ge"&gt;1．如何输入一个数组或表格&lt;/h3&gt;
&lt;p&gt;通常，一个格式化后的表格比单纯的文字或排版后的文字更具有可读性。数组和表格均以 &lt;code&gt;begin{array}&lt;/code&gt; 开头，并在其后定义列数及每一列的文本对齐属性，&lt;code&gt;c l r&lt;/code&gt; 分别代表居中、左对齐及右对齐。若需要插入垂直分割线，在定义式中插入 &lt;code&gt;|&lt;/code&gt; ，若要插入水平分割线，在下一行输入前插入 &lt;code&gt;\hline&lt;/code&gt; 。与矩阵相似，每行元素间均须要插入 &lt;code&gt;&amp;amp;&lt;/code&gt; ，每行元素以 &lt;code&gt;\\&lt;/code&gt; 结尾，最后以 &lt;code&gt;end{array}&lt;/code&gt; 结束数组。 
使用单个数组或表格时无需声明 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|lcr}
n &amp;amp; \text{左对齐} &amp;amp; \text{居中对齐} &amp;amp; \text{右对齐} \\
\hline
1 &amp;amp; 0.24 &amp;amp; 1 &amp;amp; 125 \\
2 &amp;amp; -1 &amp;amp; 189 &amp;amp; -8 \\
3 &amp;amp; -20 &amp;amp; 2000 &amp;amp; 1+10i
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|lcr}
n &amp;amp; \text{左对齐} &amp;amp; \text{居中对齐} &amp;amp; \text{右对齐} \\
\hline
1 &amp;amp; 0.24 &amp;amp; 1 &amp;amp; 125 \\
2 &amp;amp; -1 &amp;amp; 189 &amp;amp; -8 \\
3 &amp;amp; -20 &amp;amp; 2000 &amp;amp; 1+10i
\end{array}
$$&lt;/div&gt;
&lt;h3 id="2.ru he shu ru yi ge qian tao de shu zu huo biao ge"&gt;2．如何输入一个嵌套的数组或表格&lt;/h3&gt;
&lt;p&gt;多个数组/表格可&lt;strong&gt;互相嵌套&lt;/strong&gt; 并组成一组数组/一组表格。 
使用嵌套前必须声明 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% outer vertical array of arrays 外层垂直表格&lt;/span&gt;
&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="c"&gt;% inner horizontal array of arrays 内层水平表格&lt;/span&gt;
    &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;cc&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c"&gt;% inner array of minimum values 内层"最小值"数组&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;cccc&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;hline&lt;/span&gt;
        &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;
        &lt;span class="c"&gt;% inner array of maximum values 内层"最大值"数组&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;cccc&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;hline&lt;/span&gt;
        &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="c"&gt;% 内层第一行表格组结束&lt;/span&gt;
    &lt;span class="o"&gt;\\&lt;/span&gt;
    &lt;span class="c"&gt;% inner array of delta values 内层第二行Delta值数组&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;cccc&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Delta&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;hline&lt;/span&gt;
        &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c"&gt;% 内层第二行表格组结束&lt;/span&gt;
&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
% outer vertical array of arrays 外层垂直表格
\begin{array}{c}
    % inner horizontal array of arrays 内层水平表格
    \begin{array}{cc}
        % inner array of minimum values 内层"最小值"数组
        \begin{array}{c|cccc}
        \text{min} &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\\
        \hline
        0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
        1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\\
        2 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 2\\
        3 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3
        \end{array}
    &amp;amp;
        % inner array of maximum values 内层"最大值"数组
        \begin{array}{c|cccc}
        \text{max}&amp;amp;0&amp;amp;1&amp;amp;2&amp;amp;3\\
        \hline
        0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\\
        1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\\
        2 &amp;amp; 2 &amp;amp; 2 &amp;amp; 2 &amp;amp; 3\\
        3 &amp;amp; 3 &amp;amp; 3 &amp;amp; 3 &amp;amp; 3
        \end{array}
    \end{array}
    % 内层第一行表格组结束
    \\
    % inner array of delta values 内层第二行Delta值数组
        \begin{array}{c|cccc}
        \Delta&amp;amp;0&amp;amp;1&amp;amp;2&amp;amp;3\\
        \hline
        0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\\
        1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2\\
        2 &amp;amp; 2 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1\\
        3 &amp;amp; 3 &amp;amp; 2 &amp;amp; 1 &amp;amp; 0
        \end{array}
        % 内层第二行表格组结束
\end{array}
$$&lt;/div&gt;
&lt;h3 id="3.ru he shu ru yi ge fang cheng zu"&gt;3．如何输入一个方程组&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\begin{array}&amp;hellip;\end{array}&lt;/code&gt; 和 &lt;code&gt;\left\{&amp;hellip;\right.&lt;/code&gt; 来创建一个方程组。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\left\{ 
\begin{array}{c}
a_1x+b_1y+c_1z=d_1 \\ 
a_2x+b_2y+c_2z=d_2 \\ 
a_3x+b_3y+c_3z=d_3
\end{array}
\right. 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\left\{ 
\begin{array}{c}
a_1x+b_1y+c_1z=d_1 \\ 
a_2x+b_2y+c_2z=d_2 \\ 
a_3x+b_3y+c_3z=d_3
\end{array}
\right. 
$$&lt;/div&gt;
&lt;p&gt;或者使用条件表达式组 &lt;code&gt;\begin{cases}&amp;hellip;\end{cases}&lt;/code&gt; 来实现相同效果：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{cases}
a_1x+b_1y+c_1z=d_1 \\ 
a_2x+b_2y+c_2z=d_2 \\ 
a_3x+b_3y+c_3z=d_3
\end{cases}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{cases}
a_1x+b_1y+c_1z=d_1 \\ 
a_2x+b_2y+c_2z=d_2 \\ 
a_3x+b_3y+c_3z=d_3
\end{cases}
$$&lt;/div&gt;
&lt;h2 id="liu , lian fen shu shi yong can kao_1"&gt;六、连分数使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge lian fen shi"&gt;1．如何输入一个连分式&lt;/h3&gt;
&lt;p&gt;就像输入分式时使用 &lt;code&gt;\frac&lt;/code&gt; 一样，使用 &lt;code&gt;\cfrac&lt;/code&gt; 来创建一个连分数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x = a_0 + \cfrac{1^2}{a_1
          + \cfrac{2^2}{a_2
          + \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
x = a_0 + \cfrac{1^2}{a_1
          + \cfrac{2^2}{a_2
          + \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}}
$$&lt;/div&gt;
&lt;p&gt;不要使用普通的 &lt;code&gt;\frac&lt;/code&gt; 或 &lt;code&gt;\over&lt;/code&gt; 来创建，否则会看起来 &lt;strong&gt;很恶心&lt;/strong&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x = a_0 + \frac{1^2}{a_1
          + \frac{2^2}{a_2
          + \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
x = a_0 + \frac{1^2}{a_1
          + \frac{2^2}{a_2
          + \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}}
$$&lt;/div&gt;
&lt;p&gt;当然，你可以使用 &lt;code&gt;\frac&lt;/code&gt; 来表达连分数的 &lt;strong&gt;紧缩记法&lt;/strong&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x = a_0 + \frac{1^2}{a_1+}
          \frac{2^2}{a_2+}
          \frac{3^2}{a_3 +} \frac{4^4}{a_4 +} \cdots
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
x = a_0 + \frac{1^2}{a_1+}
          \frac{2^2}{a_2+}
          \frac{3^2}{a_3 +} \frac{4^4}{a_4 +} \cdots
$$&lt;/div&gt;
&lt;p&gt;连分数通常都太大以至于不易排版，所以建议在连分数前后声明 &lt;code&gt;$$&lt;/code&gt; 符号，或使用像 &lt;code&gt;[a0;a1,a2,a3,&amp;hellip;]&lt;/code&gt; 一样的紧缩记法。&lt;/p&gt;
&lt;h2 id="qi , jiao huan tu biao shi yong can kao_1"&gt;七、交换图表使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge jiao huan tu biao"&gt;1．如何输入一个交换图表&lt;/h3&gt;
&lt;p&gt;使用一行 &lt;code&gt;$ \require{AMScd} $&lt;/code&gt; 语句来允许交换图表的显示。 
声明交换图表后，语法与矩阵相似，在开头使用 &lt;code&gt;begin{CD}&lt;/code&gt;，在结尾使用 &lt;code&gt;end{CD}&lt;/code&gt;，在中间插入图表元素，每个元素之间插入 &lt;code&gt;&amp;amp;&lt;/code&gt; ，并在每行结尾处使用&lt;code&gt;\\&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$\require{AMScd}$
\begin{CD}
    A @&amp;gt;a&amp;gt;&amp;gt; B\\
    @V b V V\# @VV c V\\
    C @&amp;gt;&amp;gt;d&amp;gt; D
\end{CD}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
$\require{AMScd}$
\begin{CD}
    A @&amp;gt;a&amp;gt;&amp;gt; B\\
    @V b V V\# @VV c V\\
    C @&amp;gt;&amp;gt;d&amp;gt; D
\end{CD}
$$&lt;/div&gt;
&lt;p&gt;其中，&lt;code&gt;@&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 代表右箭头、&lt;code&gt;@&amp;lt;&amp;lt;&amp;lt;&lt;/code&gt; 代表左箭头、&lt;code&gt;@VVV&lt;/code&gt; 代表下箭头、&lt;code&gt;@AAA&lt;/code&gt; 代表上箭头、&lt;code&gt;@=&lt;/code&gt; 代表水平双实线、&lt;code&gt;@|&lt;/code&gt; 代表竖直双实线、&lt;code&gt;@.&lt;/code&gt;代表没有箭头。 
在 &lt;code&gt;@&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 的 &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 之间任意插入文字即代表该箭头的注释文字。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{CD}
    A @&amp;gt;&amp;gt;&amp;gt; B @&amp;gt;{\text{very long label}}&amp;gt;&amp;gt; C \\
    @. @AAA @| \\
    D @= E @&amp;lt;&amp;lt;&amp;lt; F
\end{CD}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{CD}
    A @&amp;gt;&amp;gt;&amp;gt; B @&amp;gt;{\text{very long label}}&amp;gt;&amp;gt; C \\
    @. @AAA @| \\
    D @= E @&amp;lt;&amp;lt;&amp;lt; F
\end{CD}
$$&lt;/div&gt;
&lt;p&gt;在本例中， "very long label"自动延长了它所在箭头以及对应箭头的长度。&lt;/p&gt;
&lt;h2 id="ba , yi xie te shu de zhu yi shi xiang_1"&gt;八、一些特殊的注意事项&lt;/h2&gt;
&lt;p&gt;有些小窍门会让数学公式显得更好看，强迫症和完美主义者会喜欢下面的内容。&lt;/p&gt;
&lt;h3 id="1. mou xie fen shu de xian shi wen ti"&gt;1. 某些分数的显示问题&lt;/h3&gt;
&lt;p&gt;在以&lt;code&gt;e&lt;/code&gt;为底的指数函数、极限和积分中尽量不要使用 &lt;code&gt;\frac&lt;/code&gt; 符号：它会使整段函数看起来很怪，而且可能产生歧义。也正是因此它在专业数学排版中几乎从不出现。 
横着写这些分式，中间使用斜线间隔 &lt;code&gt;/&lt;/code&gt; （用斜线代替分数线）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
e^{i\frac{\pi}2} \quad e^{\frac{i\pi}2}&amp;amp; e^{i\pi/2} \\
\int_{-\frac\pi2}^\frac\pi2 \sin x\,dx &amp;amp; \int_{-\pi/2}^{\pi/2}\sin x\,dx \\
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
e^{i\frac{\pi}2} \quad e^{\frac{i\pi}2}&amp;amp; e^{i\pi/2} \\
\int_{-\frac\pi2}^\frac\pi2 \sin x\,dx &amp;amp; \int_{-\pi/2}^{\pi/2}\sin x\,dx \\
\end{array}
$$&lt;/div&gt;
&lt;h3 id="2. liu chu he li de jian ge"&gt;2. 留出合理的间隔&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;|&lt;/code&gt; 符号在被当作分隔符时会产生过于狭窄的间隔，因此在需要分隔时最好使用 &lt;code&gt;\mid&lt;/code&gt; 来代替它。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\{x|x^2\in\Bbb Z\} &amp;amp; \{x\mid x^2\in\Bbb Z\} \\
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\{x|x^2\in\Bbb Z\} &amp;amp; \{x\mid x^2\in\Bbb Z\} \\
\end{array}
$$&lt;/div&gt;
&lt;h3 id="3. duo zhong ji fen fu hao de xian shi"&gt;3. 多重积分符号的显示&lt;/h3&gt;
&lt;p&gt;使用多重积分符号时，不要多次使用 &lt;code&gt;\int&lt;/code&gt; 来声明，直接使用 &lt;code&gt;\iint&lt;/code&gt; 来表示 二重积分 ，使用 &lt;code&gt;\iiint&lt;/code&gt; 来表示 三重积分 等。对于无限次积分，可以用 &lt;code&gt;\int \cdots \int&lt;/code&gt; 表示。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\int\int_S f(x)\,dy\,dx &amp;amp; \iint_S f(x)\,dy\,dx \\
\int\int\int_V f(x)\,dz\,dy\,dx &amp;amp; \iiint_V f(x)\,dz\,dy\,dx
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\int\int_S f(x)\,dy\,dx &amp;amp; \iint_S f(x)\,dy\,dx \\
\int\int\int_V f(x)\,dz\,dy\,dx &amp;amp; \iiint_V f(x)\,dz\,dy\,dx
\end{array}
$$&lt;/div&gt;
&lt;h3 id="4. duo ge wei fen fu hao de xian shi"&gt;4. 多个微分符号的显示&lt;/h3&gt;
&lt;p&gt;在微分符号前加入 &lt;code&gt;\&lt;/code&gt;, 来插入一个小的间隔空隙；没有 &lt;code&gt;\&lt;/code&gt;, 符号的话， 将会把不同的微分符号堆在一起。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\iiint_V f(x){\rm d}z {\rm d}y {\rm d}x &amp;amp; \iiint_V f(x)\,{\rm d}z\,{\rm d}y\,{\rm d}x
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\iiint_V f(x){\rm d}z {\rm d}y {\rm d}x &amp;amp; \iiint_V f(x)\,{\rm d}z\,{\rm d}y\,{\rm d}x
\end{array}
$$&lt;/div&gt;
&lt;p&gt;感谢您花费时间阅读这份指导手册，本手册内容可能有疏漏之处，欢迎更改指正。 &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Pandoc's Markdown 語法中文翻譯</title><link href="https://freeopen.github.io/posts/pandocs-markdown-yu-fa-zhong-wen-fan-yi" rel="alternate"></link><published>2017-05-11T00:00:00+08:00</published><updated>2017-05-11T00:00:00+08:00</updated><author><name>John MacFarlane; Tzeng Yuxio(translated)</name></author><id>tag:freeopen.github.io,2017-05-11:/posts/pandocs-markdown-yu-fa-zhong-wen-fan-yi</id><summary type="html">&lt;h2 id="qian yan"&gt;前言&lt;/h2&gt;
&lt;p&gt;這份文件是 &lt;a href="http://johnmacfarlane.net/pandoc/"&gt;Pandoc&lt;/a&gt; 版本 Markdown 語法的中文翻譯。Pandoc 本身是由 &lt;a href="http://johnmacfarlane.net/"&gt;John MacFarlane&lt;/a&gt; 所開發的文件轉換工具，可以在 HTML, Markdown, PDF, TeX...等等格式之間進行轉換。有許多喜歡純文字編輯的人，利用 Pandoc 來進行論文的撰寫或投影片製作。但除了轉換的功能外，Pandoc 所定義的 Markdown 擴充語法也是這套工具的一大亮點，在 Pandoc 的官方使用說明文件中，光是其針對 Markdown 格式的擴充就佔了整整一半左右的篇幅。 &lt;/p&gt;
&lt;p&gt;本文件翻譯自 &lt;a href="http://johnmacfarlane.net/pandoc/README.html#pandocs-markdown"&gt;Pandoc - Pandoc User&amp;rsquo;s Guide&lt;/a&gt; 中的 "Pandoc's markdown" 一節。你可以看看&lt;a href="https://raw.github.com/tzengyuxio/pages/gh-pages/pandoc/pandoc.markdown"&gt;這份文件的原始檔&lt;/a&gt;、產生文件&lt;a href="https://github.com/tzengyuxio/pages/blob/gh-pages/pandoc/pm-template.html5"&gt;所使用的 HTML 範本&lt;/a&gt;，以及&lt;a href="https://github.com/tzengyuxio/pages/blob/gh-pages/pandoc/convert.sh"&gt;轉換時的命令參數 …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h2 id="qian yan"&gt;前言&lt;/h2&gt;
&lt;p&gt;這份文件是 &lt;a href="http://johnmacfarlane.net/pandoc/"&gt;Pandoc&lt;/a&gt; 版本 Markdown 語法的中文翻譯。Pandoc 本身是由 &lt;a href="http://johnmacfarlane.net/"&gt;John MacFarlane&lt;/a&gt; 所開發的文件轉換工具，可以在 HTML, Markdown, PDF, TeX...等等格式之間進行轉換。有許多喜歡純文字編輯的人，利用 Pandoc 來進行論文的撰寫或投影片製作。但除了轉換的功能外，Pandoc 所定義的 Markdown 擴充語法也是這套工具的一大亮點，在 Pandoc 的官方使用說明文件中，光是其針對 Markdown 格式的擴充就佔了整整一半左右的篇幅。 &lt;/p&gt;
&lt;p&gt;本文件翻譯自 &lt;a href="http://johnmacfarlane.net/pandoc/README.html#pandocs-markdown"&gt;Pandoc - Pandoc User&amp;rsquo;s Guide&lt;/a&gt; 中的 "Pandoc's markdown" 一節。你可以看看&lt;a href="https://raw.github.com/tzengyuxio/pages/gh-pages/pandoc/pandoc.markdown"&gt;這份文件的原始檔&lt;/a&gt;、產生文件&lt;a href="https://github.com/tzengyuxio/pages/blob/gh-pages/pandoc/pm-template.html5"&gt;所使用的 HTML 範本&lt;/a&gt;，以及&lt;a href="https://github.com/tzengyuxio/pages/blob/gh-pages/pandoc/convert.sh"&gt;轉換時的命令參數&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;以下翻譯開始。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="pandoc's markdown"&gt;Pandoc's markdown&lt;/h2&gt;
&lt;p&gt;與 John Gruber 的 原始 &lt;a href="http://daringfireball.net/projects/markdown/"&gt;markdown&lt;/a&gt; 相比，Pandoc 版本的 markdown 在語法上有額外的擴充與些許的修正。這份文件解釋了這些語法，並指出其與原始 markdown 的差異所在。除非特別提到，不然這些差異均可藉由使用 &lt;code&gt;markdown_strict&lt;/code&gt; 而非 &lt;code&gt;markdown&lt;/code&gt; 的格式來關閉。單獨一項擴充也可透過 &lt;code&gt;+EXTENSION&lt;/code&gt; 或 &lt;code&gt;-EXTENSION&lt;/code&gt; 的方式來開啟或關閉。例如，&lt;code&gt;markdown_strict+footnotes&lt;/code&gt; 表示加上腳註擴充的原始 markdown，而 &lt;code&gt;markdown-footnotes-pipe_tables&lt;/code&gt; 則是拿掉了腳註與管線表格擴充的 pandoc markdown。&lt;/p&gt;
&lt;h2 id="zhe xue"&gt;哲學&lt;/h2&gt;
&lt;p&gt;Markdown 是針對易於書寫與閱讀的目標而設計的，特別是在易於閱讀這點上尤為重要：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一份 Markdown 格式的文件應該要能以純文字形式直接發表，並且一眼看過去不存在任何標記用的標籤或格式指令。
&lt;small&gt;&lt;a href="http://daringfireball.net/projects/markdown/syntax#philosophy"&gt;John Gruber&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這項原則同樣也是 pandoc 在制訂表格、腳註以及其他擴充的語法時，所依循的規範。&lt;/p&gt;
&lt;p&gt;然而，pandoc 的目標與原始 markdown 的最初目標有著方向性的不同。在 markdown 原本的設計中，HTML 是其主要輸出對象；然而 pandoc 則是針對多種輸出格式而設計。因此，雖然 pandoc 同樣也允許直接嵌入 HTML 標籤，但並不鼓勵這樣的作法，取而代之的是 pandoc 提供了許多非 HTML 的方式，來讓使用者輸入像是定義清單、表格、數學公式以及腳註等諸如此類的重要文件元素。&lt;/p&gt;
&lt;h2 id="duan luo"&gt;段落&lt;/h2&gt;
&lt;p&gt;一個段落指的是一行以上的文字，跟在一行以上的空白行之後。換行字元會被當作是空白處理，因此你可以依自己喜好排列段落文字。如果你需要強制換行，在行尾放上兩個以上的空白字元即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;escaped_line_breaks&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一個反斜線後跟著一個換行字元，同樣也有強制換行的效果。&lt;/p&gt;
&lt;h2 id="biao ti"&gt;標題&lt;/h2&gt;
&lt;p&gt;有兩種不同形式的標題語法，Setext 以及 atx。&lt;/p&gt;
&lt;h3 id="setext feng ge biao ti"&gt;Setext 風格標題&lt;/h3&gt;
&lt;p&gt;Setext 風格的標題是由一行文字底下接著一行 &lt;code&gt;=&lt;/code&gt; 符號（用於一階標題）或 &lt;code&gt;-&lt;/code&gt; 符號（用於二階標題）所構成：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gh"&gt;A level-one header&lt;/span&gt;
&lt;span class="gh"&gt;==================&lt;/span&gt;

&lt;span class="gh"&gt;A level-two header&lt;/span&gt;
&lt;span class="gh"&gt;------------------&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;標題的文字可以包含行內格式，例如強調（見下方 [行內格式] 一節）。&lt;/p&gt;
&lt;h3 id="atx feng ge biao ti"&gt;Atx 風格標題&lt;/h3&gt;
&lt;p&gt;Atx 風格的標題是由一到六個 &lt;code&gt;#&lt;/code&gt; 符號以及一行文字所組成，你可以在文字後面加上任意數量的 &lt;code&gt;#&lt;/code&gt; 符號。由行首起算的 &lt;code&gt;#&lt;/code&gt; 符號數量決定了標題的階層：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## A level-two header

### A level-three header ###
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如同 setext 風格標題，這裡的標題文字同樣可包含行內格式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# A level-one header with a [link](/url) and *emphasis*
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;blank_before_header&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原始 markdown 語法在標題之前並不需要預留空白行。Pandoc 則需要（除非標題位於文件最開始的地方）。這是因為以 &lt;code&gt;#&lt;/code&gt; 符號開頭的情況在一般文字段落中相當常見，這會導致非預期的標題。例如下面的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;I like several of their flavors of ice cream:
#22, for example, and #5.
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="html, latex yu  context de biao ti shi bie fu"&gt;HTML, LaTeX 與 ConTeXt 的標題識別符&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;header_attributes&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在標題文字所在行的行尾，可以使用以下語法為標題加上屬性：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{#identifier .class .class key=value key=value}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;雖然這個語法也包含加入類別 (class) 以及鍵／值形式的屬性 (attribute)，但目前只有識別符 (identifier/ID) 在輸出時有實際作用（且只在部分格式的輸出，包括：HTML, LaTeX, ConTeXt, Textile, AsciiDoc）。舉例來說，下面是將標題加上 &lt;code&gt;foo&lt;/code&gt; 識別符的幾種方法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# My header {#foo}

## My header ##    {#foo}

My other header   {#foo}
---------------
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;（此語法與 &lt;a href="http://www.michelf.com/projects/php-markdown/extra/"&gt;PHP Markdown Extra&lt;/a&gt; 相容。）&lt;/p&gt;
&lt;p&gt;具有 &lt;code&gt;unnumbered&lt;/code&gt; 類別的標題將不會被編號，即使 &lt;code&gt;--number-sections&lt;/code&gt; 的選項是開啟的。單一連字符號 (&lt;code&gt;-&lt;/code&gt;) 等同於 &lt;code&gt;.unnumbered&lt;/code&gt;，且更適用於非英文文件中。因此，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# My header {-}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;與下面這行是等價的&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# My header {.unnumbered}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;auto_identifiers&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;沒有明確指定 ID（識別符）的標題將會依據其標題文字，自動指派一個獨一無二的 ID。由標題文字推導 ID 的規則如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;移除所有格式，連結等。&lt;/li&gt;
&lt;li&gt;移除所有標點符號，除了底線、連字符號與句號。&lt;/li&gt;
&lt;li&gt;以連字符號取代所有空白與換行字元。&lt;/li&gt;
&lt;li&gt;將所有英文字母轉為小寫。&lt;/li&gt;
&lt;li&gt;移除第一個字元前的所有內容（ID 不能以數字或標點符號開頭）。&lt;/li&gt;
&lt;li&gt;如果剩下為空字串，則使用 &lt;code&gt;section&lt;/code&gt; 作為 ID。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下是一些範例，&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Header&lt;/th&gt;
&lt;th&gt;Identifier&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Header identifiers in HTML&lt;/td&gt;
&lt;td&gt;&lt;code&gt;header-identifiers-in-html&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Dogs&lt;/em&gt;?--in &lt;em&gt;my&lt;/em&gt; house?&lt;/td&gt;
&lt;td&gt;&lt;code&gt;dogs--in-my-house&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[HTML], [S5], or [RTF]?&lt;/td&gt;
&lt;td&gt;&lt;code&gt;html-s5-or-rtf&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3. Applications&lt;/td&gt;
&lt;td&gt;&lt;code&gt;applications&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;&lt;code&gt;section&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在大多數情況下，這些規則應該讓人能夠直接從標題文字推導出 ID。唯一的例外是當有多個標題具有同樣文字的情況；在這情況下，第一個標題的 ID 仍舊是透過以上規則推導而得；第二個則是在同樣 ID 後加上 &lt;code&gt;-1&lt;/code&gt;；第三個加上 &lt;code&gt;-2&lt;/code&gt;；以此類推。&lt;/p&gt;
&lt;p&gt;在開啟 &lt;code&gt;--toc|--table-of-contents&lt;/code&gt; 的選項時，這些 ID 是用來產生目錄 (Table of Contents) 所需的頁面連結。此外，這些 ID 也提供了一個簡便的方式來輸入跳到指定章節的連結。一個以 ID 產生的連結，其使用的語法看起來就像下面的例子：
    :::md
    See the section on
    &lt;a href="#header-identifiers-in-html-latex-and-context"&gt;header identifiers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;然而要注意的一點是，只有在以 HTML、LaTeX 與 ConTeXt 格式輸出時，才能以這種方式產生對應的章節連結。&lt;/p&gt;
&lt;p&gt;如果指定了 &lt;code&gt;--section-divs&lt;/code&gt; 選項，則每一個小節都會以 &lt;code&gt;div&lt;/code&gt; 標籤包住（或是 &lt;code&gt;section&lt;/code&gt; 標籤，如果有指定 &lt;code&gt;--html5&lt;/code&gt; 選項的話），並且 ID 會被附加在用來包住小節的 &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt;（或是 &lt;code&gt;&amp;lt;section&amp;gt;&lt;/code&gt;）標籤，而非附加在標題上。這使得整個小節都可以透過 javascript 來操作，或是採用不同的 CSS 設定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;implicit_header_references&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc 假設每個標題都定義了其參考連結，因此，相較於以下的連結語法&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你也可以單純只寫&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;][]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;section&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果有多個標題具有同樣文字，對應的參考只會連結到第一個符合的標題，這時若要連結到其他符合的標題，就必須以先前提到的方式，明確指定連結到該標題的 ID。&lt;/p&gt;
&lt;p&gt;與其他一般參考連結不同的是，這些參考連結是大小寫有別的。&lt;/p&gt;
&lt;p&gt;注意：如果你有明確定義了任何一個標題的標示符，那麼選項 &lt;code&gt;implicit_header_references&lt;/code&gt; 就沒有作用。&lt;/p&gt;
&lt;h2 id="qu kuai yin yan_1"&gt;區塊引言&lt;/h2&gt;
&lt;p&gt;Markdown 使用 email 的習慣來建立引言區塊。一個引言區塊可以由一或多個段落或其他的區塊元素（如清單或標題）組成，並且其行首均是由一個 &lt;code&gt;&amp;gt;&lt;/code&gt; 符號加上一個空白作為開頭。（&lt;code&gt;&amp;gt;&lt;/code&gt; 符號不一定要位在該行最左邊，但也不能縮進超過三個空白）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; This is a block quote. This
&amp;amp;gt; paragraph has two lines.
&amp;amp;gt;
&amp;amp;gt; 1. This is a list inside a block quote.
&amp;amp;gt; 2. Second item.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;有一個「偷懶」的形式：你只需要在引言區塊的第一行行首輸入 &lt;code&gt;&amp;gt;&lt;/code&gt; 即可，後面的行首可以省略符號：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; This is a block quote. This
paragraph has two lines.

&amp;amp;gt; 1. This is a list inside a block quote.
2. Second item.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;由於區塊引言可包含其他區塊元素，而區塊引言本身也是區塊元素，所以，引言是可以嵌套入其他引言的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; This is a block quote.
&amp;amp;gt;
&amp;amp;gt; &amp;amp;gt; A block quote within a block quote.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;blank_before_blockquote&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原始 markdown 語法在區塊引言之前並不需要預留空白行。Pandoc 則需要（除非區塊引言位於文件最開始的地方）。這是因為以 &lt;code&gt;&amp;gt;&lt;/code&gt; 符號開頭的情況在一般文字段落中相當常見（也許由於斷行所致），這會導致非預期的格式。因此，除非是指定為 &lt;code&gt;markdown_strict&lt;/code&gt; 格式，不然以下的語法在 pandoc 中將不會產生出嵌套區塊引言：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; This is a block quote.
&amp;amp;gt;&amp;amp;gt; Nested.
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="zi mian (dai ma )qu kuai"&gt;字面（代碼）區塊&lt;/h2&gt;
&lt;h3 id="suo jin dai ma qu kuai"&gt;縮進代碼區塊&lt;/h3&gt;
&lt;p&gt;一段以四個空白（或一個 tab）縮進的文字區塊會被視為字面區塊 (Verbatim Block)：換句話說，特殊字元並不會轉換為任何格式，單純只以字面形式呈現，而所有的空白與換行也都會被保留。例如，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    if (a &amp;amp;gt; 3) {
      moveShip(5 * gravity, DOWN);
    }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;位於行首的縮排（四個空白或一個 tab）並不會被視為字面區塊的一部分，因此在輸出時會被移除掉。&lt;/p&gt;
&lt;p&gt;注意：在字面文字之間的空白行並不需要也以四個空白字元做開頭。&lt;/p&gt;
&lt;h3 id="wei lan dai ma qu kuai"&gt;圍欄代碼區塊&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;fenced_code_blocks&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了標準的縮進代碼區塊外，Pandoc 也支援了&lt;strong&gt;圍欄&lt;/strong&gt; (&lt;em&gt;fenced&lt;/em&gt;) 代碼區塊的語法。這區塊需以包含三個以上波浪線 (&lt;code&gt;~&lt;/code&gt;) 或反引號 (&lt;code&gt;`&lt;/code&gt;) 的一行作為開始，並以同樣符號且至少同樣長度的一行作為結束。所有介於開始與結束之間的文字行都會視為代碼。不需要額外的縮進：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~~~~~~~
if (a &amp;amp;gt; 3) {
  moveShip(5 * gravity, DOWN);
}
~~~~~~~
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如同一般的代碼區塊，圍欄代碼區塊與其前後的文字之間必須以空白行作間隔。&lt;/p&gt;
&lt;p&gt;如果代碼本身也包含了一整行的波浪線或反引號，那麼只要在區塊首尾處使用更長的波浪線或反引號即可：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~~~~~~~~~~~~~~~~
~~~~~~~~~~
code including tildes
~~~~~~~~~~
~~~~~~~~~~~~~~~~
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你也可以選擇性地使用以下語法附加屬性到代碼區塊上：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;~~~~&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;#mycode&lt;/span&gt; &lt;span class="err"&gt;.haskell&lt;/span&gt; &lt;span class="err"&gt;.numberLines&lt;/span&gt; &lt;span class="err"&gt;startFrom="100"&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="nt"&gt;qsort&lt;/span&gt; &lt;span class="cp"&gt;[]&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[]&lt;/span&gt;
&lt;span class="nt"&gt;qsort&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;qsort&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;
               &lt;span class="nt"&gt;qsort&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;=&lt;/span&gt; &lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這裡的 &lt;code&gt;mycode&lt;/code&gt; 為 ID，&lt;code&gt;haskell&lt;/code&gt; 與 &lt;code&gt;numberLines&lt;/code&gt; 是類別，而 &lt;code&gt;startsFrom&lt;/code&gt; 則是值為 &lt;code&gt;100&lt;/code&gt; 的屬性。有些輸出格式可以利用這些資訊來作語法高亮。目前有使用到這些資訊的輸出格式僅有 HTML 與 LaTeX。如果指定的輸出格式及語言類別有支援語法高亮，那麼上面那段代碼區塊將會以高亮並帶有行號的方式呈現。（要查詢支援的程式語言清單，可在命令列輸入 &lt;code&gt;pandoc --version&lt;/code&gt;。）反之若無支援，則上面那段代碼區塊則會以下面的形式呈現：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;lt;pre id="mycode" class="haskell numberLines" startFrom="100"&amp;amp;gt;
  &amp;amp;lt;code&amp;amp;gt;
  ...
  &amp;amp;lt;/code&amp;amp;gt;
&amp;amp;lt;/pre&amp;amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;下面這個是針對代碼區塊只有指定程式語言屬性的簡便形式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;```haskell
qsort [] = []
```
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這與下面這行的效果是相同的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;``` {.haskell}
qsort [] = []
```
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;要取消所有語法高亮，使用 &lt;code&gt;--no-highlight&lt;/code&gt; 選項。要設定語法高亮的配色，則使用 &lt;code&gt;--highlight-style&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id="xing qu kuai_1"&gt;行區塊&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;line_blocks&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;行區塊是一連串以豎線 (&lt;code&gt;|&lt;/code&gt;) 加上一個空格所構成的連續行。行與行間的區隔在輸出時將會以原樣保留，行首的空白字元數目也一樣會被保留；反之，這些行將會以 markdown 的格式處理。這個語法在輸入詩句或地址時很有幫助。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;| The limerick packs laughs anatomical
| In space that is quite economical.
|    But the good ones I've seen
|    So seldom are clean
| And the clean ones so seldom are comical

| 200 Main St.
| Berkeley, CA 94718
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果有需要的話，書寫時也可以將完整一行拆成多行，但後續行必須以空白作為開始。下面範例的前兩行在輸出時會被視為一整行：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;| The Right Honorable Most Venerable and Righteous Samuel L.
  Constable, Jr.
| 200 Main St.
| Berkeley, CA 94718
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這是從 &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/introduction.html"&gt;reStructuredText&lt;/a&gt; 借來的語法。&lt;/p&gt;
&lt;h2 id="qing dan"&gt;清單&lt;/h2&gt;
&lt;h3 id="wu xu qing dan"&gt;無序清單&lt;/h3&gt;
&lt;p&gt;無序清單是以項目符號作列舉的清單。每條項目都以項目符號 (&lt;code&gt;*&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt; 或 &lt;code&gt;-&lt;/code&gt;) 作開頭。下面是個簡單的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* one
* two
* three
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這會產生一個「緊湊」清單。如果你想要一個「寬鬆」清單，也就是說以段落格式處理每個項目內的文字內容，那麼只要在每個項目間加上空白行即可：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* one

* two

* three
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;項目符號不能直接從行首最左邊處輸入，而必須以一至三個空白字元作縮進。項目符號後必須跟著一個空白字元。&lt;/p&gt;
&lt;p&gt;清單項目中的接續行，若與該項目的第一行文字對齊（在項目符號之後），看上去會較為美觀：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* here is my first
  list item.
* and my second.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;但 markdown 也允許以下「偷懶」的格式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* here is my first
list item.
* and my second.
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="si ge kong bai gui ze"&gt;四個空白規則&lt;/h3&gt;
&lt;p&gt;一個清單項目可以包含多個段落以及其他區塊等級的內容。然而，後續的段落必須接在空白行之後，並且以四個空白或一個 tab 作縮進。因此，如果項目裡第一個段落與後面段落對齊的話（也就是項目符號前置入兩個空白），看上去會比較整齊美觀：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  * First paragraph.

    Continued.

  * Second paragraph. With a code block, which must be indented
    eight spaces:

        { code }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;清單項目也可以包含其他清單。在這情況下前置的空白行是可有可無的。嵌套清單必須以四個空白或一個 tab 作縮進：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* fruits
    + apples
        - macintosh
        - red delicious
    + pears
    + peaches
* vegetables
    + brocolli
    + chard
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上一節提到，markdown 允許你以「偷懶」的方式書寫，項目的接續行可以不和第一行對齊。不過，如果一個清單項目中包含了多個段落或是其他區塊元素，那麼每個元素的第一行都必須縮進對齊。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+ A lazy, lazy, list
item.

+ Another one; this looks
bad but is legal.

    Second paragraph of second
list item.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;儘管針對接續段落的「四個空白規則」是出自於官方的 &lt;a href=""&gt;markdown syntax guide&lt;/a&gt;，但是作為對應參考用的 &lt;code&gt;Markdown.pl&lt;/code&gt; 實作版本中並未遵循此一規則。所以當輸入時若接續段落的縮進少於四個空白時，pandoc 所輸出的結果會與 &lt;code&gt;Markdown.pl&lt;/code&gt; 的輸出有所出入。&lt;/p&gt;
&lt;p&gt;在 &lt;a href=""&gt;markdown syntax guide&lt;/a&gt; 中並未明確表示「四個空白規則」是否一體適用於 &lt;strong&gt;所有&lt;/strong&gt; 位於清單項目裡的區塊元素上；規範文件中只提及了段落與代碼區塊。但文件暗示了此規則適用於所有區塊等級的內容（包含嵌套清單），並且 pandoc 以此方向進行解讀與實作。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;http://daringfireball.net/projects/markdown/syntax#list
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="you xu qing dan"&gt;有序清單&lt;/h3&gt;
&lt;p&gt;有序清單與無序清單相類似，唯一的差別在於清單項目是以列舉編號作開頭，而不是項目符號。&lt;/p&gt;
&lt;p&gt;在原始 markdown 中，列舉編號是阿拉伯數字後面接著一個句點與空白。數字本身代表的數值會被忽略，因此下面兩個清單並無差別：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1.  one
2.  two
3.  three
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上下兩個清單的輸出是相同的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;5.  one
7.  two
1.  three
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;fancy_lists&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;與原始 markdown 不同的是，Pandoc 除了使用阿拉伯數字作為有序清單的編號外，也可以使用大寫或小寫的英文字母，以及羅馬數字。清單標記可以用括號包住，也可以單獨一個右括號，抑或是句號。如果清單標記是大寫字母接著一個句號，句號後請使用至少兩個空白字元。&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;startnum&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了清單標記外，Pandoc 也能判讀清單的起始編號，這兩項資訊都會保留於輸出格式中。舉例來說，下面的輸入可以產生一個從編號 9 開始，以單括號為編號標記的清單，底下還跟著一個小寫羅馬數字的子清單：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; 9)  Ninth
10)  Tenth
11)  Eleventh
       i. subone
      ii. subtwo
     iii. subthree
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;當遇到不同形式的清單標記時，Pandoc 會重新開始一個新的清單。所以，以下的輸入會產生三份清單：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(2) Two
(5) Three
1.  Four
*   Five
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果需要預設的有序清單標記符號，可以使用 &lt;code&gt;#.&lt;/code&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#.  one
#.  two
#.  three
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="ding yi qing dan"&gt;定義清單&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;definition_lists&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc 支援定義清單，其語法的靈感來自於 &lt;a href="http://www.michelf.com/projects/php-markdown/extra/"&gt;PHP Markdown Extra&lt;/a&gt; 以及 &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/introduction.html"&gt;reStructuredText&lt;/a&gt;：&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Term 1

:   Definition 1

Term 2 with *inline markup*

:   Definition 2

        { some code, part of Definition 2 }

    Third paragraph of definition 2.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;每個專有名詞 (term) 都必須單獨存在於一行，後面可以接著一個空白行，也可以省略，但一定要接上一或多筆定義內容。一筆定義需由一個冒號或波浪線作開頭，可以接上一或兩個空白作為縮進。定義本身的內容主體（包括接在冒號或波浪線後的第一行）應該以四個空白縮進。一個專有名詞可以有多個定義，而每個定義可以包含一或多個區塊元素（段落、代碼區塊、清單等），每個區塊元素都要縮進四個空白或一個 tab。&lt;/p&gt;
&lt;p&gt;如果你在定義內容後面留下空白行（如同上面的範例），那麼該段定義會被當作段落處理。在某些輸出格式中，這意謂著成對的專有名詞與定義內容間會有較大的空白間距。在定義與定義之間，以及定義與下個專有名詞間不要留空白行，即可產生一個比較緊湊的定義清單：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Term 1
  ~ Definition 1
Term 2
  ~ Definition 2a
  ~ Definition 2b
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="bian hao fan li qing dan"&gt;編號範例清單&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;example_lists&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;這個特別的清單標記 &lt;code&gt;@&lt;/code&gt; 可以用來產生連續編號的範例清單。清單中第一個以 &lt;code&gt;@&lt;/code&gt; 標記的項目會被編號為 '1'，接著編號為 '2'，依此類推，直到文件結束。範例項目的編號不會侷限於單一清單中，而是文件中所有以 &lt;code&gt;@&lt;/code&gt; 為標記的項目均會次序遞增其編號，直到最後一個。舉例如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(@)  My first example will be numbered (1).
(@)  My second example will be numbered (2).

Explanation of examples.

(@)  My third example will be numbered (3).
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;編號範例可以加上標籤，並且在文件的其他地方作參照：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(@good)  This is a good example.

As (@good) illustrates, ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;標籤可以是由任何英文字母、底線或是連字符號所組成的字串。&lt;/p&gt;
&lt;h3 id="jin cou yu kuan song qing dan"&gt;緊湊與寬鬆清單&lt;/h3&gt;
&lt;p&gt;在與清單相關的「邊界處理」上，Pandoc 與 &lt;code&gt;Markdown.pl&lt;/code&gt; 有著不同的處理結果。考慮如下代碼：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+   First
+   Second:
    -   Fee
    -   Fie
    -   Foe

+   Third
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Pandoc 會將以上清單轉換為「緊湊清單」（在 "First", "Second" 或 "Third" 之中沒有 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 標籤），而 markdown 則會在 "Second" 與 "Third" （但不包含 "First"）裡面置入 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 標籤，這是因為 "Third" 之前的空白行而造成的結果。Pandoc 依循著一個簡單規則：如果文字後面跟著空白行，那麼就會被視為段落。既然 "Second" 後面是跟著一個清單，而非空白行，那麼就不會被視為段落了。至於子清單的後面是不是跟著空白行，那就無關緊要了。（注意：即使是設定為 &lt;code&gt;markdown_strict&lt;/code&gt; 格式，Pandoc 仍是依以上方式處理清單項目是否為段落的判定。這個處理方式與 markdown 官方語法規範裡的描述一致，然而卻與 &lt;code&gt;Markdown.pl&lt;/code&gt; 的處理不同。）&lt;/p&gt;
&lt;h3 id="jie shu yi ge qing dan"&gt;結束一個清單&lt;/h3&gt;
&lt;p&gt;如果你在清單之後放入一個縮排的代碼區塊，會有什麼結果？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-   item one
-   item two

    { my code block }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;問題大了！這邊 pandoc（其他的 markdown 實作也是如此）會將 &lt;code&gt;{ my code block }&lt;/code&gt; 視為 &lt;code&gt;item two&lt;/code&gt; 這個清單項目的第二個段落來處理，而不會將其視為一個代碼區塊。&lt;/p&gt;
&lt;p&gt;要在 &lt;code&gt;item two&lt;/code&gt; 之後「切斷」清單，你可以插入一些沒有縮排、輸出時也不可見的內容，例如 HTML 的註解：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-   item one
-   item two

&amp;amp;lt;!-- end of list --&amp;amp;gt;

    { my code block }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;當你想要兩個各自獨立的清單，而非一個大且連續的清單時，也可以運用同樣的技巧：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1.  one
2.  two
3.  three

&amp;amp;lt;!-- --&amp;amp;gt;

1.  uno
2.  dos
3.  tres
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="fen ge xian_1"&gt;分隔線&lt;/h2&gt;
&lt;p&gt;一行中若包含三個以上的 &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 符號（中間可以以空白字元分隔），則會產生一條分隔線：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;*  *  *  *

---------------
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="biao ge"&gt;表格&lt;/h2&gt;
&lt;p&gt;有四種表格的形式可以使用。前三種適用於等寬字型的編輯環境，例如 Courier。第四種則不需要直行的對齊，因此可以在比例字型的環境下使用。&lt;/p&gt;
&lt;h3 id="jian dan biao ge"&gt;簡單表格&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;simple_tables&lt;/code&gt;, &lt;code&gt;table_captions&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;簡單表格看起來像這樣子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Right     Left     Center     Default
-------     ------ ----------   -------
     12     12        12            12
    123     123       123          123
      1     1          1             1

Table:  Demonstration of simple table syntax.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;表頭與資料列分別以一行為單位。直行的對齊則依照表頭的文字和其底下虛線的相對位置來決定：&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果虛線與表頭文字的右側有切齊，而左側比表頭文字還長，則該直行為靠右對齊。&lt;/li&gt;
&lt;li&gt;如果虛線與表頭文字的左側有切齊，而右側比表頭文字還長，則該直行為靠左對齊。&lt;/li&gt;
&lt;li&gt;如果虛線的兩側都比表頭文字長，則該直行為置中對齊。&lt;/li&gt;
&lt;li&gt;如果虛線與表頭文字的兩側都有切齊，則會套用預設的對齊方式（在大多數情況下，這將會是靠左對齊）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;表格底下必須接著一個空白行，或是一行虛線後再一個空白行。表格標題為可選的（上面的範例中有出現）。標題需是一個以 &lt;code&gt;Table:&lt;/code&gt; （或單純只有 &lt;code&gt;:&lt;/code&gt;）開頭作為前綴的段落，輸出時前綴的這部份會被去除掉。表格標題可以放在表格之前或之後。&lt;/p&gt;
&lt;p&gt;表頭也可以省略，在省略表頭的情況下，表格下方必須加上一行虛線以清楚標明表格的範圍。例如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------     ------ ----------   -------
     12     12        12             12
    123     123       123           123
      1     1          1              1
-------     ------ ----------   -------
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;當省略表頭時，直行的對齊會以表格內容的第一行資料列決定。所以，以上面的表格為例，各直行的對齊依序會是靠右、靠左、置中以及靠右對齊。&lt;/p&gt;
&lt;h3 id="duo xing biao ge"&gt;多行表格&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;multiline_tables&lt;/code&gt;, &lt;code&gt;table_captions&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;多行表格允許表頭與表格資料格的文字能以複數行呈現（但不支援橫跨多欄或縱跨多列的資料格）。以下為範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------------------------------------------------------------
 Centered   Default           Right Left
  Header    Aligned         Aligned Aligned
----------- ------- --------------- -------------------------
   First    row                12.0 Example of a row that
                                    spans multiple lines.

  Second    row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.
-------------------------------------------------------------

Table: Here's the caption. It, too, may span
multiple lines.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;看起來很像簡單表格，但兩者間有以下差別：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在表頭文字之前，必須以一列虛線作為開頭（除非有省略表頭）。&lt;/li&gt;
&lt;li&gt;必須以一列虛線作為表格結尾，之後接一個空白行。&lt;/li&gt;
&lt;li&gt;資料列與資料列之間以空白行隔開。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在多行表格中，表格分析器會計算各直行的欄寬，並在輸出時盡可能維持各直行在原始文件中的相對比例。因此，要是你覺得某些欄位在輸出時不夠寬，你可以在 markdown 的原始檔中加寬一點。&lt;/p&gt;
&lt;p&gt;和簡單表格一樣，表頭在多行表格中也是可以省略的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;----------- ------- --------------- -------------------------
   First    row                12.0 Example of a row that
                                    spans multiple lines.

  Second    row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.
----------- ------- --------------- -------------------------

: Here's a multiline table without headers.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;多行表格中可以單只包含一個資料列，但該資料列之後必須接著一個空白行（然後才是標示表格結尾的一行虛線）。如果沒有此空白行，此表格將會被解讀成簡單表格。&lt;/p&gt;
&lt;h3 id="ge kuang biao ge"&gt;格框表格&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;grid_tables&lt;/code&gt;, &lt;code&gt;table_captions&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;格框表格看起來像這樣：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;: Sample grid table.

+---------------+---------------+--------------------+
| Fruit         | Price         | Advantages         |
+===============+===============+====================+
| Bananas       | $1.34         | - built-in wrapper |
|               |               | - bright color     |
+---------------+---------------+--------------------+
| Oranges       | $2.10         | - cures scurvy     |
|               |               | - tasty            |
+---------------+---------------+--------------------+
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;以 &lt;code&gt;=&lt;/code&gt; 串成的一行區分了表頭與表格本體，這在沒有表頭的表格中也是可以省略的。在格框表格中的資料格可以包含任意的區塊元素（複數段落、代碼區塊、清單等等）。不支援對齊，也不支援橫跨多欄或縱跨多列的資料格。格框表格可以在 &lt;a href="http://table.sourceforge.net/"&gt;Emacs table mode&lt;/a&gt; 下輕鬆建立。&lt;/p&gt;
&lt;h3 id="guan xian biao ge"&gt;管線表格&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;pipe_tables&lt;/code&gt;, &lt;code&gt;table_captions&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;管線表格看起來像這樣：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;Right&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;Left&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;Default&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;Center&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="s s-Atom"&gt;------:&lt;/span&gt;&lt;span class="p"&gt;|:-&lt;/span&gt;&lt;span class="s s-Atom"&gt;----&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="s s-Atom"&gt;---------&lt;/span&gt;&lt;span class="p"&gt;|:-&lt;/span&gt;&lt;span class="s s-Atom"&gt;-----:&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;    &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;    &lt;span class="mi"&gt;12&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="mi"&gt;123&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="mi"&gt;123&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="mi"&gt;123&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="mi"&gt;123&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;

  &lt;span class="s s-Atom"&gt;:&lt;/span&gt; &lt;span class="nv"&gt;Demonstration&lt;/span&gt; &lt;span class="s s-Atom"&gt;of&lt;/span&gt; &lt;span class="s s-Atom"&gt;simple&lt;/span&gt; &lt;span class="s s-Atom"&gt;table&lt;/span&gt; &lt;span class="s s-Atom"&gt;syntax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這個語法與 &lt;a href=""&gt;PHP markdown extra 中的表格語法&lt;/a&gt; 相同。開始與結尾的管線字元是可選的，但各直行間則必須以管線區隔。上面範例中的冒號表明了對齊方式。表頭可以省略，但表頭下的水平虛線必須保留，因為虛線上定義了資料欄的對齊方式。&lt;/p&gt;
&lt;p&gt;因為管線界定了各欄之間的邊界，表格的原始碼並不需要像上面例子中各欄之間保持直行對齊。所以，底下一樣是個完全合法（雖然醜陋）的管線表格：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fruit| price
-----|-----:
apple|2.05
pear|1.37
orange|3.09
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;管線表格的資料格不能包含如段落、清單之類的區塊元素，也不能包含複數行文字。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;http://michelf.ca/projects/php-markdown/extra/#table
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意：Pandoc 也可以看得懂以下形式的管線表格，這是由 Emacs 的 orgtbl-mod 所繪製：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;| One | Two   |
|-----+-------|
| my  | table |
| is  | nice  |
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;主要的差別在於以 &lt;code&gt;+&lt;/code&gt; 取代了部分的 &lt;code&gt;|&lt;/code&gt;。其他的 orgtbl 功能並未支援。如果要指定非預設的直行對齊形式，你仍然需要在上面的表格中自行加入冒號。&lt;/p&gt;
&lt;h2 id="wen jian biao ti qu kuai_1"&gt;文件標題區塊&lt;/h2&gt;
&lt;p&gt;（譯註：本節中提到的「標題」均指 Title，而非 Headers）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;pandoc_title_block&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果檔案以文件標題（Title）區塊開頭&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% title&lt;/span&gt;
&lt;span class="c"&gt;% author(s) (separated by semicolons)&lt;/span&gt;
&lt;span class="c"&gt;% date&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這部份將不會作為一般文字處理，而會以書目資訊的方式解析。（這可用在像是單一 LaTeX 或是 HTML 輸出文件的書名上。）這個區塊僅能包含標題，或是標題與作者，或是標題、作者與日期。如果你只想包含作者卻不想包含標題，或是只有標題與日期而沒有作者，你得利用空白行：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;%&lt;/span&gt;
&lt;span class="c"&gt;% Author&lt;/span&gt;

&lt;span class="c"&gt;% My title&lt;/span&gt;
&lt;span class="c"&gt;%&lt;/span&gt;
&lt;span class="c"&gt;% June 15, 2006&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;標題可以包含多行文字，但接續行必須以空白字元開頭，像是：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% My title&lt;/span&gt;
  &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;multiple&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果文件有多個作者，作者也可以分列在不同行並以空白字元作開頭，或是以分號間隔，或是兩者並行。所以，下列各種寫法得到的結果都是相同的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% Author One&lt;/span&gt;
  &lt;span class="n"&gt;Author&lt;/span&gt; &lt;span class="n"&gt;Two&lt;/span&gt;

&lt;span class="c"&gt;% Author One; Author Two&lt;/span&gt;

&lt;span class="c"&gt;% Author One;&lt;/span&gt;
  &lt;span class="n"&gt;Author&lt;/span&gt; &lt;span class="n"&gt;Two&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;日期就只能寫在一行之內。&lt;/p&gt;
&lt;p&gt;所有這三個 metadata 欄位都可以包含標準的行內格式（斜體、連結、腳註等等）。&lt;/p&gt;
&lt;p&gt;文件標題區塊一定會被分析處理，但只有在 &lt;code&gt;--standaline&lt;/code&gt; (&lt;code&gt;-s&lt;/code&gt;) 選項被設定時才會影響輸出內容。在輸出 HTML 時，文件標題會出現的地方有兩個：一個是在文件的 &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; 區塊裡－－這會顯示在瀏覽器的視窗標題上－－另外一個是文件的 &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; 區塊最前面。位於 &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; 裡的文件標題可以選擇性地加上前綴文字（透過 &lt;code&gt;--title-prefix&lt;/code&gt; 或 &lt;code&gt;-T&lt;/code&gt; 選項）。而在 &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; 裡的文件標題會以 H1 元素呈現，並附帶 "title" 類別 (class)，這樣就能藉由 CSS 來隱藏顯示或重新定義格式。如果以 &lt;code&gt;-T&lt;/code&gt; 選項指定了標題前綴文字，卻沒有設定文件標題區塊裡的標題，那麼前綴文字本身就會被當作是 HTML 的文件標題。&lt;/p&gt;
&lt;p&gt;而 man page 的輸出器會分析文件標題區塊的標題行，以解出標題、man page section number，以及其他頁眉 (header) 頁腳 (footer) 所需要的資訊。一般會假設標題行的第一個單字為標題，標題後也許會緊接著一個以括號包住的單一數字，代表 section number（標題與括號之間沒有空白）。在此之後的其他文字則為頁腳與頁眉文字。頁腳與頁眉文字之間是以單獨的一個管線符號 (&lt;code&gt;|&lt;/code&gt;) 作為區隔。所以，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% PANDOC(1)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;將會產生一份標題為 &lt;code&gt;PANDOC&lt;/code&gt; 且 section 為 1 的 man page。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% PANDOC(1) Pandoc User Manuals&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;產生的 man page 會再加上 "Pandoc User Manuals" 在頁腳處。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% PANDOC(1) Pandoc User Manuals | Version 4.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;產生的 man page 會再加上 "Version 4.0" 在頁眉處。&lt;/p&gt;
&lt;h2 id="fan xie xian tiao tuo zi yuan"&gt;反斜線跳脫字元&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;all_symbols_escapable&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了在代碼區塊或行內代碼之外，任何標點符號或空白字元前面只要加上一個反斜線，都能使其保留字面原義，而不會進行格式的轉義解讀。因此，舉例來說，下面的寫法&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;*\*hello\**
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;輸出後會得到&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;lt;em&amp;amp;gt;*hello*&amp;amp;lt;/em&amp;amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;而不是&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;lt;strong&amp;amp;gt;hello&amp;amp;lt;/strong&amp;amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這條規則比原始的 markdown 規則來得好記許多，原始規則中，只有以下字元才有支援反斜線跳脫，不作進一步轉義：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\`*_{}[]()&amp;amp;gt;#+-.!
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;（然而，如果使用了 &lt;code&gt;markdown_strict&lt;/code&gt; 格式，那麼就會採用原始的 markdown 規則）&lt;/p&gt;
&lt;p&gt;一個反斜線之後的空白字元會被解釋為不斷行的空白 (nonbreaking space)。這在 TeX 的輸出中會顯示為 &lt;code&gt;~&lt;/code&gt;，而在 HTML 與 XML 則是顯示為 &lt;code&gt;\&amp;amp;#160;&lt;/code&gt; 或 &lt;code&gt;\&amp;amp;nbsp;&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;一個反斜線之後的換行字元（例如反斜線符號出現在一行的最尾端）則會被解釋為強制換行。這在 TeX 的輸出中會顯示為 &lt;code&gt;\\&lt;/code&gt;，而在 HTML 裡則是 &lt;code&gt;&amp;lt;br /&amp;gt;&lt;/code&gt;。相對於原始 markdown 是以在行尾加上兩個空白字元這種「看不見」的方式進行強制換行，反斜線接換行字元會是比較好的替代方案。&lt;/p&gt;
&lt;p&gt;反斜線跳脫字元在代碼上下文中不起任何作用。&lt;/p&gt;
&lt;h2 id="zhi hui xing biao dian fu hao"&gt;智慧型標點符號&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果指定了 &lt;code&gt;--smart&lt;/code&gt; 選項，pandoc 將會輸出正式印刷用的標點符號，像是將 straight quotes 轉換為 curly quotes&lt;sup id="fnref:T1"&gt;&lt;a class="footnote-ref" href="#fn:T1" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;、&lt;code&gt;---&lt;/code&gt; 轉為破折號 (em-dashes)，&lt;code&gt;--&lt;/code&gt; 轉為連接號 (en-dashes)，以及將 &lt;code&gt;...&lt;/code&gt; 轉為刪節號。不斷行空格 (Nonbreaking spaces) 將會插入某些縮寫詞之後，例如 "Mr."。&lt;/p&gt;
&lt;p&gt;注意：如果你的 LaTeX template 使用了 &lt;code&gt;csquotes&lt;/code&gt; 套件，pandoc 會自動偵測並且使用 &lt;code&gt;\enquote{...}&lt;/code&gt; 在引言文字上。&lt;/p&gt;
&lt;h2 id="xing nei ge shi"&gt;行內格式&lt;/h2&gt;
&lt;h3 id="qiang diao"&gt;強調&lt;/h3&gt;
&lt;p&gt;要 &lt;em&gt;強調&lt;/em&gt; 某些文字，只要以 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 符號前後包住即可，像這樣：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This text is _emphasized with underscores_, and this
is *emphasized with asterisks*.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;重複兩個 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 符號以產生 &lt;strong&gt;更強烈的強調&lt;/strong&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This is **strong emphasis** and __with underscores__.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;一個前後以空白字元包住，或是前面加上反斜線的 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 符號，都不會轉換為強調格式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This is * not emphasized *, and \*neither is this\*.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;intraword_underscores&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因為 &lt;code&gt;_&lt;/code&gt; 字元有時會使用在單字或是 ID 之中，所以 pandoc 不會把被字母包住的 &lt;code&gt;_&lt;/code&gt; 解讀為強調標記。如果有需要特別強調單字中的一部分，就用 &lt;code&gt;*&lt;/code&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;feas*ible*, not feas*able*.
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="shan chu xian"&gt;刪除線&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension:  &lt;code&gt;strikeout&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要將一段文字加上水平線作為刪除效果，將該段文字前後以 &lt;code&gt;~~&lt;/code&gt; 包住即可。例如，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This ~~is deleted text.~~
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="shang biao yu xia biao"&gt;上標與下標&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;superscript&lt;/code&gt;, &lt;code&gt;subscript&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要輸入上標可以用 &lt;code&gt;^&lt;/code&gt; 字元將要上標的文字包起來；要輸入下標可以用 &lt;code&gt;~&lt;/code&gt; 字元將要下標的文字包起來。直接看範例，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;H~2~O is a liquid.  2^10^ is 1024.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果要上標或下標的文字中包含了空白，那麼這個空白字元之前必須加上反斜線。（這是為了避免一般使用下的 &lt;code&gt;~&lt;/code&gt; 和 &lt;code&gt;^&lt;/code&gt; 在非預期的情況下產生出意外的上標或下標。）所以，如果你想要讓字母 P 後面跟著下標文字 'a cat'，那麼就要輸入 &lt;code&gt;P~a\ cat~&lt;/code&gt;，而不是 &lt;code&gt;P~a cat~&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id="zi mian wen zi"&gt;字面文字&lt;/h3&gt;
&lt;p&gt;要讓一小段文字直接以其字面形式呈現，可以用反引號將其包住：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;What is the difference between `&amp;amp;gt;&amp;amp;gt;=` and `&amp;amp;gt;&amp;amp;gt;`?
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果字面文字中也包含了反引號，那就使用雙重反引號包住：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Here is a literal backtick `` ` ``.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;（在起始反引號後的空白以及結束反引號前的空白都會被忽略。）&lt;/p&gt;
&lt;p&gt;一般性的規則如下，字面文字區段是以連續的反引號字元作為開始（反引號後的空白字元為可選），一直到同樣數目的反引號字元出現才結束（反引號前的空白字元也為可選）。&lt;/p&gt;
&lt;p&gt;要注意的是，反斜線跳脫字元（以及其他 markdown 結構）在字面文字的上下文中是沒有效果的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This is a backslash followed by an asterisk: `\*`.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;inline_code_attributes&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;與[圍欄代碼區塊]一樣，字面文字也可以附加屬性：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;`&amp;amp;lt;$&amp;amp;gt;`{.haskell}
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="shu xue_1"&gt;數學&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;tex_math_dollars&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所有介於兩個 &lt;code&gt;$&lt;/code&gt; 字元之間的內容將會被視為 TeX 數學公式處理。開頭的 &lt;code&gt;$&lt;/code&gt; 右側必須立刻接上任意文字，而結尾 &lt;code&gt;$&lt;/code&gt; 的左側同樣也必須緊挨著文字。這樣一來，&lt;code&gt;$20,000 and $30,000&lt;/code&gt; 就不會被當作數學公式處理了。如果基於某些原因，有必須使用 &lt;code&gt;$&lt;/code&gt; 符號將其他文字括住的需求時，那麼可以在 &lt;code&gt;$&lt;/code&gt; 前使用反斜線跳脫字元，這樣 &lt;code&gt;$&lt;/code&gt; 就不會被當作數學公式的分隔符。&lt;/p&gt;
&lt;p&gt;TeX 數學公式會在所有輸出格式中印出。至於會以什麼方式演算編排 (render) 則取決於輸出的格式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Markdown, LaTeX, Org-Mode, ConTeXt
  ~ 公式會以字面文字呈現在兩個 &lt;code&gt;$&lt;/code&gt; 符號之間。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;reStructuredText
  ~ 公式會使用 &lt;a href="http://www.american.edu/econ/itex2mml/mathhack.rst"&gt;此處&lt;/a&gt; 所描述的 &lt;code&gt;:math:&lt;/code&gt; 這個 "interpreted text role" 來進行演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AsciiDoc
  ~ 公式會以 &lt;code&gt;latexmath:[...]&lt;/code&gt; 演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Texinfo
  ~ 公式會在 &lt;code&gt;@math&lt;/code&gt; 指令中演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;groff man
  ~ 公式會以去掉 &lt;code&gt;$&lt;/code&gt; 後的字面文字演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MediaWiki
  ~ 公式會在 &lt;code&gt;&amp;lt;math&amp;gt;&lt;/code&gt; 標籤中演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Textile
  ~ 公式會在 &lt;code&gt;&amp;lt;span class="math"&amp;gt;&lt;/code&gt; 標籤中演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RTF, OpenDocument, ODT
  ~ 如果可以的話，公式會以 unicode 字元演算編排，不然就直接使用字面字元。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Docbook
  ~ 如果使用了 &lt;code&gt;--mathml&lt;/code&gt; 旗標，公式就會在 &lt;code&gt;inlineequation&lt;/code&gt; 或 &lt;code&gt;informalequation&lt;/code&gt; 標籤中使用 mathml 演算編排。否則就會盡可能使用 unicode 字元演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Docx
  ~ 公式會以 OMML 數學標記的方式演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FictionBook2
  ~ 如果有使用 &lt;code&gt;--webtex&lt;/code&gt; 選項，公式會以 Google Charts 或其他相容的網路服務演算編排為圖片，並下載嵌入於電子書中。否則就會以字面文字顯示。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;HTML, Slidy, DZSlides, S5, EPUB
  ~ 公式會依照以下命令列選項的設置，以不同的方法演算編排為 HTML 代碼。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;預設方式是將 TeX 數學公式盡可能地以 unicode 字元演算編排，如同 RTF、DocBook 以及 OpenDocument 的輸出。公式會被放在附有屬性 &lt;code&gt;class="math"&lt;/code&gt; 的 &lt;code&gt;span&lt;/code&gt; 標籤內，所以可以在需要時給予不同的樣式，使其突出於周遭的文字內容。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--latexmathml&lt;/code&gt; 選項，TeX 數學公式會被顯示於 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 字元中，並放在附帶 &lt;code&gt;LaTeX&lt;/code&gt; 類別的 &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; 標籤裡。這段內容會用 &lt;a href="http://math.etsu.edu/LaTeXMathML/"&gt;LaTeXMathML&lt;/a&gt; script 演算編排為數學公式。（這個方法無法適用於所有瀏覽器，但在 Firefox 中是有效的。在不支援 LaTeXMathML 的瀏覽器中，TeX 數學公式會單純的以兩個 &lt;code&gt;$&lt;/code&gt; 字元間的字面文字呈現。）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--jsmath&lt;/code&gt; 選項，TeX數學公式會放在 &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; 標籤（用於行內數學公式）或 &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; 標籤（用於區塊數學公式）中，並附帶類別屬性 &lt;code&gt;math&lt;/code&gt;。這段內容會使用 &lt;a href="http://www.math.union.edu/~dpvc/jsmath/"&gt;jsMath&lt;/a&gt; script 來演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--mimetex&lt;/code&gt; 選項，&lt;a href="http://www.forkosh.com/mimetex.html"&gt;mimeTeX&lt;/a&gt; CGI script 會被呼叫來產生每個 TeX 數學公式的圖片。這適用於所有瀏覽器。&lt;code&gt;--mimetex&lt;/code&gt; 選項有一個可選的 URL 參數。如果沒有指定 URL，它會假設 mimeTeX CGI script 的位置在 &lt;code&gt;/cgi-bin/mimetex.cig&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--gladtex&lt;/code&gt; 選項，TeX 數學公式在 HTML 的輸出中會被 &lt;code&gt;&amp;lt;eq&amp;gt;&lt;/code&gt; 標籤包住。產生的 &lt;code&gt;htex&lt;/code&gt; 檔案之後可以透過 &lt;a href="http://ans.hsh.no/home/mgg/gladtex/"&gt;gladTeX&lt;/a&gt; 處理，這會針對每個數學公式生成圖片，並於最後生成一個包含這些圖片連結的 &lt;code&gt;html&lt;/code&gt; 檔案。所以，整個處理流程如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pandoc -s --gladtex myfile.txt -o myfile.htex
gladtex -d myfile-images myfile.htex
# produces myfile.html and images in myfile-images
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--webtex&lt;/code&gt; 選項，TeX 數學公式會被轉換為 &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; 標籤並連結到一個用以轉換公式為圖片的外部 script。公式將會編碼為 URL 可接受格式並且與指定的 URL 參數串接。如果沒有指定 URL，那麼將會使用 Google Chart API (&lt;code&gt;http://chart.apis.google.com/chart?cht=tx&amp;amp;chl=&lt;/code&gt;)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--mathjax&lt;/code&gt; 選項，TeX 數學公式將會被包在 &lt;code&gt;\(...\)&lt;/code&gt;（用於行內數學公式）或 &lt;code&gt;\[...\]&lt;/code&gt;（用於區塊數學公式）之間顯示，並且放在附帶類別 &lt;code&gt;math&lt;/code&gt; 的 &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; 標籤之中。這段內容會使用 &lt;a href="http://www.mathjax.org/"&gt;MathJax&lt;/a&gt; script 演算編排為頁面上的數學公式。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="raw html"&gt;Raw HTML&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;raw_html&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Markdown 允許你在文件中的任何地方插入原始 HTML（或 DocBook）指令（除了在字面文字上下文處，此時的 &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt; 和 &lt;code&gt;&amp;amp;&lt;/code&gt; 都會按其字面意義顯示）。（技術上而言這不算擴充功能，因為原始 markdown 本身就有提供此功能，但做成擴充形式便可以在有特殊需要的時候關閉此功能。）&lt;/p&gt;
&lt;p&gt;輸出 HTML, S5, Slidy, Slideous, DZSlides, EPUB, Markdown 以及 Textile 等格式時，原始 HTML 代碼會不作修改地保留至輸出檔案中；而其他格式的輸出內容則會將原始 HTML 代碼去除掉。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;markdown_in_html_blocks&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原始 markdown 允許你插入 HTML「區塊」：所謂的 HTML 區塊是指，上下各由一個空白行所隔開，開始與結尾均由所在行最左側開始的一連串對稱均衡的 HTML 標籤。在這個區塊中，任何內容都會當作是 HTML 來分析，而不再視為 markdown；所以（舉例來說），&lt;code&gt;*&lt;/code&gt; 符號就不再代表強調。&lt;/p&gt;
&lt;p&gt;當指定格式為 &lt;code&gt;markdown_strict&lt;/code&gt; 時，Pandoc 會以上述方式處理；但預設情況下，Pandoc 能夠以 markdown 語法解讀 HTML 區塊標籤中的內容。舉例說明，Pandoc 能夠將底下這段&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;tr&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;td&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;*&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;td&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;td&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//google.com)&amp;amp;lt;/td&amp;amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;tr&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;轉換為&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;table&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;tr&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;td&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;em&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;one&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;em&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;td&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;td&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"http://google.com"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;a link&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;td&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;tr&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;table&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;而 &lt;code&gt;Markdown.pl&lt;/code&gt; 則是保留該段原樣。&lt;/p&gt;
&lt;p&gt;這個規則只有一個例外：那就是介於 &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; 與 &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; 之間的文字都不會被拿來當作 markdown 解讀。&lt;/p&gt;
&lt;p&gt;這邊與原始 markdown 的分歧，主要是為了讓 markdown 能夠更便利地混入 HTML 區塊元素。比方說，一段 markdown 文字可以用 &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; 標籤將其前後包住來進行樣式指定，而不用擔心裡面的 markdown 不會被解譯到。&lt;/p&gt;
&lt;h2 id="raw tex"&gt;Raw TeX&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;raw_tex&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了 HTML 之外，pandoc 也接受文件中嵌入原始 LaTeX, TeX 以及 ConTeXt 代碼。行內 TeX 指令會被保留並不作修改地輸出至 LaTeX 與 ConTeXt 格式中。所以，舉例來說，你可以使用 LaTeX 來導入 BibTeX 的引用文獻：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This result was proved in \cite{jones.1967}.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;請注意在 LaTeX 環境下時，像是底下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{tabular}{|l|l|}\hline
Age &amp;amp;amp; Frequency \\ \hline
18--25  &amp;amp;amp; 15 \\
26--35  &amp;amp;amp; 33 \\
36--45  &amp;amp;amp; 22 \\ \hline
\end{tabular}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;位在 &lt;code&gt;begin&lt;/code&gt; 與 &lt;code&gt;end&lt;/code&gt; 標籤之間的內容，都會被當作是原始 LaTeX 資料解讀，而不會視為 markdown。&lt;/p&gt;
&lt;p&gt;行內 LaTeX 在輸出至 Markdown, LaTeX 及 ConTeXt 之外的格式時會被忽略掉。&lt;/p&gt;
&lt;h2 id="latex ju ji"&gt;LaTeX 巨集&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;latex_macros&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;當輸出格式不是 LaTeX 時，pandoc 會分析 LaTeX 的 &lt;code&gt;\newcommand&lt;/code&gt; 和 &lt;code&gt;\renewcommand&lt;/code&gt; 定義，並套用其產生的巨集到所有 LaTeX 數學公式中。所以，舉例來說，下列指令對於所有的輸出格式均有作用，而非僅僅作用於 LaTeX 格式： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\newcommand{\tuple}[1]{\langle #1 \rangle}

$\tuple{a, b, c}$
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在 LaTeX 的輸出中，&lt;code&gt;\newcommand&lt;/code&gt; 定義會單純不作修改地保留至輸出結果。&lt;/p&gt;
&lt;h2 id="lian jie"&gt;連結&lt;/h2&gt;
&lt;p&gt;Markdown 接受以下數種指定連結的方式。&lt;/p&gt;
&lt;h3 id="zi dong lian jie"&gt;自動連結&lt;/h3&gt;
&lt;p&gt;如果你用角括號將一段 URL 或是 email 位址包起來，它會自動轉換成連結：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="nt"&gt;google&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;com&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;sam&lt;/span&gt;&lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="k"&gt;green&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;eggs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;ham&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="xing nei lian jie"&gt;行內連結&lt;/h3&gt;
&lt;p&gt;一個行內連結包含了位在方括號中的連結文字，以及方括號後以圓括號包起來的 URL。（你可以選擇性地在 URL 後面加入連結標題，標題文字要放在引號之中。）&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;one&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//fsf.org "click here for a good time!").&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;方括號與圓括號之間不能有空白。連結文字可以包含格式（例如強調），但連結標題則否。&lt;/p&gt;
&lt;h3 id="can kao lian jie"&gt;參考連結&lt;/h3&gt;
&lt;p&gt;一個 &lt;strong&gt;明確&lt;/strong&gt; 的參考連結包含兩個部分，連結本身以及連結定義，其中連結定義可以放在文件的任何地方（不論是放在連結所在處之前或之後）。&lt;/p&gt;
&lt;p&gt;連結本身是由兩組方括號所組成，第一組方括號中為連結文字，第二組為連結標籤。（在兩個方括號間可以有空白。）連結定義則是以方括號框住的連結標籤作開頭，後面跟著一個冒號一個空白，再接著一個 URL，最後可以選擇性地（在一個空白之後）加入由引號或是圓括號包住的連結標題。&lt;/p&gt;
&lt;p&gt;以下是一些範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[my label 1]: /foo/bar.html  "My title, optional"
[my label 2]: /foo
[my label 3]: http://fsf.org (The free software foundation)
[my label 4]: /bar#special  'A title in single quotes'
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;連結的 URL 也可以選擇性地以角括號包住：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;my&lt;/span&gt; &lt;span class="nx"&gt;label&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="nt"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;baz&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;連結標題可以放在第二行：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[my label 3]: http://fsf.org
  "The free software foundation"
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;需注意連結標籤並不區分大小寫。所以下面的例子會建立合法的連結：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Here&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;FOO&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;baz&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在一個 &lt;strong&gt;隱性&lt;/strong&gt; 參考連結中，第二組方括號的內容是空的，甚至可以完全地略去：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;See&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;website&lt;/span&gt;&lt;span class="p"&gt;][],&lt;/span&gt; &lt;span class="n"&gt;or&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;website&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;website&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//foo.bar.baz&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意：在 &lt;code&gt;Markdown.pl&lt;/code&gt; 以及大多數其他 markdown 實作中，參考連結的定義不能存在於嵌套結構中，例如清單項目或是區塊引言。Pandoc lifts this arbitrary seeming restriction。所以雖然下面的語法在幾乎所有其他實作中都是錯誤的，但在 pandoc 中可以正確處理：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; My block [quote].
&amp;amp;gt;
&amp;amp;gt; [quote]: /foo
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="nei bu lian jie"&gt;內部連結&lt;/h3&gt;
&lt;p&gt;要連結到同一份文件的其他章節，可使用自動產生的 ID（參見 [HTML, LaTeX 與 ConTeXt 的標題識別符] 一節後半）。例如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;See the [Introduction](#introduction).
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;或是&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;See the [Introduction].

[Introduction]: #introduction
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;內部連結目前支援的格式有 HTML（包括 HTML slide shows 與 EPUB）、LaTeX 以及 ConTeXt。&lt;/p&gt;
&lt;h2 id="tu pian_1"&gt;圖片&lt;/h2&gt;
&lt;p&gt;在連結語法的前面加上一個 &lt;code&gt;!&lt;/code&gt; 就是圖片的語法了。連結文字將會作為圖片的替代文字（alt text）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;la&lt;/span&gt; &lt;span class="n"&gt;lune&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;lalune&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="s"&gt;"Voyage to the moon"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;movie&lt;/span&gt; &lt;span class="n"&gt;reel&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;movie&lt;/span&gt; &lt;span class="n"&gt;reel&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gif&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="fu shang shuo ming de tu pian"&gt;附上說明的圖片&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;implicit_figures&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一個圖片若自身單獨存在一個段落中，那麼將會以附上圖片說明 (caption) 的圖表 (figure) 形式呈現。&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;（在 LaTeX 中，會使用圖表環境；在 HTML 中，圖片會被放在具有 &lt;code&gt;figure&lt;/code&gt; 類別的 &lt;code&gt;div&lt;/code&gt; 元素中，並會附上一個具有 &lt;code&gt;caption&lt;/code&gt; 類別的 &lt;code&gt;p&lt;/code&gt; 元素。）圖片的替代文字同時也會用來作為圖片說明。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;![This is the caption](/url/of/image.png)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你只是想要個一般的行內圖片，那麼只要讓圖片不是段落裡唯一的元素即可。一個簡單的方法是在圖片後面插入一個不斷行空格：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;![This image won't be a figure](/url/of/image.png)\
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="jiao zhu_1"&gt;腳註&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;footnotes&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc's markdown 支援腳註功能，使用以下的語法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Here is a footnote reference,[^1] and another.[^longnote]

[^1]: Here is the footnote.

[^longnote]: Here's one with multiple blocks.

    Subsequent paragraphs are indented to show that they
belong to the previous footnote.

        { some.code }

    The whole paragraph can be indented, or just the first
    line.  In this way, multi-paragraph footnotes work like
    multi-paragraph list items.

This paragraph won't be part of the note, because it
isn't indented.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;腳註參考用的 ID 不得包含空白、tabs 或換行字元。這些 ID 只會用來建立腳註位置與腳註文字的對應關連；在輸出時，腳註將會依序遞增編號。&lt;/p&gt;
&lt;p&gt;腳註本身不需要放在文件的最後面。它們可以放在文件裡的任何地方，但不能被放入區塊元素（清單、區塊引言、表格等）之中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;inline_notes&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc 也支援了行內腳註（儘管，與一般腳註不同，行內腳註不能包含多個段落）。其語法如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Here is an inline note.^[Inlines notes are easier to write, since
you don't have to pick an identifier and move down to type the
note.]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;行內與一般腳註可以自由交錯使用。&lt;/p&gt;
&lt;h2 id="yin yong"&gt;引用&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;citations&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc 能夠以數種形式自動產生引用與參考書目（使用 Andrea Rossato 的 &lt;code&gt;hs-citeproc&lt;/code&gt;）。為了使用這項功能，你需要一個下列其中一種格式的參考書目資料庫：&lt;/p&gt;
&lt;p&gt;Format            File extension
  ------------      --------------
  MODS              .mods
  BibLaTeX          .bib
  BibTeX            .bibtex
  RIS               .ris
  EndNote           .enl
  EndNote XML       .xml
  ISI               .wos
  MEDLINE           .medline
  Copac             .copac
  JSON citeproc     .json&lt;/p&gt;
&lt;p&gt;需注意的是副檔名 &lt;code&gt;.bib&lt;/code&gt; 一般而言同時適用於 BibTeX 與 BibLaTeX 的檔案，不過你可以使用 &lt;code&gt;.bibtex&lt;/code&gt; 來強制指定 BibTeX。&lt;/p&gt;
&lt;p&gt;你需要使用命令列選項 &lt;code&gt;--bibliography&lt;/code&gt; 來指定參考書目檔案（如果有多個書目檔就得反覆指定）。&lt;/p&gt;
&lt;p&gt;預設情況下，pandoc 會在引用文獻與參考書目中使用芝加哥「作者－日期」格式。要使用其他的格式，你需要用 &lt;code&gt;--csl&lt;/code&gt; 選項來指定一個 &lt;a href="http://CitationStyles.org"&gt;CSL&lt;/a&gt; 1.0 格式的檔案。關於建立與修改 CSL 格式的入門可以在 &lt;a href="http://citationstyles.org/downloads/primer.html"&gt;http://citationstyles.org/downloads/primer.html&lt;/a&gt; 這邊找到。&lt;a href="https://github.com/citation-style-language/styles"&gt;https://github.com/citation-style-language/styles&lt;/a&gt; 是 CSL 格式的檔案庫。也可以在 &lt;a href="http://zotero.org/styles"&gt;http://zotero.org/styles&lt;/a&gt; 以簡單的方式瀏覽。&lt;/p&gt;
&lt;p&gt;引用資訊放在方括號中，以分號區隔。每一條引用都會有個 key，由 &lt;code&gt;@&lt;/code&gt; 加上資料庫中的引用 ID 組成，並且可以選擇性地包含前綴、定位以及後綴。以下是一些範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Blah blah [see @doe99, pp. 33-35; also @smith04, ch. 1].

Blah blah [@doe99, pp. 33-35, 38-39 and *passim*].

Blah blah [@smith04; @doe99].
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在 &lt;code&gt;@&lt;/code&gt; 前面的減號 (&lt;code&gt;-&lt;/code&gt;) 將會避免作者名字在引用中出現。這可以用在已經提及作者的文章場合中：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Smith says blah [-@smith04].
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你也可以在文字中直接插入引用資訊，方式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;@smith04 says blah.

@smith04 [p. 33] says blah.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果引用格式檔需要產生一份引用作品的清單，這份清單會被放在文件的最後面。一般而言，你需要以一個適當的標題結束你的文件：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;last paragraph...

# References
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如此一來參考書目就會被放在這個標題後面了。&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;之所以有這條規則，主要是要避免以人名頭文字縮寫作為開頭的段落所帶來的混淆，像是&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;B. Russell was an English philosopher.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這樣就不會被當作清單項目了。&lt;/p&gt;
&lt;p&gt;這條規則並不會避免以下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(C) 2007 Joe Smith
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這樣的敘述被解釋成清單項目。在這情形下，可以使用反斜線：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(C\) 2007 Joe Smith
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="http://www.justatheory.com/computers/markup/modest-markdown-proposal.html"&gt;David Wheeler&lt;/a&gt; 對於 markdown 的建議也同時影響了我。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;這個方案是由 Michel Fortin 在 &lt;a href="http://six.pairlist.net/pipermail/markdown-discuss/2005-March/001097.html"&gt;Markdown discussion list&lt;/a&gt; 的討論中所提出。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:T1"&gt;
&lt;p&gt;譯註：straight quotes 指的是左右兩側都長得一樣的引號，例如我們直接在鍵盤上打出來的單引號或雙引號；curly quotes 則是左右兩側不同，有從兩側向內包夾視覺效果的引號。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:T1" rev="footnote" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;這項功能尚未在 RTF, OpenDocument 或 ODT 格式上實現。在這些格式中，你會得到一個在段落中只包含自己的圖片，而無圖片說明。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:5" rev="footnote" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content></entry></feed>