<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Freeopen - 机器学习</title><link href="https://freeopen.github.io/" rel="alternate"></link><link href="https://freeopen.github.io/feeds/ji-qi-xue-xi.atom.xml" rel="self"></link><id>https://freeopen.github.io/</id><updated>2018-05-08T00:00:00+08:00</updated><entry><title>概率图模型 HMM、MEMM、CRF</title><link href="https://freeopen.github.io/posts/hmm-memm-crf" rel="alternate"></link><published>2018-05-08T00:00:00+08:00</published><updated>2018-05-08T00:00:00+08:00</updated><author><name>Scofield</name></author><id>tag:freeopen.github.io,2018-05-08:/posts/hmm-memm-crf</id><summary type="html">&lt;p&gt;&lt;a href="https://www.zhihu.com/question/35866596/answer/236886066"&gt;转自知乎&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 此文不错，有些小错，顺手改了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="yi , preface"&gt;&lt;strong&gt;一、Preface&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;之前刚接触NLP时做相关的任务，也必然地涉及到了序列处理任务，然后自然要接触到概率图模型。当时在全网搜中文资料，陆续失望地发现竟然真的没有讲得清楚的博文，发现基本是把李航老师书里或CRF tutorial等资料的文字论述和公式抄来抄去的。当然，没有说别人讲的是错的，只是觉得，要是没有把东西说的让读者看得懂，那也是没意义啊。或者有些吧，就是讲了一大堆的东西，貌似也明白了啥，但还是不能让我很好的理解CRF这些模型究竟是个啥，完了还是有一头雾水散不开的感觉。试想，一堆公式扔过来，没有个感性理解的过渡，怎么可能理解的了。我甚至觉得，如果博客让人看不懂，那说明要么自己没理解透要么就是思维不清晰讲不清楚。所以默想，深水区攻坚还是要靠自己，然后去做调研做research，所以就写了个这个学习记录。&lt;/p&gt;
&lt;p&gt;所以概率图的研究学习思考列入了我的任务清单。不过平时的时间又非常的紧，只能陆陆续续的思考着，所以时间拖得也真是长啊。&lt;/p&gt;
&lt;p&gt;这是个学习笔记。相比其他的学习模型，概率图貌似确实是比较难以理解的。这里我基本全部用自己的理解加上自己的语言习惯表达出来，off the official form，表达尽量接地气。我会尽量将我所有理解过程中的每个关键小细节都详细描述出来，以使对零基础的初学者友好 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://www.zhihu.com/question/35866596/answer/236886066"&gt;转自知乎&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 此文不错，有些小错，顺手改了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="yi , preface"&gt;&lt;strong&gt;一、Preface&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;之前刚接触NLP时做相关的任务，也必然地涉及到了序列处理任务，然后自然要接触到概率图模型。当时在全网搜中文资料，陆续失望地发现竟然真的没有讲得清楚的博文，发现基本是把李航老师书里或CRF tutorial等资料的文字论述和公式抄来抄去的。当然，没有说别人讲的是错的，只是觉得，要是没有把东西说的让读者看得懂，那也是没意义啊。或者有些吧，就是讲了一大堆的东西，貌似也明白了啥，但还是不能让我很好的理解CRF这些模型究竟是个啥，完了还是有一头雾水散不开的感觉。试想，一堆公式扔过来，没有个感性理解的过渡，怎么可能理解的了。我甚至觉得，如果博客让人看不懂，那说明要么自己没理解透要么就是思维不清晰讲不清楚。所以默想，深水区攻坚还是要靠自己，然后去做调研做research，所以就写了个这个学习记录。&lt;/p&gt;
&lt;p&gt;所以概率图的研究学习思考列入了我的任务清单。不过平时的时间又非常的紧，只能陆陆续续的思考着，所以时间拖得也真是长啊。&lt;/p&gt;
&lt;p&gt;这是个学习笔记。相比其他的学习模型，概率图貌似确实是比较难以理解的。这里我基本全部用自己的理解加上自己的语言习惯表达出来，off the official form，表达尽量接地气。我会尽量将我所有理解过程中的每个关键小细节都详细描述出来，以使对零基础的初学者友好。包括理论的来龙去脉，抽象具象化，模型的构成，模型的训练过程，会注重类比的学习。&lt;/p&gt;
&lt;p&gt;根据现有资料，我是按照概率图模型将HMM，MEMM，CRF放在这里一起对比学习。之所以把他们拿在一起，是因为他们都用于标注问题。并且之所以放在概率图框架下，是完全因为自己top-down思维模式使然。另外，概率图下还有很多的模型，这儿只学习标注模型。&lt;/p&gt;
&lt;p&gt;正儿八经的，我对这些个概率图模型有了彻悟，是从我明白了生成式模型与判别式模型的那一刻。一直在思考从概率图模型角度讲他们的区别到底在哪。&lt;/p&gt;
&lt;p&gt;另外，篇幅略显长，但咱们不要急躁，好好看完这篇具有良好的上下文的笔记，那肯定是能理解的，或者就多看几遍。&lt;/p&gt;
&lt;p&gt;个人学习习惯就是，&lt;strong&gt;要尽可能地将一群没有结构的知识点融会贯通，再用一条树状结构的绳将之串起来，结构化，就是说要成体系，这样把绳子头一拎所有的东西都能拿起来&lt;/strong&gt;。学习嘛，应该要是一个熵减的过程，卓有成效的学习应该是混乱度越来越小！这个思维方式对我影响还是蛮大的。&lt;/p&gt;
&lt;p&gt;在正式内容之前，还是先要明确下面这一点，最好脑子里形成一个定势：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;统计机器学习所有的模型（个别instant model和优化算法以及其他的特种工程知识点除外）的工作流程都是如此：&lt;br/&gt;
a.训练模型参数，得到模型（由参数唯一确定），&lt;br/&gt;
b.预测给定的测试数据。&lt;br/&gt;
拿这个流程去挨个学习模型，思路上会非常顺畅。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;除此之外，对初学者的关于机器学习的入门学习方式也顺带表达一下(empirical speaking)：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a.完整特征工程竞赛&lt;br/&gt;
b.野博客理论入门理解&lt;br/&gt;
c.再回到代码深入理解模型内部&lt;br/&gt;
d.再跨理论，查阅经典理论巨作。这时感性理性都有一定高度，会遇到很多很大的理解上的疑惑，这时3大经典可能就可以发挥到最大作用了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;很多beginer，就比如说学CRF模型，然后一上来就摆一套复杂的公式，什么我就问，这能理解的了吗？这是正确的开启姿势吗？当然了，也要怪那些博主，直接整一大堆核心公式，实际上读者的理解门槛可能就是一个过渡性的细枝末节而已。没有上下文的教育肯定是失败的（这一点我又想吐槽国内绝大部分本科的院校教育模式）。所以说带有完整上下文信息以及过程来龙去脉交代清楚才算到位吧。&lt;/p&gt;
&lt;p&gt;而不是一上来就死啃被人推荐的&amp;ldquo;经典资料&amp;rdquo;，这一点相信部分同学会理解。好比以前本科零基础学c++ JAVA，上来就看primr TIJ，结果浪费了时间精力一直在门外兜圈。总结方法吸取教训，应该快速上手代码，才是最高效的。经典最好是用来查阅的工具书，我目前是李航周志华和经典的那3本迭代轮询看了好多轮，经常会反复查询某些model或理论的来龙去脉；有时候要查很多相关的东西，看这些书还是难以贯通，然后发现有些人的博客写的会更容易去理解。所以另外，学习资料渠道也要充分才行。&lt;/p&gt;
&lt;p&gt;最后提示一下，&lt;strong&gt;请务必按照标题层级结构和目录一级一级阅读，防止跟丢。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="er , prerequisite"&gt;&lt;strong&gt;二、Prerequisite&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id="2.1 gai lu tu"&gt;&lt;strong&gt;2.1 概率图&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;之前刚接触CRF时，一上来试图越过一堆繁琐的概率图相关概念，不过sad to say, 这是后面的前驱知识，后面还得反过来补这个点。所以若想整体把握，系统地拿下这一块，应该还是要越过这块门槛的。&lt;/p&gt;
&lt;p&gt;当然了，一开始只需略略快速看一篇，后面可再返过来补查。&lt;/p&gt;
&lt;h3 id="2.1.1 gai lan"&gt;&lt;strong&gt;2.1.1 概览&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构（个人非常喜欢这种类型的图）：  &lt;/p&gt;
&lt;p&gt;&lt;img data-caption="" data-rawheight="336" data-rawwidth="631" data-size="normal" src="https://freeopen.github.io/images/v2.jpg" width="631"/&gt;&lt;/p&gt;
&lt;p&gt;在概率图模型中，数据(样本)由公式 &lt;span class="math"&gt;\(G=(V,E)\)&lt;/span&gt; 建模表示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(V\)&lt;/span&gt; 表示节点，即随机变量（放在此处的，可以是一个token或者一个label），具体地，用 &lt;span class="math"&gt;\(Y = (y_1, \cdots, y_n)\)&lt;/span&gt; 为随机变量建模，注意 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 现在是代表了一批随机变量（想象对应一条sequence，包含了很多的token）， &lt;span class="math"&gt;\(P(Y)\)&lt;/span&gt; 为这些随机变量的分布；  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(E\)&lt;/span&gt; 表示边，即概率依赖关系。具体咋理解，还是要在后面结合HMM或CRF的graph具体解释。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="2.1.2 you xiang tu  vs. wu xiang tu"&gt;&lt;strong&gt;2.1.2 有向图 vs. 无向图&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;上图可以看到，贝叶斯网络（信念网络）都是有向的，马尔科夫网络无向。所以，贝叶斯网络适合为有单向依赖的数据建模，马尔科夫网络适合实体之间互相依赖的建模。具体地，他们的核心差异表现在如何求 &lt;span class="math"&gt;\(P=(Y)\)&lt;/span&gt; ，即怎么表示 &lt;span class="math"&gt;\(Y=(y_1,\cdots,y_n)\)&lt;/span&gt; 这个的联合概率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 有向图&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于有向图模型，这么求联合概率： &lt;span class="math"&gt;\(P(x_1, \cdots, x_n )=\prod_{i=0}P(x_i | \pi(x_{i}))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;举个例子，对于下面的这个有向图的随机变量(注意，这个图我画的还是比较广义的)：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/2.jpg" width="453"&gt;
&lt;br/&gt;
&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;应该这样表示他们的联合概率:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(x_1, \cdots, x_n )=P(x_1)&amp;middot;P(x_2|x_1 )&amp;middot;P(x_3|x_2 )&amp;middot;P(x_4|x_2 )&amp;middot;P(x_5|x_3,x_4 )\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;应该很好理解吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 无向图&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于无向图，我看资料一般就指马尔科夫网络(注意，这个图我画的也是比较广义的)。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/3.jpg" width="260"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;如果一个graph太大，可以用因子分解将 &lt;img alt="P=(Y)" src="https://www.zhihu.com/equation?tex=P%3D%28Y%29"/&gt; 写为若干个联合概率的乘积。咋分解呢，将一个图分为若干个&amp;ldquo;小团&amp;rdquo;，注意每个团必须是&amp;ldquo;最大团&amp;rdquo;（就是里面任何两个点连在了一块，具体&amp;hellip;&amp;hellip;算了不解释，有点&amp;ldquo;最大连通子图&amp;rdquo;的感觉），则有：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Y )=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c} )\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;其中, &lt;span class="math"&gt;\(Z(x) = \sum_{Y} \prod_{c}\psi_{c}(Y_{c} )\)&lt;/span&gt;，公式应该不难理解吧，归一化是为了让结果算作概率。&lt;/p&gt;
&lt;p&gt;所以像上面的无向图：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Y )=\frac{1}{Z(x)} ( \psi_{1}(X_{1}, X_{3}, X_{4} ) &amp;middot; \psi_{2}(X_{2}, X_{3}, X_{4} ) )\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中， &lt;span class="math"&gt;\(\psi_{c}(Y_{c} )\)&lt;/span&gt; 是一个最大团 &lt;img alt="C" src="https://www.zhihu.com/equation?tex=C"/&gt; 上随机变量们的联合概率，一般取指数函数的：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\psi_{c}(Y_{c} ) = e^{-E(Y_{c})} =e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)}\)&lt;/span&gt;
好了，管这个东西叫做&lt;code&gt;势函数&lt;/code&gt;。注意 &lt;span class="math"&gt;\(e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)}\)&lt;/span&gt; 是否有看到CRF的影子。&lt;/p&gt;
&lt;p&gt;那么概率无向图的联合概率分布可以在因子分解下表示为：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(Y )=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c} ) = \frac{1}{Z(x)} \prod_{c} e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)} = \frac{1}{Z(x)} e^{\sum_{c}\sum_{k}\lambda_{k}f_{k}(y_{i},y_{i-1},x,i)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意，这里的理解还蛮重要的，注意递推过程，敲黑板，这是CRF的开端！&lt;br/&gt;
这个由&lt;code&gt;Hammersly-Clifford law&lt;/code&gt;保证，具体不展开。&lt;/p&gt;
&lt;h3 id="2.1.3 ma er ke fu jia she &amp;amp;ma er ke fu xing"&gt;&lt;strong&gt;2.1.3 马尔科夫假设&amp;amp;马尔科夫性&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;这个也属于前馈知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 马尔科夫假设&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;额应该是齐次马尔科夫假设，这样假设：马尔科夫链 &lt;span class="math"&gt;\((x_{1},\cdots,x_{n})\)&lt;/span&gt; 里的 &lt;span class="math"&gt;\(x_{i}\)&lt;/span&gt; 总是只受 &lt;span class="math"&gt;\(x_{i-1}\)&lt;/span&gt; 一个人的影响。&lt;br/&gt;
马尔科夫假设这里相当于就是个1-gram。&lt;/p&gt;
&lt;p&gt;马尔科夫过程呢？即，在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。最简单的马尔科夫过程是一阶的，即只依赖于前一个状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 马尔科夫性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;马尔科夫性是是保证或者判断概率图是否为概率无向图的条件。&lt;/p&gt;
&lt;p&gt;三点内容：a. 成对，b. 局部，c. 全局。&lt;/p&gt;
&lt;p&gt;我觉得这个不用展开。&lt;/p&gt;
&lt;h3 id="2.2 pan bie shi (discriminative)mo xing  vs. sheng cheng shi (generative)mo xing"&gt;&lt;strong&gt;2.2 判别式(discriminative)模型 vs. 生成式(generative)模型&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在监督学习下，模型可以分为判别式模型与生成式模型。&lt;/p&gt;
&lt;p&gt;重点来了。上面有提到，我理解了HMM、CRF模型的区别是从理解了判别式模型与生成式模型的那刻，并且瞬间对其他的模型有一个恍然大悟。我记得是一年前就开始纠结这两者的区别，但我只能说，栽在了一些烂博客上，大部分都没有自己的insightful理解，也就是一顿官话，也真是难以理解。后来在知乎上一直琢磨别人的答案，然后某日早晨终于豁然开朗，就是这种感觉。&lt;/p&gt;
&lt;p&gt;好了，我要用自己的理解来转述两者的区别了below。&lt;/p&gt;
&lt;p&gt;先问个问题，根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT&amp;hellip;&amp;hellip;）与B批模型（NB、LDA&amp;hellip;&amp;hellip;），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。  &lt;/p&gt;
&lt;p&gt;2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; (注意 &lt;span class="math"&gt;\(X\)&lt;/span&gt; 包含所有的特征 &lt;span class="math"&gt;\(x_{i}\)&lt;/span&gt; ， &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; ，再结合新样本给的 &lt;span class="math"&gt;\(X\)&lt;/span&gt; ，通过条件概率就能出来 &lt;span class="math"&gt;\(Y\)&lt;/span&gt;：&lt;br/&gt;
&lt;span class="math"&gt;\(P(Y|X) = \frac{P(X,Y)}{P(X)}\)&lt;/span&gt;
好了，应该说清楚了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;1. 判别式模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：&lt;strong&gt;判别模型是直接对&lt;/strong&gt; &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; &lt;strong&gt;建模&lt;/strong&gt;，就是说，直接根据X特征来对Y建模训练。&lt;/p&gt;
&lt;p&gt;具体地，我的训练过程是确定构件 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; 模型里面&amp;ldquo;复杂映射关系&amp;rdquo;中的参数，完了再去inference一批新的sample。&lt;/p&gt;
&lt;p&gt;所以判别式模型的特征总结如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; 建模&lt;/li&gt;
&lt;li&gt;对所有的样本只构建一个模型，确认总体判别边界&lt;/li&gt;
&lt;li&gt;观测到输入什么特征，就预测最可能的label&lt;/li&gt;
&lt;li&gt;另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;2. 生成式模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; ，也就是说，&lt;strong&gt;我在训练阶段是只对&lt;/strong&gt; &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt;&lt;strong&gt;建模&lt;/strong&gt;，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt;，导出 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; ,但这已经不属于建模阶段了。&lt;/p&gt;
&lt;p&gt;结合NB过一遍生成式模型的工作流程。学习阶段，建模： &lt;span class="math"&gt;\(P(X,Y)=P(X|Y)P(Y)\)&lt;/span&gt; （当然，NB具体流程去隔壁参考）,然后 &lt;span class="math"&gt;\(P(Y|X) = \frac{P(X,Y)}{P(X)}\)&lt;/span&gt; 。&lt;br/&gt;
另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。&lt;/p&gt;
&lt;p&gt;所以生成式总结下有如下特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; 建模&lt;/li&gt;
&lt;li&gt;这里我们主要讲分类问题，所以是要对每个label(&lt;span class="math"&gt;\(y_{i}\)&lt;/span&gt;) 都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）&lt;/li&gt;
&lt;li&gt;中间生成联合分布，并可生成采样数据。&lt;/li&gt;
&lt;li&gt;生成式模型的优点在于，所包含的信息非常齐全，我称之为&amp;ldquo;上帝信息&amp;rdquo;，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。&lt;br/&gt;
最后identity the picture below:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/4.jpg" width="80%"&gt;
&lt;br/&gt;
&lt;/img&gt;&lt;/p&gt;
&lt;h3 id="2.3 xu lie jian mo"&gt;&lt;strong&gt;2.3 序列建模&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;为了号召零门槛理解，现在解释如何为序列问题建模。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/5.jpg" width="80%"&gt;
&lt;br/&gt;
&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;序列包括时间序列以及general sequence，但两者无异。连续的序列在分析时也会先离散化处理。常见的序列有如：时序数据、本文句子、语音数据、等等。&lt;/p&gt;
&lt;p&gt;广义下的序列有这些特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点之间有关联依赖性/无关联依赖性&lt;/li&gt;
&lt;li&gt;序列的节点是随机的/确定的&lt;/li&gt;
&lt;li&gt;序列是线性变化/非线性的&lt;/li&gt;
&lt;li&gt;&amp;hellip;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对不同的序列有不同的问题需求，常见的序列建模方法总结有如下：&lt;/p&gt;
&lt;p&gt;1. 拟合，预测未来节点（或走势分析）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a. 常规序列建模方法：AR、MA、ARMA、ARIMA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;b. 回归拟合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;c. Neural Networks&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2. 判断不同序列类别，即分类问题：HMM、CRF、General Classifier（ML models、NN models）&lt;/p&gt;
&lt;p&gt;3. 不同时序对应的状态的分析，即序列标注问题：HMM、CRF、RecurrentNNs&lt;/p&gt;
&lt;p&gt;在本篇文字中，我们只关注在2. &amp;amp; 3.类问题下的建模过程和方法。&lt;/p&gt;
&lt;h2 id="san , hmm_1"&gt;&lt;strong&gt;三、HMM&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;最早接触的是HMM。较早做过一个项目，关于声波手势识别，跟声音识别的机制一样，使用的正是HMM的一套方法。后来又用到了 &lt;em&gt;kalman filter&lt;/em&gt;，之后做序列标注任务接触到了CRF，所以整个概率图模型还是接触的方面还蛮多。&lt;/p&gt;
&lt;h3 id="3.1 li jie hmm"&gt;&lt;strong&gt;3.1 理解HMM&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在2.2、2.3中提序列的建模问题时，我们只是讨论了常规的序列数据，e.g., &lt;span class="math"&gt;\((X_{1},\cdots,X_{n})\)&lt;/span&gt; ,像2.3的图片那样。像这种序列一般用马尔科夫模型就可以胜任。实际上我们碰到的更多的使用HMM的场景是每个节点 &lt;span class="math"&gt;\(X_{i}\)&lt;/span&gt; 下还附带着另一个节点 &lt;span class="math"&gt;\(Y_{i}\)&lt;/span&gt; ，正所谓&lt;strong&gt;隐含&lt;/strong&gt;马尔科夫模型，那么除了正常的节点，还要将&lt;strong&gt;隐含状态节点&lt;/strong&gt;也得建模进去。正儿八经地，将 &lt;span class="math"&gt;\(X_{i} 、 Y_{i}\)&lt;/span&gt; 换成 &lt;span class="math"&gt;\(i_{i} 、o_{i}\)&lt;/span&gt; ,并且他们的名称变为状态节点、观测节点。状态节点正是我的隐状态。&lt;/p&gt;
&lt;p&gt;HMM属于典型的生成式模型。对照2.1的讲解，应该是要从训练数据中学到数据的各种分布，那么有哪些分布呢以及是什么呢？直接正面回答的话，正是&lt;strong&gt;HMM的5要素&lt;/strong&gt;，其中有3个就是整个数据的不同角度的概率分布：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt; ，隐藏状态集 &lt;span class="math"&gt;\(N = \lbrace q_{1}, \cdots, q_{N} \rbrace\)&lt;/span&gt; , 我的隐藏节点不能随意取，只能限定取包含在隐藏状态集中的符号。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M\)&lt;/span&gt;，观测集 &lt;span class="math"&gt;\(M = \lbrace v_{1}, \cdots, v_{M} \rbrace\)&lt;/span&gt; , 同样我的观测节点不能随意取，只能限定取包含在观测状态集中的符号。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A\)&lt;/span&gt; ，状态转移概率矩阵，这个就是其中一个概率分布。他是个矩阵， &lt;span class="math"&gt;\(A= [a_{ij}]_{N \times N}\)&lt;/span&gt; （N为隐藏状态集元素个数），其中 &lt;span class="math"&gt;\(a_{ij} = P(i_{t+1}|i_{t})， i_{t}\)&lt;/span&gt; 即第i个隐状态节点,即所谓的状态转移嘛。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(B\)&lt;/span&gt; ，观测概率矩阵，这个就是另一个概率分布。他是个矩阵， &lt;span class="math"&gt;\(B = [b_{ij}]_{N \times M}\)&lt;/span&gt; （&lt;span class="math"&gt;\(N\)&lt;/span&gt;为隐藏状态集元素个数，&lt;span class="math"&gt;\(M\)&lt;/span&gt;为观测集元素个数），其中 &lt;span class="math"&gt;\(b_{ij} = P(o_{t}|i_{t})， o_{t}\)&lt;/span&gt; 即第&lt;span class="math"&gt;\(i\)&lt;/span&gt;个观测节点，&lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 即第&lt;span class="math"&gt;\(i\)&lt;/span&gt;个隐状态节点，即所谓的观测概率（发射概率）嘛。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(&amp;pi;\)&lt;/span&gt; ，指模型在初始时刻各状态(来自状态集&lt;span class="math"&gt;\(N\)&lt;/span&gt;)出现的概率。通常，第一个隐状态节点 &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt;的隐状态可由EM方法学得,故&lt;span class="math"&gt;\(&amp;pi;\)&lt;/span&gt;在初始化时可随机给定。(&lt;em&gt;这里原句读不通，由freeopenn修订&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以图看起来是这样的：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/6.jpg" width="415"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;看的很清楚，我的模型先去学习要确定以上5要素，之后在inference阶段的工作流程是：首先，隐状态节点 &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 是不能直接观测到的数据节点， &lt;span class="math"&gt;\(o_{t}\)&lt;/span&gt; 才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系， &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 在A的指导下生成下一个隐状态节点 &lt;span class="math"&gt;\(i_{t+1}\)&lt;/span&gt; ，并且 &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 在 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 的指导下生成依赖于该 &lt;span class="math"&gt;\(i_{t}\)&lt;/span&gt; 的观测节点 &lt;span class="math"&gt;\(o_{t}\)&lt;/span&gt; , 并且我只能观测到序列 &lt;span class="math"&gt;\((o_{1}, \cdots, o_{i})\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;好，举例子说明（序列标注问题，POS，标注集BES）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;input: "学习出一个模型，然后再预测出一条指定"  &lt;/p&gt;
&lt;p&gt;expected output: 学/B 习/E 出/S 一/B 个/E 模/B 型/E ，/S 然/B 后/E 再/E 预/B 测/E &amp;hellip;&amp;hellip;  &lt;/p&gt;
&lt;p&gt;其中，input里面所有的char构成的字表，形成观测集 &lt;span class="math"&gt;\(M\)&lt;/span&gt; ，因为字序列在inference阶段是我所能看见的；标注集BES构成隐藏状态集 &lt;span class="math"&gt;\(N\)&lt;/span&gt; ，这是我无法直接获取的，也是我的预测任务；至于 &lt;span class="math"&gt;\(A、B、&amp;pi;\)&lt;/span&gt; ，这些概率分布信息（上帝信息）都是我在学习过程中所确定的参数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;然后一般初次接触的话会疑问：为什么要这样？&amp;hellip;&amp;hellip;好吧，就应该是这样啊，根据具有同时带着隐藏状态节点和观测节点的类型的序列，在HMM下就是这样子建模的。&lt;/p&gt;
&lt;p&gt;下面来点高层次的理解：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模 &lt;span class="math"&gt;\(P(O,I) = \sum_{t=1}^{T}P(I_{t} | I_{t-1})P(O_{t} | I_{t})\)&lt;/span&gt; (注意，这个公式不在模型运行的任何阶段能体现出来，只是我们都去这么来表示HMM是个生成式模型，他的联合概率 &lt;span class="math"&gt;\(P(O,I)\)&lt;/span&gt; 就是这么计算的)。&lt;/li&gt;
&lt;li&gt;并且B中 &lt;span class="math"&gt;\(b_{ij} = P(o_{t}|i_{t})\)&lt;/span&gt; ，这意味着o对i有依赖性。&lt;/li&gt;
&lt;li&gt;在A中， &lt;span class="math"&gt;\(a_{ij} = P(i_{t+1}|i_{t})\)&lt;/span&gt; ，也就是说只遵循了一阶马尔科夫假设，1-gram。试想，如果数据的依赖超过1-gram，那肯定HMM肯定是考虑不进去的。这一点限制了HMM的性能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="3.2 mo xing yun xing guo cheng"&gt;&lt;strong&gt;3.2 模型运行过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;模型的运行过程（工作流程）对应了HMM的3个问题。&lt;/p&gt;
&lt;h3 id="3.2.1 xue xi xun lian guo cheng"&gt;&lt;strong&gt;3.2.1 学习训练过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;对照2.1的讲解，HMM学习训练的过程，就是找出数据的分布情况，也就是模型参数的确定。&lt;/p&gt;
&lt;p&gt;主要学习算法按照训练数据除了观测状态序列 &lt;span class="math"&gt;\((o_{1}, \cdots, o_{i})\)&lt;/span&gt; 是否还有隐状态序列 &lt;span class="math"&gt;\((i_{1}, \cdots, i_{i})\)&lt;/span&gt; 分为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;极大似然估计, with 隐状态序列&lt;/li&gt;
&lt;li&gt;Baum-Welch(前向后向), without 隐状态序列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感觉不用做很多的介绍，都是很实实在在的算法，看懂了就能理解。简要提一下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 极大似然估计&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的。所以极大似然估计法是非常常用的学习算法，我见过的很多代码里面也是这么计算的。比较简单。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step1. 算A&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$\hat{a_{ij}} = \frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;step2. 算B&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$\hat{b_{j}}(k) = \frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;step3. 直接估计 &lt;span class="math"&gt;\(&amp;pi;\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;比如说，在代码里计算完了就是这样的：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/7.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/8.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/9.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Baum-Welch(前向后向)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;就是一个EM的过程，如果你对EM的工作流程有经验的话，对这个Baum-Welch一看就懂。EM的过程就是初始化一套值，然后迭代计算，根据结果再调整值，再迭代，最后收敛&amp;hellip;&amp;hellip;好吧，这个理解是没有捷径的，去隔壁钻研EM吧。&lt;/p&gt;
&lt;p&gt;这里只提一下核心。因为我们手里没有隐状态序列 &lt;span class="math"&gt;\((i_{1}, \cdots, i_{i})\)&lt;/span&gt; 信息，所以我先必须给初值 &lt;span class="math"&gt;\(a_{ij}^{0}, b_{j}(k)^{0}, \pi^{0}\)&lt;/span&gt; ，初步确定模型，然后再迭代计算出 &lt;span class="math"&gt;\(a_{ij}^{n}, b_{j}(k)^{n}, \pi^{n}\)&lt;/span&gt; ,中间计算过程会用到给出的观测状态序列 &lt;span class="math"&gt;\((o_{1}, \cdots, o_{i})\)&lt;/span&gt;。另外，收敛性由EM的XXX定理保证。&lt;/p&gt;
&lt;h3 id="3.2.2 xu lie biao zhu (jie ma )guo cheng"&gt;&lt;strong&gt;3.2.2 序列标注（解码）过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;好了，学习完了HMM的分布参数，也就确定了一个HMM模型。需要注意的是，这个HMM是对我这一批全部的数据进行训练所得到的参数。&lt;/p&gt;
&lt;p&gt;序列标注问题也就是&amp;ldquo;预测过程&amp;rdquo;，通常称为解码过程。对应了序列建模问题3.。对于序列标注问题，我们只需要学习出一个HMM模型即可，后面所有的新的sample我都用这一个HMM去apply。&lt;/p&gt;
&lt;p&gt;我们的目的是，在学习后已知了 &lt;span class="math"&gt;\(P(Q,O)\)&lt;/span&gt; ,现在要求出 &lt;span class="math"&gt;\(P(Q|O)\)&lt;/span&gt; ，进一步&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Q_{max} = argmax_{allQ}\frac{P(Q,O)}{P(O)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再直白点就是，我现在要在给定的观测序列下找出一条隐状态序列，条件是这个隐状态序列的概率是最大的那个。&lt;/p&gt;
&lt;p&gt;具体地，都是用Viterbi算法解码，是用DP思想减少重复的计算。Viterbi也是满大街的，不过要说的是，Viterbi不是HMM的专属，也不是任何模型的专属，他只是恰好被满足了被HMM用来使用的条件。谁知，现在大家都把Viterbi跟HMM捆绑在一起了, shame。&lt;/p&gt;
&lt;p&gt;Viterbi计算有向无环图的一条最大路径，应该还好理解。如图：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/10.jpg" width="418"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;关键是注意，每次工作热点区只涉及到t 与 t-1,这对应了DP的无后效性的条件。如果对某些同学还是很难理解，请参考&lt;a href="https://www.zhihu.com/question/20136144"&gt;这个答案&lt;/a&gt;下@Kiwee的回答吧。&lt;/p&gt;
&lt;h3 id="3.2.3 xu lie gai lu guo cheng"&gt;&lt;strong&gt;3.2.3 序列概率过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;我通过HMM计算出序列的概率又有什么用？针对这个点我把这个问题详细说一下。&lt;/p&gt;
&lt;p&gt;实际上，序列概率过程对应了序列建模问题2.，即序列分类。&lt;br/&gt;
在3.2.2第一句话我说，在序列标注问题中，我用一批完整的数据训练出了一支HMM模型即可。好，那在序列分类问题就不是训练一个HMM模型了。我应该这么做（结合语音分类识别例子）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目标：识别声音是A发出的还是B发出的。&lt;br/&gt;
HMM建模过程：&lt;br/&gt;
1. 训练：我将所有A说的语音数据作为dataset_A,将所有B说的语音数据作为dataset_B（当然，先要分别对dataset A ,B做预处理encode为元数据节点，形成sequences）,然后分别用dataset_A、dataset_B去训练出HMM_A/HMM_B&lt;br/&gt;
2. inference：来了一条新的sample（sequence），我不知道是A的还是B的，没问题，分别用HMM_A/HMM_B计算一遍序列的概率得到 &lt;span class="math"&gt;\(P_{A}(S)、P_{B}(S)\)&lt;/span&gt; ，比较两者大小，哪个概率大说明哪个更合理，更大概率作为目标类别。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以，本小节的理解重点在于，&lt;strong&gt;如何对一条序列计算其整体的概率&lt;/strong&gt;。即目标是计算出 &lt;span class="math"&gt;\(P(O|&amp;lambda;)\)&lt;/span&gt; 。这个问题前辈们在他们的经典中说的非常好了，比如参考李航老师整理的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接计算法（穷举搜索）&lt;/li&gt;
&lt;li&gt;前向算法&lt;/li&gt;
&lt;li&gt;后向算法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后面两个算法采用了DP思想，减少计算量，即每一次直接引用前一个时刻的计算结果以避免重复计算，跟Viterbi一样的技巧。&lt;/p&gt;
&lt;p&gt;还是那句，因为这篇文档不是专门讲算法细节的，所以不详细展开这些。毕竟，所有的科普HMM、CRF的博客貌似都是在扯这些算法，妥妥的街货，就不搬运了。&lt;/p&gt;
&lt;h2 id="si , memm_1"&gt;&lt;strong&gt;四、MEMM&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;MEMM，即最大熵马尔科夫模型，这个是在接触了HMM、CRF之后才知道的一个模型。说到MEMM这一节时，得转换思维了，因为现在这MEMM属于判别式模型。&lt;/p&gt;
&lt;p&gt;不过有一点很尴尬，MEMM貌似被使用或者讲解引用的不及HMM、CRF。&lt;/p&gt;
&lt;h3 id="4.1 li jie memm"&gt;&lt;strong&gt;4.1 理解MEMM&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;这里还是啰嗦强调一下，MEMM正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，比如说序列求概率（分类）问题，我直接考虑找出函数分类边界。这一点跟HMM的思维方式发生了很大的变化，如果不对这一点有意识，那么很难理解为什么MEMM、CRF要这么做。&lt;/p&gt;
&lt;p&gt;HMM中，观测节点 &lt;span class="math"&gt;\(o_{i}\)&lt;/span&gt; 依赖隐藏状态节点 &lt;span class="math"&gt;\(i_{i}\)&lt;/span&gt; ,也就意味着我的观测节点只依赖当前时刻的隐藏状态。但在更多的实际场景下，观测序列是需要很多的特征来刻画的，比如说，我在做NER时，我的标注 &lt;span class="math"&gt;\(i_{i}\)&lt;/span&gt; 不仅跟当前状态 &lt;span class="math"&gt;\(o_{i}\)&lt;/span&gt; 相关，而且还跟前后标注 &lt;span class="math"&gt;\(o_{j}(j \neq i)\)&lt;/span&gt; 相关，比如字母大小写、词性等等。&lt;/p&gt;
&lt;p&gt;为此，提出来的MEMM模型就是能够直接允许&lt;strong&gt;&amp;ldquo;定义特征&amp;rdquo;&lt;/strong&gt;，直接学习条件概率，即 &lt;span class="math"&gt;\(P(i_{i}|i_{i-1},o_{i}) (i = 1,\cdots,n)\)&lt;/span&gt; , 总体为：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(I|O) = \prod_{t=1}^{n}P(i_{i}|i_{i-1},o_{i}), i = 1,\cdots,n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;并且， &lt;span class="math"&gt;\(P(i|i^{'},o)\)&lt;/span&gt; 这个概率通过最大熵分类器建模（取名MEMM的原因）:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(i|i^{'},o) = \frac{1}{Z(o,i^{'})} exp(\sum_{a})\lambda_{a}f_{a}(o,i)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;重点来了，这是ME的内容，也是理解MEMM的关键： &lt;span class="math"&gt;\(Z(o,i^{'})\)&lt;/span&gt; 这部分是归一化； &lt;span class="math"&gt;\(f_{a}(o,i)\)&lt;/span&gt; 是&lt;strong&gt;特征函数&lt;/strong&gt;，具体点，这个函数是需要去定义的; &lt;span class="math"&gt;\(&amp;lambda;\)&lt;/span&gt; 是特征函数的权重，这是个未知参数，需要从训练阶段学习而得。&lt;/p&gt;
&lt;p&gt;比如我可以这么定义特征函数：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation} f_{a}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定条件}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;/div&gt;
&lt;p&gt;其中，特征函数 &lt;span class="math"&gt;\(f_{a}(o,i)\)&lt;/span&gt; 个数可任意制定， &lt;span class="math"&gt;\((a = 1, \cdots, n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以总体上，MEMM的建模公式这样：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(I|O) = \prod_{t=1}^{n}\frac{ exp(\sum_{a})\lambda_{a}f_{a}(o,i) }{Z(o,i_{i-1})} , i = 1,\cdots,n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;是的，公式这部分之所以长成这样，是由ME模型决定的。&lt;/p&gt;
&lt;p&gt;请务必注意，理解&lt;strong&gt;判别模型&lt;/strong&gt;和&lt;strong&gt;定义特征&lt;/strong&gt;两部分含义，这已经涉及到CRF的雏形了。&lt;/p&gt;
&lt;p&gt;所以说，他是判别式模型，直接对条件概率建模。 上图：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/11.jpg" width="415"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;MEMM需要两点注意：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;与HMM的 &lt;span class="math"&gt;\(o_{i}\)&lt;/span&gt; 依赖 &lt;span class="math"&gt;\(i_{i}\)&lt;/span&gt; 不一样，MEMM当前隐藏状态 &lt;span class="math"&gt;\(i_{i}\)&lt;/span&gt; 应该是依赖当前时刻的观测节点 &lt;span class="math"&gt;\(o_{i}\)&lt;/span&gt; 和上一时刻的隐藏节点 &lt;span class="math"&gt;\(i_{i-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;需要注意，之所以图的箭头这么画，是由MEMM的公式决定的，而公式是creator定义出来的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;好了，走一遍完整流程。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;step1. 先预定义特征函数 &lt;span class="math"&gt;\(f_{a}(o,i)\)&lt;/span&gt; ，&lt;br/&gt;
step2. 在给定的数据上，训练模型，确定参数，即确定了MEMM模型&lt;br/&gt;
step3. 用确定的模型做序列标注问题或者序列求概率问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="4.2 mo xing yun xing guo cheng"&gt;&lt;strong&gt;4.2 模型运行过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;MEMM模型的工作流程也包括了学习训练问题、序列标注问题、序列求概率问题。&lt;/p&gt;
&lt;h3 id="4.2.1 xue xi xun lian guo cheng"&gt;&lt;strong&gt;4.2.1 学习训练过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;一套MEMM由一套参数唯一确定，同样地，我需要通过训练数据学习这些参数。MEMM模型很自然需要学习里面的特征权重&amp;lambda;。&lt;/p&gt;
&lt;p&gt;不过跟HMM不用的是，因为HMM是生成式模型，参数即为各种概率分布元参数，数据量足够可以用最大似然估计。而判别式模型是用函数直接判别，学习边界，MEMM即通过特征函数来界定。但同样，MEMM也有极大似然估计方法、梯度下降、牛顿迭代发、拟牛顿下降、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。&lt;/p&gt;
&lt;p&gt;嗯，具体详细求解过程貌似问题不大。&lt;/p&gt;
&lt;h3 id="4.2.2 xu lie biao zhu guo cheng"&gt;&lt;strong&gt;4.2.2 序列标注过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;还是跟HMM一样的，用学习好的MEMM模型，在新的sample（观测序列 &lt;span class="math"&gt;\(o_{1}, \cdots, o_{i}\)&lt;/span&gt; ）上找出一条概率最大最可能的隐状态序列 &lt;span class="math"&gt;\(i_{1}, \cdots, i_{i}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。&lt;/p&gt;
&lt;h3 id="4.2.3 xu lie qiu gai lu guo cheng"&gt;&lt;strong&gt;4.2.3 序列求概率过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的MEMM，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。&lt;/p&gt;
&lt;p&gt;应该可以不用展开，吧&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;h3 id="4.3 biao zhu pian zhi ?"&gt;&lt;strong&gt;4.3 标注偏置？&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;MEMM讨论的最多的是他的labeling bias 问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 现象&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;是从街货上烤过来的&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/12.jpg" width="558"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;用Viterbi算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2。 解码过程细节（需要会viterbi算法这个前提）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;P(1-&amp;gt; 1-&amp;gt; 1-&amp;gt; 1)= 0.4 x 0.45 x 0.5 = 0.09 ，&lt;br/&gt;
P(2-&amp;gt;2-&amp;gt;2-&amp;gt;2)= 0.2 X 0.3 X 0.3 = 0.018，&lt;br/&gt;
P(1-&amp;gt;2-&amp;gt;1-&amp;gt;2)= 0.6 X 0.2 X 0.5 = 0.06，&lt;br/&gt;
P(1-&amp;gt;1-&amp;gt;2-&amp;gt;2)= 0.4 X 0.55 X 0.3 = 0.066&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但是得到的最优的状态转换路径是1-&amp;gt;1-&amp;gt;1-&amp;gt;1，为什么呢？因为状态2可以转换的状态比状态1要多，从而使转移概率降低,即MEMM倾向于选择拥有更少转移的状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 解释原因&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;直接看MEMM公式：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(I|O) = \prod_{t=1}^{n}\frac{ exp[(\sum_{a})\lambda_{a}f_{a}(o,i)] }{Z(o,i_{i-1})} , i = 1,\cdots,n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(&amp;sum;\)&lt;/span&gt; 求和的作用在概率中是归一化，但是这里归一化放在了指数内部，管这叫local归一化。 来了，viterbi求解过程，是用dp的状态转移公式（MEMM的没展开，请参考CRF下面的公式），因为是局部归一化，所以MEMM的viterbi的转移公式的第二部分出现了问题，导致dp无法正确的递归到全局的最优。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\delta_{i+1} = max_{1 \le j \le m}\lbrace \delta_{i}(I) + \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i) \rbrace\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="wu , crf_1"&gt;&lt;strong&gt;五、CRF&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;我觉得一旦有了一个清晰的工作流程，那么按部就班地，没有什么很难理解的地方，因为整体框架已经胸有成竹了，剩下了也只有添砖加瓦小修小补了。有了上面的过程基础，CRF也是类似的，只是有方法论上的细微区别。&lt;/p&gt;
&lt;h3 id="5.1 li jie crf"&gt;&lt;strong&gt;5.1 理解CRF&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;请看第一张概率图模型构架图，CRF上面是马尔科夫随机场（马尔科夫网络），而条件随机场是在给定的随机变量 &lt;span class="math"&gt;\(X\)&lt;/span&gt; （具体，对应观测序列 &lt;span class="math"&gt;\(o_{1}, \cdots, o_{i}\)&lt;/span&gt; ）条件下，随机变量 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; （具体，对应隐状态序列 &lt;span class="math"&gt;\(i_{1}, \cdots, i_{i}\)&lt;/span&gt; ）的马尔科夫随机场。&lt;br/&gt;
广义的CRF的定义是： 满足 &lt;span class="math"&gt;\(P(Y_{v}|X,Y_{w},w \neq v) = P(Y_{v}|X,Y_{w},w \sim v)\)&lt;/span&gt; 的马尔科夫随机场叫做条件随机场（CRF）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen注:&lt;br/&gt;
&lt;span class="math"&gt;\(Y_{w},w \neq v\)&lt;/span&gt; 表示除&lt;span class="math"&gt;\(v\)&lt;/span&gt;以外观测集中的所有节点，&lt;br/&gt;
&lt;span class="math"&gt;\(Y_{w},w \sim v\)&lt;/span&gt; 表示观测集中&lt;span class="math"&gt;\(v\)&lt;/span&gt;的邻接节点，  &lt;/p&gt;
&lt;p&gt;下面是另一种表达方式：&lt;br/&gt;
&lt;span class="math"&gt;\(P(Y_v|X,Y_{V\backslash\{v\}}) = P(Y_v|X,Y_{n(v)}) \\\)&lt;/span&gt; 
其中：&lt;br/&gt;
&lt;span class="math"&gt;\(Y_{V\backslash\{v\}}\)&lt;/span&gt; 表示除&lt;span class="math"&gt;\(v\)&lt;/span&gt;以外的&lt;span class="math"&gt;\(V\)&lt;/span&gt;中所有节点， &lt;br/&gt;
&lt;span class="math"&gt;\(Y_{n(v)}\)&lt;/span&gt; 表示结点&lt;span class="math"&gt;\(v\)&lt;/span&gt;的邻接节点  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;不过一般说CRF为序列建模，就专指CRF线性链（linear chain CRF）：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/13.jpg" width="415"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;在2.1.2中有提到过，概率无向图的联合概率分布可以在因子分解下表示为：&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y | X)=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c}|X ) = \frac{1}{Z(x)} \prod_{c} e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)} = \frac{1}{Z(x)} e^{\sum_{c}\sum_{k}\lambda_{k}f_{k}(y_{i},y_{i-1},x,i)}$$&lt;/div&gt;
&lt;p&gt;而在线性链CRF示意图中，每一个（ &lt;span class="math"&gt;\(I_{i} \sim O_{i}\)&lt;/span&gt; ）对为一个最大团,即在上式中 &lt;span class="math"&gt;\(c = i\)&lt;/span&gt; 。并且线性链CRF满足 &lt;span class="math"&gt;\(P(I_{i}|O,I_{1},\cdots, I_{n}) = P(I_{i}|O,I_{i-1},I_{i+1})\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;所以CRF的建模公式如下：&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$P(I | O)=\frac{1}{Z(O)} \prod_{i}\psi_{i}(I_{i}|O ) = \frac{1}{Z(O)} \prod_{i} e^{\sum_{k}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)} = \frac{1}{Z(O)} e^{\sum_{i}\sum_{k}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;/div&gt;
&lt;p&gt;我要敲黑板了，这个公式是非常非常关键的，注意递推过程啊，我是怎么从 &lt;span class="math"&gt;\(&amp;prod;\)&lt;/span&gt; 跳到 &lt;span class="math"&gt;\(e^{\sum}\)&lt;/span&gt; 的。&lt;/p&gt;
&lt;p&gt;不过还是要多啰嗦一句，想要理解CRF，必须判别式模型的概念要深入你心。
正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，因
为我创造出来就是为了这个分边界的目的的。比如说序列求概率（分类）问
题，我直接考虑找出函数分类边界。所以才为什么会有这个公式。所以再看
到这个公式也别懵逼了，he was born for discriminating the given data
from different classes. 就这样。不过待会还会具体介绍特征函数部分的东西。&lt;/p&gt;
&lt;p&gt;除了建模总公式，关键的CRF重点概念在MEMM中已强调过：&lt;strong&gt;判别式模型&lt;/strong&gt;、&lt;strong&gt;特征函数&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 特征函数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上面给出了CRF的建模公式：&lt;/p&gt;
&lt;div class="math"&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;下标 &lt;em&gt;i&lt;/em&gt; 表示我当前所在的节点（token）位置。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;下标 &lt;em&gt;k&lt;/em&gt; 表示我这是第几个特征函数，并且每个特征函数都附属一个权重 &lt;span class="math"&gt;\(\lambda_{k}\)&lt;/span&gt; ，也就是这么回事，每个团里面，我将为 &lt;span class="math"&gt;\(token_{i}\)&lt;/span&gt; 构造M个特征，每个特征执行一定的限定作用，然后建模时我再为每个特征函数加权求和。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Z(O)\)&lt;/span&gt; 是用来归一化的，为什么？想想LR以及softmax为何有归一化呢，一样的嘛，形成概率值。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;再来个重要的理解。 &lt;span class="math"&gt;\(P(I|O)\)&lt;/span&gt; 这个表示什么？具体地，表示了在给定的一条观测序列 &lt;span class="math"&gt;\(O=(o_{1},\cdots, o_{i})\)&lt;/span&gt; 条件下，我用CRF所求出来的隐状态序列 &lt;span class="math"&gt;\(I=(i_{1},\cdots, i_{i})\)&lt;/span&gt; 的概率，注意，这里的 &lt;span class="math"&gt;\(I\)&lt;/span&gt; 是一条序列，有多个元素（一组随机变量），而至于观测序列 &lt;span class="math"&gt;\(O=(o_{1},\cdots, o_{i})\)&lt;/span&gt; ，它可以是一整个训练语料的所有的观测序列；也可以是在inference阶段的一句sample，比如说对于序列标注问题，我对一条sample进行预测，可能能得到 &lt;span class="math"&gt;\(P_{j}(I | O)（j=1,&amp;hellip;,J)\)&lt;/span&gt;,  &lt;span class="math"&gt;\(J\)&lt;/span&gt;条隐状态&lt;span class="math"&gt;\(I\)&lt;/span&gt;，但我肯定最终选的是最优概率的那条（by viterbi）。这一点希望你能理解。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于CRF，可以为他定义两款特征函数：转移特征&amp;amp;状态特征。 我们将建模总公式展开：&lt;/p&gt;
&lt;div class="math"&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}=\frac{1}{Z(O)} e^{ [ \sum_{i}^{T}\sum_{j}^{J}\lambda_{j}t_{j}(O,I_{i-1},I_{i},i) + \sum_{i}^{T}\sum_{l}^{L}\mu_{l}s_{l}(O,I_{i},i) ] }$$&lt;/div&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(t_{j}\)&lt;/span&gt; 为i处的转移特征，对应权重 &lt;span class="math"&gt;\(\lambda_{j}\)&lt;/span&gt; ,每个 &lt;span class="math"&gt;\(token_{i}\)&lt;/span&gt; 都有J个特征,转移特征针对的是前后token之间的限定。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\begin{equation} t_{k=1}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定转移条件，比如前一个token是&amp;lsquo;I&amp;rsquo;}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(s_l\)&lt;/span&gt;为i 处的状态特征，对应权重&lt;span class="math"&gt;\(&amp;mu;_l\)&lt;/span&gt;，每个&lt;span class="math"&gt;\(token_i\)&lt;/span&gt;都有L个特征  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\begin{equation} s_{l=1}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定状态条件，比如当前token的POS是&amp;lsquo;V&amp;rsquo;}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不过一般情况下，我们不把两种特征区别的那么开，合在一起：&lt;/p&gt;
&lt;div class="math"&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;/div&gt;
&lt;p&gt;满足特征条件就取值为1，否则没贡献，甚至你还可以让他打负分，充分惩罚。&lt;/p&gt;
&lt;p&gt;再进一步理解的话，我们需要把特征函数部分抠出来：&lt;/p&gt;
&lt;div class="math"&gt;$$Score = \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)$$&lt;/div&gt;
&lt;p&gt;是的，我们为 &lt;span class="math"&gt;\(token_{i}\)&lt;/span&gt; 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值&amp;hellip;&amp;hellip;完了又扯到了log线性模型。现在稍作解释：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;log-linear models take the following form:&lt;br/&gt;
&lt;span class="math"&gt;\(P(y|x;\omega) = \frac{ exp(\omega&amp;middot;\phi(x,y)) }{ \sum_{y^{'}\in Y }exp(\omega&amp;middot;\phi(x,y^{&amp;lsquo;})) }\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我觉得对LR或者sotfmax熟悉的对这个应该秒懂。然后CRF完美地满足这个形式，所以又可以归入到了log-linear models之中。&lt;/p&gt;
&lt;h3 id="5.2 mo xing yun xing guo cheng"&gt;&lt;strong&gt;5.2 模型运行过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;模型的工作流程，跟MEMM是一样的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step1. 先预定义特征函数 &lt;span class="math"&gt;\(f_{a}(o,i)\)&lt;/span&gt; ，&lt;/li&gt;
&lt;li&gt;step2. 在给定的数据上，训练模型，确定参数 &lt;span class="math"&gt;\(\lambda_{k}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;step3. 用确定的模型做&lt;code&gt;序列标注问题&lt;/code&gt;或者&lt;code&gt;序列求概率问题&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可能还是没做到100%懂，结合例子说明：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="5.2.1 xue xi xun lian guo cheng"&gt;&lt;strong&gt;5.2.1 学习训练过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;一套CRF由一套参数&amp;lambda;唯一确定（先定义好各种特征函数）。&lt;/p&gt;
&lt;p&gt;同样，CRF用极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。其实能用在log-linear models上的求参方法都可以用过来。&lt;/p&gt;
&lt;p&gt;嗯，具体详细求解过程貌似问题不大。&lt;/p&gt;
&lt;h3 id="5.2.2 xu lie biao zhu guo cheng"&gt;&lt;strong&gt;5.2.2 序列标注过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;还是跟HMM一样的，用学习好的CRF模型，在新的sample（观测序列 &lt;span class="math"&gt;\(o_{1}, \cdots, o_{i}\)&lt;/span&gt; ）上找出一条概率最大最可能的隐状态序列 &lt;span class="math"&gt;\(i_{1}, \cdots, i_{i}\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。&lt;/p&gt;
&lt;p&gt;啰嗦一下，我们就定义i处的局部状态为 &lt;span class="math"&gt;\(\delta_{i}(I)\)&lt;/span&gt; ,表示在位置i处的隐状态的各种取值可能为 &lt;em&gt;I&lt;/em&gt; ，然后递推位置i+1处的隐状态，写出来的DP转移公式为：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\delta_{i+1} = max_{1 \le j \le m}\lbrace \delta_{i}(I) + \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i) \rbrace\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里没写规范因子 &lt;span class="math"&gt;\(Z(O)\)&lt;/span&gt; 是因为不规范化不会影响取最大值后的比较。&lt;/p&gt;
&lt;p&gt;具体还是不展开为好。&lt;/p&gt;
&lt;h3 id="5.2.3 xu lie qiu gai lu guo cheng"&gt;&lt;strong&gt;5.2.3 序列求概率过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的CRF，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。只是貌似很少看到拿CRF或者MEMM来做分类的，直接用网络模型不就完了不&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;应该可以不用展开，吧&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;h3 id="5.3 crf++fen xi"&gt;&lt;strong&gt;5.3 CRF++分析&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;本来做task用CRF++跑过baseline,后来在对CRF做调研时，非常想透析CRF++的工作原理，以identify以及verify做的各种假设猜想。当然，也看过其他的CRF实现源码。&lt;/p&gt;
&lt;p&gt;所以干脆写到这里来，结合CRF++实例讲解过程。&lt;/p&gt;
&lt;p&gt;有一批语料数据，并且已经tokenized好了：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nuclear&lt;br/&gt;
theory&lt;br/&gt;
devoted&lt;br/&gt;
major&lt;br/&gt;
efforts&lt;br/&gt;
&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;并且我先确定了13个标注元素：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;B_MAT&lt;br/&gt;
B_PRO&lt;br/&gt;
B_TAS&lt;br/&gt;
E_MAT&lt;br/&gt;
E_PRO&lt;br/&gt;
E_TAS&lt;br/&gt;
I_MAT&lt;br/&gt;
I_PRO&lt;br/&gt;
I_TAS&lt;br/&gt;
O&lt;br/&gt;
S_MAT&lt;br/&gt;
S_PRO&lt;br/&gt;
S_TAS&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;1. 定义模板&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按道理应该是定义特征函数才对吧？好的，在CRF++下，应该是先定义特征模板，然后用模板自动批量产生大量的特征函数。我之前也蛮confused的，用完CRF++还以为模板就是特征，后面就搞清楚了：每一条模板将在每一个token处生产若干个特征函数。&lt;/p&gt;
&lt;p&gt;CRF++的模板（template）有U系列（unigram）、B系列(bigram)，不过我至今搞不清楚B系列的作用，因为U模板都可以完成2-gram的作用。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;U00:%x[-2,0]&lt;br/&gt;
U01:%x[-1,0]&lt;br/&gt;
U02:%x[0,0]&lt;br/&gt;
U03:%x[1,0]&lt;br/&gt;
U04:%x[2,0]  &lt;/p&gt;
&lt;p&gt;U05:%x[-2,0]/%x[-1,0]/%x[0,0]&lt;br/&gt;
U06:%x[-1,0]/%x[0,0]/%x[1,0]&lt;br/&gt;
U07:%x[0,0]/%x[1,0]/%x[2,0]&lt;br/&gt;
U08:%x[-1,0]/%x[0,0]&lt;br/&gt;
U09:%x[0,0]/%x[1,0]  &lt;/p&gt;
&lt;p&gt;B&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以，U00 - U09 我定义了10个模板。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 产生特征函数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;是的，会产生大量的特征。 U00 - U04的模板产生的是状态特征函数；U05 - U09的模板产生的是转移特征函数。&lt;/p&gt;
&lt;p&gt;在CRF++中，每个特征都会try每个标注label（这里有13个），总共将生成 &lt;span class="math"&gt;\(N * L = i * k^{'} * L\)&lt;/span&gt; 个特征函数以及对应的权重出来。N表示每一套特征函数 &lt;span class="math"&gt;\(N= i * k^{'}\)&lt;/span&gt; ，L表示标注集元素个数。&lt;/p&gt;
&lt;p&gt;比如训练好的CRF模型的部分特征函数是这样存储的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;22607 B&lt;br/&gt;
790309 U00:%&lt;br/&gt;
3453892 U00:%)&lt;br/&gt;
2717325 U00:&amp;amp;&lt;br/&gt;
2128269 U00:'t&lt;br/&gt;
2826239 U00:(0.3534&lt;br/&gt;
2525055 U00:(0.593&amp;ndash;1.118&lt;br/&gt;
197093 U00:(1)&lt;br/&gt;
2079519 U00:(1)L=14w2&amp;minus;12w&amp;minus;F&amp;mu;&amp;nu;aFa&amp;mu;&amp;nu;&lt;br/&gt;
2458547 U00:(1)&amp;delta;n=&amp;int;&amp;minus;&amp;infin;En+1&amp;rho;&amp;tilde;(E)dE&amp;minus;n&lt;br/&gt;
1766024 U00:(1.0g&lt;br/&gt;
2679261 U00:(1.1wt%)&lt;br/&gt;
1622517 U00:(100)&lt;br/&gt;
727701 U00:(1000&amp;ndash;5000A)&lt;br/&gt;
2626520 U00:(10a)&lt;br/&gt;
2626689 U00:(10b)&lt;br/&gt;
&amp;hellip;&amp;hellip;&lt;br/&gt;
2842814 U07:layer/thicknesses/Using&lt;br/&gt;
2847533 U07:layer/thicknesses/are&lt;br/&gt;
2848651 U07:layer/thicknesses/in&lt;br/&gt;
331539 U07:layer/to/the&lt;br/&gt;
1885871 U07:layer/was/deposited&lt;br/&gt;
&amp;hellip;&amp;hellip;（数量非常庞大）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其实也就是对应了这样些个特征函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;func1 = if (output = B and feature="U02:一") return 1 else return 0&lt;br/&gt;
func2 = if (output = M and feature="U02:一") return 1 else return 0&lt;br/&gt;
func3 = if (output = E and feature="U02:一") return 1 else return 0&lt;br/&gt;
func4 = if (output = S and feature="U02:一") return 1 else return 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;比如模板U06会从语料中one by one逐句抽出这些各个特征：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一/个/人/&amp;hellip;&amp;hellip;&lt;br/&gt;
个/人/走/&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;3. 求参&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对上述的各个特征以及初始权重进行迭代参数学习。&lt;/p&gt;
&lt;p&gt;在CRF++ 训练好的模型里，权重是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;0.3972716048310705&lt;br/&gt;
0.5078838237171732&lt;br/&gt;
0.6715316559507898&lt;br/&gt;
-0.4198827647512405&lt;br/&gt;
-0.4233310655891150&lt;br/&gt;
-0.4176580083832543&lt;br/&gt;
-0.4860489836004728&lt;br/&gt;
-0.6156475863742051&lt;br/&gt;
-0.6997919485753300&lt;br/&gt;
0.8309956709647820&lt;br/&gt;
0.3749695682658566&lt;br/&gt;
0.2627347894057647&lt;br/&gt;
0.0169732441379157&lt;br/&gt;
0.3972716048310705&lt;br/&gt;
0.5078838237171732&lt;br/&gt;
0.6715316559507898&lt;br/&gt;
&amp;hellip;&amp;hellip;（数量非常庞大，与每个label的特征函数对应，我这有300W个）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;4. 预测解码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;结果是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nuclear B_TAS&lt;br/&gt;
theory E_TAS&lt;br/&gt;
devoted O&lt;br/&gt;
major O&lt;br/&gt;
efforts O&lt;br/&gt;
&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="5.4 lstm+crf"&gt;&lt;strong&gt;5.4 LSTM+CRF&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;LSTM+CRF这个组合其实我在知乎上答过问题，然后顺便可以整合到这里来。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、perspectively&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大家都知道，LSTM已经可以胜任序列标注问题了，为每个token预测一个label（LSTM后面接:分类器）；而CRF也是一样的，为每个token预测一个label。&lt;/p&gt;
&lt;p&gt;但是，他们的预测机理是不同的。CRF是全局范围内统计归一化的条件状态转移概率矩阵，再预测出一条指定的sample的每个token的label；LSTM（RNNs，不区分here）是依靠神经网络的超强非线性拟合能力，在训练时将samples通过复杂到让你窒息的高阶高纬度异度空间的非线性变换，学习出一个模型，然后再预测出一条指定的sample的每个token的label。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2、LSTM+CRF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;既然LSTM都OK了，为啥researchers搞一个LSTM+CRF的hybrid model?&lt;/p&gt;
&lt;p&gt;哈哈，因为a single LSTM预测出来的标注有问题啊！举个segmentation例子(BES; char level)，plain LSTM 会搞出这样的结果：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;input&lt;/strong&gt;: "学习出一个模型，然后再预测出一条指定"&lt;br/&gt;
&lt;strong&gt;expected output&lt;/strong&gt;: 学/B 习/E 出/S 一/B 个/E 模/B 型/E ，/S 然/B 后/E 再/E 预/B 测/E &amp;hellip;&amp;hellip;&lt;br/&gt;
&lt;strong&gt;real output&lt;/strong&gt;: 学/B 习/E 出/S 一/B 个/B 模/B 型/E ，/S 然/B 后/B 再/E 预/B 测/E &amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看到不，用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。当然了，CRF还能学到更多的限定特征，那越多越好啊！&lt;/p&gt;
&lt;p&gt;好了，那就把CRF接到LSTM上面，把LSTM在time_step上把每一个hidden_state的tensor输入给CRF，让LSTM负责在CRF的特征限定下，依照新的loss function，学习出一套新的非线性变换空间。&lt;/p&gt;
&lt;p&gt;最后，不用说，结果还真是好多了呢。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//github.com/scofield7419/sequence-labeling-BiLSTM-CRF"&gt;LSTM+CRF codes&lt;/a&gt;, here. Go just take it.&lt;/p&gt;
&lt;h2 id="liu , zong jie_1"&gt;&lt;strong&gt;六、总结&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id="1. zong ti dui bi"&gt;&lt;strong&gt;1. 总体对比&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;应该看到了熟悉的图了，现在看这个图的话，应该可以很清楚地get到他所表达的含义了。这张图的内容正是按照生成式&amp;amp;判别式来区分的，NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/14.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;h3 id="2. hmm vs. memm vs. crf"&gt;&lt;strong&gt;2. HMM vs. MEMM vs. CRF&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;将三者放在一块做一个总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;HMM -&amp;gt; MEMM： HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MEMM -&amp;gt; CRF:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;HMM、MEMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去（这点问题应该只有HMM）。
CRF属于无向图，没有这种依赖性，克服此问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="3. machine learning models vs. sequential models"&gt;&lt;strong&gt;3. Machine Learning models vs. Sequential models&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;为了一次将概率图模型理解的深刻到位，我们需要再串一串，更深度与原有的知识体系融合起来。&lt;/p&gt;
&lt;p&gt;机器学习模型，按照学习的范式或方法，以及加上自己的理解，给常见的部分的他们整理分了分类（主流上，都喜欢从训练样本的歧义型分，当然也可以从其他角度来）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;一、监督：{

1.1 分类算法(线性和非线性)：{

    感知机

    KNN

    概率{
        朴素贝叶斯（NB）
        Logistic Regression（LR）
        最大熵MEM（与LR同属于对数线性分类模型）
    }

    支持向量机(SVM)

    决策树(ID3、CART、C4.5)

    assembly learning{
        Boosting{
            Gradient Boosting{
                GBDT
                xgboost（传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）；xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。）
            }
            AdaBoost
        }   
        Bagging{
            随机森林
        }
        Stacking
    }

    &amp;hellip;&amp;hellip;
}

1.2 概率图模型：{
    HMM
    MEMM（最大熵马尔科夫）
    CRF
    &amp;hellip;&amp;hellip;
}

1.3 回归预测：{
    线性回归
    树回归
    Ridge岭回归
    Lasso回归
    &amp;hellip;&amp;hellip;
}

&amp;hellip;&amp;hellip;  
}

二、非监督：{
2.1 聚类：{
    1. 基础聚类
        K&amp;mdash;mean
        二分k-mean
        K中值聚类
        GMM聚类
    2. 层次聚类
    3. 密度聚类
    4. 谱聚类()
}

2.2 主题模型:{
    pLSA
    LDA隐含狄利克雷分析
}

2.3 关联分析：{
    Apriori算法
    FP-growth算法
}

2.4 降维：{
    PCA算法
    SVD算法
    LDA线性判别分析
    LLE局部线性嵌入
}

2.5 异常检测：
&amp;hellip;&amp;hellip;
}

三、半监督学习

四、迁移学习
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;（注意到，没有把神经网络体系加进来。因为NNs的范式很灵活，不太适用这套分法，largely, off this framework）&lt;/p&gt;
&lt;p&gt;Generally speaking，机器学习模型，尤其是有监督学习，一般是为一条sample预测出一个label，作为预测结果。 但与典型常见的机器学习模型不太一样，序列模型（概率图模型）是试图为一条sample里面的每个基本元数据分别预测出一个label。这一点，往往是beginner伊始难以理解的。&lt;/p&gt;
&lt;p&gt;具体的实现手段差异，就是：ML models通过直接预测得出label；Sequential models是给每个token预测得出label还没完，还得将他们每个token对应的labels进行组合，具体的话，用viterbi来挑选最好的那个组合。&lt;/p&gt;
&lt;h2 id="over_1"&gt;&lt;strong&gt;over&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;有了这道开胃菜，接下来，读者可以完成这些事情：完善细节算法、阅读原著相关论文达到彻底理解、理解相关拓展概念、理论创新&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;hope those hlpe!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欢迎留言！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有错误之处请多多指正，谢谢！&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="referrences:"&gt;&lt;strong&gt;Referrences:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;《统计学习方法》，李航&lt;/p&gt;
&lt;p&gt;《统计自然语言处理》，宗成庆&lt;/p&gt;
&lt;p&gt;《 An Introduction to Conditional Random Fields for Relational Learning》， Charles Sutton， Andrew McCallum&lt;/p&gt;
&lt;p&gt;《Log-Linear Models, MEMMs, and CRFs》，ichael Collins&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/question/35866596"&gt;如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/en-heng/p/6201893.html"&gt;【中文分词】最大熵马尔可夫模型MEMM - Treant - 博客园&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//github.com/timvieira/crf"&gt;timvieira/crf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//github.com/shawntan/python-crf"&gt;shawntan/python-crf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=http%3A//videolectures.net/cikm08_elkan_llmacrf/"&gt;Log-linear Models and Conditional Random Fields&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/55755fc649b1"&gt;如何轻松愉快地理解条件随机场（CRF）？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/7068574.html"&gt;条件随机场CRF(三) 模型学习与维特比算法解码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/question/20279019"&gt;crf++里的特征模板得怎么理解？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=http%3A//www.hankcs.com/ml/crf-code-analysis.html"&gt;CRF++代码分析-码农场&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=http%3A//blog.csdn.net/aws3217150/article/details/69212445"&gt;CRF++源码解读 - CSDN博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=http%3A//www.hankcs.com/nlp/the-crf-model-format-description.html"&gt;CRF++模型格式说明-码农场&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/syx-1987/p/4077325.html"&gt;标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较&amp;lt;转&amp;gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;编辑于 2018-03-21&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Y_{w},w \neq v\)&lt;/span&gt; 表示除&lt;span class="math"&gt;\(v\)&lt;/span&gt;以外观测集中的所有&lt;/p&gt;
&lt;/blockquote&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Estimator 编程指南</title><link href="https://freeopen.github.io/posts/estimator-bian-cheng-zhi-nan" rel="alternate"></link><published>2018-03-04T00:00:00+08:00</published><updated>2018-03-04T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2018-03-04:/posts/estimator-bian-cheng-zhi-nan</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;2018-04-12 第一次修订, 新增"多GPU下的写法"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：代码适用于 TF1.4 ~ TF1.7 。&lt;/p&gt;
&lt;p&gt;为什么要使用Estimator, 仅&lt;a href="https://www.tensorflow.org/programmers_guide/estimators"&gt;官方文档&lt;/a&gt;里提到的第一条优点就让我不得不重视它。
大意是不管你在本地环境还是分布式环境，不管你用一个或多个CPU、GPU还是TPU训练模型，你的模型代码不需要做任何改变。&lt;/p&gt;
&lt;p&gt;但看了一些Estimator教程，不怎么满意。因为大部分介绍的方法过于简单，仅适用于实验环境。当你面对大数据、复杂模型和机能限制时，发现那些方法就不灵了。
所以就自己写了一本，方便自查自检。这篇文章，会随着本人的打怪升级等级进行增补。&lt;/p&gt;
&lt;h2 id="estimator"&gt;Estimator&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;img src="/images/image2.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;Estimator 作为高层API，可以让我们写出结构清晰的代码。你有两种方法通过 estimator 来构建模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-mode Estimator: 创建指定类型的模型，如上图，它们分别是线性分类和回归模型、深度神经网络分类和回归模型，还有线性和深度混合的分类、回归模型。&lt;/li&gt;
&lt;li&gt;自定义 Estimator: 按传统的方法写模型，然后用 model_fn 函数封装 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;2018-04-12 第一次修订, 新增"多GPU下的写法"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：代码适用于 TF1.4 ~ TF1.7 。&lt;/p&gt;
&lt;p&gt;为什么要使用Estimator, 仅&lt;a href="https://www.tensorflow.org/programmers_guide/estimators"&gt;官方文档&lt;/a&gt;里提到的第一条优点就让我不得不重视它。
大意是不管你在本地环境还是分布式环境，不管你用一个或多个CPU、GPU还是TPU训练模型，你的模型代码不需要做任何改变。&lt;/p&gt;
&lt;p&gt;但看了一些Estimator教程，不怎么满意。因为大部分介绍的方法过于简单，仅适用于实验环境。当你面对大数据、复杂模型和机能限制时，发现那些方法就不灵了。
所以就自己写了一本，方便自查自检。这篇文章，会随着本人的打怪升级等级进行增补。&lt;/p&gt;
&lt;h2 id="estimator"&gt;Estimator&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;img src="/images/image2.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;Estimator 作为高层API，可以让我们写出结构清晰的代码。你有两种方法通过 estimator 来构建模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-mode Estimator: 创建指定类型的模型，如上图，它们分别是线性分类和回归模型、深度神经网络分类和回归模型，还有线性和深度混合的分类、回归模型。&lt;/li&gt;
&lt;li&gt;自定义 Estimator: 按传统的方法写模型，然后用 model_fn 函数封装&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 指定类型模型的估计器举例&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNNClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# 定义好的特征列 &lt;/span&gt;
   &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;           &lt;span class="c1"&gt;# 两个隐藏层, 每层10个神经元&lt;/span&gt;
   &lt;span class="n"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                     &lt;span class="c1"&gt;# 输出3个类别&lt;/span&gt;
   &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# 存 checkpoints 的路径&lt;/span&gt;

&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;file_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FILE_TRAIN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;     &lt;span class="c1"&gt;# 训练数据文件路径&lt;/span&gt;
        &lt;span class="n"&gt;perform_shuffle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;     &lt;span class="c1"&gt;# 打乱数据&lt;/span&gt;
        &lt;span class="n"&gt;repeat_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;          &lt;span class="c1"&gt;# 重复8次&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 自定义模型的估计器举例&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                       &lt;span class="c1"&gt;# batch数量的特征，是input_fn 函数的输出&lt;/span&gt;
   &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                         &lt;span class="c1"&gt;# batch数量的标签，是input_fn 函数的输出&lt;/span&gt;
   &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;                          &lt;span class="c1"&gt;# tf.estimator.ModeKeys.TRAIN / EVAL / PREDICT&lt;/span&gt;

  &lt;span class="c1"&gt;# 用特征列（feature_columns)定义输入层 &lt;/span&gt;
  &lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;# 模型定义部分&lt;/span&gt;
  &lt;span class="o"&gt;...&lt;/span&gt;

  &lt;span class="c1"&gt;# 返回值被EstimatorSpec封装，返回训练时关心的loss和train_op&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
     &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Estimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;model_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;               &lt;span class="c1"&gt;# 自定义模型的封装函数&lt;/span&gt;
   &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# 存 checkpoints 的路径&lt;/span&gt;

&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILE_TRAIN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;repeat_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;仔细研究上面两段最简代码，Estimator的编程结构就呼之欲出了，看下图：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/Unknown.jpg" width="60%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;input_fn读入数据，预处理后输出到estimator，再由estimator执行训练、评估或预测等任务.
这里的estimator(估计器）就是模型的抽象，它可以直接定义模型或使用外部模型。 特殊的地方在于，
数据送进estimator时，常常被feature_columns（由特征列组成的列表, 与input_fn输出的数据一一对应）
做二次封装. 注意，特征列是使用estimator的主要方法之一，并不是必须.&lt;/p&gt;
&lt;p&gt;任务分解如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input_fn: 从原始数据文件读取数据，然后清洗数据、打乱顺序等，用迭代器分批输出特征和对应的标签。  &lt;/li&gt;
&lt;li&gt;feature_columns: 特征工程，使数据便于模型训练。&lt;/li&gt;
&lt;li&gt;模型定义: 简单的模型可考虑用estimator直接定义；自定义模型的话，须封装进model_fn函数，输入层传入feature_columns, 输出用tf.estimator.EstimatorSpec封装。&lt;/li&gt;
&lt;li&gt;最后，用estimator把上面三项组织在一起，做训练、评估、预测等任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="input_fn"&gt;input_fn&lt;/h2&gt;
&lt;p&gt;原始数据一般工整的很少，所以要把input_fn写好，还是蛮难的。&lt;/p&gt;
&lt;h3 id="xiao shu ju de qing kuang"&gt;小数据的情况&lt;/h3&gt;
&lt;p&gt;通常实验性的项目采用小规模的数据，这时只需要简单把数据载入内存作训练即可, 我们可以用numpy、pandas等通用工具来处理数据。
假如原始数据是csv文件，选用pandas读入并作预处理：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 定义列名&lt;/span&gt;
&lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;'symboling'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="s1"&gt;'normalized-losses'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="s1"&gt;'make'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
     &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="s1"&gt;'price'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# 为每列指定类型.&lt;/span&gt;
&lt;span class="n"&gt;dtypes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;'symboling'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="s1"&gt;'normalized-losses'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="s1"&gt;'make'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
     &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="s1"&gt;'price'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# 读入文件，空数据用 ？号填充.&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'filename.csv'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;na_values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'?'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 清理数据: 如果发现价格为空就删除该行.&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rows'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'any'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;subset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'price'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# 补足数据: 把其他列的空值填充为缺省值&lt;/span&gt;
&lt;span class="c1"&gt;# 把float32类型的列放入列表 float_columns&lt;/span&gt;
&lt;span class="n"&gt;float_columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# 对数值列来说，如果发现空值就填充 0 &lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;float_columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;float_columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'columns'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 构建字符串列，如果方向空值(NaN)就填充''(空串).&lt;/span&gt;
&lt;span class="n"&gt;string_columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;string_columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;string_columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'columns'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样，数据变得比较工整了。假设最后一项price是label，前面的都是特征，按照习惯，数据被分割成训练和评估数据, 
它们分别被叫做 training_data、training_label 和 eval_data、eval_label, 它们的类型都是dataframe.&lt;/p&gt;
&lt;p&gt;定义训练和评估的input_fn&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# num_epochs=None -&amp;gt; 数据无限循环&lt;/span&gt;
&lt;span class="c1"&gt;# shuffle   =True -&amp;gt; 打乱数据&lt;/span&gt;
&lt;span class="n"&gt;training_input_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pandas_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 评估时，数据不需要被打乱，所以shuffle=False &lt;/span&gt;
&lt;span class="n"&gt;eval_input_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pandas_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eval_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eval_label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样，input_fn就快速的写好了。&lt;/p&gt;
&lt;h3 id="da shu ju de qing kuang"&gt;大数据的情况&lt;/h3&gt;
&lt;p&gt;pandas一次性把数据载入内存中，不适合大数据量的情形。
面对大规模数据时，需要给数据和模型之间接上管道，然后打开水龙头，按照你想要的流量把数据传入模型。
这时，TF提供的 dataset api 就派上用场了。&lt;/p&gt;
&lt;h4&gt;Dataset API 的结构&lt;/h4&gt;
&lt;p&gt;&lt;p align="center"&gt;
&lt;img src="/images/image7.jpg" width="70%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TextLineDataset: 从文本文件每次读一行.&lt;/li&gt;
&lt;li&gt;TFRecordDataset: 从 TFRecord 文件读取记录.&lt;/li&gt;
&lt;li&gt;FixedLengthRecordDataset: 从二进制文件读取固定大小的记录.&lt;/li&gt;
&lt;li&gt;Iterator: 从dataset中每次读取一笔(一般为batch条)数据.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Dataset版的input_fn&lt;/h4&gt;
&lt;p&gt;假设我们手上有一堆人口普查数据，其中年收入是字符串类型，形如&amp;ldquo;&amp;gt;50k&amp;rdquo;, 我们的目标是预测人们的年收入是大于5万还是小于等于5万。
input_fn函数的写法如下：&lt;/p&gt;
&lt;p&gt;首先定义CSV文件中每行数据的列名和缺省值，字典格式.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;csv_defaults&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collections&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderedDict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'age'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'workclass'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fnlwgt'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'education'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'education-num'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'marital-status'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'occupation'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'relationship'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'race'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'sex'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'capital-gain'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'capital-loss'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'hours-per-week'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'native-country'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'income'&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;接下来是一段通用代码, 具体见代码注释.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 按行解码 CSV 文件.&lt;/span&gt;
&lt;span class="c1"&gt;# 读入一行数据，对于每列如果有数据就用原值，如果没数据就用缺省值;&lt;/span&gt;
&lt;span class="c1"&gt;# 返回字典格式的键值对&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;csv_decoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;parsed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_defaults&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_defaults&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;parsed&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# 过滤器，滤掉空行，该函数后面要用&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;filter_empty_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;not_equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string_split&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;','&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 创建训练的input_fn&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;create_train_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;    
        &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextLineDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# 从文件创建数据集&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_empty_lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="c1"&gt;# 滤掉空行&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_decoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# 解析每行&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;buffer_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;# 每1000行打乱顺序&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;                          &lt;span class="c1"&gt;# 无限重复&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 

        &lt;span class="c1"&gt;# 迭代器，每次取batch个数据, 这里为32&lt;/span&gt;
        &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_one_shot_iterator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# 分离出label值，并转成 true/false 形式&lt;/span&gt;
        &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'income'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s2"&gt;" &amp;gt;50K"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;

&lt;span class="c1"&gt;# 创建测试的input_fn, 注意与前面的区别&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;create_test_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;    
        &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextLineDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_empty_lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_decoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_one_shot_iterator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'income'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s2"&gt;" &amp;gt;50K"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; 

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;

&lt;span class="c1"&gt;# 从input_fn中取出数据，每sess.run一次next_batch，就取出一批&lt;/span&gt;
&lt;span class="n"&gt;train_input_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_train_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;next_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_input_fn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'education'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面的代码为什么 create_train_input_fn() 套 input_fn() 呢？
回忆这句：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILE_TRAIN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;repeat_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;因为calssifier.train()中，input_fn要求接的是一个函数，而input_fn() 返回的是特征和标签，所以前面要接上&lt;code&gt;lambda:&lt;/code&gt;.
如果采用现在函数套函数的结构, 那么这句前面的lambda就可以去掉, 走个例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_input_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_train_input_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILE_TRAIN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_input_fn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="te zheng lie_1"&gt;特征列&lt;/h2&gt;
&lt;p&gt;特征列实质上是对input_fn()输出的数据做的二次封装，它是做特征工程的强力工具之一。
特征列好比一种约定，它规定了estimator使用input_fn传入的数据具备什么样的形式, 
主要目的是令特征数据变得更方便机器运算。&lt;/p&gt;
&lt;p&gt;关于特征列，一共涉及10个函数（图中底层的3个矩形框, 缺weighted_categorical_column）。
按大类分为类别特征列和密集特征列（以下也简称为&amp;ldquo;类别列&amp;rdquo;和&amp;ldquo;密集列&amp;rdquo;）。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/3_.jpg" width="70%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;其中，&lt;code&gt;buchetized_column&lt;/code&gt;位于中间，表示它作为中介把密集列(通常是&lt;code&gt;numeric_column&lt;/code&gt;)转为类别列。对于类别列而言，除了&lt;code&gt;categorical_column_with_hash_buchet&lt;/code&gt;和&lt;code&gt;crossed_column&lt;/code&gt;外，其余三种均把输入的特征数据处理为one-hot结构。&lt;/p&gt;
&lt;p&gt;10个函数可对应9种特征列，我们约定中文称谓如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;numeric_column : 数值列&lt;/li&gt;
&lt;li&gt;bucketized_column : 分区列 &lt;/li&gt;
&lt;li&gt;indicator_column : 指示列&lt;/li&gt;
&lt;li&gt;embedding_column : 嵌入列&lt;/li&gt;
&lt;li&gt;categorical_column_with_identity : 类别ID列&lt;/li&gt;
&lt;li&gt;categorical_column_with_vocabulary(file or list) : 类别词表列&lt;/li&gt;
&lt;li&gt;categorical_column_with_hash_bucket : 类别哈希列&lt;/li&gt;
&lt;li&gt;crossed_column : 合成列&lt;/li&gt;
&lt;li&gt;weighted_categorical_column : 权重类别列&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="shu zhi lie"&gt;数值列&lt;/h3&gt;
&lt;p&gt;以鸢尾花分类问题举例，其输入特征 SepalLength, SepalWidth, PetalLength, PetalWidth （萼片的长宽、花瓣的长宽）就是数值类型。用法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 缺省为tf.float32的标量.&lt;/span&gt;
&lt;span class="n"&gt;numeric_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"SepalLength"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;数值列的缺省类型为 tf.float32, 如果想指定类型，则：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 用tf.float64的标量表示.&lt;/span&gt;
&lt;span class="n"&gt;numeric_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"SepalLength"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;缺省情况下，numeric_column 返回一个单值数据，如果要返回向量数据，则需指定shape值：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 用10维向量来表示，其中每个元素的类型为 tf.float32.&lt;/span&gt;
&lt;span class="n"&gt;vector_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Bowling"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 用10x5的矩阵来表示.&lt;/span&gt;
&lt;span class="n"&gt;matrix_feature_column&lt;/span&gt;
   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"MyMatrix"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="fen qu lie"&gt;分区列&lt;/h3&gt;
&lt;p&gt;如果要把一个数值分成不同区间，比如按年份划分：
&lt;p align="center"&gt;
&lt;img src="/images/4_.jpg" width="60%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;划分后的结果为one-hot向量形式。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;区间&lt;/th&gt;
&lt;th align="center"&gt;表示为&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 1960&lt;/td&gt;
&lt;td align="center"&gt;[1, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;= 1960 且 &amp;lt; 1980&lt;/td&gt;
&lt;td align="center"&gt;[0, 1, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;= 1980 且 &amp;lt; 2000&lt;/td&gt;
&lt;td align="center"&gt;[0, 0, 1, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt; 2000&lt;/td&gt;
&lt;td align="center"&gt;[0, 0, 0, 1]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 原始输入是一个名为Year的数值列.&lt;/span&gt;
&lt;span class="n"&gt;numeric_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Year"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 以1960、1980、2000年来划分区间&lt;/span&gt;
&lt;span class="n"&gt;bucketized_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bucketized_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;source_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numeric_feature_column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;boundaries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1960&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1980&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="lei bie idlie"&gt;类别Id列&lt;/h3&gt;
&lt;p&gt;如图，所谓类别Id列是指把左边的单值数据转换为右边的one-hot矢量形式。
&lt;p align="center"&gt;
&lt;img src="/images/5_.jpg" width="40%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;比如我们用0、1、2、3分别表示童装、数码、运动和食品四类商品：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0=&amp;ldquo;kitchenware&amp;rdquo;&lt;/li&gt;
&lt;li&gt;1="electronics"&lt;/li&gt;
&lt;li&gt;2="sport"&lt;/li&gt;
&lt;li&gt;3="food"&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# key后跟的列名与input_fn()中的列名一致，&lt;/span&gt;
&lt;span class="c1"&gt;# 其值域为[0, num_buckets)间的整数。&lt;/span&gt;
&lt;span class="n"&gt;identity_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical_column_with_identity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'procduct_class'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;num_buckets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="c1"&gt;# 本例中, 'Integer_1' 或 'Integer_2' 皆可替换到上句的 key 之后&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="o"&gt;...&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;...&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;({&lt;/span&gt; &lt;span class="s1"&gt;'Integer_1'&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;..&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;etc&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;..&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Integer_2'&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Label_values&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="lei bie ci biao lie"&gt;类别词表列&lt;/h3&gt;
&lt;p&gt;在NLP任务中，我们不会把词条直接输入模型，而是首先把它转换成数值或向量。类别词表列可以把词条转换为one-hot向量形式，如下图：
&lt;p align="center"&gt;
&lt;img src="/images/6_.jpg" width="50%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;从列表创建一个词表列：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;vocabulary_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical_column_with_vocabulary_list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"feature_name_from_input_fn"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;vocabulary_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"kitchenware"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"electronics"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"sports"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从文件创建一个词表列：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;vocabulary_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical_column_with_vocabulary_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"feature_name_from_input_fn"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;vocabulary_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"product_class.txt"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;vocabulary_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# product_class.txt 的文件内容如下：&lt;/span&gt;
&lt;span class="n"&gt;kitchenware&lt;/span&gt;
&lt;span class="n"&gt;electronics&lt;/span&gt;
&lt;span class="n"&gt;sports&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="lei bie ha xi lie"&gt;类别哈希列&lt;/h3&gt;
&lt;p&gt;如果待分类的数据量很大，势必会消耗很大内存。tensorflow提供一种用哈希表分类的方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hashed_feature_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categorical_column_with_hash_bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"feature_name_from_input_fn"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;hash_buckets_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 把特征值哈希分布到100个位置&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
&lt;img src="/images/7_.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;当分类数量大于哈希分布尺寸的时候，必然会有几个特征指向同一个哈希位置。如图所示，&lt;code&gt;kitchenware&lt;/code&gt;和&lt;code&gt;sports&lt;/code&gt;的哈希值同为12，这没有关系，模型可以通过你提供的其他特征进一步区分到底是&lt;code&gt;kitchenware&lt;/code&gt;还是&lt;code&gt;sports&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id="he cheng lie"&gt;合成列&lt;/h3&gt;
&lt;p&gt;有时我们需要组合多个特征为一个特征，这种特征叫合成特征。组合方式通常采用相乘或求笛卡尔积，特征组合有助于表示非线性关系。举个例子，假设我们的模型要计算北京的房产价格，而房产价格与它所处的位置密切相关，而对于位置而言，我们需要用经纬度两个数据同时标定，因此这个经纬度就构成了合成特征。假设我们把北京均匀的纵横切100x100刀，这样就会产生10000个可区分的矩形区域。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 将经纬度转换为[0, 100)范围内的整型值&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# 从数据集读入经纬度&lt;/span&gt;
    &lt;span class="n"&gt;latitude&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;   &lt;span class="c1"&gt;# A tf.float32 value&lt;/span&gt;
    &lt;span class="n"&gt;longitude&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# A tf.float32 value&lt;/span&gt;

    &lt;span class="c1"&gt;# 返回的字典包含经纬度及其它特征，经纬度的值为0到99的整型值&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;"latitude"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;latitude&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"longitude"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;longitude&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;

&lt;span class="c1"&gt;# 用np.linspace把纬度区间分成100等份&lt;/span&gt;
&lt;span class="c1"&gt;# 然后把100等份的列表定义为区间列.&lt;/span&gt;
&lt;span class="n"&gt;latitude_buckets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;33.641336&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;33.887157&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;latitude_fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bucketized_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'latitude'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;latitude_buckets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;longitude_buckets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;84.558798&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;84.287259&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;longitude_fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bucketized_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numeric_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'longitude'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;longitude_buckets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 用fc_longitude x fc_latitude创建交叉特征.&lt;/span&gt;
&lt;span class="n"&gt;fc_beijing_boxed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;crossed_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;latitude_fc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;longitude_fc&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;hash_bucket_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 把10000个分区哈希分布到1000个位置&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;创建合成特征的方法为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从input_fn的返回值中取得待组合的特征名，本例中为&lt;code&gt;latitude&lt;/code&gt;和&lt;code&gt;longitude&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;做组合的这些特征必须先转换成one-hot形式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由&lt;code&gt;latitude_fc&lt;/code&gt;和&lt;code&gt;longitude_fc&lt;/code&gt;组成的合成列的形式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(0,0),(0,1)...  (0,99)
(1,0),(1,1)...  (1,99)
&amp;hellip;, &amp;hellip;,          ...
(99,0),(99,1)...(99, 99)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，使用合成列后，仍需在模型中包含你用来合成特征列的原始特征列，它们负责在哈希冲突时，作为附加特征来进一步做类别区分。&lt;/p&gt;
&lt;h3 id="zhi shi lie"&gt;指示列&lt;/h3&gt;
&lt;p&gt;指示列和后面要说的嵌入列均不能直接作为特征给模型使用，它的数据来源于类别特征列，即类别特征列是它的输入。
为什么要作这样的设计？因为estimator执行深度神经网络的任务时，只能使用密集特征列，而类别特征列为稀疏列，需要用指示列或嵌入列作下变换才能被使用。
至于指示列封装后，数据变成什么样子，我在官方文档中没找到，以后知道了再补充。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# 创建某种类型的类别特征列&lt;/span&gt;

&lt;span class="c1"&gt;# 定义一个指示列，该列中的每个元素为one-hot向量. &lt;/span&gt;
&lt;span class="n"&gt;indicator_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;indicator_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="qian ru lie"&gt;嵌入列&lt;/h3&gt;
&lt;p&gt;如果类别数据量很大，比如上百万、上亿等，这时采用one-hot来表示就不经济了。记得词嵌入模型中的词向量吗，用一组浮点数来代替one-hot形式来表示一个词条，这种形式在这里被叫做嵌入列，这种方法明显的好处就是令向量维度变得很小。&lt;/p&gt;
&lt;p&gt;如下图，假设我们有81个不同的单词，采用one-hot形式需要81维的向量，而采用嵌入列则仅需要3维向量就能表达。
&lt;p align="center"&gt;
&lt;img src="/images/image9.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;那么，在嵌入列产生的向量中的浮点数是如何确定的呢？通常，由训练数据学得。嵌入列可以提升模型的表达能力，一定程度描述类别间的关系。&lt;/p&gt;
&lt;p&gt;如何确定表示81个类别只需要3维呢？有个简单的公式来算出：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{2}\log_2(n)$$&lt;/div&gt;
&lt;p&gt;
&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$n^{0.25}  \tag {等价公式}$$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 类别数的0.25次方&lt;/span&gt;
&lt;span class="n"&gt;embedding_dimensions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;number_of_categories&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mf"&gt;0.25&lt;/span&gt;

&lt;span class="n"&gt;categorical_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# 创建一个类别列.&lt;/span&gt;

&lt;span class="c1"&gt;# 再把这个类别列转为一个嵌入列.&lt;/span&gt;
&lt;span class="c1"&gt;# 这意味着把one-hot向量转为指定维度的向量.&lt;/span&gt;
&lt;span class="n"&gt;embedding_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dimension&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;embedding_dimensions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，这仅仅是个一般规则，你也可以自行设定你希望的维度数。&lt;/p&gt;
&lt;h3 id="quan zhong lei bie lie"&gt;权重类别列&lt;/h3&gt;
&lt;p&gt;有时会遇到一种配对特征，特征一是本体，特征二是本体对应的权重（或出现频率）。
这就是权重类别列的使用场景。
下面是从Tensorflow源码里抠出例子，话说有个&lt;code&gt;tf.Example&lt;/code&gt;对象，它的proto形式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# proto 
[
  features {
    feature {
      key: "terms"
      value {bytes_list {value: "very" value: "model"}}
    }
    feature {
      key: "frequencies"
      value {float_list {value: 0.3 value: 0.1}}
    }
  },
  features {
    feature {
      key: "terms"
      value {bytes_list {value: "when" value: "course" value: "human"}}
    }
    feature {
      key: "frequencies"
      value {float_list {value: 0.4 value: 0.1 value: 0.2}}
    }
  }
]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;考虑到proto格式熟悉的人不多，我们把上面的内容简化一下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;terms      : ["very", "model"]
frequencies: [  0.3 ,    0.1 ]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;容易看出，这两组数据有伴生关系，下面的代码通过权重类别列函数把该特征组合在一起.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;categorical_column_with_hash_bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;column_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'terms'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hash_bucket_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;weighted_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weighted_categorical_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;categorical_column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight_feature_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'frequencies'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;weighted_column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;make_parse_example_spec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;linear_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;linear_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="shi yong te zheng lie"&gt;使用特征列&lt;/h3&gt;
&lt;p&gt;我们须把多个特征列封装成一个列表，才能作为参数拿给估计器(estimator)用。
在使用特征列时，要注意区分特征列类型和模型类型。特征列只有两种类型，类别列和密集列；
模型也分两种，线性模型和深度模型。具体如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LinearClassifier&lt;/code&gt; 和 &lt;code&gt;LinearRegressor&lt;/code&gt;:&lt;ul&gt;
&lt;li&gt;适用所有类型的特征列&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DNNClassifier&lt;/code&gt; 和 &lt;code&gt;DNNRegressor&lt;/code&gt;:&lt;ul&gt;
&lt;li&gt;仅适用于密集列，如要使用类别列，须经过 &lt;code&gt;indicator_column&lt;/code&gt; or或&lt;code&gt;embedding_column&lt;/code&gt;做二次封装&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DNNLinearCombinedClassifier&lt;/code&gt; 和&lt;code&gt;DNNLinearCombinedRegressor&lt;/code&gt;:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;linear_feature_columns&lt;/code&gt; 参数适用所有类型特征列.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dnn_feature_columns&lt;/code&gt; 参数仅适用密集列, 用法和 &lt;code&gt;DNNClassifier&lt;/code&gt; 及 &lt;code&gt;DNNRegressor&lt;/code&gt;的用法一致.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;DNNLinearCombinedClassifier&lt;/code&gt;的代码举例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNNLinearCombinedClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'/tmp/census_model'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;linear_feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;base_columns&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;crossed_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dnn_feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;deep_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dnn_hidden_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="mo xing ding yi_1"&gt;模型定义&lt;/h2&gt;
&lt;h3 id="yu ding yi"&gt;预定义&lt;/h3&gt;
&lt;p align="center"&gt;
&lt;img src="/images/image2.jpg" width="75%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;预定义模型没什么好讲，看看文档就能秒懂，如下面的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 含2个隐藏层的深度神经网络&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNNClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# 定义好的特征列 &lt;/span&gt;
   &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;           &lt;span class="c1"&gt;# 两个隐藏层, 每层10个神经元&lt;/span&gt;
   &lt;span class="n"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                     &lt;span class="c1"&gt;# 输出3个类别&lt;/span&gt;
   &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                  &lt;span class="c1"&gt;# 存 checkpoints 的路径&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="zi ding yi"&gt;自定义&lt;/h3&gt;
&lt;p&gt;写自定义模型时，其实和传统的写法差不多，只是有些小地方要注意一下。&lt;/p&gt;
&lt;p&gt;基本思路是，定义模型函数，它接收从input_fn()传来的特征和标签，输出由tf.estimator.EstimatorSpec封装后的结果, 
函数体主要做两件事情，一件是定义模型，一件是通过分支语句分别实现训练、评估和预测。&lt;/p&gt;
&lt;p&gt;我喜欢的结构是把模型单独定义成一个类，然后再用mode_fn()来调用它, 
mode_fn()的返回用tf.estimator.EstimatorSpec封装，详见下面的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Sample_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                       &lt;span class="c1"&gt;# batch数量的特征，是input_fn 函数的输出&lt;/span&gt;
   &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                         &lt;span class="c1"&gt;# batch数量的标签，是input_fn 函数的输出&lt;/span&gt;
   &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;                          &lt;span class="c1"&gt;# tf.estimator.ModeKeys.TRAIN / EVAL / PREDICT&lt;/span&gt;

    &lt;span class="c1"&gt;# 用特征列（feature_columns)定义输入层 &lt;/span&gt;
    &lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# 定义模型实例 &lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Sample_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TRAIN&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

        &lt;span class="c1"&gt;# 给训练准确度命名，并使它被tf日志记录&lt;/span&gt;
        &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;identity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'train_accuracy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'train_accuracy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PREDICT&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EVAL&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你可能发现，我好像没用到特征列。
如果在自定义的模型中想用特征列这个工具（再次强调，不是必须），只需在模型的输入层调用下面这个函数即可：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.feature_column.linear_model(features, feature_columns, ...)&lt;/code&gt;：如果定义线性模型的话用这个，输出是预测结果.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.feature_column.input_layer(features, feature_columns, ...)&lt;/code&gt;：深度模型用这个.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中features 来自input_fn 的输出， feature_columns 是由多个特征列组成的列表。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 自定义一个超简单的模型&lt;/span&gt;
&lt;span class="c1"&gt;# 输入层&lt;/span&gt;
&lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 隐藏层: h1，h2&lt;/span&gt;
&lt;span class="c1"&gt;# 10个神经元，relu激活函数，input_layer作为输入参数&lt;/span&gt;
&lt;span class="n"&gt;h1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 输出层，3个输出&lt;/span&gt;
&lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;合并写法&lt;/h4&gt;
&lt;p&gt;这是tensorflow 官网给出的一种写法, 只用了一个return, 返回内容的判断放在了前面的 if 分支，你可以根据自己的喜好选择不同写法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TRAIN&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt;
      &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EVAL&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TRAIN&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModeKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PREDICT&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EstimatorSpec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="zu zhuang_1"&gt;组装&lt;/h2&gt;
&lt;p&gt;准备好上面的内容后，就可以把model_fn()和input_fn组装在一起了。方法是用estimator实例化
估计器对象，然后用这个对象分别进行训练、评估、预测即可。&lt;/p&gt;
&lt;p&gt;在组装时，我们还要加入一些常规的东西。比如设置checkpoint的保存规则，定义一些观测变量，
方便在训练时用TensorBoard观察，还有超参数等等。具体请看示例注释：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 设置tf输出哪种类别的日志，不同类别详细程度不同&lt;/span&gt;
&lt;span class="c1"&gt;# tf.logging.后的可选值为DEBUG, INFO, WARN, ERROR, or FATAL.&lt;/span&gt;
&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# TensorFlow 版本检查，estimator 要求1.4以上 &lt;/span&gt;
&lt;span class="n"&gt;tf_version&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__version__&lt;/span&gt;
&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"TensorFlow version: {}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf_version&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="s2"&gt;"1.4"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;tf_version&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"TensorFlow r1.4 or later is needed"&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_function&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="c1"&gt;# flags 携带准备传入模型函数和输入函数的参数。 &lt;/span&gt;

  &lt;span class="c1"&gt;# 设置训练时每隔多少秒保存一下checkpoint.&lt;/span&gt;
  &lt;span class="n"&gt;run_config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RunConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;save_checkpoints_secs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="c1"&gt;# 生成classifier实例&lt;/span&gt;
  &lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Estimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;model_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run_config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="s1"&gt;'resnet_size'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;'data_format'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_format&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;'batch_size'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="c1"&gt;# 每训练 flags.epochs_per_eval 轮更新一下日志内容.&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_epochs&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs_per_eval&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tensors_to_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;'learning_rate'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'learning_rate'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;'cross_entropy'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'cross_entropy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;'train_accuracy'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'train_accuracy'&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="c1"&gt;# 设置每跑100个迭代器，打印一下日志。&lt;/span&gt;
    &lt;span class="n"&gt;logging_hook&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LoggingTensorHook&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;tensors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tensors_to_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Starting a training cycle.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn_train&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs_per_eval&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_fn_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hooks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;logging_hook&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Starting to evaluate.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# 评估模型并打印结果&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;input_fn_eval&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;eval_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_fn_eval&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="duo gpuxia de xie fa"&gt;多GPU下的写法&lt;/h2&gt;
&lt;p&gt;在多GPU的情况下，代码需要做几点小变化。&lt;/p&gt;
&lt;p&gt;首先, 检查batch_size的数量，它必须能被GPU的数量整除。其目的是让每批的输入数量被平均分配到各个GPU上。&lt;/p&gt;
&lt;p&gt;其次，在model_fn函数中的训练部分封装优化器。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;          &lt;span class="c1"&gt;# 原优化器的定义不变&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'multi_gpu'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
      &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TowerOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，在main函数部分封装模型函数(model_fn).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multi_gpu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;model_function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replicate_model_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;model_fn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss_reduction&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Reduction&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MEAN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="qi ta"&gt;其他&lt;/h2&gt;
&lt;h3 id="san ge import"&gt;三个import&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;absolute_import&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;常看到这三个置于顶上的import 语句，一直没有深究它们有什么用，今天查了下资料，发现这三句都是针对python 2.X版本的情况，分别作用如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;print_function: print语句必须使用函数形式，而 print &amp;lsquo;test&amp;rsquo;这句在这种条件下就会报错。&lt;/li&gt;
&lt;li&gt;division: 精确除法，即python2.x版本中，3/4=0（截断除法），有了这句, 3/4=0.75， 而3//4=0&lt;/li&gt;
&lt;li&gt;absolute_import: 绝对路径，解决自定义包与缺省包名字冲突的问题，如你不小心自定义了string，有了这句， import string时引用系统的，没有这句就引用你本地的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="guan yu chao can shu"&gt;关于超参数&lt;/h3&gt;
&lt;p&gt;既然训练始终要调参，不如把参数提前定义好，比如下面这个参数类。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MymodelArgParser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;argparse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MymodelArgParser&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s1"&gt;'--multi_gpu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'store_true'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'If set, run across all available GPUs.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s1"&gt;'--batch_size'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Number of images to process in a batch'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;还有一种方法也比较优雅，它把参数存成json文件，运行时载入参数即可，见下面的几个功能函数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;codecs&lt;/span&gt;

&lt;span class="c1"&gt;# 从指定目录载入参数&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;load_hparams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;"""Load hparams from an existing model directory."""&lt;/span&gt;
  &lt;span class="n"&gt;hparams_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"hparams"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"# Loading hparams from &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;codecs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getreader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"utf-8"&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"rb"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;hparams_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;hparams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HParams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;hparams_values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"  can't load hparams file"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;hparams&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

&lt;span class="c1"&gt;# 保存参数到json文件&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;save_hparams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;"""Save hparams."""&lt;/span&gt;
  &lt;span class="n"&gt;hparams_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"hparams"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"  saving hparams to &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;codecs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getwriter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"utf-8"&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"wb"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_json&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# 用hparams_path里的新值覆盖hparams的老值&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maybe_parse_standard_hparams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;hparams&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"# Loading standard hparams from &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hparams_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"r"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;hparams&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="zhu cheng xu ru kou"&gt;主程序入口&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FLAGS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_function&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;parser&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MymodelArgParser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;FLAGS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;unparsed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_known_args&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;unparsed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>TensorLayer 教程</title><link href="https://freeopen.github.io/posts/tensorlayer-jiao-cheng" rel="alternate"></link><published>2017-09-09T00:00:00+08:00</published><updated>2017-09-09T00:00:00+08:00</updated><author><name>Xiaoping Fan(译), freeopen(修订)</name></author><id>tag:freeopen.github.io,2017-09-09:/posts/tensorlayer-jiao-cheng</id><summary type="html">&lt;p&gt;&lt;a href="https://github.com/zsdonghao/tensorlayer/blob/master/docs/user/tutorial.rst"&gt;原文&lt;/a&gt; | &lt;a href="https://github.com/shorxp/tensorlayer-chinese/blob/master/docs/user/tutorial.rst"&gt;原译文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 为什么研究 TensorLayer ? &lt;br/&gt;
在TF的编码中，常看到model被封装成class，进一步研究，发现为了简化模型的创建，有各种封装库，包括Keras, Tflearn, TFSlim, tf.contrib.learn(或称skflow) 等。
其中TFSlim和skflow属TensorFlow自带，考虑到TFSlim中有现成的模型，就修改了一个试试，发现坑不小。我需要首先把数据源转成TFRecord格式，然后用模型训练后报错（out of range), 然后我就首先检查数据源的格式是否有问题，我采用TFSlim的Data
provider来输出数据，可能我的数据维度太大，程序不报错，但一直死在那里不出结果，而同样我在使用numpy输出数据源数据时，没有任何问题，也没什么延时，瞬间我感觉到TFSlim的封装模式不仅把问题复杂化了，甚至还降低了程序的性能。Keras据说跑TF性能较慢，Tflearn感觉活跃度较低，考虑到TensorLayer是skflow上的再次封装，且不丧失灵活性，加上这篇最主要的教程写得真心不错，所以我发愿要好好研究一番。 &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;为什么我总喜欢修订一些长文翻译？&lt;br/&gt;
技术翻译文章经常在阅读时有不知所云的现象，我总结的原因主要有三：一是译者没有理解作者意图，甚至连文章都没读懂，就开始直译 …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://github.com/zsdonghao/tensorlayer/blob/master/docs/user/tutorial.rst"&gt;原文&lt;/a&gt; | &lt;a href="https://github.com/shorxp/tensorlayer-chinese/blob/master/docs/user/tutorial.rst"&gt;原译文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 为什么研究 TensorLayer ? &lt;br/&gt;
在TF的编码中，常看到model被封装成class，进一步研究，发现为了简化模型的创建，有各种封装库，包括Keras, Tflearn, TFSlim, tf.contrib.learn(或称skflow) 等。
其中TFSlim和skflow属TensorFlow自带，考虑到TFSlim中有现成的模型，就修改了一个试试，发现坑不小。我需要首先把数据源转成TFRecord格式，然后用模型训练后报错（out of range), 然后我就首先检查数据源的格式是否有问题，我采用TFSlim的Data
provider来输出数据，可能我的数据维度太大，程序不报错，但一直死在那里不出结果，而同样我在使用numpy输出数据源数据时，没有任何问题，也没什么延时，瞬间我感觉到TFSlim的封装模式不仅把问题复杂化了，甚至还降低了程序的性能。Keras据说跑TF性能较慢，Tflearn感觉活跃度较低，考虑到TensorLayer是skflow上的再次封装，且不丧失灵活性，加上这篇最主要的教程写得真心不错，所以我发愿要好好研究一番。 &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;为什么我总喜欢修订一些长文翻译？&lt;br/&gt;
技术翻译文章经常在阅读时有不知所云的现象，我总结的原因主要有三：一是译者没有理解作者意图，甚至连文章都没读懂，就开始直译，导致文理不通；
二是对技术术语不统一的称谓，本来英文是一个词，翻成中文后变成几个词，导致读者以为是不同的东西；三是过度翻译，把没必要或该省略的词也翻出来，反而影响理解。
比如vanilla RNN 中的vanilla, 并不代表&amp;ldquo;香草&amp;rdquo;，而是表示一个&amp;ldquo;普通的&amp;rdquo;或&amp;ldquo;原始的&amp;rdquo;意思，有时把这个词加进译文反而觉得多余，删掉最好。还有本文中&amp;ldquo;理解机器翻译－实现细节&amp;rdquo;部分的第6个段落，encoder_input_size 这个词，其实就是个代码变量，原译文居然也把它翻译出来，没有必要。我一般也是看到一点改一点，不保证能穷尽译文的所有地方，欢迎读者批评指正。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于深度学习，该教程会引导您使用MNIST数据集构建不同的手写数字的分类器，
这可以说是神经网络的 "Hello World" 。
对于强化学习，我们将让计算机根据屏幕输入来学习打乒乓球。
对于自然语言处理。我们从词嵌套（word embedding）开始，然后再实现语言建模和机器翻译。
此外，TensorLayer的Tutorial包含了所有TensorFlow官方深度学习教程的模块化实现，因此你可以对照TensorFlow深度学习教程&lt;a href="https://www.tensorflow.org/versions/master/tutorials/index.html"&gt;(英文&lt;/a&gt;&lt;a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/"&gt;|中文)&lt;/a&gt;来学习 。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;若你已经对TensorFlow非常熟悉，阅读 &lt;code&gt;InputLayer&lt;/code&gt; 和 &lt;code&gt;DenseLayer&lt;/code&gt; 的源代码可让您很好地理解 TensorLayer 是如何工作的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="zai wo men kai shi zhi qian"&gt;在我们开始之前&lt;/h2&gt;
&lt;p&gt;本教程假定您在神经网络和 TensorFlow (TensorLayer在它的基础上构建的)方面具有一定的基础。
您可以尝试从&lt;a href="http://deeplearning.stanford.edu/tutorial/"&gt;Deeplearning Tutorial&lt;/a&gt; 同时进行学习。&lt;/p&gt;
&lt;p&gt;对于人工神经网络更系统的介绍，我们推荐 Andrej Karpathy 等人所著的 &lt;a href="http://cs231n.github.io/"&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt;
和 Michael Nielsen 的 &lt;a href="http://neuralnetworksanddeeplearning.com/"&gt;Neural Networks and Deep Learning&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;要了解TensorFlow的更多内容，请阅读 &lt;a href="https://www.tensorflow.org/versions/r0.9/tutorials/index.html"&gt;TensorFlow tutorial&lt;/a&gt; 。
您不需要会它的全部，只要知道TensorFlow是如何工作的，就能够使用TensorLayer。
如果您是TensorFlow的新手，建议你阅读整个教程。&lt;/p&gt;
&lt;h2 id="tensorlayerhen jian dan"&gt;TensorLayer很简单&lt;/h2&gt;
&lt;p&gt;下面的代码是TensorLayer的一个简单例子，来自 &lt;code&gt;tutorial_mnist_simple.py&lt;/code&gt; 。
我们提供了很多方便的函数（如： &lt;code&gt;fit()&lt;/code&gt; ，&lt;code&gt;test()&lt;/code&gt; ），但如果你想了解更多实现细节，或想成为机器学习领域的专家，我们鼓励
您尽可能地直接使用TensorFlow原本的方法如 &lt;code&gt;sess.run()&lt;/code&gt; 来训练模型，请参考  &lt;code&gt;tutorial_mnist.py&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorlayer&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tl&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveSession&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# 准备数据&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                                &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_mnist_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# 定义 placeholder&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'x'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'y_'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 定义模型&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InputLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'input_layer'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DropoutLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'drop1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;act&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DropoutLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'drop2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;act&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DropoutLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'drop3'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;act&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;identity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'output_layer'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 定义损失函数和衡量指标&lt;/span&gt;
&lt;span class="c1"&gt;# tl.cost.cross_entropy 在内部使用 tf.nn.sparse_softmax_cross_entropy_with_logits() 实现 softmax&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;
&lt;span class="n"&gt;cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'cost'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;correct_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 定义 optimizer&lt;/span&gt;
&lt;span class="n"&gt;train_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_params&lt;/span&gt;
&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.999&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-08&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_locking&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 初始化 session 中的所有参数&lt;/span&gt;
&lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_global_variables&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 列出模型信息&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print_params&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print_layers&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# 训练模型&lt;/span&gt;
&lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_epoch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;print_freq&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eval_train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 评估模型&lt;/span&gt;
&lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 把模型保存成 .npz 文件&lt;/span&gt;
&lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_npz&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_params&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'model.npz'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="yun xing mnistli zi"&gt;运行MNIST例子&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;img src="/images/mnist.jpeg" width="80%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;在本教程的第一部分，我们仅仅运行TensorLayer内置的MNIST例子。
MNIST数据集包含了60000个28x28像素的手写数字图片，它通常用于训练各种图片识别系统。&lt;/p&gt;
&lt;p&gt;我们假设您已经按照 &lt;code&gt;installation&lt;/code&gt; 安装好了TensorLayer。如果您还没有，请复制一个TensorLayer的source目录到终端中，并进入该文件夹，
然后运行 &lt;code&gt;tutorial_mnist.py&lt;/code&gt; 这个例子脚本：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_mnist.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果所有设置都正确，您将得到下面的结果：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;GPU&lt;/span&gt; &lt;span class="n"&gt;MEM&lt;/span&gt; &lt;span class="n"&gt;Fraction&lt;/span&gt; &lt;span class="mf"&gt;0.300000&lt;/span&gt;
  &lt;span class="n"&gt;Downloading&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;idx3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ubyte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;
  &lt;span class="n"&gt;Downloading&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;idx1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ubyte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;
  &lt;span class="n"&gt;Downloading&lt;/span&gt; &lt;span class="n"&gt;t10k&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;idx3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ubyte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;
  &lt;span class="n"&gt;Downloading&lt;/span&gt; &lt;span class="n"&gt;t10k&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;idx1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ubyte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;

  &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
  &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
  &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
  &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="n"&gt;float32&lt;/span&gt;   &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;int64&lt;/span&gt;

  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;InputLayer&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DropoutLayer&lt;/span&gt; &lt;span class="n"&gt;drop1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.800000&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DenseLayer&lt;/span&gt; &lt;span class="n"&gt;relu1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DropoutLayer&lt;/span&gt; &lt;span class="n"&gt;drop2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.500000&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DenseLayer&lt;/span&gt; &lt;span class="n"&gt;relu2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DropoutLayer&lt;/span&gt; &lt;span class="n"&gt;drop3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.500000&lt;/span&gt;
  &lt;span class="n"&gt;tensorlayer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Instantiate&lt;/span&gt; &lt;span class="n"&gt;DenseLayer&lt;/span&gt; &lt;span class="n"&gt;output_layer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;identity&lt;/span&gt;

  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000053&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000043&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.035558&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000008&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000041&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.035371&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000469&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000432&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.049895&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1276810&lt;/span&gt;

  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"dropout/mul_1:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Relu:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"dropout_1/mul_1:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Relu_1:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"dropout_2/mul_1:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"add_2:0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000100&lt;/span&gt;
  &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;

  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.342539&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.330111&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.298098&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.910700&lt;/span&gt;
  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.356471&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.085225&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.097082&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.971700&lt;/span&gt;
  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.352137&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.040741&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.070149&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.978600&lt;/span&gt;
  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.350814&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.022995&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.060471&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.982800&lt;/span&gt;
  &lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt; &lt;span class="mf"&gt;0.350996&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
    &lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.013713&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.055777&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.983700&lt;/span&gt;
  &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这个例子脚本允许您从 &lt;code&gt;if__name__=='__main__':&lt;/code&gt; 中选择不同的模型进行尝试，包括多层神经网络（Multi-Layer Perceptron），
退出（Dropout），退出连接（DropConnect），堆栈式降噪自编码器（Stacked Denoising Autoencoder）和卷积神经网络（CNN）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  main_test_layers(model='relu')
  main_test_denoise_AE(model='relu')
  main_test_stacked_denoise_AE(model='relu')
  main_test_cnn_layer()
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="li jie mnistli zi"&gt;理解MNIST例子&lt;/h2&gt;
&lt;p&gt;现在就让我们看看它是如何做到的！跟着下面的步骤，打开源代码。&lt;/p&gt;
&lt;h3 id="xu yan"&gt;序言&lt;/h3&gt;
&lt;p&gt;您可能会首先注意到，除TensorLayer之外，我们还导入了Numpy和TensorFlow：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorlayer&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tl&lt;/span&gt;
  &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorlayer.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;set_keep&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这是因为TensorLayer是建立在TensorFlow上的，TensorLayer设计的初衷是为了简化工作并提供帮助而不是取代TensorFlow。
所以您会需要一起使用TensorLayer和一些常见的TensorFlow代码。&lt;/p&gt;
&lt;p&gt;请注意，当使用降噪自编码器(Denoising Autoencoder)时，代码中的 &lt;code&gt;set_keep&lt;/code&gt; 被当作用来访问保持概率(Keeping Probabilities)的占位符。&lt;/p&gt;
&lt;h3 id="zai ru shu ju"&gt;载入数据&lt;/h3&gt;
&lt;p&gt;下面第一部分的代码首先定义了 &lt;code&gt;load_mnist_dataset()&lt;/code&gt; 函数。
其目的是为了下载MNIST数据集（如果还未下载），并且返回标准numpy数列通过numpy array的格式。
到这里还没有涉及TensorLayer，所以我们可以把它简单看作：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  X_train, y_train, X_val, y_val, X_test, y_test = \
                    tl.files.load_mnist_dataset(shape=(-1,784))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;X_train.shape&lt;/code&gt; 为 &lt;code&gt;(50000,784)&lt;/code&gt;，可以理解成共有50000张图片并且每张图片有784个像素点。
&lt;code&gt;Y_train.shape&lt;/code&gt; 为 &lt;code&gt;(50000,)&lt;/code&gt; ，它是一个和 &lt;code&gt;X_train&lt;/code&gt; 长度相同的向量，用于给出每幅图的数字标签，即这些图片所包含的位于0-9之间的数字（如果画这些数字的人没有想乱画别的东西）。&lt;/p&gt;
&lt;p&gt;另外对于卷积神经网络的例子，MNIST还可以按下面的4D版本来载入：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  X_train, y_train, X_val, y_val, X_test, y_test = \
              tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;X_train.shape&lt;/code&gt; 为 &lt;code&gt;(50000,28,28,1)&lt;/code&gt; ，这代表了50000张图片，每张图片使用一个通道（Channel），28行，28列。
通道为1是因为它是灰度图像，每个像素只能有一个值。&lt;/p&gt;
&lt;h3 id="jian li mo xing"&gt;建立模型&lt;/h3&gt;
&lt;p&gt;来到这里，就轮到TensorLayer来一显身手了！TensorLayer允许您通过创建，堆叠或者合并图层(Layers)来定义任意结构的神经网络。
每一层都知道它在网络中的直接输入层, 而每层的输出同时作为该层和整个网络的一个句柄，
通常是我们要传递给其余代码的唯一东西。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 上段的最后一句翻译，我认为原文中的output layer说法有问题，因为output layer通常指网络的最后一层, 或者表示当前层的下一层，这里应该表示为&amp;ldquo;每层的输出&amp;rdquo;才符合上下文，也符合代码逻辑。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;正如上文提到的， &lt;code&gt;tutorial_mnist.py&lt;/code&gt; 支持四类模型，我们通过同样的接口实现，只需简单的替换一下函数即可。
首先，我们将定义一个结构固定的多层次感知器（Multi-Layer Perceptron），所有的步骤都会详细的讲解。
然后，我们会实现一个去噪自编码器(Denosing Autoencoding)。
接着，我们要将所有去噪自编码器堆叠起来并对他们进行监督微调(Supervised Fine-tune)。
最后，我们将展示如何去创建一个卷积神经网络(Convolutional Neural Network)。&lt;/p&gt;
&lt;p&gt;此外，如果您有兴趣，我们还提供了一个简化版的MNIST例子在 &lt;code&gt;tutorial_mnist_simple.py&lt;/code&gt; 中，和一个
CIFAR-10数据集的卷积神经网络(CNN)的例子在 &lt;code&gt;tutorial_cifar10_tfrecord.py&lt;/code&gt; 中, 供你参考。&lt;/p&gt;
&lt;h3 id="duo ceng shen jing wang luo"&gt;多层神经网络&lt;/h3&gt;
&lt;p&gt;第一个脚本 &lt;code&gt;main_test_layers()&lt;/code&gt; ，创建了一个具有两个隐藏层，每层800个单元的多层次感知器，并且具有10个单元的SOFTMAX输出层紧随其后。
它对输入数据采用20%的退出率(dropout)并且对隐藏层应用50%的退出率(dropout)。&lt;/p&gt;
&lt;p&gt;为了提供数据给这个网络，TensorFlow占位符(placeholder)需要按如下定义。
在这里 &lt;code&gt;None&lt;/code&gt; 是指在编译之后，网络将接受任意批规模(batchsize)的数据
&lt;code&gt;x&lt;/code&gt; 是用来存放 &lt;code&gt;X_train&lt;/code&gt; 数据的并且 &lt;code&gt;y_&lt;/code&gt; 是用来存放 &lt;code&gt;y_train&lt;/code&gt; 数据的。
如果你已经知道批规模，那就不需要这种灵活性了。您可以在这里给出批规模，特别是对于卷积层，这样可以运用TensorFlow一些优化功能。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    x = tf.placeholder(tf.float32, shape=[None, 784], name='x')
    y_ = tf.placeholder(tf.int64, shape=[None, ], name='y_')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在TensorLayer中每个神经网络的基础是一个 &lt;code&gt;InputLayer&lt;/code&gt; 实例。它代表了将要提供(feed)给网络的输入数据。
值得注意的是 &lt;code&gt;InputLayer&lt;/code&gt; 并不依赖任何特定的数据。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.InputLayer(x, name='input_layer')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在添加第一层隐藏层之前，我们要对输入数据应用20%的退出率(dropout)。
这里我们是通过一个 &lt;code&gt;DropoutLayer&lt;/code&gt; 的实例来实现的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;请注意构造函数的第一个参数是输入层，第二个参数是激活值的保持概率(keeping probability for the activation value)
现在我们要继续构造第一个800个单位的全连接的隐藏层。
尤其是当要堆叠一个 &lt;code&gt;DenseLayer&lt;/code&gt; 时，要特别注意。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.DenseLayer(network, n_units=800, act = tf.nn.relu, name='relu1')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;同样，构造函数的第一个参数意味着这我们正在 &lt;code&gt;network&lt;/code&gt; 之上堆叠 &lt;code&gt;network&lt;/code&gt; 。
&lt;code&gt;n_units&lt;/code&gt; 简明得给出了全连接层的单位数。
&lt;code&gt;act&lt;/code&gt; 指定了一个激活函数，这里的激活函数有一部分已经被定义在了&lt;code&gt;tensorflow.nn&lt;/code&gt; 和  &lt;code&gt;tensorlayer.activation&lt;/code&gt; 中。
我们在这里选择了整流器(rectifier)，我们将得到ReLUs。
我们现在来添加50%的退出率，以及另外800个单位的稠密层(dense layer)，和50%的退出率：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')
    network = tl.layers.DenseLayer(network, n_units=800, act = tf.nn.relu, name='relu2')
    network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，我们加入&lt;code&gt;n_units&lt;/code&gt;等于分类个数的全连接的输出层。注意，&lt;code&gt;cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))&lt;/code&gt; 在内部实现 Softmax，以提高计算效率，因此最后一层的输出为 identity ，更多细节请参考 &lt;code&gt;tl.cost.cross_entropy()&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.DenseLayer(network,
                                  n_units=10,
                                  act = tl.activation.identity,
                                  name='output_layer')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如上所述，因为每一层都被链接到了它的输入层，所以我们只需要在TensorLayer中将输出层接入一个网络：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    y = network.outputs
    y_op = tf.argmax(tf.nn.softmax(y), 1)
    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在这里，&lt;code&gt;network.outputs&lt;/code&gt; 是网络的10个特征的输出(按照一个热门的格式)。
&lt;code&gt;y_op&lt;/code&gt; 是代表类索引的整数输出， &lt;code&gt;cost&lt;/code&gt; 是目标和预测标签的交叉熵。&lt;/p&gt;
&lt;h3 id="jiang zao zi bian ma qi"&gt;降噪自编码器&lt;/h3&gt;
&lt;p&gt;自编码器是一种无监督学习（Unsupervisered Learning）模型，可从数据中学习出更好的表达，
目前已经用于逐层贪婪的预训练（Greedy layer-wise pre-train）。
有关自编码器内容，请参考教程 &lt;a href="http://deeplearning.stanford.edu/tutorial/"&gt;Deeplearning Tutorial&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;脚本 &lt;code&gt;main_test_denoise_AE()&lt;/code&gt; 实现了有50%的腐蚀率(corrosion rate)的降噪自编码器。
这个自编码器可以按如下方式定义，这里的 &lt;code&gt;DenseLayer&lt;/code&gt; 代表了一个自编码器：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    network = tl.layers.InputLayer(x, name='input_layer')
    network = tl.layers.DropoutLayer(network, keep=0.5, name='denoising1')
    network = tl.layers.DenseLayer(network, n_units=200, act=tf.nn.sigmoid, name='sigmoid1')
    recon_layer1 = tl.layers.ReconLayer(network,
                                        x_recon=x,
                                        n_units=784,
                                        act=tf.nn.sigmoid,
                                        name='recon_layer1')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练 &lt;code&gt;DenseLayer&lt;/code&gt; ，只需要运行 &lt;code&gt;ReconLayer.Pretrain()&lt;/code&gt; 即可。
如果要使用降噪自编码器，腐蚀层(corrosion layer)(&lt;code&gt;DropoutLayer&lt;/code&gt;)的名字需要按后面说的指定。
如果要保存特征图像，设置 &lt;code&gt;save&lt;/code&gt; 为 True 。
根据不同的架构和应用这里可以设置许多预训练的度量(metric)&lt;/p&gt;
&lt;p&gt;对于 sigmoid型激活函数来说，自编码器可以用KL散度来实现。
而对于整流器(Rectifier)来说，对激活函数输出的L1正则化能使得输出变得稀疏。
所以 &lt;code&gt;ReconLayer&lt;/code&gt; 默认只对整流激活函数(ReLU)提供KLD和交叉熵这两种损失度量，而对sigmoid型激活函数提供均方误差以及激活输出的L1范数这两种损失度量。
我们建议您修改 &lt;code&gt;ReconLayer&lt;/code&gt; 来实现自己的预训练度量。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    recon_layer1.pretrain(sess,
                          x=x,
                          X_train=X_train,
                          X_val=X_val,
                          denoise_name='denoising1',
                          n_epoch=200,
                          batch_size=128,
                          print_freq=10,
                          save=True,
                          save_name='w1pre_')
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;此外，脚本 &lt;code&gt;main_test_stacked_denoise_AE()&lt;/code&gt; 展示了如何将多个自编码器堆叠到一个网络，然后进行微调。&lt;/p&gt;
&lt;h3 id="juan ji shen jing wang luo"&gt;卷积神经网络&lt;/h3&gt;
&lt;p&gt;最后，&lt;code&gt;main_test_cnn_layer()&lt;/code&gt; 脚本创建了两个CNN层和最大汇流阶段(max pooling stages)，一个全连接的隐藏层和一个全连接的输出层。&lt;/p&gt;
&lt;p&gt;首先，我们用 &lt;code&gt;Conv2dLayer&lt;/code&gt;添加一个卷积层，它带有32个5x5的过滤器，紧接是2x2维度的最大池化。接着是64个5x5的过滤器的卷积层和同样的最大池化。之后，用｀FlattenLayer｀把4维输出转为1维向量，和50%的dropout在最后的隐层中。这里的&lt;code&gt;?&lt;/code&gt;表示每批数量(batch_size)。&lt;/p&gt;
&lt;p&gt;注，&lt;code&gt;tutorial_mnist.py&lt;/code&gt; 中介绍了针对初学者的简化版的 CNN API。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;network = tl.layers.InputLayer(x, name='input_layer')
network = tl.layers.Conv2dLayer(network,
                        act = tf.nn.relu,
                        shape = [5, 5, 1, 32],  # 32 features for each 5x5 patch
                        strides=[1, 1, 1, 1],
                        padding='SAME',
                        name ='cnn_layer1')     # output: (?, 28, 28, 32)
network = tl.layers.PoolLayer(network,
                        ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1],
                        padding='SAME',
                        pool = tf.nn.max_pool,
                        name ='pool_layer1',)   # output: (?, 14, 14, 32)
network = tl.layers.Conv2dLayer(network,
                        act = tf.nn.relu,
                        shape = [5, 5, 32, 64], # 64 features for each 5x5 patch
                        strides=[1, 1, 1, 1],
                        padding='SAME',
                        name ='cnn_layer2')     # output: (?, 14, 14, 64)
network = tl.layers.PoolLayer(network,
                        ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1],
                        padding='SAME',
                        pool = tf.nn.max_pool,
                        name ='pool_layer2',)   # output: (?, 7, 7, 64)
network = tl.layers.FlattenLayer(network, name='flatten_layer')
                                                # output: (?, 3136)
network = tl.layers.DropoutLayer(network, keep=0.5, name='drop1')
                                                # output: (?, 3136)
network = tl.layers.DenseLayer(network, n_units=256, act = tf.nn.relu, name='relu1')
                                                # output: (?, 256)
network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')
                                                # output: (?, 256)
network = tl.layers.DenseLayer(network, n_units=10, act = tl.identity, name='output_layer')
                                                # output: (?, 10)
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;对于专家们来说， &lt;code&gt;Conv2dLayer&lt;/code&gt; 将使用 &lt;code&gt;tensorflow.nn.conv2d&lt;/code&gt; ,TensorFlow默认的卷积方式来创建一个卷积层。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="xun lian mo xing"&gt;训练模型&lt;/h3&gt;
&lt;p&gt;在 &lt;code&gt;tutorial_mnist.py&lt;/code&gt; 脚本的其余部分，仅使用交叉熵代价函数来对MNIST数据集进行训练循环。&lt;/p&gt;
&lt;h4&gt;数据集迭代&lt;/h4&gt;
&lt;p&gt;迭代函数分别对inputs 和 targets 两个numpy数组，按照小批量的数量尺度进行迭代。
更多有关迭代函数的说明，可以在 &lt;code&gt;tensorlayer.iterate&lt;/code&gt; 中找到。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    tl.iterate.minibatches(inputs, targets, batchsize, shuffle=False)
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;损失和更新公式&lt;/h4&gt;
&lt;p&gt;我们继续创建一个在训练中被最小化的损失表达式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    y = network.outputs
    y_op = tf.argmax(tf.nn.softmax(y), 1)
    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;举 &lt;code&gt;main_test_layers()&lt;/code&gt; 这个例子来说，更多的成本或者正则化方法可以被应用在这里。
如果要在权重矩阵中应用最大模(max-norm)方法，你可以添加下列代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cost = cost + tl.cost.maxnorm_regularizer(1.0)(network.all_params[0]) +
                  tl.cost.maxnorm_regularizer(1.0)(network.all_params[2])
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;根据要解决的问题，您会需要使用不同的损失函数，更多有关损失函数的说明请见： &lt;code&gt;tensorlayer.cost&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;有了模型和定义的损失函数之后，我们就可以创建用于训练网络的更新公式。
接下去，我们将使用TensorFlow的优化器如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    train_params = network.all_params
    train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
        epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为了训练网络，我们需要提供数据和保持概率给 &lt;code&gt;feed_dict&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    feed_dict = {x: X_train_a, y_: y_train_a}
    feed_dict.update( network.all_drop )
    sess.run(train_op, feed_dict=feed_dict)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;同时为了进行验证和测试，我们这里用了略有不同的方法。
所有的Dropout，退连(DropConnect)，腐蚀层(Corrosion Layers)都将被禁用。
&lt;code&gt;tl.utils.dict_to_one&lt;/code&gt; 将会设置所有 &lt;code&gt;network.all_drop&lt;/code&gt; 值为1。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    dp_dict = tl.utils.dict_to_one( network.all_drop )
    feed_dict = {x: X_test_a, y_: y_test_a}
    feed_dict.update(dp_dict)
    err, ac = sess.run([cost, acc], feed_dict=feed_dict)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，作为一个额外的监测量，我们需要创建一个分类准确度的公式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    correct_prediction = tf.equal(tf.argmax(y, 1), y_)
    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;下一步？&lt;/h4&gt;
&lt;p&gt;在 &lt;code&gt;tutorial_cifar10_tfrecord.py&lt;/code&gt; 中我们还有更高级的图像分类的例子。
请阅读代码及注释，用以明白如何来生成更多的训练数据以及什么是局部响应正则化。
在这之后，您可以尝试着去实现 &lt;a href="http://doi.org/10.3389/fpsyg.2013.00124"&gt;残差网络(Residual Network)&lt;/a&gt;。
&lt;em&gt;小提示：您可能会用到Layer.outputs。&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="yun xing ping pang qiu li zi_1"&gt;运行乒乓球例子&lt;/h2&gt;
&lt;p&gt;在本教程的第二部分，我们将运行一个深度强化学习的例子，它在Karpathy的两篇博客 &lt;a href="http://karpathy.github.io/2016/05/31/rl/"&gt;Deep Reinforcement Learning:Pong from Pixels&lt;/a&gt; 有介绍。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_atari_pong.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在运行教程代码之前 你需要安装 &lt;a href="https://gym.openai.com/docs"&gt;OpenAI gym environment&lt;/a&gt; ,它是强化学习的一个标杆。
如果一切设置正确，您将得到一个类似以下的输出：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  [2016-07-12 09:31:59,760] Making new env: Pong-v0
    tensorlayer:Instantiate InputLayer input_layer (?, 6400)
    tensorlayer:Instantiate DenseLayer relu1: 200, relu
    tensorlayer:Instantiate DenseLayer output_layer: 3, identity
    param 0: (6400, 200) (mean: -0.000009, median: -0.000018 std: 0.017393)
    param 1: (200,) (mean: 0.000000, median: 0.000000 std: 0.000000)
    param 2: (200, 3) (mean: 0.002239, median: 0.003122 std: 0.096611)
    param 3: (3,) (mean: 0.000000, median: 0.000000 std: 0.000000)
    num of params: 1280803
    layer 0: Tensor("Relu:0", shape=(?, 200), dtype=float32)
    layer 1: Tensor("add_1:0", shape=(?, 3), dtype=float32)
  episode 0: game 0 took 0.17381s, reward: -1.000000
  episode 0: game 1 took 0.12629s, reward: 1.000000  !!!!!!!!
  episode 0: game 2 took 0.17082s, reward: -1.000000
  episode 0: game 3 took 0.08944s, reward: -1.000000
  episode 0: game 4 took 0.09446s, reward: -1.000000
  episode 0: game 5 took 0.09440s, reward: -1.000000
  episode 0: game 6 took 0.32798s, reward: -1.000000
  episode 0: game 7 took 0.74437s, reward: -1.000000
  episode 0: game 8 took 0.43013s, reward: -1.000000
  episode 0: game 9 took 0.42496s, reward: -1.000000
  episode 0: game 10 took 0.37128s, reward: -1.000000
  episode 0: game 11 took 0.08979s, reward: -1.000000
  episode 0: game 12 took 0.09138s, reward: -1.000000
  episode 0: game 13 took 0.09142s, reward: -1.000000
  episode 0: game 14 took 0.09639s, reward: -1.000000
  episode 0: game 15 took 0.09852s, reward: -1.000000
  episode 0: game 16 took 0.09984s, reward: -1.000000
  episode 0: game 17 took 0.09575s, reward: -1.000000
  episode 0: game 18 took 0.09416s, reward: -1.000000
  episode 0: game 19 took 0.08674s, reward: -1.000000
  episode 0: game 20 took 0.09628s, reward: -1.000000
  resetting env. episode reward total was -20.000000. running mean: -20.000000
  episode 1: game 0 took 0.09910s, reward: -1.000000
  episode 1: game 1 took 0.17056s, reward: -1.000000
  episode 1: game 2 took 0.09306s, reward: -1.000000
  episode 1: game 3 took 0.09556s, reward: -1.000000
  episode 1: game 4 took 0.12520s, reward: 1.000000  !!!!!!!!
  episode 1: game 5 took 0.17348s, reward: -1.000000
  episode 1: game 6 took 0.09415s, reward: -1.000000
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这个例子让电脑从屏幕输入来学习如何像人类一样打乒乓球。
在经过15000个序列的训练之后，计算机就可以赢得20%的比赛。
在20000个序列的训练之后，计算机可以赢得35%的比赛，
我们可以看到计算机学的越来越快，这是因为它有更多的胜利的数据来进行训练。
如果您用30000个序列来训练它，那么它会一直赢。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  render = False
  resume = False
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果您想显示游戏过程，那就设置 &lt;code&gt;render&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 。
当您再次运行该代码，您可以设置 &lt;code&gt;resume&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt;,那么代码将加载现有的模型并且会基于它进行训练。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/pong_game.jpeg" width="30%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;h2 id="li jie qiang hua xue xi"&gt;理解强化学习&lt;/h2&gt;
&lt;h3 id="ping pang qiu"&gt;乒乓球&lt;/h3&gt;
&lt;p&gt;要理解强化学习，我们要让电脑学习如何从原始的屏幕输入(像素输入)打乒乓球。
在我们开始之前，我们强烈建议您去浏览一个著名的博客叫做 &lt;a href="http://karpathy.github.io/2016/05/31/rl/"&gt;Deep Reinforcement Learning:pong from Pixels&lt;/a&gt; ,
这是使用python numpy库和OpenAI gym environment=来实现的一个深度强化学习的最简实现。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_atari_pong.py
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="ce lue wang luo (policy network)"&gt;策略网络(Policy Network)&lt;/h3&gt;
&lt;p&gt;在深度强化学习中，Policy Network 等同于 深度神经网络。
它是我们的选手(或者说&amp;ldquo;代理人(agent)&amp;rdquo;），它的输出告诉我们应该做什么(向上移动或向下移动)：
在Karpathy的代码中，他只定义了2个动作，向上移动和向下移动，并且仅使用单个simgoid输出：
为了使我们的教程更具有普遍性，我们使用3个 softmax 输出来定义向上移动，向下移动和停止(什么都不做)3个动作。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    # observation for training
    states_batch_pl = tf.placeholder(tf.float32, shape=[None, D])

    network = tl.layers.InputLayer(states_batch_pl, name='input_layer')
    network = tl.layers.DenseLayer(network, n_units=H,
                                    act = tf.nn.relu, name='relu1')
    network = tl.layers.DenseLayer(network, n_units=3,
                            act = tl.activation.identity, name='output_layer')
    probs = network.outputs
    sampling_prob = tf.nn.softmax(probs)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后我们的代理人就一直打乒乓球。它计算不同动作的概率，
并且之后会从这个均匀的分布中选取样本(动作)。
因为动作被1,2和3代表，但是softmax输出应该从0开始，所以我们从-1计算这个标签的价值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    prob = sess.run(
        sampling_prob,
        feed_dict={states_batch_pl: x}
    )
    # action. 1: STOP  2: UP  3: DOWN
    action = np.random.choice([1,2,3], p=prob.flatten())
    ...
    ys.append(action - 1)
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="ce lue bi jin (policy gradient)"&gt;策略逼近(Policy Gradient)&lt;/h3&gt;
&lt;p&gt;策略梯度下降法是一个end-to-end的算法，它直接学习从状态映射到动作的策略函数。
一个近似最优的策略可以通过最大化预期的奖励来直接学习。
策略函数的参数(例如，在乒乓球例子终使用的策略网络的参数)在预期奖励的近似值的引导下能够被训练和学习。
换句话说，我们可以通过更新它的参数来逐步调整策略函数，这样它能从给定的状态做出一系列行为来获得更高的奖励。&lt;/p&gt;
&lt;p&gt;策略迭代的一个替代算法就是深度Q-learning(DQN)。
他是基于Q-learning,学习一个映射状态和动作到一些值的价值函数的算法(叫Q函数)。
DQN采用了一个深度神经网络来作为Q函数的逼近来代表Q函数。
训练是通过最小化时序差分(temporal-difference)误差来实现。
一个名为&amp;ldquo;再体验(experience replay)&amp;rdquo;的神经生物学的启发式机制通常和DQN一起被使用来帮助提高非线性函数的逼近的稳定性&lt;/p&gt;
&lt;p&gt;您可以阅读以下文档，来得到对强化学习更好的理解：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html"&gt;Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf"&gt;Deep Reinforcement Learning. David Silver, Google DeepMind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;UCL Course on RL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;强化深度学习近些年来最成功的应用就是让模型去学习玩Atari的游戏。 AlphaGO同时也是使用类似的策略逼近方法来训练他们的策略网络而战胜了世界级的专业围棋选手。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"&gt;Atari - Playing Atari with Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html"&gt;Atari - Human-level control through deep reinforcement learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html"&gt;AlphaGO - Mastering the game of Go with deep neural networks and tree search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;数据集迭代&lt;/h4&gt;
&lt;p&gt;在强化学习中，我们把每场比赛所产生的所有决策来作为一个序列 (up,up,stop,...,down)。在乒乓球游戏中，比赛是在某一方达到21分后结束的，所以一个序列可能包含几十个决策。
然后我们可以设置一个批规模的大小，每一批包含一定数量的序列，基于这个批规模来更新我们的模型。
在本教程中，我们把每批规模设置成10个序列。使用RMSProp训练一个具有200个单元的隐藏层的2层策略网络&lt;/p&gt;
&lt;h4&gt;损失和更新公式&lt;/h4&gt;
&lt;p&gt;接着我们创建一个在训练中被最小化的损失公式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    actions_batch_pl = tf.placeholder(tf.int32, shape=[None])
    discount_rewards_batch_pl = tf.placeholder(tf.float32, shape=[None])
    loss = tl.rein.cross_entropy_reward_loss(probs, actions_batch_pl,
                                                  discount_rewards_batch_pl)
    ...
    ...
    sess.run(
        train_op,
        feed_dict={
            states_batch_pl: epx,
            actions_batch_pl: epy,
            discount_rewards_batch_pl: disR
        }
    )
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;一batch的损失和一个batch内的策略网络的所有输出，所有的我们做出的动作和相应的被打折的奖励有关
我们首先通过累加被打折的奖励和实际输出和真实动作的交叉熵计算每一个动作的损失。
最后的损失是所有动作的损失的和。&lt;/p&gt;
&lt;h3 id="xia yi bu ?"&gt;下一步?&lt;/h3&gt;
&lt;p&gt;上述教程展示了您如何去建立自己的代理人，end-to-end。
虽然它有很合理的品质，但它的默认参数不会给你最好的代理人模型。
这有一些您可以优化的内容。&lt;/p&gt;
&lt;p&gt;首先，与传统的MLP模型不同，比起 &lt;a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt; 更好的是我们可以使用CNNs来采集屏幕信息&lt;/p&gt;
&lt;p&gt;另外这个模型默认参数没有调整，您可以更改学习率，衰退率，或者用不同的方式来初始化您的模型的权重。&lt;/p&gt;
&lt;p&gt;最后，您可以尝试不同任务(游戏)的模型。&lt;/p&gt;
&lt;h2 id="yun xing word2vecli zi_1"&gt;运行Word2Vec例子&lt;/h2&gt;
&lt;p&gt;在教程的这一部分，我们训练一个词嵌套矩阵，每个词可以通过矩阵中唯一的行向量来表示。
在训练结束时，意思类似的单词会有相识的词向量。
在代码的最后，我们通过把单词放到一个平面上来可视化，我们可以看到相似的单词会被聚集在一起。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_word2vec_basic.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果一切设置正确，您最后会得到如下的可视化图。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/tsne.png" width="100%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;h2 id="li jie ci qian tao"&gt;理解词嵌套&lt;/h2&gt;
&lt;h3 id="ci qian tao (qian ru )"&gt;词嵌套（嵌入）&lt;/h3&gt;
&lt;p&gt;我们强烈建议您先阅读Colah的博客 &lt;a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"&gt;Word Representations&lt;/a&gt; &lt;a href="http://dataunion.org/9331.html"&gt;(中文翻译)&lt;/a&gt; ，
以理解为什么我们要使用一个向量来表示一个单词。更多Word2vec的细节可以在 &lt;a href="http://arxiv.org/abs/1411.2738"&gt;Word2vec Parameter Learning Explained&lt;/a&gt; 中找到。&lt;/p&gt;
&lt;p&gt;基本来说，训练一个嵌套矩阵是一个非监督学习的过程。一个单词使用唯一的ID来表示，而这个ID号就是嵌套矩阵的行号（row index），对应的行向量就是用来表示该单词的，使用向量来表示单词可以更好地表达单词的意思。比如，有4个单词的向量， &lt;code&gt;woman &amp;minus; man = queen - king&lt;/code&gt; ，这个例子中可以看到，嵌套矩阵中有一个纬度是用来表示性别的。&lt;/p&gt;
&lt;p&gt;定义一个Word2vec词嵌套矩阵如下。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # train_inputs is a row vector, a input is an integer id of single word.
  # train_labels is a column vector, a label is an integer id of single word.
  # valid_dataset is a column vector, a valid set is an integer id of single word.
  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

  # Look up embeddings for inputs.
  emb_net = tl.layers.Word2vecEmbeddingInputlayer(
          inputs = train_inputs,
          train_labels = train_labels,
          vocabulary_size = vocabulary_size,
          embedding_size = embedding_size,
          num_sampled = num_sampled,
          nce_loss_args = {},
          E_init = tf.random_uniform_initializer(minval=-1.0, maxval=1.0),
          E_init_args = {},
          nce_W_init = tf.truncated_normal_initializer(
                            stddev=float(1.0/np.sqrt(embedding_size))),
          nce_W_init_args = {},
          nce_b_init = tf.constant_initializer(value=0.0),
          nce_b_init_args = {},
          name ='word2vec_layer',
      )
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;数据迭代和损失函数&lt;/h4&gt;
&lt;p&gt;Word2vec使用负采样（Negative sampling）和Skip-gram模型进行训练。
噪音对比估计损失（NCE）会帮助减少损失函数的计算量，加快训练速度。
Skip-Gram 将文本（context）和目标（target）反转，尝试从目标单词预测目标文本单词。
我们使用 &lt;code&gt;tl.nlp.generate_skip_gram_batch&lt;/code&gt; 函数来生成训练数据，如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # NCE损失函数由 Word2vecEmbeddingInputlayer 提供
  cost = emb_net.nce_cost
  train_params = emb_net.all_params

  train_op = tf.train.AdagradOptimizer(learning_rate, initial_accumulator_value=0.1,
            use_locking=False).minimize(cost, var_list=train_params)

  data_index = 0
  while (step &amp;lt; num_steps):
    batch_inputs, batch_labels, data_index = tl.nlp.generate_skip_gram_batch(
                  data=data, batch_size=batch_size, num_skips=num_skips,
                  skip_window=skip_window, data_index=data_index)
    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}
    _, loss_val = sess.run([train_op, cost], feed_dict=feed_dict)
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;加载已训练好的的词嵌套矩阵&lt;/h4&gt;
&lt;p&gt;在训练嵌套矩阵的最后，我们保存矩阵及其词汇表、单词转ID字典、ID转单词字典。
然后，当下次做实际应用时，可以想下面的代码中那样加载这个已经训练好的矩阵和字典，
参考 &lt;code&gt;tutorial_generate_text.py&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  vocabulary_size = 50000
  embedding_size = 128
  model_file_name = "model_word2vec_50k_128"
  batch_size = None

  print("Load existing embedding matrix and dictionaries")
  all_var = tl.files.load_npy_to_any(name=model_file_name+'.npy')
  data = all_var['data']; count = all_var['count']
  dictionary = all_var['dictionary']
  reverse_dictionary = all_var['reverse_dictionary']

  tl.nlp.save_vocab(count, name='vocab_'+model_file_name+'.txt')

  del all_var, data, count

  load_params = tl.files.load_npz(name=model_file_name+'.npz')

  x = tf.placeholder(tf.int32, shape=[batch_size])
  y_ = tf.placeholder(tf.int32, shape=[batch_size, 1])

  emb_net = tl.layers.EmbeddingInputlayer(
                  inputs = x,
                  vocabulary_size = vocabulary_size,
                  embedding_size = embedding_size,
                  name ='embedding_layer')

  tl.layers.initialize_global_variables(sess)

  tl.files.assign_params(sess, [load_params[0]], emb_net)
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="yun xing ptbli zi_1"&gt;运行PTB例子&lt;/h2&gt;
&lt;p&gt;Penn TreeBank（PTB）数据集被用在很多语言建模（Language Modeling）的论文中，包括"Empirical Evaluation and Combination of Advanced Language Modeling Techniques"和
&amp;ldquo;Recurrent Neural Network Regularization&amp;rdquo;。该数据集的训练集有929k个单词，验证集有73K个单词，测试集有82k个单词。
在它的词汇表刚好有10k个单词。&lt;/p&gt;
&lt;p&gt;PTB例子是为了展示如何用递归神经网络（Recurrent Neural Network）来进行语言建模的。&lt;/p&gt;
&lt;p&gt;给一句话 "I am from Imperial College London", 这个模型可以从中学习出如何从&amp;ldquo;from Imperial College&amp;rdquo;来预测出&amp;ldquo;Imperial College London&amp;rdquo;。也就是说，它根据之前输入的单词序列来预测出下一步输出的单词序列，在刚才的例子中 &lt;code&gt;num_steps (序列长度，sequence length)&lt;/code&gt; 为 3。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_ptb_lstm.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;该脚本提供三种设置(小，中，大)，越大的模型有越好的建模性能，您可以修改下面的代码片段来选择不同的模型设置。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  flags.DEFINE_string(
      "model", "small",
      "A type of model. Possible options are: small, medium, large.")
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果您选择小设置，您将会看到：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Epoch: 1 Learning rate: 1.000
  0.004 perplexity: 5220.213 speed: 7635 wps
  0.104 perplexity: 828.871 speed: 8469 wps
  0.204 perplexity: 614.071 speed: 8839 wps
  0.304 perplexity: 495.485 speed: 8889 wps
  0.404 perplexity: 427.381 speed: 8940 wps
  0.504 perplexity: 383.063 speed: 8920 wps
  0.604 perplexity: 345.135 speed: 8920 wps
  0.703 perplexity: 319.263 speed: 8949 wps
  0.803 perplexity: 298.774 speed: 8975 wps
  0.903 perplexity: 279.817 speed: 8986 wps
  Epoch: 1 Train Perplexity: 265.558
  Epoch: 1 Valid Perplexity: 178.436
  ...
  Epoch: 13 Learning rate: 0.004
  0.004 perplexity: 56.122 speed: 8594 wps
  0.104 perplexity: 40.793 speed: 9186 wps
  0.204 perplexity: 44.527 speed: 9117 wps
  0.304 perplexity: 42.668 speed: 9214 wps
  0.404 perplexity: 41.943 speed: 9269 wps
  0.504 perplexity: 41.286 speed: 9271 wps
  0.604 perplexity: 39.989 speed: 9244 wps
  0.703 perplexity: 39.403 speed: 9236 wps
  0.803 perplexity: 38.742 speed: 9229 wps
  0.903 perplexity: 37.430 speed: 9240 wps
  Epoch: 13 Train Perplexity: 36.643
  Epoch: 13 Valid Perplexity: 121.475
  Test Perplexity: 116.716
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;PTB例子证明了递归神经网络能够实现语言建模，但是这个例子并没有做什么实际的事情。
在做具体应用之前，您应该浏览这个例子的代码和下一章 &amp;ldquo;理解 LSTM&amp;rdquo; 来学好递归神经网络的基础。
之后，您将学习如何用递归神经网络来生成文本，如何实现语言翻译和问题应答系统。&lt;/p&gt;
&lt;h2 id="li jie lstm"&gt;理解LSTM&lt;/h2&gt;
&lt;h3 id="di gui shen jing wang luo  (recurrent neural network)"&gt;递归神经网络 (Recurrent Neural Network)&lt;/h3&gt;
&lt;p&gt;我们认为Andrey Karpathy的博客 &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;Understand Recurrent Neural Network&lt;/a&gt; 是了解递归神经网络最好的材料。
读完这个博客后，Colah的博客 &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understand LSTM Network&lt;/a&gt; 能帮助你了解LSTM。
我们在这里不介绍更多关于递归神经网络的内容，所以在你继续下面的内容之前，请先阅读我们建议阅读的博客。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/karpathy_rnn.jpeg" width="100%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;图片由Andrey Karpathy提供&lt;/p&gt;
&lt;h3 id="tong bu shu ru yu shu chu xu lie  (synced sequence input and output)"&gt;同步输入与输出序列 (Synced sequence input and output)&lt;/h3&gt;
&lt;p&gt;PTB例子中的模型是一个典型的同步输入与输出，Karpathy 把它描述为
&amp;ldquo;(5) 同步序列输入与输出(例如视频分类中我们希望对每一帧进行标记)。&amp;ldquo;&lt;/p&gt;
&lt;p&gt;模型的构建如下，第一层是词嵌套层（嵌入），把每一个单词转换成对应的词向量，在该例子中没有使用预先训练好的
嵌套矩阵。第二，堆叠两层LSTM，使用Dropout来实现规则化，防止overfitting。
最后，使用全连接层输出一序列的softmax输出。&lt;/p&gt;
&lt;p&gt;第一层LSTM的输出形状是 [batch_size, num_steps, hidden_size]，这是为了让下一层LSTM可以堆叠在其上面。
第二层LSTM的输出形状是 [batch_size&lt;em&gt;num_steps, hidden_size]，这是为了让输出层（全连接层 Dense）可以堆叠在其上面。
然后计算每个样本的softmax输出，样本总数为 n_examples = batch_size&lt;/em&gt;num_steps。&lt;/p&gt;
&lt;p&gt;若想要更进一步理解该PTB教程，您也可以阅读 &lt;a href="https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html#recurrent-neural-networks"&gt;TensorFlow 官方的PTB教程&lt;/a&gt; ，中文翻译请见极客学院。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  network = tl.layers.EmbeddingInputlayer(
              inputs = x,
              vocabulary_size = vocab_size,
              embedding_size = hidden_size,
              E_init = tf.random_uniform_initializer(-init_scale, init_scale),
              name ='embedding_layer')
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop1')
  network = tl.layers.RNNLayer(network,
              cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
              cell_init_args={'forget_bias': 0.0},
              n_hidden=hidden_size,
              initializer=tf.random_uniform_initializer(-init_scale, init_scale),
              n_steps=num_steps,
              return_last=False,
              name='basic_lstm_layer1')
  lstm1 = network
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop2')
  network = tl.layers.RNNLayer(network,
              cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
              cell_init_args={'forget_bias': 0.0},
              n_hidden=hidden_size,
              initializer=tf.random_uniform_initializer(-init_scale, init_scale),
              n_steps=num_steps,
              return_last=False,
              return_seq_2d=True,
              name='basic_lstm_layer2')
  lstm2 = network
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop3')
  network = tl.layers.DenseLayer(network,
              n_units=vocab_size,
              W_init=tf.random_uniform_initializer(-init_scale, init_scale),
              b_init=tf.random_uniform_initializer(-init_scale, init_scale),
              act = tl.activation.identity, name='output_layer')
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;数据迭代&lt;/h4&gt;
&lt;p&gt;batch_size 数值可以被视为并行计算的数量。
如下面的例子所示，第一个 batch 使用 0 到 9 来学习序列信息。
第二个 batch 使用 10 到 19 来学习序列。
所以它忽略了 9 到 10 之间的信息。
只当我们 bath_size 设为 1，它才使用 0 到 20 之间所有的序列信息来学习。&lt;/p&gt;
&lt;p&gt;这里的 batch_size 的意思与 MNIST 例子略有不同。
在 MNIST 例子，batch_size 是每次迭代中我们使用的样本数量，
而在 PTB 的例子中，batch_size 是为加快训练速度的并行进程数。&lt;/p&gt;
&lt;p&gt;虽然当 batch_size &amp;gt; 1 时有些信息将会被忽略，
但是如果你的数据是足够长的（一个语料库通常有几十亿个字），被忽略的信息不会影响最终的结果。&lt;/p&gt;
&lt;p&gt;在PTB教程中，我们设置了 batch_size = 20，所以，我们将整个数据集拆分成 20 段（segment）。
在每一轮（epoch）的开始时，我们有 20 个初始化的 LSTM 状态（State），然后分别对 20 段数据进行迭代学习。&lt;/p&gt;
&lt;p&gt;训练数据迭代的例子如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  train_data = [i for i in range(20)]
  for batch in tl.iterate.ptb_iterator(train_data, batch_size=2, num_steps=3):
      x, y = batch
      print(x, '\n',y)
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  ... [[ 0  1  2] &amp;lt;---x                       1st subset/ iteration
  ...  [10 11 12]]
  ... [[ 1  2  3] &amp;lt;---y
  ...  [11 12 13]]
  ...
  ... [[ 3  4  5]  &amp;lt;--- 1st batch input       2nd subset/ iteration
  ...  [13 14 15]] &amp;lt;--- 2nd batch input
  ... [[ 4  5  6]  &amp;lt;--- 1st batch target
  ...  [14 15 16]] &amp;lt;--- 2nd batch target
  ...
  ... [[ 6  7  8]                             3rd subset/ iteration
  ...  [16 17 18]]
  ... [[ 7  8  9]
  ...  [17 18 19]]
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; 这个例子可以当作词嵌套矩阵的预训练。
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;损失和更新公式&lt;/h4&gt;
&lt;p&gt;损失函数是一系列输出cross entropy的均值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # 更多细节请见 tensorlayer.cost.cross_entropy_seq()
  def loss_fn(outputs, targets, batch_size, num_steps):
      # Returns the cost function of Cross-entropy of two sequences, implement
      # softmax internally.
      # outputs : 2D tensor [batch_size*num_steps, n_units of output layer]
      # targets : 2D tensor [batch_size, num_steps], need to be reshaped.
      # n_examples = batch_size * num_steps
      # so
      # cost is the averaged cost of each mini-batch (concurrent process).
      loss = tf.nn.seq2seq.sequence_loss_by_example(
          [outputs],
          [tf.reshape(targets, [-1])],
          [tf.ones([batch_size * num_steps])])
      cost = tf.reduce_sum(loss) / batch_size
      return cost

  # Cost for Training
  cost = loss_fn(network.outputs, targets, batch_size, num_steps)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在训练时，该例子在若干个epoch之后（由 &lt;code&gt;max_epoch&lt;/code&gt; 定义），才开始按比例下降学习率（learning rate），新学习率是前一个epoch的学习率乘以一个下降率（由 &lt;code&gt;lr_decay&lt;/code&gt; 定义）。
此外，截断反向传播（truncated backpropagation）截断了&lt;/p&gt;
&lt;p&gt;为使学习过程易于处理，通常的做法是将反向传播的梯度在（按时间）展开的步骤上照一个固定长度( &lt;code&gt;num_steps&lt;/code&gt; )截断。 通过在一次迭代中的每个时刻上提供长度为 &lt;code&gt;num_steps&lt;/code&gt; 的输入和每次迭代完成之后反向传导，这会很容易实现。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # 截断反响传播 Truncated Backpropagation for training
  with tf.variable_scope('learning_rate'):
      lr = tf.Variable(0.0, trainable=False)
  tvars = tf.trainable_variables()
  grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                    max_grad_norm)
  optimizer = tf.train.GradientDescentOptimizer(lr)
  train_op = optimizer.apply_gradients(zip(grads, tvars))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果当前epoch值大于 &lt;code&gt;max_epoch&lt;/code&gt; ，则把当前学习率乘以 &lt;code&gt;lr_decay&lt;/code&gt; 来降低学习率。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  new_lr_decay = lr_decay ** max(i - max_epoch, 0.0)
  sess.run(tf.assign(lr, learning_rate * new_lr_decay))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在每一个epoch的开始之前，LSTM的状态要被重置为零状态；在每一个迭代之后，LSTM状态都会被改变，所以要把最新的LSTM状态
作为下一个迭代的初始化状态。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  # 在每一个epoch之前，把所有LSTM状态设为零状态
  state1 = tl.layers.initialize_rnn_state(lstm1.initial_state)
  state2 = tl.layers.initialize_rnn_state(lstm2.initial_state)
  for step, (x, y) in enumerate(tl.iterate.ptb_iterator(train_data,
                                              batch_size, num_steps)):
      feed_dict = {input_data: x, targets: y,
                  lstm1.initial_state: state1,
                  lstm2.initial_state: state2,
                  }
      # 启用dropout
      feed_dict.update( network.all_drop )
      # 把新的状态作为下一个迭代的初始状态
      _cost, state1, state2, _ = sess.run([cost,
                                      lstm1.final_state,
                                      lstm2.final_state,
                                      train_op],
                                      feed_dict=feed_dict
                                      )
      costs += _cost; iters += num_steps
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;预测&lt;/h4&gt;
&lt;p&gt;在训练完模型之后，当我们预测下一个输出时，我们不需要考虑序列长度了，因此 &lt;code&gt;batch_size&lt;/code&gt; 和 &lt;code&gt;num_steps&lt;/code&gt; 都设为 1 。
然后，我们可以一步一步地输出下一个单词，而不是通过一序列的单词来输出一序列的单词。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  input_data_test = tf.placeholder(tf.int32, [1, 1])
  targets_test = tf.placeholder(tf.int32, [1, 1])
  ...
  network_test, lstm1_test, lstm2_test = inference(input_data_test,
                        is_training=False, num_steps=1, reuse=True)
  ...
  cost_test = loss_fn(network_test.outputs, targets_test, 1, 1)
  ...
  print("Evaluation")
  # 测试
  # go through the test set step by step, it will take a while.
  start_time = time.time()
  costs = 0.0; iters = 0
  # 与训练时一样，设置所有LSTM状态为零状态
  state1 = tl.layers.initialize_rnn_state(lstm1_test.initial_state)
  state2 = tl.layers.initialize_rnn_state(lstm2_test.initial_state)
  for step, (x, y) in enumerate(tl.iterate.ptb_iterator(test_data,
                                          batch_size=1, num_steps=1)):
      feed_dict = {input_data_test: x, targets_test: y,
                  lstm1_test.initial_state: state1,
                  lstm2_test.initial_state: state2,
                  }
      _cost, state1, state2 = sess.run([cost_test,
                                      lstm1_test.final_state,
                                      lstm2_test.final_state],
                                      feed_dict=feed_dict
                                      )
      costs += _cost; iters += 1
  test_perplexity = np.exp(costs / iters)
  print("Test Perplexity: %.3f took %.2fs" % (test_perplexity, time.time() - start_time))
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="xia yi bu ?_1"&gt;下一步？&lt;/h3&gt;
&lt;p&gt;您已经明白了同步序列输入和序列输出（Synced sequence input and output）。
现在让我们思考下序列输入单一输出的情况（Sequence input and one output），
LSTM 也可以学会通过给定一序列输入如 &amp;ldquo;我来自北京，我会说.." 来输出
一个单词 "中文"。&lt;/p&gt;
&lt;p&gt;请仔细阅读并理解 &lt;code&gt;tutorial_generate_text.py&lt;/code&gt; 的代码，它讲了如何加载一个已经训练好的词嵌套矩阵，
以及如何给定机器一个文档，让它来学习文字自动生成。&lt;/p&gt;
&lt;p&gt;Karpathy的博客：
"(3) Sequence input (e.g. sentiment analysis where a given sentence is
classified as expressing positive or negative sentiment). "&lt;/p&gt;
&lt;h2 id="yun xing ji qi fan yi li zi_1"&gt;运行机器翻译例子&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  python tutorial_translate.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;该脚本将训练一个神经网络来把英文翻译成法文。
如果一切正常，您将看到：
- 下载WMT英文-法文翻译数据库，包括训练集和测试集。
- 通过训练集创建英文和法文的词汇表。
- 把训练集和测试集的单词转换成数字ID表示。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Prepare raw data
  Load or Download WMT English-to-French translation &amp;gt; wmt
  Training data : wmt/giga-fren.release2
  Testing data : wmt/newstest2013

  Create vocabularies
  Vocabulary of French : wmt/vocab40000.fr
  Vocabulary of English : wmt/vocab40000.en
  Creating vocabulary wmt/vocab40000.fr from data wmt/giga-fren.release2.fr
    processing line 100000
    processing line 200000
    processing line 300000
    processing line 400000
    processing line 500000
    processing line 600000
    processing line 700000
    processing line 800000
    processing line 900000
    processing line 1000000
    processing line 1100000
    processing line 1200000
    ...
    processing line 22500000
  Creating vocabulary wmt/vocab40000.en from data wmt/giga-fren.release2.en
    processing line 100000
    ...
    processing line 22500000

  ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;首先，我们从WMT'15网站上下载英语-法语翻译数据。训练数据和测试数据如下。
训练数据用于训练模型，测试数据用于评估该模型。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  wmt/training-giga-fren.tar  &amp;lt;-- 英文－法文训练集 (2.6GB)
                                  giga-fren.release2.* 从该文件解压出来
  wmt/dev-v2.tgz              &amp;lt;-- 多种语言的测试集 (21.4MB)
                                  newstest2013.* 从该文件解压出来

  wmt/giga-fren.release2.fr   &amp;lt;-- 法文训练集 (4.57GB)
  wmt/giga-fren.release2.en   &amp;lt;-- 英文训练集 (3.79GB)

  wmt/newstest2013.fr         &amp;lt;-- 法文测试集 (393KB)
  wmt/newstest2013.en         &amp;lt;-- 英文测试集 (333KB)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;所有 &lt;code&gt;giga-fren.release2.*&lt;/code&gt; 是训练数据， &lt;code&gt;giga-fren.release2.fr&lt;/code&gt; 内容如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Il a transform&amp;eacute; notre vie | Il a transform&amp;eacute; la soci&amp;eacute;t&amp;eacute; | Son fonctionnement | La technologie, moteur du changement Accueil | Concepts | Enseignants | Recherche | Aper&amp;ccedil;u | Collaborateurs | Web HHCC | Ressources | Commentaires Mus&amp;eacute;e virtuel du Canada
  Plan du site
  R&amp;eacute;troaction
  Cr&amp;eacute;dits
  English
  Qu&amp;rsquo;est-ce que la lumi&amp;egrave;re?
  La d&amp;eacute;couverte du spectre de la lumi&amp;egrave;re blanche Des codes dans la lumi&amp;egrave;re Le spectre &amp;eacute;lectromagn&amp;eacute;tique Les spectres d&amp;rsquo;&amp;eacute;mission Les spectres d&amp;rsquo;absorption Les ann&amp;eacute;es-lumi&amp;egrave;re La pollution lumineuse
  Le ciel des premiers habitants La vision contemporaine de l'Univers L&amp;rsquo;astronomie pour tous
  Bande dessin&amp;eacute;e
  Liens
  Glossaire
  Observatoires
  ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;giga-fren.release2.en&lt;/code&gt; 内容如下，我们可以看到单词或者句子用 &lt;code&gt;|&lt;/code&gt; 或 &lt;code&gt;\n&lt;/code&gt; 来分隔。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Changing Lives | Changing Society | How It Works | Technology Drives Change Home | Concepts | Teachers | Search | Overview | Credits | HHCC Web | Reference | Feedback Virtual Museum of Canada Home Page
  Site map
  Feedback
  Credits
  Fran&amp;ccedil;ais
  What is light ?
  The white light spectrum Codes in the light The electromagnetic spectrum Emission spectra Absorption spectra Light-years Light pollution
  The sky of the first inhabitants A contemporary vison of the Universe Astronomy for everyone
  Cartoon
  Links
  Glossary
  Observatories
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;测试数据 &lt;code&gt;newstest2013.en&lt;/code&gt; 和 &lt;code&gt;newstest2013.fr&lt;/code&gt; 如下所示：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  newstest2013.en :
  A Republican strategy to counter the re-election of Obama
  Republican leaders justified their policy by the need to combat electoral fraud.
  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.

  newstest2013.fr :
  Une strat&amp;eacute;gie r&amp;eacute;publicaine pour contrer la r&amp;eacute;&amp;eacute;lection d'Obama
  Les dirigeants r&amp;eacute;publicains justifi&amp;egrave;rent leur politique par la n&amp;eacute;cessit&amp;eacute; de lutter contre la fraude &amp;eacute;lectorale.
  Or, le Centre Brennan consid&amp;egrave;re cette derni&amp;egrave;re comme un mythe, affirmant que la fraude &amp;eacute;lectorale est plus rare aux &amp;Eacute;tats-Unis que le nombre de personnes tu&amp;eacute;es par la foudre.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;下载完数据之后，开始创建词汇表文件。
从训练数据 &lt;code&gt;giga-fren.release2.fr&lt;/code&gt; 和 &lt;code&gt;giga-fren.release2.en&lt;/code&gt;创建 &lt;code&gt;vocab40000.fr&lt;/code&gt; 和 &lt;code&gt;vocab40000.en&lt;/code&gt; 这个过程需要较长一段时间，数字 &lt;code&gt;40000&lt;/code&gt; 代表了词汇库的大小。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vocab40000.fr&lt;/code&gt; (381KB) 按下列所示地按每行一个单词的方式存储（one-item-per-line）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  _PAD
  _GO
  _EOS
  _UNK
  de
  ,
  .
  '
  la
  et
  des
  les
  &amp;agrave;
  le
  du
  l
  en
  )
  d
  0
  (
  00
  pour
  dans
  un
  que
  une
  sur
  au
  0000
  a
  par
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;vocab40000.en&lt;/code&gt; (344KB) 也是如此。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  _PAD
  _GO
  _EOS
  _UNK
  the
  .
  ,
  of
  and
  to
  in
  a
  )
  (
  0
  for
  00
  that
  is
  on
  The
  0000
  be
  by
  with
  or
  :
  as
  "
  000
  are
  ;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;接着我们开始创建英文和法文的数字化（ID）训练集和测试集。这也要较长一段时间。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Tokenize data
  Tokenizing data in wmt/giga-fren.release2.fr  &amp;lt;-- Training data of French
    tokenizing line 100000
    tokenizing line 200000
    tokenizing line 300000
    tokenizing line 400000
    ...
    tokenizing line 22500000
  Tokenizing data in wmt/giga-fren.release2.en  &amp;lt;-- Training data of English
    tokenizing line 100000
    tokenizing line 200000
    tokenizing line 300000
    tokenizing line 400000
    ...
    tokenizing line 22500000
  Tokenizing data in wmt/newstest2013.fr        &amp;lt;-- Testing data of French
  Tokenizing data in wmt/newstest2013.en        &amp;lt;-- Testing data of English
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，我们所有的文件如下所示：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  wmt/training-giga-fren.tar  &amp;lt;-- 英文－法文训练集 (2.6GB)
                                  giga-fren.release2.* 从该文件解压出来
  wmt/dev-v2.tgz              &amp;lt;-- 多种语言的测试集 (21.4MB)
                                  newstest2013.* 从该文件解压出来

  wmt/giga-fren.release2.fr   &amp;lt;-- 法文训练集 (4.57GB)
  wmt/giga-fren.release2.en   &amp;lt;-- 英文训练集 (3.79GB)

  wmt/newstest2013.fr         &amp;lt;-- 法文测试集 (393KB)
  wmt/newstest2013.en         &amp;lt;-- 英文测试集 (333KB)

  wmt/vocab40000.fr           &amp;lt;-- 法文词汇表 (381KB)
  wmt/vocab40000.en           &amp;lt;-- 英文词汇表 (344KB)

  wmt/giga-fren.release2.ids40000.fr   &amp;lt;-- 数字化法文训练集 (2.81GB)
  wmt/giga-fren.release2.ids40000.en   &amp;lt;-- 数字化英文训练集 (2.38GB)

  wmt/newstest2013.ids40000.fr         &amp;lt;-- 数字化法文训练集 (268KB)
  wmt/newstest2013.ids40000.en         &amp;lt;-- 数字化英文测试集 (232KB)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;现在，把数字化的数据读入buckets中，并计算不同buckets中数据样本的个数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Read development (test) data into buckets
  dev data: (5, 10) [[13388, 4, 949], [23113, 8, 910, 2]]
  en word_ids: [13388, 4, 949]
  en context: [b'Preventing', b'the', b'disease']
  fr word_ids: [23113, 8, 910, 2]
  fr context: [b'Pr\xc3\xa9venir', b'la', b'maladie', b'_EOS']

  Read training data into buckets (limit: 0)
    reading data line 100000
    reading data line 200000
    reading data line 300000
    reading data line 400000
    reading data line 500000
    reading data line 600000
    reading data line 700000
    reading data line 800000
    ...
    reading data line 22400000
    reading data line 22500000
  train_bucket_sizes: [239121, 1344322, 5239557, 10445326]
  train_total_size: 17268326.0
  train_buckets_scale: [0.013847375825543252, 0.09169638099257565, 0.3951164693091849, 1.0]
  train data: (5, 10) [[1368, 3344], [1089, 14, 261, 2]]
  en word_ids: [1368, 3344]
  en context: [b'Site', b'map']
  fr word_ids: [1089, 14, 261, 2]
  fr context: [b'Plan', b'du', b'site', b'_EOS']

  the num of training data in each buckets: [239121, 1344322, 5239557, 10445326]
  the num of training data: 17268326
  train_buckets_scale: [0.013847375825543252, 0.09169638099257565, 0.3951164693091849, 1.0]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后开始训练模型，当 &lt;code&gt;steps_per_checkpoint = 10&lt;/code&gt; 时，您将看到：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;steps_per_checkpoint = 10&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Create Embedding Attention Seq2seq Model

  global step 10 learning rate 0.5000 step-time 22.26 perplexity 12761.50
    eval: bucket 0 perplexity 5887.75
    eval: bucket 1 perplexity 3891.96
    eval: bucket 2 perplexity 3748.77
    eval: bucket 3 perplexity 4940.10
  global step 20 learning rate 0.5000 step-time 20.38 perplexity 28761.36
    eval: bucket 0 perplexity 10137.01
    eval: bucket 1 perplexity 12809.90
    eval: bucket 2 perplexity 15758.65
    eval: bucket 3 perplexity 26760.93
  global step 30 learning rate 0.5000 step-time 20.64 perplexity 6372.95
    eval: bucket 0 perplexity 1789.80
    eval: bucket 1 perplexity 1690.00
    eval: bucket 2 perplexity 2190.18
    eval: bucket 3 perplexity 3808.12
  global step 40 learning rate 0.5000 step-time 16.10 perplexity 3418.93
    eval: bucket 0 perplexity 4778.76
    eval: bucket 1 perplexity 3698.90
    eval: bucket 2 perplexity 3902.37
    eval: bucket 3 perplexity 22612.44
  global step 50 learning rate 0.5000 step-time 14.84 perplexity 1811.02
    eval: bucket 0 perplexity 644.72
    eval: bucket 1 perplexity 759.16
    eval: bucket 2 perplexity 984.18
    eval: bucket 3 perplexity 1585.68
  global step 60 learning rate 0.5000 step-time 19.76 perplexity 1580.55
    eval: bucket 0 perplexity 1724.84
    eval: bucket 1 perplexity 2292.24
    eval: bucket 2 perplexity 2698.52
    eval: bucket 3 perplexity 3189.30
  global step 70 learning rate 0.5000 step-time 17.16 perplexity 1250.57
    eval: bucket 0 perplexity 298.55
    eval: bucket 1 perplexity 502.04
    eval: bucket 2 perplexity 645.44
    eval: bucket 3 perplexity 604.29
  global step 80 learning rate 0.5000 step-time 18.50 perplexity 793.90
    eval: bucket 0 perplexity 2056.23
    eval: bucket 1 perplexity 1344.26
    eval: bucket 2 perplexity 767.82
    eval: bucket 3 perplexity 649.38
  global step 90 learning rate 0.5000 step-time 12.61 perplexity 541.57
    eval: bucket 0 perplexity 180.86
    eval: bucket 1 perplexity 350.99
    eval: bucket 2 perplexity 326.85
    eval: bucket 3 perplexity 383.22
  global step 100 learning rate 0.5000 step-time 18.42 perplexity 471.12
    eval: bucket 0 perplexity 216.63
    eval: bucket 1 perplexity 348.96
    eval: bucket 2 perplexity 318.20
    eval: bucket 3 perplexity 389.92
  global step 110 learning rate 0.5000 step-time 18.39 perplexity 474.89
    eval: bucket 0 perplexity 8049.85
    eval: bucket 1 perplexity 1677.24
    eval: bucket 2 perplexity 936.98
    eval: bucket 3 perplexity 657.46
  global step 120 learning rate 0.5000 step-time 18.81 perplexity 832.11
    eval: bucket 0 perplexity 189.22
    eval: bucket 1 perplexity 360.69
    eval: bucket 2 perplexity 410.57
    eval: bucket 3 perplexity 456.40
  global step 130 learning rate 0.5000 step-time 20.34 perplexity 452.27
    eval: bucket 0 perplexity 196.93
    eval: bucket 1 perplexity 655.18
    eval: bucket 2 perplexity 860.44
    eval: bucket 3 perplexity 1062.36
  global step 140 learning rate 0.5000 step-time 21.05 perplexity 847.11
    eval: bucket 0 perplexity 391.88
    eval: bucket 1 perplexity 339.09
    eval: bucket 2 perplexity 320.08
    eval: bucket 3 perplexity 376.44
  global step 150 learning rate 0.4950 step-time 15.53 perplexity 590.03
    eval: bucket 0 perplexity 269.16
    eval: bucket 1 perplexity 286.51
    eval: bucket 2 perplexity 391.78
    eval: bucket 3 perplexity 485.23
  global step 160 learning rate 0.4950 step-time 19.36 perplexity 400.80
    eval: bucket 0 perplexity 137.00
    eval: bucket 1 perplexity 198.85
    eval: bucket 2 perplexity 276.58
    eval: bucket 3 perplexity 357.78
  global step 170 learning rate 0.4950 step-time 17.50 perplexity 541.79
    eval: bucket 0 perplexity 1051.29
    eval: bucket 1 perplexity 626.64
    eval: bucket 2 perplexity 496.32
    eval: bucket 3 perplexity 458.85
  global step 180 learning rate 0.4950 step-time 16.69 perplexity 400.65
    eval: bucket 0 perplexity 178.12
    eval: bucket 1 perplexity 299.86
    eval: bucket 2 perplexity 294.84
    eval: bucket 3 perplexity 296.46
  global step 190 learning rate 0.4950 step-time 19.93 perplexity 886.73
    eval: bucket 0 perplexity 860.60
    eval: bucket 1 perplexity 910.16
    eval: bucket 2 perplexity 909.24
    eval: bucket 3 perplexity 786.04
  global step 200 learning rate 0.4901 step-time 18.75 perplexity 449.64
    eval: bucket 0 perplexity 152.13
    eval: bucket 1 perplexity 234.41
    eval: bucket 2 perplexity 249.66
    eval: bucket 3 perplexity 285.95
  ...
  global step 980 learning rate 0.4215 step-time 18.31 perplexity 208.74
    eval: bucket 0 perplexity 78.45
    eval: bucket 1 perplexity 108.40
    eval: bucket 2 perplexity 137.83
    eval: bucket 3 perplexity 173.53
  global step 990 learning rate 0.4173 step-time 17.31 perplexity 175.05
    eval: bucket 0 perplexity 78.37
    eval: bucket 1 perplexity 119.72
    eval: bucket 2 perplexity 169.11
    eval: bucket 3 perplexity 202.89
  global step 1000 learning rate 0.4173 step-time 15.85 perplexity 174.33
    eval: bucket 0 perplexity 76.52
    eval: bucket 1 perplexity 125.97
    eval: bucket 2 perplexity 150.13
    eval: bucket 3 perplexity 181.07
  ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;经过350000轮训练模型之后，您可以将代码中的 &lt;code&gt;main_train()&lt;/code&gt; 换为 &lt;code&gt;main_decode()&lt;/code&gt; 来使用训练好的翻译器，
您输入一个英文句子，程序将输出一个对应的法文句子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Reading model parameters from wmt/translate.ckpt-350000
  &amp;gt;  Who is the president of the United States?
  Qui est le pr&amp;eacute;sident des &amp;Eacute;tats-Unis ?
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="li jie ji qi fan yi"&gt;理解机器翻译&lt;/h2&gt;
&lt;h3 id="seq2seq"&gt;Seq2seq&lt;/h3&gt;
&lt;p&gt;序列到序列模型（Seq2seq）通常被用来转换一种语言到另一种语言。
但实际上它能用来做很多您可能无法想象的事情，比如我们可以将一个长的句子翻译成意思一样但短且简单的句子，
再比如，从莎士比亚的语言翻译成现代英语。若用上卷积神经网络(CNN)的话，我们能将视频翻译成句子，则自动看一段视频给出该视频的文字描述（Video captioning）。&lt;/p&gt;
&lt;p&gt;如果你只是想用 Seq2seq，你只需要考虑训练集的格式，比如如何切分单词、如何数字化单词等等。
所以，在本教程中，我们将讨论很多如何整理训练集。&lt;/p&gt;
&lt;h4&gt;基础&lt;/h4&gt;
&lt;p&gt;序列到序列模型是一种多对多（Many to many）的模型，但与PTB教程中的同步序列输入与输出(Synced sequence input and output）不一样，Seq2seq是在输入了整个序列之后，才开始输出新的序列（非同步）。
该教程用了下列两种最新的方法来提高准确度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把输入序列倒转输入（Reversing the inputs）&lt;/li&gt;
&lt;li&gt;注意机制（Attention mechanism）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了要加快训练速度，我们使用了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;softmax 抽样（Sampled softmax）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Karpathy的博客是这样描述Seq2seq的："(4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French)."&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/basic_seq2seq.png" width="80%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;如上图所示，编码器输入（encoder input），解码器输入（decoder input）以及输出目标（targets）如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   encoder_input =  A    B    C
   decoder_input =  &amp;lt;go&amp;gt; W    X    Y    Z
   targets       =  W    X    Y    Z    &amp;lt;eos&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Note：在代码实现中，targets的长度比decoder_input的长度小一，更多实现细节将在下文说明。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;文献&lt;/h4&gt;
&lt;p&gt;该英语-法语的机器翻译例子使用了多层递归神经网络以及注意机制。
该模型和如下论文中一样：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1412.7449"&gt;Grammar as a Foreign Language&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该例子采用了 softmax 抽样（sampled softmax）来解决当词汇表很大时计算量大的问题。
在该例子中，&lt;code&gt;target_vocab_size=4000&lt;/code&gt; ，若词汇量小于 &lt;code&gt;512&lt;/code&gt; 时用普通的softmax cross entropy即可。
Softmax 抽样在这篇论文的第三小节中描述:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1412.2007"&gt;On Using Very Large Target Vocabulary for Neural Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下文章讲述了把输入序列倒转（Reversing the inputs）和多层神递归神经网络用在Seq2seq的翻译应用非常成功：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1409.3215"&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下文章讲述了注意机制（Attention Mechanism）让解码器可以更直接地得到每一个输入的信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1409.0473"&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下文章讲述了另一种Seq2seq模型，则使用双向编码器（Bi-directional encoder）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1409.0473"&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="shi xian xi jie"&gt;实现细节&lt;/h3&gt;
&lt;h4&gt;Bucketing and Padding&lt;/h4&gt;
&lt;p&gt;Bucketing 是一种能有效处理不同句子长度的方法，为什么使用Bucketing，在 &lt;a href="https://www.zhihu.com/question/42057513"&gt;知乎&lt;/a&gt;上已经有很好的回答了。&lt;/p&gt;
&lt;p&gt;当将英文翻译成法文的时，我们有不同长度的英文句子输入（长度为 &lt;code&gt;L1&lt;/code&gt; ），以及不同长度的法文句子输出，（长度为 &lt;code&gt;L2&lt;/code&gt; ）。
我们原则上要建立每一种长度的可能性，则有很多个 &lt;code&gt;(L1, L2+1)&lt;/code&gt; ，其中 &lt;code&gt;L2&lt;/code&gt; 加一是因为有 GO 标志符。&lt;/p&gt;
&lt;p&gt;为了减少 bucket 的数量以及为句子找到最合适的 bucket，若 bucket 大于句子的长度，我们则使用 PAD 标志符填充之。&lt;/p&gt;
&lt;p&gt;为了提高效率，我们只使用几个 bucket，然后使用 padding 来让句子匹配到最相近的 bucket 中。
在该例子中，我们使用如下 4 个 buckets。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果输入的是一个有 &lt;code&gt;3&lt;/code&gt; 个单词的英文句子，对应的法文输出有 &lt;code&gt;6&lt;/code&gt; 个单词，
那么改数据将被放在第一个 bucket 中并且把 encoder inputs 和 decoder inputs 通过 padding 来让其长度变成 &lt;code&gt;5&lt;/code&gt; 和 &lt;code&gt;10&lt;/code&gt; 。
如果我们有 &lt;code&gt;8&lt;/code&gt; 个单词的英文句子，及 &lt;code&gt;18&lt;/code&gt; 个单词的法文句子，它们会被放到 &lt;code&gt;(20, 25)&lt;/code&gt; 的 bucket 中。&lt;/p&gt;
&lt;p&gt;换句话说，bucket &lt;code&gt;(I,O)&lt;/code&gt; 是 (encoder_input_size，decoder_inputs_size) 。&lt;/p&gt;
&lt;p&gt;给出一对数字化训练样本 &lt;code&gt;[["I", "go", "."], ["Je", "vais", "."]]&lt;/code&gt; ，我们把它转换为 &lt;code&gt;(5,10)&lt;/code&gt; 。
编码器输入（encoder inputs）的训练数据为  &lt;code&gt;[PAD PAD "." "go" "I"]&lt;/code&gt; ，而解码器的输入（decoder inputs）为 &lt;code&gt;[GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]&lt;/code&gt; 。
而输出目标（targets）是解码器输入（decoder inputs）平移一位。 &lt;code&gt;target_weights&lt;/code&gt; 是输出目标（targets）的掩码。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  bucket = (I, O) = (5, 10)
  encoder_inputs = [PAD PAD "." "go" "I"]                       &amp;lt;-- 5  x batch_size
  decoder_inputs = [GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD] &amp;lt;-- 10 x batch_size
  target_weights = [1   1     1     1   0 0 0 0 0 0 0]          &amp;lt;-- 10 x batch_size
  targets        = ["Je" "vais" "." EOS PAD PAD PAD PAD PAD]    &amp;lt;-- 9  x batch_size
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在该代码中，一个句子是由一个列向量表示，假设 &lt;code&gt;batch_size = 3&lt;/code&gt; ， &lt;code&gt;bucket = (5, 10)&lt;/code&gt; ，训练集如下所示。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  encoder_inputs    decoder_inputs    target_weights    targets
  0    0    0       1    1    1       1    1    1       87   71   16748
  0    0    0       87   71   16748   1    1    1       2    3    14195
  0    0    0       2    3    14195   0    1    1       0    2    2
  0    0    3233    0    2    2       0    0    0       0    0    0
  3    698  4061    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中 0 : _PAD    1 : _GO     2 : _EOS      3 : _UNK&lt;/p&gt;
&lt;p&gt;在训练过程中，解码器输入是目标，而在预测过程中，下一个解码器的输入是最后一个解码器的输出。&lt;/p&gt;
&lt;p&gt;在训练过程中，编码器输入（decoder inputs）就是目标输出（targets）；
当使用模型时，下一个编码器输入（decoder inputs）是上一个解码器输出（ decoder output）。&lt;/p&gt;
&lt;h4&gt;特殊标志符、标点符号与阿拉伯数字&lt;/h4&gt;
&lt;p&gt;该例子中的特殊标志符是：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="n"&gt;_PAD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;"_PAD"&lt;/span&gt;
  &lt;span class="n"&gt;_GO&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;"_GO"&lt;/span&gt;
  &lt;span class="n"&gt;_EOS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;"_EOS"&lt;/span&gt;
  &lt;span class="n"&gt;_UNK&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;"_UNK"&lt;/span&gt;
  &lt;span class="n"&gt;PAD_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;      &lt;span class="o"&gt;&amp;lt;--&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="n"&gt;number&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;
  &lt;span class="n"&gt;GO_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
  &lt;span class="n"&gt;EOS_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="n"&gt;UNK_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
  &lt;span class="n"&gt;_START_VOCAB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;_PAD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_GO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_EOS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_UNK&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;          ID号    意义
  _PAD    0       Padding, empty word
  _GO     1       decoder_inputs 的第一个元素
  _EOS    2       targets 的结束符
  _UNK    3       不明单词（Unknown word），没有在词汇表出现的单词被标记为3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;对于阿拉伯数字，建立词汇表时与数字化数据集时的 &lt;code&gt;normalize_digits&lt;/code&gt; 必须是一致的，若
&lt;code&gt;normalize_digits=True&lt;/code&gt; 所有阿拉伯数字都将被 &lt;code&gt;0&lt;/code&gt; 代替。比如 &lt;code&gt;123&lt;/code&gt; 被 &lt;code&gt;000&lt;/code&gt; 代替，&lt;code&gt;9&lt;/code&gt; 被 &lt;code&gt;0&lt;/code&gt;代替
，&lt;code&gt;1990-05&lt;/code&gt; 被 &lt;code&gt;0000-00` 代替，最后&lt;/code&gt;000&lt;code&gt;，&lt;/code&gt;0&lt;code&gt;，&lt;/code&gt;0000-00&lt;code&gt;等将在词汇库中(看&lt;/code&gt;vocab40000.en`` )。&lt;/p&gt;
&lt;p&gt;反之，如果 &lt;code&gt;normalize_digits=False&lt;/code&gt; ，不同的阿拉伯数字将会放入词汇表中，那么词汇表就变得十分大了。
本例子中寻找阿拉伯数字使用的正则表达式是 &lt;code&gt;_DIGIT_RE = re.compile(br"\d")&lt;/code&gt; 。(详见 &lt;code&gt;tl.nlp.create_vocabulary()&lt;/code&gt; 和 &lt;code&gt;`tl.nlp.data_to_token_ids()&lt;/code&gt; )&lt;/p&gt;
&lt;p&gt;对于分离句子成独立单词，本例子使用正则表达式 &lt;code&gt;_WORD_SPLIT = re.compile(b"([.,!?\"':;)(])")&lt;/code&gt; ，
这意味着使用这几个标点符号 &lt;code&gt;[ . , ! ? " ' : ; ) ( ]&lt;/code&gt; 以及空格来分割句子，详情请看 &lt;code&gt;tl.nlp.basic_tokenizer()&lt;/code&gt; 。这个分割方法是 &lt;code&gt;tl.nlp.create_vocabulary()&lt;/code&gt; 和  &lt;code&gt;tl.nlp.data_to_token_ids()&lt;/code&gt; 的默认方法。&lt;/p&gt;
&lt;p&gt;所有的标点符号，比如 &lt;code&gt;. , ) (&lt;/code&gt; 在英文和法文数据库中都会被全部保留下来。&lt;/p&gt;
&lt;h4&gt;Softmax 抽样 (Sampled softmax)&lt;/h4&gt;
&lt;p&gt;softmax抽样是一种词汇表很大（Softmax 输出很多）的时候用来降低损失（cost）计算量的方法。
与从所有输出中计算 cross-entropy 相比，这个方法只从 &lt;code&gt;num_samples&lt;/code&gt; 个输出中计算 cross-entropy。&lt;/p&gt;
&lt;h4&gt;损失和更新函数&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;EmbeddingAttentionSeq2seqWrapper&lt;/code&gt; 内部实现了 SGD optimizer。&lt;/p&gt;
&lt;h3 id="xia yi bu ?_2"&gt;下一步？&lt;/h3&gt;
&lt;p&gt;您可以尝试其他应用。&lt;/p&gt;
&lt;h2 id="fan yi dui zhao_1"&gt;翻译对照&lt;/h2&gt;
&lt;p&gt;Stacked Denosing Autoencoder 堆栈式降噪自编码器&lt;/p&gt;
&lt;p&gt;Word Embedding               词嵌套、词嵌入&lt;/p&gt;
&lt;p&gt;Iteration                    迭代&lt;/p&gt;
&lt;p&gt;Natural Language Processing  自然语言处理&lt;/p&gt;
&lt;p&gt;Sparse                       稀疏的&lt;/p&gt;
&lt;p&gt;Cost function                损失函数&lt;/p&gt;
&lt;p&gt;Regularization               规则化、正则化&lt;/p&gt;
&lt;p&gt;Tokenization                 数字化&lt;/p&gt;
&lt;p&gt;Truncated backpropagation    截断反向传播&lt;/p&gt;
&lt;h2 id="geng duo xin xi"&gt;更多信息&lt;/h2&gt;
&lt;p&gt;TensorLayer 还能做什么？请继续阅读本文档。&lt;/p&gt;
&lt;p&gt;最后，API 参考列表和说明如下：&lt;/p&gt;
&lt;p&gt;layers (&lt;code&gt;tensorlayer.layers&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;activation (&lt;code&gt;tensorlayer.activation&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;natural language processing (&lt;code&gt;tensorlayer.nlp&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;reinforcement learning (&lt;code&gt;tensorlayer.rein&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;cost expressions and regularizers (&lt;code&gt;tensorlayer.cost&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;load and save files (&lt;code&gt;tensorlayer.files&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;operating system (&lt;code&gt;tensorlayer.ops&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;helper functions (&lt;code&gt;tensorlayer.utils&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;visualization (&lt;code&gt;tensorlayer.visualize&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;iteration functions (&lt;code&gt;tensorlayer.iterate&lt;/code&gt;),&lt;/p&gt;
&lt;p&gt;preprocessing functions (&lt;code&gt;tensorlayer.prepro&lt;/code&gt;),&lt;/p&gt;</content></entry><entry><title>TensorFlow 的 MNIST 教程</title><link href="https://freeopen.github.io/posts/tensorflow-de-mnist-jiao-cheng" rel="alternate"></link><published>2017-08-02T00:00:00+08:00</published><updated>2017-08-02T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2017-08-02:/posts/tensorflow-de-mnist-jiao-cheng</id><summary type="html">&lt;p&gt;&lt;a href="https://www.tensorflow.org/get_started/mnist/pros"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这篇文章对初学tensorflow的朋友来说，有很好的参考作用。曾经在网上看过本教程的翻译稿，但版本偏老。本文翻译时，tensorflow的版本为1.3.0-rc1. 新版相较于老版，示例代码有变化，文字说明也有增补。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。
在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。&lt;/p&gt;
&lt;p&gt;这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看新手指南.&lt;/p&gt;
&lt;h2 id="guan yu ben jiao cheng"&gt;关于本教程&lt;/h2&gt;
&lt;p&gt;本教程第一部分讲解 &lt;a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_softmax.py"&gt;mnist_softmax.py&lt;/a&gt; 代码, 一个简单的tensorflow模型实现。第二部分介绍一些提高精度的方法。&lt;/p&gt;
&lt;p&gt;你可以从本教程拷贝和粘贴代码块到你的python环境，或者下载完整代码 &lt;a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_deep.py"&gt;mnist_deep.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;我们将完成如下目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于图片像素，创建一个softmax回归函数的模型来识别MNIST数字。&lt;/li&gt;
&lt;li&gt;用tensorflow训练模型识别数字。&lt;/li&gt;
&lt;li&gt;用测试数据检查模型精度&lt;/li&gt;
&lt;li&gt;构建、训练和测试一个多层卷积神经网络来提升模型精度。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="an zhuang"&gt;安装&lt;/h2&gt;
&lt;p&gt;在创建模型之前，我们会先加载MNIST数据集，然后启动一个TensorFlow的session。&lt;/p&gt;
&lt;h3 id="jia zai mnistshu ju"&gt;加载MNIST数据&lt;/h3&gt;
&lt;p&gt;为了方便起见，我们已经准备了一个脚本来自动下载和导入MNIST数据集。它会自动创建一个'MNIST_data'的目录来存储数据。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://www.tensorflow.org/get_started/mnist/pros"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这篇文章对初学tensorflow的朋友来说，有很好的参考作用。曾经在网上看过本教程的翻译稿，但版本偏老。本文翻译时，tensorflow的版本为1.3.0-rc1. 新版相较于老版，示例代码有变化，文字说明也有增补。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。
在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。&lt;/p&gt;
&lt;p&gt;这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看新手指南.&lt;/p&gt;
&lt;h2 id="guan yu ben jiao cheng"&gt;关于本教程&lt;/h2&gt;
&lt;p&gt;本教程第一部分讲解 &lt;a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_softmax.py"&gt;mnist_softmax.py&lt;/a&gt; 代码, 一个简单的tensorflow模型实现。第二部分介绍一些提高精度的方法。&lt;/p&gt;
&lt;p&gt;你可以从本教程拷贝和粘贴代码块到你的python环境，或者下载完整代码 &lt;a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_deep.py"&gt;mnist_deep.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;我们将完成如下目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于图片像素，创建一个softmax回归函数的模型来识别MNIST数字。&lt;/li&gt;
&lt;li&gt;用tensorflow训练模型识别数字。&lt;/li&gt;
&lt;li&gt;用测试数据检查模型精度&lt;/li&gt;
&lt;li&gt;构建、训练和测试一个多层卷积神经网络来提升模型精度。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="an zhuang"&gt;安装&lt;/h2&gt;
&lt;p&gt;在创建模型之前，我们会先加载MNIST数据集，然后启动一个TensorFlow的session。&lt;/p&gt;
&lt;h3 id="jia zai mnistshu ju"&gt;加载MNIST数据&lt;/h3&gt;
&lt;p&gt;为了方便起见，我们已经准备了一个脚本来自动下载和导入MNIST数据集。它会自动创建一个'MNIST_data'的目录来存储数据。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'MNIST_data'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里，&lt;code&gt;mnist&lt;/code&gt;是一个轻量级的类。它以Numpy数组的形式存储着训练、校验和测试数据集。同时提供了一个函数，用于在迭代中获得minibatch，后面我们将会用到。&lt;/p&gt;
&lt;h3 id="yun xing tensorflowde interactivesession"&gt;运行TensorFlow的InteractiveSession&lt;/h3&gt;
&lt;p&gt;Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。&lt;/p&gt;
&lt;p&gt;这里，我们使用更加方便的InteractiveSession类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些计算图，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用InteractiveSession，那么你需要在启动session之前构建整个计算图，然后启动该计算图。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveSession&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="ji suan tu"&gt;计算图&lt;/h3&gt;
&lt;p&gt;为了在Python中进行高效的数值计算，我们通常会使用像NumPy一类的库，将一些诸如矩阵乘法的耗时操作在Python环境的外部来计算，这些计算通常会通过其它语言并用更为高效的代码来实现。&lt;/p&gt;
&lt;p&gt;但遗憾的是，每一个操作切换回Python环境时仍需要不小的开销。如果你想在GPU或者分布式环境中计算时，这一开销更加可怖，这一开销主要可能是用来进行数据迁移。&lt;/p&gt;
&lt;p&gt;TensorFlow也是在Python外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在Python外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在Python外部。这与Theano或Torch的做法类似。&lt;/p&gt;
&lt;p&gt;因此Python代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。详情请查看基本用法中的计算图表一节。&lt;/p&gt;
&lt;h2 id="gou jian  softmax hui gui mo xing_1"&gt;构建 Softmax 回归模型&lt;/h2&gt;
&lt;p&gt;在这一节中我们将建立一个拥有一个线性层的softmax回归模型。在下一节，我们会将其扩展为一个拥有多层卷积网络的softmax回归模型。&lt;/p&gt;
&lt;h3 id="zhan wei fu (placeholders)"&gt;占位符(placeholders)&lt;/h3&gt;
&lt;p&gt;我们通过为输入图像和目标输出类别创建节点，来构建计算图。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里的&lt;code&gt;x&lt;/code&gt;和&lt;code&gt;y_&lt;/code&gt;并不是特定的值，相反，他们都只是一个&lt;code&gt;占位符&lt;/code&gt;，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。&lt;/p&gt;
&lt;p&gt;输入图片&lt;code&gt;x&lt;/code&gt;是一个2维的浮点数张量。这里，分配给它的shape为[None, 784]，其中784是一张展平的MNIST图片的维度。None表示其值大小不定，在这里作为第一个维度值，用以指代batch的大小，意即x的数量不定。输出类别值y_也是一个2维张量，其中每一行为一个10维的one-hot向量,用于代表对应某一MNIST图片的类别。&lt;/p&gt;
&lt;p&gt;虽然placeholder的shape参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。&lt;/p&gt;
&lt;h3 id="bian liang"&gt;变量&lt;/h3&gt;
&lt;p&gt;我们现在为模型定义权重W和偏置b。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：变量。一个变量代表着TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用Variable来表示。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;我们在调用&lt;code&gt;tf.Variable&lt;/code&gt;的时候传入初始值。
在这个例子里，我们把&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;都初始化为零向量。&lt;code&gt;W&lt;/code&gt;是一个784x10的矩阵（因为我们有784个特征和10个输出值）。&lt;code&gt;b&lt;/code&gt;是一个10维的向量（因为我们有10个分类）。&lt;/p&gt;
&lt;p&gt;变量需要通过session初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个变量,可以一次性为所有变量完成此操作。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_all_variables&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="lei bie yu ce yu sun shi han shu_1"&gt;类别预测与损失函数&lt;/h2&gt;
&lt;p&gt;现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片&lt;code&gt;x&lt;/code&gt;和权重矩阵&lt;code&gt;W&lt;/code&gt;相乘，加上偏置&lt;code&gt;b&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以很容易的为训练过程指定最小化误差用的损失函数，损失表示在一个样本上模型预测有多差; 我们试图在所有的样本上最小化这个损失。这里， 我们的损失函数是目标类别和softmax激活函数之间的交叉熵。在教程开始，我们使用如下公式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，&lt;code&gt;tf.nn.softmax_cross_entropy_with_logits&lt;/code&gt;在内部用softmax规范化模型，
并对所有分类求和，而&lt;code&gt;tf.reduce_mean&lt;/code&gt;对这些和求平均 。&lt;/p&gt;
&lt;h2 id="xun lian mo xing"&gt;训练模型&lt;/h2&gt;
&lt;p&gt;我们已经定义好模型和训练用的损失函数，那么用TensorFlow进行训练就很简单了。因为TensorFlow知道整个计算图，它可以使用自动微分法找到对于各个变量的损失的梯度值。TensorFlow有大量内置的优化算法 这个例子中，我们用最速下降法让交叉熵下降，步长为0.5.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。&lt;/p&gt;
&lt;p&gt;返回的&lt;code&gt;train_step&lt;/code&gt;操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行&lt;code&gt;train_step&lt;/code&gt;来完成。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;每一步迭代，我们都会加载100个训练样本，然后执行一次&lt;code&gt;train_step&lt;/code&gt;，并通过&lt;code&gt;feed_dict&lt;/code&gt;将&lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y_&lt;/code&gt; 张量占位符用训练训练数据替代。
注意，在计算图中，你可以用&lt;code&gt;feed_dict&lt;/code&gt;来替代任何张量，并不仅限于替换占位符。&lt;/p&gt;
&lt;h3 id="ping gu mo xing"&gt;评估模型&lt;/h3&gt;
&lt;p&gt;那么我们的模型性能如何呢？&lt;/p&gt;
&lt;p&gt;首先让我们找出那些预测正确的标签。&lt;code&gt;tf.argmax&lt;/code&gt; 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如&lt;code&gt;tf.argmax(y,1)&lt;/code&gt;返回的是模型对于任一输入&lt;code&gt;x&lt;/code&gt;预测到的标签值，而&lt;code&gt;tf.argmax(y_,1)&lt;/code&gt; 代表正确的标签，我们可以用 &lt;code&gt;tf.equal&lt;/code&gt; 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：[True, False, True, True]变为[1,0,1,1]，计算出平均值为0.75。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，我们可以计算出在测试数据上的准确率，大概是92%。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="gou jian duo ceng juan ji wang luo_1"&gt;构建多层卷积网络&lt;/h2&gt;
&lt;p&gt;在MNIST上只有92% 的准确率，实在太糟糕。在这节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。&lt;/p&gt;
&lt;h3 id="quan zhong chu shi hua"&gt;权重初始化&lt;/h3&gt;
&lt;p&gt;为了创建这个模型，我们需要创建大量的权重和偏置项。
这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。
由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;initial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;initial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="juan ji he chi hua"&gt;卷积和池化&lt;/h3&gt;
&lt;p align="center"&gt;
&lt;img src="/images/convgaus.gif"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：卷积，从数学角度说，指计算一个函数通过另一个函数时，有多少重叠的积分。也可以视为通过相乘的方式将两个函数进行混合。详见上图,
红色曲线（函数f）下的区域是积分，蓝色曲线(函数g)从左向右缓慢移动，绿色曲线下的区域是红色曲线和蓝色曲线的卷积，灰色阴影表示在绿色垂直线位置时，红色曲线和蓝色曲线的卷积, 其值为f(a)*g(x-a)。 &lt;br&gt; &lt;br/&gt;
下面再给个我非常喜欢的说明，帮助理解这个公式，写得非常有意思。  &lt;br&gt; &lt;br/&gt;
比如说你的老板命令你干活，你却到楼下打台球去了，后来被老板发现，他非常气愤，扇了你一巴掌（注意，这就是输入信号，脉冲），于是你的脸上会渐渐地（贱贱地）鼓起来一个包，你的脸就是一个系统，而鼓起来的包就是你的脸对巴掌的响应，好，这样就和信号系统建立起来意义对应的联系。下面还需要一些假设来保证论证的严谨：假定你的脸是线性时不变系统，也就是说，无论什么时候老板打你一巴掌，打在你脸的同一位置（这似乎要求你的脸足够光滑，如果你说你长了很多青春痘，甚至整个脸皮处处连续处处不可导，那难度太大了，我就无话可说了哈哈），你的脸上总是会在相同的时间间隔内鼓起来一个相同高度的包来，并且假定以鼓起来的包的大小作为系统输出。好了，那么，下面可以进入核心内容&amp;mdash;&amp;mdash;卷积了！  &lt;br&gt; &lt;br/&gt;
如果你每天都到楼下去打台球，那么老板每天都要扇你一巴掌，不过当老板打你一巴掌后，你5分钟就消肿了，所以时间长了，你甚至就适应这种生活了&amp;hellip;&amp;hellip;如果有一天，老板忍无可忍，以0.5秒的间隔开始不间断的扇你的过程，这样问题就来了，第一次扇你鼓起来的包还没消肿，第二个巴掌就来了，你脸上的包就可能鼓起来两倍高，老板不断扇你，脉冲不断作用在你脸上，效果不断叠加了，这样这些效果就可以求和了，结果就是你脸上的包的高度随时间变化的一个函数了（注意理解）；如果老板再狠一点，频率越来越高，以至于你都辨别不清时间间隔了，那么，求和就变成积分了。可以这样理解，在这个过程中的某一固定的时刻，你的脸上的包的鼓起程度和什么有关呢？和之前每次打你都有关！但是各次的贡献是不一样的，越早打的巴掌，贡献越小，所以这就是说，某一时刻的输出是之前很多次输入乘以各自的衰减系数之后的叠加而形成某一点的输出，然后再把不同时刻的输出点放在一起，形成一个函数，这就是卷积，卷积之后的函数就是你脸上的包的大小随时间变化的函数。本来你的包几分钟就可以消肿，可是如果连续打，几个小时也消不了肿了，这难道不是一种平滑过程么？反映到剑桥大学的公式上，f(a)就是第a个巴掌，g(x-a)就是第a个巴掌在x时刻的作用程度，乘起来再叠加就ok了，大家说是不是这个道理呢？&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？
在这个实例里，我们会一直使用普通版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。
我们的池化用简单传统的2x2大小的模板做最大池化。为了代码更简洁，我们把这部分抽象成一个函数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                        &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="di yi ceng juan ji"&gt;第一层卷积&lt;/h3&gt;
&lt;p&gt;现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为了用这一层，我们把&lt;code&gt;x&lt;/code&gt;变成一个4维向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，
如果是rgb彩色图，则为3)。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这里&lt;code&gt;x&lt;/code&gt;是2维矩阵（m, 784), 其中m表示样本数量，所以下面的reshape中的 &lt;code&gt;-1&lt;/code&gt;的位置实质表示的是m值，此处的&lt;code&gt;-1&lt;/code&gt;可以推断出m值，在具体执行时，将用m值替换.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;我们把&lt;code&gt;x_image&lt;/code&gt;和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行最大池化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;h_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_pool1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="di er ceng juan ji"&gt;第二层卷积&lt;/h3&gt;
&lt;p&gt;为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;h_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="quan lian jie ceng"&gt;全连接层&lt;/h3&gt;
&lt;p&gt;现在，图片尺寸缩小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;h_pool2_flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;h_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool2_flat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_fc1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_fc1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="dropout"&gt;Dropout&lt;/h3&gt;
&lt;p&gt;为了减少过拟合，我们在输出层之前加入&lt;a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"&gt;dropout&lt;/a&gt;。我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。
这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 
TensorFlow的&lt;code&gt;tf.nn.dropout&lt;/code&gt;操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;keep_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_fc1_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="shu chu ceng"&gt;输出层&lt;/h3&gt;
&lt;p&gt;最后，我们添加一个softmax层，就像前面的单层 softmax 回归一样。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W_fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;y_conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_fc1_drop&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_fc2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_fc2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="xun lian he ping gu mo xing"&gt;训练和评估模型&lt;/h3&gt;
&lt;p&gt;这个模型的效果如何呢？&lt;/p&gt;
&lt;p&gt;为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，
这个模型和上个模型的不同之处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只是我们会用更加复杂的&lt;a href="http://arxiv.org/abs/1412.6980"&gt;ADAM优化器&lt;/a&gt;来做梯度最速下降，&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;feed_dict&lt;/code&gt;中加入额外的参数&lt;code&gt;keep_prob&lt;/code&gt;来控制dropout比例。&lt;/li&gt;
&lt;li&gt;然后每100次迭代输出一次日志。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们使用了 &lt;code&gt;tf.Session&lt;/code&gt; 而不是 &lt;code&gt;tf.InteractiveSession&lt;/code&gt;. 将创建图(模型规格)和评估图(模型匹配)的处理分开, 这样便产生更清晰的代码。tf.Session被with代码块创建，以至于一旦块退出时它将自动销毁。&lt;/p&gt;
&lt;p&gt;随时运行此代码。请注意, 它会进行2万次训练迭代, 并且需要一段时间 (可能长达半小时), 这取决于你的处理器。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_conv&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;correct_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_conv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;train_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'step &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;, training accuracy &lt;/span&gt;&lt;span class="si"&gt;%g&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_accuracy&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'test accuracy &lt;/span&gt;&lt;span class="si"&gt;%g&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;以上代码，在最终测试集上的准确率大概是99.2%。&lt;/p&gt;
&lt;p&gt;目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。&lt;/p&gt;
&lt;p&gt;如果你想用力鼓励一下，欢迎&lt;a href="../../pay.html"&gt;打赏&lt;/a&gt;，官方建议零售价 &amp;yen;2 。&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;在小型卷积网络中，有没有dropout对性能影响不大。Dropout 通常可以降低过拟合，它常常被用于大型的神经网络中。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content></entry><entry><title>Neural Machine Translation (seq2seq) 教程</title><link href="https://freeopen.github.io/posts/neural-machine-translation-seq2seq-jiao-cheng" rel="alternate"></link><published>2017-07-18T00:00:00+08:00</published><updated>2017-07-18T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2017-07-18:/posts/neural-machine-translation-seq2seq-jiao-cheng</id><summary type="html">&lt;p&gt;2018-01-31 第一次修订&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/nmt"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="jie shao"&gt;介绍&lt;/h2&gt;
&lt;p&gt;序列到序列(seq2seq)模型 (&lt;a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"&gt;Sutskever et al., 2014&lt;/a&gt;, &lt;a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf"&gt;Cho et al., 2014&lt;/a&gt;) 在诸如机器翻译、语音识别和文本概括等任务中取得了巨大成功. 本教程为读者提供对 seq2seq 模型的全面理解，并展示如何从头构建一个有竞争力的 seq2seq 模型. 我们专注于神经机器翻译（NMT）任务，这是一个很好的、已获得广泛&lt;a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;成功&lt;/a&gt;的 seq2seq 模型的试验台. 所含的代码轻量、高质、实用，并整合了最新的研究思路。我们通过以下方式达成此目标 :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用最新的 解码器 / attention wrapper &lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops"&gt;API&lt;/a&gt;, TensorFlow 1.2 数据迭代器&lt;/li&gt;
&lt;li&gt;结合我们在建立循环神经网络和序列到序列模型方面的强大专长&lt;/li&gt;
&lt;li&gt;提供一些巧思来构建最好的 NMT 模型 …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;2018-01-31 第一次修订&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/nmt"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="jie shao"&gt;介绍&lt;/h2&gt;
&lt;p&gt;序列到序列(seq2seq)模型 (&lt;a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"&gt;Sutskever et al., 2014&lt;/a&gt;, &lt;a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf"&gt;Cho et al., 2014&lt;/a&gt;) 在诸如机器翻译、语音识别和文本概括等任务中取得了巨大成功. 本教程为读者提供对 seq2seq 模型的全面理解，并展示如何从头构建一个有竞争力的 seq2seq 模型. 我们专注于神经机器翻译（NMT）任务，这是一个很好的、已获得广泛&lt;a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;成功&lt;/a&gt;的 seq2seq 模型的试验台. 所含的代码轻量、高质、实用，并整合了最新的研究思路。我们通过以下方式达成此目标 :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用最新的 解码器 / attention wrapper &lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops"&gt;API&lt;/a&gt;, TensorFlow 1.2 数据迭代器&lt;/li&gt;
&lt;li&gt;结合我们在建立循环神经网络和序列到序列模型方面的强大专长&lt;/li&gt;
&lt;li&gt;提供一些巧思来构建最好的 NMT 模型，并复制一个谷歌神经机器翻译系统 &lt;a href="https://research.google.com/pubs/pub45610.html"&gt;Google&amp;rsquo;s NMT (GNMT) system&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们认为，重要的是提供人们可以轻松复制的基准. 因此，我们提供了完整的实验结果，并对以下公开的数据集进行了预训练 :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;小规模&lt;/em&gt;: 英语-越南语平行语料库(133K 句子对,TED 对话), 由 &lt;a href="https://sites.google.com/site/iwsltevaluation2015/"&gt;IWSLT Evaluation Campaign&lt;/a&gt; 提供.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;大规模&lt;/em&gt;: 德语-英语平行语料库(4.5M 句子对) , 由 &lt;a href="http://www.statmt.org/wmt16/translation-task.html"&gt;WMT Evaluation Campaign&lt;/a&gt; 提供.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们首先建立关于 seq2seq 模型的一些基本知识, 说明如何构建和训练一个普通的 NMT 模型. 第二部分将详细介绍采用注意力机制（attention mechanism) 建立一个较好的 NMT 模型. 然后，我们将讨论构建更好 NMT 模型（包括速度和翻译质量）的技巧，比如 TensorFlow 的最佳实践（batching, bucketing）, 双向 RNNs 和 定向搜索.&lt;/p&gt;
&lt;h2 id="ji chu"&gt;基础&lt;/h2&gt;
&lt;h3 id="shen jing ji qi fan yi de bei jing"&gt;神经机器翻译的背景&lt;/h3&gt;
&lt;p&gt;回到过去，传统的基于短语的翻译系统通过将语句拆成多个小块，然后再一小块一小块的翻译。这导致不流畅的翻译结果，并不十分像我们人类的翻译。我们是先读懂整个句子，再翻译出来。神经机器翻译(NMT)就是在模仿这种方式！&lt;/p&gt;
&lt;p&gt;具体来说,  NMT 系统首先使用 &lt;em&gt;编码器&lt;/em&gt; 读取源句来构建一个 &lt;a href="https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence"&gt;"thought" 向量&lt;/a&gt; , 一个表示句子意义的数字序列; 然后，&lt;em&gt;解码器&lt;/em&gt; 处理这个向量输出翻译结果
, 如图 1 . 这通常被称为 &lt;em&gt;编码器 - 解码器结构&lt;/em&gt;. 以这种方式, NMT 解决了传统的基于短语翻译的遗留问题: 它可以捕获语言的 &lt;em&gt;远程依赖性&lt;/em&gt; , 比如，词性、语法结构等，并生成顺畅的翻译，如 &lt;a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;Google Neural Machine Translation systems&lt;/a&gt; 所示.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：上文的 &lt;em&gt;"thought"&lt;/em&gt; 只是个比喻，不要当真. 机器学习建立在统计学方法的基础上，跟&amp;ldquo;思考&amp;rdquo;没有半毛钱关系，至于理解人类语言中表达的意义,那更是遥远得离谱的事情.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/encdec.jpg" width="90%"/&gt;
&lt;br/&gt;
图1. &lt;b&gt;编码器-解码器结构&lt;/b&gt; &amp;ndash; NMT 的通用示例. 编码器转换源语句为&amp;ldquo;含义&amp;rdquo;向量，再由&lt;i&gt;解码器&lt;/i&gt;产生翻译结果.
&lt;/p&gt;
&lt;p&gt;NMT 模型的具体结构有所不同. 对顺序数据而言，大多数 NMT 模型的一个自然选择是采用循环神经网络 (RNN).
通常，RNN 同时使用编码器和解码器. 然而，RNN 模型在以下方面有所不同: (a) &lt;em&gt;方向性&lt;/em&gt; &amp;ndash;  单向或双向; (b) &lt;em&gt;深度&lt;/em&gt; &amp;ndash; 单层或多层;  (c) &lt;em&gt;类型&lt;/em&gt; &amp;ndash; 常见的有 RNN,  Long Short-term Memory (LSTM),  或 gated recurrent unit
(GRU). 有兴趣的读者可以在这篇&lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;博文&lt;/a&gt;上找到有关 RNNs 和 LSTM 的更多信息 .&lt;/p&gt;
&lt;p&gt;在本教程中, 我们将考察一个 &lt;em&gt;深度多层 RNN&lt;/em&gt; 的例子，它是单向的，并使用 LSTM 作为循环单元. 如图 2. 在这个例子中，我们将 "I am a student"  翻译成 "Je suis &amp;eacute;tudiant". 在高层上, 这个 NMT 模型由两个循环神经网络组成: &lt;em&gt;编码器&lt;/em&gt;
RNN 简单的吃进输入文字，不做任何预测; 
另一方面，&lt;em&gt;解码器&lt;/em&gt;在预测下一个单词时处理目标句子.&lt;/p&gt;
&lt;p&gt;更多信息, 请参阅 &lt;a href="https://github.com/lmthang/thesis"&gt;Luong (2016)&lt;/a&gt; .&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/seq2seq.jpg" width="58%"/&gt;
&lt;br/&gt;
图2. &lt;b&gt;神经机器翻译&lt;/b&gt; &amp;ndash; 一个深度循环网络的例子，把源语句 "I am a student" 翻译成目标语句 
 "Je suis &amp;eacute;tudiant". 这里, "&amp;lt;s&amp;gt;" 表示解码处理的开始,
而 "&amp;lt;/s&amp;gt;" 表示解码结束.
&lt;/p&gt;
&lt;h3 id="an zhuang jiao cheng"&gt;安装教程&lt;/h3&gt;
&lt;p&gt;要安装本教程, 你需要在系统上安装 TensorFlow. 本教程撰写时 TensorFlow 的版本为 &lt;strong&gt;1.2.1&lt;/strong&gt; .&lt;br/&gt;
安装 TensorFlow 请参阅 &lt;a href="https://www.tensorflow.org/install/"&gt;installation instructions here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一旦安装了 TensorFlow, 你就可以运行以下脚本下载本教程的源码了:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/tensorflow/nmt/
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="xun lian  - ru he jian li wo men de di yi ge  nmt xi tong"&gt;训练 &amp;ndash; 如何建立我们的第一个 NMT 系统&lt;/h3&gt;
&lt;p&gt;让我们先走进构建 NMT 模型的核心代码，一会儿我们将详细解释图 2 . 我们晚点再来看完整代码及数据准备部分. 这部分的代码文件为
&lt;em&gt;&lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py"&gt;model.py&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;如图 2 的底层, 编码器和解码器的循环神经网络接收下列输入: 首先, 是待翻译的句子, 接着是一个边界标记 "&amp;lt;s&amp;gt;&amp;rdquo;，表示从编码到解码模式的转换, 最后是翻译好的句子.  为了&lt;em&gt;训练&lt;/em&gt;, 我们将为系统提供以下张量,
它们包含词汇索引和时序格式（time-major format）:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;encoder_inputs&lt;/strong&gt; [max_encoder_time, batch_size]: 原始输入字.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;decoder_inputs&lt;/strong&gt; [max_decoder_time, batch_size]: 目标输入字.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;decoder_outputs&lt;/strong&gt; [max_decoder_time, batch_size]: 目标输出字. 
这里目标输入字 &lt;em&gt;decoder_inputs&lt;/em&gt; 向左移动一个时间步长，并在右边附加一个句末标记.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了效率，我们一次训练多个句子 (batch_size). 测试时略有不同，我们稍后再行讨论.&lt;/p&gt;
&lt;h4&gt;Embedding&lt;/h4&gt;
&lt;p&gt;根据词汇的含义，模型必须首先找出源词和目标词对应的词向量表达。为使 &lt;em&gt;embedding layer&lt;/em&gt; 工作，首先为每种语言选择一个词表。
通常, 把词表的长度设为 V (即不重复的词汇数量). 而其它词则设为&amp;ldquo;unknown&amp;rdquo;标记，并赋予同样的词向量值。一种语言一套词向量, 一般通过训练学到。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Embedding&lt;/span&gt;
&lt;span class="n"&gt;embedding_encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;"embedding_encoder"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;src_vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Look up embedding:&lt;/span&gt;
&lt;span class="c1"&gt;#   encoder_inputs: [max_time, batch_size]&lt;/span&gt;
&lt;span class="c1"&gt;#   encoder_emp_inp: [max_time, batch_size, embedding_size]&lt;/span&gt;
&lt;span class="n"&gt;encoder_emb_inp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_ops&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;embedding_encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;类似的，我们可以构建 &lt;em&gt;embedding_decoder&lt;/em&gt; 和 &lt;em&gt;decoder_emb_inp&lt;/em&gt; 。注意，可以使用已训练好的词向量, 如 word2vec 或  Glove vectors, 来初始化我们的词向量. 通常，如果有大量的训练数据，我们也可以从头开始训练这些词向量。&lt;/p&gt;
&lt;h4&gt;编码器&lt;/h4&gt;
&lt;p&gt;一旦检索到这个词，就将其对应的词向量作为输入发送到主网络，该网络由两个多层 RNN 组成 ，一个针对源语言的编码器和一个针对目标语言的解码器。
这两个 RNN 原则上可以共享相同的权重；然而，实践中，我们经常使用不同的参数（这样的模型在拟合大规模训练数据集时做得更好）. 
这个 RNN &lt;em&gt;编码器&lt;/em&gt;使用 0 向量作为初始状态，建立如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Build RNN cell&lt;/span&gt;
&lt;span class="n"&gt;encoder_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicLSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Run Dynamic RNN&lt;/span&gt;
&lt;span class="c1"&gt;#   encoder_outpus: [max_time, batch_size, num_units]&lt;/span&gt;
&lt;span class="c1"&gt;#   encoder_state: [batch_size, num_units]&lt;/span&gt;
&lt;span class="n"&gt;encoder_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dynamic_rnn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;encoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_emb_inp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;sequence_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;source_seqence_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time_major&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意, 句子有不同的长度，我们通过 &lt;em&gt;source_sequence_length&lt;/em&gt; 来告诉 &lt;em&gt;dynamic_rnn&lt;/em&gt; 确切的源句长度，以避免计算上的浪费. 由于我们的输入有时序，我们设置 &lt;em&gt;time_major=True&lt;/em&gt; . 这里，我们仅建立一个单层的 LSTM &lt;em&gt;编码器单元&lt;/em&gt;. 我们将在后面的章节说明如何构建多层LSTM，增加 dropout, 和使用 attention.&lt;/p&gt;
&lt;h4&gt;解码器&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;decoder&lt;/em&gt; 也需要访问源信息，一个简单的方法是用编码器最后的隐藏状态来初始化它。如图2，我们把源语言的&amp;ldquo;student&amp;rdquo;的隐藏状态传递到解码器端。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Build RNN cell&lt;/span&gt;
&lt;span class="n"&gt;decoder_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicLSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Helper&lt;/span&gt;
&lt;span class="n"&gt;helper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TrainingHelper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder_emb_inp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;decoder_lengths&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time_major&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Decoder&lt;/span&gt;
&lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicDecoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;helper&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output_layer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;projection_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Dynamic decoding&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dynamic_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里，这个代码的核心部分是 &lt;em&gt;BasicDecoder&lt;/em&gt;，它接收 &lt;em&gt;decoder_cell&lt;/em&gt; (类似 encoder_cell)、&lt;em&gt;helper&lt;/em&gt;、前一个 &lt;em&gt;encoder_state&lt;/em&gt; 作为输入输出到 &lt;em&gt;decoder&lt;/em&gt; 对象。通过分开 decoder 和 helper，我们可以在不同代码中重用。例如，&lt;em&gt;TrainingHelper&lt;/em&gt; 可以被 &lt;em&gt;GreedyEmbeddingHelper&lt;/em&gt; 取代做贪心解码。更多内容请看&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py"&gt;helper.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;最后，我们没提到的 &lt;em&gt;projection_layer&lt;/em&gt; 是个密集矩阵，它将顶部的隐藏状态转为 V 个维度的 logit 向量。我们在图 2 的顶部展示了这个过程。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;projection_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layers_core&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tgt_vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;误差&lt;/h4&gt;
&lt;p&gt;根据上面给定的 &lt;em&gt;logits&lt;/em&gt; ，我们现在准备计算我们的训练误差:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;crossent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparse_softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;decoder_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;crossent&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;target_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里，&lt;em&gt;target_weights&lt;/em&gt; 是一个和 &lt;em&gt;decoder_outputs&lt;/em&gt; 维度一样的 0-1 矩阵.它把超出目标序列长度之外的地方填充为0。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;重要注意事项&lt;/em&gt;&lt;/strong&gt;: 值得指出的是，我们将误差除以 &lt;em&gt;batch_size&lt;/em&gt;, 所以我们的超参数对 batch_size 是不变的. 有些人将误差除以(&lt;em&gt;batch_size&lt;/em&gt; * &lt;em&gt;num_time_steps&lt;/em&gt;)，它可以降低短句的错误. 更微妙的是，我们的超参数（用于前一种方法）不能被用于后一种方法.  例如，如果两个方法都使用 1.0 为学习率的 SGD（随机梯度下降算法），后一种方法会更有效, 因为它采用了更小的 1 / &lt;em&gt;num_time_steps&lt;/em&gt; 作为学习率。&lt;/p&gt;
&lt;h4&gt;梯度计算 &amp;amp; 优化&lt;/h4&gt;
&lt;p&gt;我们现在已经定义了正向传播的 NMT 模型。计算反向传播只是几行代码的问题:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Calculate and clip gradients&lt;/span&gt;
&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;trainable_variables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;gradients&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clipped_gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip_by_global_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_gradient_norm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练 RNN 的一个重要步骤是梯度调整。这里，我们按照惯例来调整。&lt;em&gt;max_gradient_norm&lt;/em&gt; 的最大值, 通常设为 5 或 1. 最后一步是选择优化器. Adam 优化器是常用的选择.  我们也选择一个学习率，这个值常在 0.0001 到 0.001 之间; 并且可以随训练进度而减小.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Optimization&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;update_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clipped_gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在我们自己的实验中，我们采用可自动降低学习率的标准 SGD 优化器（tf.train.GradientDescentOptimizer），从而产生更好的性能。参见 &lt;a href="#评测"&gt;评测&lt;/a&gt;. &lt;/p&gt;
&lt;h3 id="dong shou  - rang wo men xun lian yi ge nmtmo xing"&gt;动手 - 让我们训练一个NMT模型&lt;/h3&gt;
&lt;p&gt;让我们训练我们的第一个 NMT 模型，把越南语翻译成英语！我们代码的入口是
&lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/nmt.py"&gt;&lt;em&gt;nmt.py&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;我们将使用 &lt;em&gt;small-scale parallel corpus of TED talks&lt;/em&gt; (133K training
examples) 进行此练习. 所有数据可在:
&lt;a href="https://nlp.stanford.edu/projects/nmt/"&gt;https://nlp.stanford.edu/projects/nmt/&lt;/a&gt;找到. 我们将使用 tst2012 作为训练数据集,  tst2013 作为测试数据集.&lt;/p&gt;
&lt;p&gt;运行下列命令下载训练 NMT 模型的数据:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;nmt/scripts/download_iwslt15.sh /tmp/nmt_data&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;运行如下命令开始训练:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir /tmp/nmt_model
python -m nmt.nmt &lt;span class="se"&gt;\&lt;/span&gt;
    --src&lt;span class="o"&gt;=&lt;/span&gt;vi --tgt&lt;span class="o"&gt;=&lt;/span&gt;en &lt;span class="se"&gt;\&lt;/span&gt;
    --vocab_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/vocab  &lt;span class="se"&gt;\&lt;/span&gt;
    --train_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/train &lt;span class="se"&gt;\&lt;/span&gt;
    --dev_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/tst2012  &lt;span class="se"&gt;\&lt;/span&gt;
    --test_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/tst2013 &lt;span class="se"&gt;\&lt;/span&gt;
    --out_dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_model &lt;span class="se"&gt;\&lt;/span&gt;
    --num_train_steps&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;12000&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --steps_per_stats&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --num_layers&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --num_units&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;128&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --dropout&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.2 &lt;span class="se"&gt;\&lt;/span&gt;
    --metrics&lt;span class="o"&gt;=&lt;/span&gt;bleu
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上述命令训练一个 2 层 LSTM seq2seq 模型，含 128 个隐藏单元和 12 轮的 embedding 操作。我们使用的 dropout 值为 0.2（维持概率在0.8 ）。如果没有错误，我们应该在我们训练时看到类似于下面的日志。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# First evaluation, global step 0
  eval dev: perplexity 17193.66
  eval test: perplexity 17193.27
# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017
  sample train data:
    src_reverse: &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt; Điều đ&amp;oacute; , dĩ nhi&amp;ecirc;n , l&amp;agrave; c&amp;acirc;u chuyện tr&amp;iacute;ch ra từ học thuyết của Karl Marx .
    ref: That , of course , was the &lt;span class="nt"&gt;&amp;lt;unk&amp;gt;&lt;/span&gt; distilled from the theories of Karl Marx . &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt; &lt;span class="nt"&gt;&amp;lt;/s&amp;gt;&lt;/span&gt;
  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00
  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00
  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00
  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00
  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;有关详细信息，请参阅 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/train.py"&gt;&lt;em&gt;train.py&lt;/em&gt;&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;我们可以在训练期间启动 Tensorboard 查看模型的统计：:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tensorboard --port &lt;span class="m"&gt;22222&lt;/span&gt; --logdir /tmp/nmt_model/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从英语到越南语的训练可以简单地改变:
&lt;code&gt;--src=en --tgt=vi&lt;/code&gt;&lt;/p&gt;
&lt;h3 id="tui li  - ru he chan sheng fan yi"&gt;推理 &amp;ndash; 如何产生翻译&lt;/h3&gt;
&lt;p&gt;当你训练你的 NMT 模型（一旦你已训练好模型），你可以得到以前未见过的源语句的翻译。这个过程称为推理。训练和推理（测试）之间有明确的区别：在推理时，我们只能访问源语句，即 encoder_inputs。解码有很多种方法, 包括贪心、采样和定向搜索几种。在这里，我们将讨论贪心解码法。&lt;/p&gt;
&lt;p&gt;这个想法很简单，我们在图 3 中说明:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们仍然以与训练期间相同的方式对源语句进行编码以获得 encoder_state，并使用该 encoder_state 来初始化解码器。&lt;/li&gt;
&lt;li&gt;一旦解码器接收到起始符号&amp;ldquo;&amp;lt;s&amp;gt;&amp;rdquo;（参见我们代码中的 tgt_sos_id ），解码（翻译）处理就开始&lt;/li&gt;
&lt;li&gt;对于解码器侧的每个时间步长，我们将 RNN 的输出视为一组 logit。我们选择最可能的字，与最大 logit 值相关联的 id 作为译出的字（这是&amp;ldquo;贪婪&amp;rdquo;行为）。例如在图 3 中，在第一个解码步骤中，词&amp;ldquo;moi&amp;rdquo;具有最高的翻译概率。然后，我们将这个词作为输入提供给下一个时间步。(译者注：由于输出的logit向量维度为词表长度V，当词表很大时计算量很大)&lt;/li&gt;
&lt;li&gt;继续第3步直到遇到句子的结尾标记&amp;ldquo;&amp;lt;/s&amp;gt;&amp;rdquo;（参见我们的代码中的 tgt_eos_id ）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/greedy_dec.jpg" width="50%"/&gt;
&lt;br/&gt;
图3. &lt;b&gt;贪心解码&lt;/b&gt; &amp;ndash; 如何用贪心搜索法训练 NMT 模型, 使源语句产生"Je suis &amp;eacute;tudiant"的翻译
&lt;/p&gt;
&lt;p&gt;第 3 步的推理与训练不同。不是总是将正确的目标词作为输入，推理使用模型预测的单词。以下是实现贪心解码的代码。它与训练解码器非常相似。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Helper&lt;/span&gt;
&lt;span class="n"&gt;helper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GreedyEmbeddingHelper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;embedding_decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;tgt_sos_id&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tgt_eos_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Decoder&lt;/span&gt;
&lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicDecoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;helper&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output_layer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;projection_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Dynamic decoding&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dynamic_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maximum_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;maximum_iterations&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;translations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_id&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里，我们使用 &lt;em&gt;GreedyEmbeddingHelper&lt;/em&gt; 代替 &lt;em&gt;TrainingHelper&lt;/em&gt;。由于我们预先不知道目标序列的长度，所以我们使用 &lt;em&gt;maximum_iterations&lt;/em&gt; 来限制翻译的长度。一个启发式的用法是采用源语句长度的两倍来解码。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;maximum_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source_sequence_length&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练好一个模型后，我们现在可以创建一个推理文件并翻译一些句子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cat &amp;gt; /tmp/my_infer_file.vi
&lt;span class="c1"&gt;# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)&lt;/span&gt;

python -m nmt.nmt &lt;span class="se"&gt;\&lt;/span&gt;
    --model_dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_model &lt;span class="se"&gt;\&lt;/span&gt;
    --inference_input_file&lt;span class="o"&gt;=&lt;/span&gt;/tmp/my_infer_file.vi &lt;span class="se"&gt;\&lt;/span&gt;
    --inference_output_file&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_model/output_infer

cat /tmp/nmt_model/output_infer &lt;span class="c1"&gt;# To view the inference as output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，只要存在训练检查点，即使模型仍在训练中，也可以运行上述命令。详见 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/inference.py"&gt;&lt;em&gt;inference.py&lt;/em&gt;&lt;/a&gt; 。&lt;/p&gt;
&lt;h2 id="jin jie_1"&gt;进阶&lt;/h2&gt;
&lt;p&gt;经历了最基本的 seq2seq 模型，让我们进一步完善它！为了建立最先进的神经机器翻译系统，我们还需要更多的&amp;ldquo;内功心法&amp;rdquo;：
&lt;em&gt;注意力机制(attention mechanism)&lt;/em&gt;，这是由 &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanau等人2015年&lt;/a&gt;首次引入，然后由 &lt;a href="https://arxiv.org/abs/1508.04025"&gt;Luong等人2015年&lt;/a&gt;完善。&lt;em&gt;注意力机制&lt;/em&gt;的关键在于，通过在翻译过程中对相关的源内容进行&amp;ldquo;关注&amp;rdquo;，建立目标和源之间的直接连接。注意力机制 的一个很好的副产品是在源和目标句子之间生成一个易于查看的对齐矩阵（如图4所示）&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/attention_vis.jpg" width="50%"/&gt;
&lt;br/&gt;
图4. &lt;b&gt;Attention 可视化&lt;/b&gt; &amp;ndash; 源语句与目标语句的词汇对齐矩阵示例， 图片来自 (Bahdanau et al., 2015).
&lt;/p&gt;
&lt;p&gt;请记住，在 seq2seq 模型中，当开始解码时，我们将最后的源状态从编码器传递到解码器。这对中短句的效果很好; 而对于长句，单个固定大小的隐藏状态会成为信息瓶颈。
注意力机制不是放弃在源 RNN 中计算的所有隐藏状态，而是提供了一种允许解码器窥视它们的方法（将它们视为源信息的动态存储器）。
通过这样做，注意力机制改善了较长句子的翻译质量。现在，注意力机制已成为事实上的标准，并已成功应用于许多其他任务（包括图像字幕生成，语音识别和文本摘要等）。&lt;/p&gt;
&lt;h3 id="zhu yi li ji zhi de bei jing"&gt;注意力机制的背景&lt;/h3&gt;
&lt;p&gt;我们现在讲讲（Luong等人，2015年）中提出的注意力机制(attention mechanism)的一个实例，该实例已被用于包括 &lt;a href="http://opennmt.net/about/"&gt;OpenNMT&lt;/a&gt; 等开源工具包在内的多个最先进的系统，以及本教程的 TF seq2seq API 中。我们还将提供注意力机制其他变种的连接。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/attention_mechanism.jpg" width="58%"/&gt;
&lt;br/&gt;
图5. &lt;b&gt;Attention mechanism&lt;/b&gt; &amp;ndash; (Luong et al., 2015)中提到的基于 attention NMT 系统的示例. 我们重点突出 attention 计算的第一个步骤. 为了清晰，我们没像图2那样显示 embedding 层和 projection 层.
&lt;/p&gt;
&lt;p&gt;如图 5 所示，attention 计算发生在每个解码器的时间步长。它包括以下步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将当前目标的隐藏状态与所有源的状态进行比较以获得 &lt;em&gt;attention weights&lt;/em&gt;（如图4所示）。&lt;/li&gt;
&lt;li&gt;基于 attention weights ，我们计算&lt;em&gt;上下文矢量&lt;/em&gt;（&lt;em&gt;context vector&lt;/em&gt;） 作为源状态的加权平均值。&lt;/li&gt;
&lt;li&gt;将上下文矢量与当前目标的隐藏状态组合以产生最终的 &lt;em&gt;attention vector&lt;/em&gt;。&lt;/li&gt;
&lt;li&gt;attention vector 作为输入发送到下一个时间步（&lt;em&gt;input feeding&lt;/em&gt;）。前三个步骤通过以下等式来总结：&lt;/li&gt;
&lt;/ol&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/attention_equation_0.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;这里，函数 &lt;code&gt;score&lt;/code&gt; 用于将目标隐藏状态 &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; 与每个源隐藏状态 &lt;span class="math"&gt;\(\overline{h}_s\)&lt;/span&gt; 进行比较，并将结果归一化以产生 attention weights（一个基于源语言的位置分布）。这里的 score 函数有多种选择; 流行的 score 函数包括等式（4）列出的乘法和加法形式。一旦完成计算，Attention vector &lt;span class="math"&gt;\(a_t\)&lt;/span&gt; 被用来导出softmax logit 和 loss。这类似于 seq2seq 模型顶层的目标隐藏状态。这个函数 &lt;code&gt;f&lt;/code&gt; 也可以采取其他形式。&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://freeopen.github.io/images/attention_equation_1.jpg" width="90%"/&gt;
&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;attention mechanisms 的各种实现可以在 &lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py"&gt;attention_wrapper.py&lt;/a&gt; 中找到.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;注意力机制有多重要?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如上述方程式所示，有许多不同的 attention 变体。这些变体取决于 score 函数和 attention 函数的形式，以及在 score 函数中是否使用上一个时间步的状态 &lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt; 而不是 &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; （源自 Bahdanau et al.,2015 的建议）。经验上，我们发现只有某些选择很重要。首先，是  attention 的基本形式，即目标语言和源语言之间的直接联系。第二，重要的是把 attention vector 送到下一个时间步，以通报网络关于过去的 attention 决定（源自Luong et al., 2015 中的示范）。最后，score 函数的选择常常会导致不同的表现。更多内容请看&lt;a href="#评测"&gt;评测结果&lt;/a&gt;部分。&lt;/p&gt;
&lt;h3 id="attention wrapper api"&gt;Attention Wrapper API&lt;/h3&gt;
&lt;p&gt;在实现 &lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py"&gt;AttentionWrapper&lt;/a&gt; 时，我们借鉴了 &lt;a href="https://arxiv.org/abs/1409.0473"&gt;(Weston et al., 2015)&lt;/a&gt; 在 &lt;em&gt;memory networks&lt;/em&gt; 方面的一些术语。本教程介绍的 attention mechanism 是只读 memory，而不是可读写的 memory。具体来说，一组源语隐藏状态（或其转换版本，如：Luong 的评分中的  &lt;span class="math"&gt;\(W\overline{h}_s\)&lt;/span&gt;，或 Bahong 的评分中的 &lt;span class="math"&gt;\(W_2\overline{h}_s\)&lt;/span&gt;）被作为 &lt;em&gt;&amp;ldquo;memory&amp;rdquo;&lt;/em&gt; 。在每个时间步骤中，我们使用当前的目标隐藏状态作为 &lt;em&gt;&amp;ldquo;query&amp;rdquo;&lt;/em&gt; 来决定要读取 memory 的哪个部分。通常，查询需要与对应于各个 memory 插槽的键值进行比较。在上述 attention mechanism 的介绍中，我们恰好将源语隐藏状态（或其转换版本，例如，Bahdanau 的评分中的 &lt;span class="math"&gt;\(W_1h_t\)&lt;/span&gt;）作为&amp;ldquo;键&amp;rdquo;值。可以通过这种记忆网络术语来启发其他形式的  attention！&lt;/p&gt;
&lt;p&gt;多亏了对 attention 的封装，使得我们扩展原始 seq2seq 代码时很方便。这部分文件参见 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/attention_model.py"&gt;&lt;em&gt;attention_model.py&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;首先，我们需要定义一个attention mechanism，例如(Luong et al., 2015):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# attention_states: [batch_size, max_time, num_units]&lt;/span&gt;
&lt;span class="n"&gt;attention_states&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Create an attention mechanism&lt;/span&gt;
&lt;span class="n"&gt;attention_mechanism&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LuongAttention&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attention_states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;memory_sequence_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;source_sequence_length&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在前面的&lt;a href="#编码器"&gt;编码器&lt;/a&gt; 部分，&lt;em&gt;encoder_outputs&lt;/em&gt; 是顶层所有源语隐藏状态的集合，其形状为 &lt;em&gt;[max_time，batch_size，num_units]&lt;/em&gt; （因为我们使用 &lt;em&gt;dynamic_rnn&lt;/em&gt;，&lt;em&gt;time_major&lt;/em&gt; 设置为 &lt;em&gt;True&lt;/em&gt; 以获得效率）。对于 attention mechanism，我们需要确保传递的&amp;ldquo;memory&amp;rdquo;是批处理的，所以我们需要转置 &lt;em&gt;attention_states&lt;/em&gt;。我们将 &lt;em&gt;source_sequence_length&lt;/em&gt; 传递给 attention machanism，以确保 attention weight 适当归一化（仅在非填充位置上）。&lt;/p&gt;
&lt;p&gt;定义了 attention mechanism 后，我们使用 &lt;em&gt;AttentionWrapper&lt;/em&gt; 来包装 decoding_cell：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;decoder_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AttentionWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;decoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attention_mechanism&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;attention_layer_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;代码的其余部分与&lt;a href="#解码器"&gt;解码器&lt;/a&gt;部分几乎相同！&lt;/p&gt;
&lt;h3 id="dong shou  - jian li ji yu  attention de  nmt mo xing"&gt;动手 &amp;ndash; 建立基于 attention 的 NMT 模型&lt;/h3&gt;
&lt;p&gt;要启用的 attention ，在训练时我们需要使用 &lt;code&gt;luong&lt;/code&gt;、&lt;code&gt;scaled_luong&lt;/code&gt;、&lt;code&gt;bahdanau&lt;/code&gt;  或 &lt;code&gt;normed_bahdanau&lt;/code&gt; 中的一个作为 &lt;code&gt;attention&lt;/code&gt; 标志的值。该标志指定了我们将要使用的 attention mechanism。此外，我们需要为 attention 模型创建一个新的目录，所以我们不能重复使用前面训练过的简单 NMT 模型。&lt;/p&gt;
&lt;p&gt;运行以下命令开始训练:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir /tmp/nmt_attention_model

python -m nmt.nmt &lt;span class="se"&gt;\&lt;/span&gt;
    --attention&lt;span class="o"&gt;=&lt;/span&gt;scaled_luong &lt;span class="se"&gt;\&lt;/span&gt;
    --src&lt;span class="o"&gt;=&lt;/span&gt;vi --tgt&lt;span class="o"&gt;=&lt;/span&gt;en &lt;span class="se"&gt;\&lt;/span&gt;
    --vocab_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/vocab  &lt;span class="se"&gt;\&lt;/span&gt;
    --train_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/train &lt;span class="se"&gt;\&lt;/span&gt;
    --dev_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/tst2012  &lt;span class="se"&gt;\&lt;/span&gt;
    --test_prefix&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_data/tst2013 &lt;span class="se"&gt;\&lt;/span&gt;
    --out_dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_attention_model &lt;span class="se"&gt;\&lt;/span&gt;
    --num_train_steps&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;12000&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --steps_per_stats&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --num_layers&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --num_units&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;128&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --dropout&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.2 &lt;span class="se"&gt;\&lt;/span&gt;
    --metrics&lt;span class="o"&gt;=&lt;/span&gt;bleu
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练后，我们可以使用相同的推理命令与新的 model_dir 进行推理：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python -m nmt.nmt &lt;span class="se"&gt;\&lt;/span&gt;
    --model_dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_attention_model &lt;span class="se"&gt;\&lt;/span&gt;
    --inference_input_file&lt;span class="o"&gt;=&lt;/span&gt;/tmp/my_infer_file.vi &lt;span class="se"&gt;\&lt;/span&gt;
    --inference_output_file&lt;span class="o"&gt;=&lt;/span&gt;/tmp/nmt_attention_model/output_infer
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="ti shi  &amp;amp; ji qiao_1"&gt;提示 &amp;amp; 技巧&lt;/h2&gt;
&lt;h3 id="xun lian , ping gu , he tui li tu"&gt;训练, 评估, 和推理图&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：&amp;ldquo;图&amp;rdquo;的定义原文，A Graph contains a set of Operation objects, which represent units of computation; and Tensor objects, which represent the units of data that flow between operations. 简单说, &amp;ldquo;图&amp;rdquo;相当于一个数学公式，其中的 tensor 相当于变量 x  ，而定义的 op 相当于连接变量的运算符号.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 TensorFlow 中构建机器学习模型时，最好建立三个独立的图：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;训练图, 其中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把来自文件或外部导入的数据批次化（即分成数量相同的很多组，每次处理一组），作为输入数据&lt;/li&gt;
&lt;li&gt;包含正向和反向传播操作.&lt;/li&gt;
&lt;li&gt;构建优化器，并加到训练中.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;评估图, 其中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;批次化输入数据.&lt;/li&gt;
&lt;li&gt;包含在训练图中用过的正向传播操作，增加了一个没在训练图中用过的评估操作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;推理图, 其中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可能不批量输入数据.&lt;/li&gt;
&lt;li&gt;不对输入数据进行子采样或批次化.&lt;/li&gt;
&lt;li&gt;从占位符读取输入数据（可以通过 &lt;em&gt;feed_dict&lt;/em&gt; 或 C ++ TensorFlow serving binary 将数据直接提供给推理图）.&lt;/li&gt;
&lt;li&gt;包括模型正向传播操作的一个子集，和一个额外的在 session.run 调用之间存储状态的特殊输入/输出（或有）.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分别构建图有几个好处:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推理图通常与其他两个很不同，因此分开构建是有意义的.&lt;/li&gt;
&lt;li&gt;由于没有反向转播操作，评估图变得更简单.&lt;/li&gt;
&lt;li&gt;数据分别提供给每个图.&lt;/li&gt;
&lt;li&gt;复用简单得多.  比如, 在评估图中，不需要用 &lt;em&gt;reuse=True&lt;/em&gt; 来打开变量的作用域，因为训练模型已经创建了这些变量.  所以相同的代码不需要加上 &lt;em&gt;reuse=&lt;/em&gt; 参数就能被复用.&lt;/li&gt;
&lt;li&gt;在分布式训练中，把训练、评估和推理的工作分开是很平常的。这就需要它们各自建立自己的图。因此，以这种方式构建的系统将为你提供分布式训练的准备。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;复杂性的主要来源是如何在单个计算机设置中跨三个图共享变量. 可以为每个图使用单独的会话来解决. 训练会话定期的保存检查点，评估会话和推断会话从检查点导入参数. 下面的例子显示了两种方法的主要区别.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;之前: 三个模型在一个图中并共享一个会话&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'root'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;train_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildTrainModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;initializer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'root'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reuse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;eval_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;eval_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildEvalModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'root'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reuse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;infer_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;inference_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildInferenceModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="n"&gt;train_input_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;train_inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;train_input_data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;EVAL_STEPS&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;data_to_eval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;eval_input_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
      &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;eval_loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;eval_inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;eval_input_data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;INFER_STEPS&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inference_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;infer_inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;infer_input_data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;之后: 三个模型在三个图中，有三个会话并共享同样的变量&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;eval_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;infer_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;train_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="n"&gt;train_iterator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;train_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildTrainModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_iterator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;initializer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;eval_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="n"&gt;eval_iterator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;eval_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildEvalModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_iterator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;infer_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="n"&gt;infer_iterator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;infer_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;infer_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BuildInferenceModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_iterator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;checkpoints_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"/tmp/model/checkpoints"&lt;/span&gt;

&lt;span class="n"&gt;train_sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;eval_sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eval_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;infer_sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;infer_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;

  &lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;EVAL_STEPS&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;checkpoint_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoints_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;data_to_eval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;eval_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_sess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;INFER_STEPS&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;checkpoint_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoints_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;infer_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;infer_sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;infer_inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;infer_input_data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;data_to_infer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;infer_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;infer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_sess&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意后一种方法是如何转换为分布式版本的。&lt;/p&gt;
&lt;p&gt;新方法的另一个区别在于，在每个 &lt;em&gt;session.run&lt;/em&gt; 调用时，我们使用有状态的迭代器对象来代替 &lt;em&gt;feed_dicts&lt;/em&gt; 提供数据（从而我们能自己批处理、切分和操作数据）。这些迭代器使"输入管道"在设置单机和分布式时，都容易很多。我们将在下一节中介绍新的输入数据管道（ 限TensorFlow 1.2 ）。&lt;/p&gt;
&lt;h3 id="shu ju shu ru guan dao"&gt;数据输入管道&lt;/h3&gt;
&lt;p&gt;在 TensorFlow 1.2 之前, 用户有三种方式把数据提供给 TensorFlow 进行训练和评估:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在每次调用 &lt;em&gt;session.run&lt;/em&gt; 时，用 &lt;em&gt;feed_dict&lt;/em&gt; 提供数据.&lt;/li&gt;
&lt;li&gt;在 &lt;em&gt;tf.train&lt;/em&gt; (e.g. &lt;em&gt;tf.train.batch&lt;/em&gt;) 和 &lt;em&gt;tf.contrib.train&lt;/em&gt; 中使用排队机制 .&lt;/li&gt;
&lt;li&gt;使用象 &lt;em&gt;tf.contrib.learn&lt;/em&gt; 或 &lt;em&gt;tf.contrib.slim&lt;/em&gt; 等高级框架中的 helper (用 #2 效率更高).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一种方法对于不熟悉 TensorFlow 的用户或只能在 Python 中需要做个性化输入（如：自己的小批次队列）的用户更容易。第二和第三种方法更标准，但灵活性稍差一些；他们还需要启动多个 python 线程（queue runners）。此外，如果使用错误的队列可能会导致死锁或不可知的错误。然而，队列比使用 &lt;em&gt;feed_dict&lt;/em&gt; 更高效，也是单机和分布式训练的标准方式。&lt;/p&gt;
&lt;p&gt;从 TensorFlow 1.2 开始，有一个新的方法可用于将数据读入 TensorFlow 模型：数据集迭代器（dataset iterators），可在 &lt;strong&gt;tf.contrib.data&lt;/strong&gt;  模块中找到。数据迭代器是灵活的、易理解的和可定制的，并能依赖 TensorFlow C ++ 运行库提供高效和多线程的读入操作。&lt;/p&gt;
&lt;p&gt;一个&lt;strong&gt;数据集&lt;/strong&gt;可以从一批数据张量、一个文件名，或包含多个文件名的一个张量来创建。举例如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Training dataset consists of multiple files.&lt;/span&gt;
&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextLineDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_files&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Evaluation dataset uses a single file, but we may&lt;/span&gt;
&lt;span class="c1"&gt;# point to a different file for each evaluation round.&lt;/span&gt;
&lt;span class="n"&gt;eval_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;eval_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextLineDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# For inference, feed input data to the dataset directly via feed_dict.&lt;/span&gt;
&lt;span class="n"&gt;infer_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_infer_examples&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;infer_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_tensor_slices&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infer_batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;所有数据集的输入处理都是类似的。包括数据的读取和清理，数据的切分（在训练和评估时）、过滤及批处理等。&lt;/p&gt;
&lt;p&gt;把每个句子转换成单词的字符串向量，例如，我们对数据集做 map 转换：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string_split&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后，我们把每个句子向量切换成一个包含向量及其动态长度的元组：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，我们对每个句子执行词汇查找。根据给定的查找表，把元组中的元素从字符串向量转换为整数向量。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;拼接两个数据集也很容易。如果有两个彼此逐行互译的文件，且每个文件都有自己的数据集，则可按以下方式创建一个新数据集来合并这两个数据集：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;source_target_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;source_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_dataset&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;不定长句子的批处理是简单明了的。接下来对 &lt;em&gt;source_target_dataset&lt;/em&gt; 数据集中的元素按 &lt;em&gt;batch_size&lt;/em&gt; 的大小规格做批次转换， 同时在每批中，把源向量和目标向量进行填充，使其长度与它们当中最长的向量长度一致。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;译者注：这句话不是人翻的，不信你去看原文，我是看了半天下面的 python 源码才搞出来，我在怀疑是我的语文不好还是作者的语文不好，哎！  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;batched_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;source_target_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;padded_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;padded_shapes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TensorShape&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;  &lt;span class="c1"&gt;# source vectors of unknown size&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TensorShape&lt;/span&gt;&lt;span class="p"&gt;([])),&lt;/span&gt;     &lt;span class="c1"&gt;# size(source)&lt;/span&gt;
                   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TensorShape&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;  &lt;span class="c1"&gt;# target vectors of unknown size&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TensorShape&lt;/span&gt;&lt;span class="p"&gt;([]))),&lt;/span&gt;    &lt;span class="c1"&gt;# size(target)&lt;/span&gt;
    &lt;span class="n"&gt;padding_values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;src_eos_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# source vectors padded on the right with src_eos_id&lt;/span&gt;
                     &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;          &lt;span class="c1"&gt;# size(source) -- unused&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tgt_eos_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# target vectors padded on the right with tgt_eos_id&lt;/span&gt;
                     &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;         &lt;span class="c1"&gt;# size(target) -- unused&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;从这个数据集传出的值是一个嵌套元组，其张量最左边的维度就是 &lt;em&gt;batch_size&lt;/em&gt; 的大小。该结构为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;iterator[0][0] 是分好批和填充好的源句子矩阵.&lt;/li&gt;
&lt;li&gt;iterator[0][1] 是分好批的源句子长度向量.&lt;/li&gt;
&lt;li&gt;iterator[1][0] 是分好批和填充好的目标句子矩阵.&lt;/li&gt;
&lt;li&gt;iterator[1][1] 是分好批的目标句子长度向量.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后, 尽量把这些长度相似的源句子打包在一起。更多内容及完整实现请参阅 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py"&gt;utils/iterator_utils.py&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;从数据集读取数据仅需三行代码：创建迭代器，获取其值，和初始化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;batched_iterator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batched_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_initializable_iterator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;source_lengths&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_lenghts&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batched_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# At initialization time.&lt;/span&gt;
&lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batched_iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;一旦迭代器被初始化，每个 &lt;em&gt;session.run&lt;/em&gt; 调用（访问源或目标张量）将从底层数据集请求下一批数据。&lt;/p&gt;
&lt;h3 id="guan yu zeng qiang  nmt mo xing de qi ta xi jie"&gt;关于增强 NMT 模型的其它细节&lt;/h3&gt;
&lt;h4&gt;双向 RNNs&lt;/h4&gt;
&lt;p&gt;在编码器端双向化通常会带来更好的性能(随着层数的增加速度会有所降低). 这里, 我们给个简单的例子，建立一个双向单层的编码器：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Construct forward and backward cells&lt;/span&gt;
&lt;span class="n"&gt;forward_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicLSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;backward_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasicLSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;bi_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bidirectional_dynamic_rnn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;forward_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;backward_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder_emb_inp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;sequence_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;source_sequence_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time_major&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;encoder_outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bi_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;变量 &lt;em&gt;encoder_outputs&lt;/em&gt; 和 &lt;em&gt;encoder_state&lt;/em&gt; 的用法与编码器那节中说的一样. 注意, 对多个双向层, 我们需要对 encoder_state 做些修改, 详见 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py"&gt;model.py&lt;/a&gt;,中的 &lt;em&gt;_build_bidirectional_rnn()&lt;/em&gt; 方法 .&lt;/p&gt;
&lt;h4&gt;定向搜索&lt;/h4&gt;
&lt;p&gt;虽然用贪心法解码能带给我们较满意的翻译质量，但用定向搜索解码能进一步提高性能。定向搜索的想法是，在翻译的同时，保留一个小小的最佳候选集以便更容易检索。这个定向的范围称为 &lt;em&gt;beam width&lt;/em&gt;; 一般这个值设为 10 就够了. 更多信息请参阅 &lt;a href="https://arxiv.org/abs/1703.01619"&gt;Neubig, (2017)&lt;/a&gt; 第 7.2.3 节。举例如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Replicate encoder infos beam_width times&lt;/span&gt;
&lt;span class="n"&gt;decoder_initial_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tile_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;encoder_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;multiplier&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hparams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;beam_width&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Define a beam-search decoder&lt;/span&gt;
&lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BeamSearchDecoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;cell&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;decoder_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;embedding_decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;start_tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;start_tokens&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;end_token&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;end_token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;initial_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;decoder_initial_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;beam_width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;beam_width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;output_layer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;projection_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;length_penalty_weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Dynamic decoding&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dynamic_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意这里对 &lt;em&gt;dynamic_decode()&lt;/em&gt; API 的调用和&lt;a href="#解码器"&gt;解码器&lt;/a&gt;那节一样。解码后，我们就能向下面那样获取翻译结果了：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;translations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predicted_ids&lt;/span&gt;
&lt;span class="c1"&gt;# Make sure translations shape is [batch_size, beam_width, time]&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time_major&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
   &lt;span class="n"&gt;translations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;translations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;关于 &lt;em&gt;_build_decoder()&lt;/em&gt; 方法的更多信息，详见 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py"&gt;model.py&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;超参数&lt;/h4&gt;
&lt;p&gt;有几个超参数可以提高性能。这里，我们根据自己的经验列出一些［免责声明：其它人可能不同意我说的］。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;优化&lt;/em&gt;&lt;/strong&gt;: Adam 优化器有些不太寻常的结构, 如果你用 SGD （随机梯度下降算法）训练，它一般会带来更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Attention&lt;/em&gt;&lt;/strong&gt;: 在编码器端，Bahdanau-style attention 通常需要双向 RNN 才运行良好; 而 Luong-style attention 适用于不同的设置. 在本教程中, 我们推荐使用 Luong 和 Bahdanau-style attentions 的改进版本： &lt;em&gt;scaled_luong&lt;/em&gt; 和 &lt;em&gt;normed bahdanau&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;多 GPU 训练&lt;/h4&gt;
&lt;p&gt;训练一个 NMT 模型可能要好几天。把不同的 RNN 层放到不同的 GPU 上，能提高训练速度。下面是在多 GPU 上创建 RNN 层的例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DeviceWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
      &lt;span class="s2"&gt;"/gpu:&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;num_gpus&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MultiRNNCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;另外，我们要在&lt;code&gt;tf.gradients&lt;/code&gt; 中启用 &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt;选项，才可以对梯度进行并行计算。 &lt;/p&gt;
&lt;p&gt;你可能注意到即使增加GPU，对基于 attention 的 NMT 模型的速度提升也很小。attention 架构的一个主要缺点是，在每一个时间步，采用顶层（即最后一层）输出来查询 attention。这就意味着每次解码时必须等前面的所以步骤完成才行；因此，我们不能简单的把 RNN 层放在多个 GPU 上来并行解码。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1609.08144.pdf"&gt;GNMT attention architecture&lt;/a&gt; 提出，通过使用底层（即第一层）输出来查询 attention 来并行解码运算。这样，每次解码就能在前面的第一层完成后开始。我们在 &lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/gnmt_model.py"&gt;GNMTAttentionMultiCell&lt;/a&gt; 中的子类 &lt;em&gt;tf.contrib.rnn.MultiRNNCell&lt;/em&gt;上实现此架构，下面是用 &lt;em&gt;GNMTAttentionMultiCell&lt;/em&gt; 创建一个解码单元的示例。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DeviceWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
      &lt;span class="s2"&gt;"/gpu:&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;num_gpus&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;attention_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;attention_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seq2seq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AttentionWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;attention_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;attention_mechanism&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;attention_layer_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# don't add an additional dense layer.&lt;/span&gt;
    &lt;span class="n"&gt;output_attention&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;span class="n"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GNMTAttentionMultiCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attention_cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="ping ce_1"&gt;评测&lt;/h2&gt;
&lt;h3 id="iwslt ying yu -yue nan yu"&gt;IWSLT 英语-越南语&lt;/h3&gt;
&lt;p&gt;样本集: 133K examples, 训练集=tst2012, 测试集=tst2013,
&lt;a href="https://github.com/tensorflow/nmt/blob/master/nmt/scripts/download_iwslt15.sh"&gt;下载脚本&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;训练细节&lt;/em&gt;&lt;/strong&gt;. 我们用双向编码器（编码器有一个双向层）训练 512 单元的 2 层 LSTM，embedding 维度为 512。LuongAttention (scale=True) 与 keep_prob 为 0.8 的 dropout 一起使用。所有参数均匀。我们采用学习率为 1.0 的 SGD 算法：训练12K 步（12 轮）；8K 步后，我们开始每 1K 步减半学习率。 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;结果&lt;/em&gt;&lt;/strong&gt;.
TODO(rzhao): 添加英语-越南语模型的 URL。&lt;/p&gt;
&lt;p&gt;以下是两个模型的平均结果
(&lt;a href="http://download.tensorflow.org/models/nmt/envi_model_1.zip"&gt;model 1&lt;/a&gt;, &lt;a href="http://download.tensorflow.org/models/nmt/envi_model_2.zip"&gt;model 2&lt;/a&gt;).
我们用 BLEU 评分来评估翻译质量 &lt;a href="http://www.aclweb.org/anthology/P02-1040.pdf"&gt;(Papineni et al., 2002)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;tst2012 (dev)&lt;/th&gt;
&lt;th align="center"&gt;test2013 (test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NMT (greedy)&lt;/td&gt;
&lt;td align="center"&gt;23.2&lt;/td&gt;
&lt;td align="center"&gt;25.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT (beam=10)&lt;/td&gt;
&lt;td align="center"&gt;23.8&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;26.1&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://stanford.edu/~lmthang/data/papers/iwslt15.pdf"&gt;(Luong &amp;amp; Manning, 2015)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;23.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;训练速度&lt;/strong&gt;: 在 &lt;em&gt;K40m&lt;/em&gt; 上 (0.37 秒每步 , 15.3K 字每秒)  &amp;amp; 在 &lt;em&gt;TitanX&lt;/em&gt; 上 (0.17 秒每步, 32.2K 字每秒) .
这里，每步时间指运行一个小批量（128个）所需的时间。对于字每秒，我们统计的是源和目标上的单词。&lt;/p&gt;
&lt;h3 id="wmt de yu -ying yu"&gt;WMT 德语-英语&lt;/h3&gt;
&lt;p&gt;样本集: 4.5M examples, 训练集=newstest2013, 测试集=newstest2015
&lt;a href="nmt/scripts/wmt16_en_de.sh"&gt;下载脚本&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;训练细节&lt;/em&gt;&lt;/strong&gt;. 我们训练的超参数与英语-越南语的实验类似，除了以下细节. 采用 &lt;a href="https://github.com/rsennrich/subword-nmt"&gt;BPE&lt;/a&gt;(32K 操作)将数据分成子字单元. 我们用双向编码器 1024（编码器有2个双向层）训练 1024 单元的 4 层 LSTM , embedding 维度为 1024. 我们训练了 359K 步 (10轮); 170K 步后, 我们开始每 17K 步减半学习率.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;结果&lt;/em&gt;&lt;/strong&gt;.
TODO(rzhao): 添加德语-英语模型的 URL.&lt;/p&gt;
&lt;p&gt;前 2 行是模型 1、2 的评价结果(&lt;a href="http://download.tensorflow.org/models/nmt/deen_model_1.zip"&gt;model 1&lt;/a&gt;,&lt;a href="http://download.tensorflow.org/models/nmt/deen_model_2.zip"&gt;model 2&lt;/a&gt;).
第 4 行是运行在 4 个 GPU 上的 GNMT attention &lt;a href="http://download.tensorflow.org/models/nmt/deen_gnmt_model_4_layer.zip"&gt;模型&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;newstest2013 (dev)&lt;/th&gt;
&lt;th align="center"&gt;newstest2015&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NMT (greedy)&lt;/td&gt;
&lt;td align="center"&gt;27.1&lt;/td&gt;
&lt;td align="center"&gt;27.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT (beam=10)&lt;/td&gt;
&lt;td align="center"&gt;28.0&lt;/td&gt;
&lt;td align="center"&gt;28.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT + GNMT attention (beam=10)&lt;/td&gt;
&lt;td align="center"&gt;29.0&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;29.9&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://matrix.statmt.org/"&gt;WMT SOTA&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;29.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这些结果表明，我们的代码为 NMT 建立了强大的基线系统。
(请注意，WMT 系统通常会使用大量的单种语料数据，我们目前没有。)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;训练速度&lt;/strong&gt;: 在 &lt;em&gt;Nvidia K40m&lt;/em&gt; 上 (2.1 秒每步, 3.4K 字每秒)  &amp;amp; 在 &lt;em&gt;Nvidia TitanX&lt;/em&gt; 上(0.7 秒每步, 8.7K 字每秒) 。
为看 GNMT attention 的速度提升效果, 我们仅在 &lt;em&gt;K40m&lt;/em&gt; 上做测试:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;1 gpu&lt;/th&gt;
&lt;th align="center"&gt;4 gpus&lt;/th&gt;
&lt;th align="center"&gt;8 gpus&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NMT (4 layers)&lt;/td&gt;
&lt;td align="center"&gt;2.2s, 3.4K&lt;/td&gt;
&lt;td align="center"&gt;1.9s, 3.9K&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT (8 layers)&lt;/td&gt;
&lt;td align="center"&gt;3.5s, 2.0K&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;2.9s, 2.4K&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT + GNMT attention (4 layers)&lt;/td&gt;
&lt;td align="center"&gt;2.6s, 2.8K&lt;/td&gt;
&lt;td align="center"&gt;1.7s, 4.3K&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMT + GNMT attention (8 layers)&lt;/td&gt;
&lt;td align="center"&gt;4.2s, 1.7K&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;1.9s, 3.8K&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这些结果表明，没有 GNMT attention，使用多 GPU 获得的效果很小。而使用 GNMT attention，我们从多 GPU 上获得了 50%-100% 的速度提升。&lt;/p&gt;
&lt;h3 id="wmt ying yu -de yu  -- wan quan bi jiao"&gt;WMT 英语-德语 &amp;mdash; 完全比较&lt;/h3&gt;
&lt;p&gt;前 2 行是 GNMT attention 模型: &lt;a href="http://download.tensorflow.org/models/nmt/ende_gnmt_model_4_layer.zip"&gt;model 1 (4 层)&lt;/a&gt;,&lt;a href="http://download.tensorflow.org/models/nmt/ende_gnmt_model_8_layer.zip"&gt;model 2 (8 层)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;newstest2014&lt;/th&gt;
&lt;th align="center"&gt;newstest2015&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Ours&lt;/em&gt; &amp;mdash; NMT + GNMT attention (4 layers)&lt;/td&gt;
&lt;td align="center"&gt;23.7&lt;/td&gt;
&lt;td align="center"&gt;26.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Ours&lt;/em&gt; &amp;mdash; NMT + GNMT attention (8 layers)&lt;/td&gt;
&lt;td align="center"&gt;24.4&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;27.6&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://matrix.statmt.org/"&gt;WMT SOTA&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;20.6&lt;/td&gt;
&lt;td align="center"&gt;24.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenNMT &lt;a href="https://arxiv.org/abs/1701.02810"&gt;(Klein et al., 2017)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;19.3&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tf-seq2seq &lt;a href="https://arxiv.org/abs/1703.03906"&gt;(Britz et al., 2017)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;22.2&lt;/td&gt;
&lt;td align="center"&gt;25.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GNMT &lt;a href="https://research.google.com/pubs/pub45610.html"&gt;(Wu et al., 2016)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;24.6&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;上面的结果表明，我们的模型在类似架构中有很强的竞争力。
注意，OpenNMT 使用较小的模型，而目前在 Transformer network &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Vaswani et al., 2017&lt;/a&gt;中获得的最佳结果为 28.4，不过这是明显不同的架构.&lt;/p&gt;
&lt;h2 id="qi ta zi yuan_1"&gt;其它资源&lt;/h2&gt;
&lt;p&gt;为深入了解神经机器翻译和序列到序列模型，我们强烈推荐下面的材料
&lt;a href="https://sites.google.com/site/acl16nmt/"&gt;Luong, Cho, Manning, (2016)&lt;/a&gt;;
&lt;a href="https://github.com/lmthang/thesis"&gt;Luong, (2016)&lt;/a&gt;;
and &lt;a href="https://arxiv.org/abs/1703.01619"&gt;Neubig, (2017)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;可用不同的工具构建 seq2seq 模型，我们每样选了一种: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stanford NMT &lt;a href="https://nlp.stanford.edu/projects/nmt/"&gt;https://nlp.stanford.edu/projects/nmt/&lt;/a&gt;&lt;em&gt;[Matlab]&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;tf-seq2seq &lt;a href="https://github.com/google/seq2seq"&gt;https://github.com/google/seq2seq&lt;/a&gt;&lt;em&gt;[TensorFlow]&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;Nemantus &lt;a href="https://github.com/rsennrich/nematus"&gt;https://github.com/rsennrich/nematus&lt;/a&gt;&lt;em&gt;[Theano]&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;OpenNMT &lt;a href="http://opennmt.net/"&gt;http://opennmt.net/&lt;/a&gt; &lt;em&gt;[Torch]&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zhi xie"&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们要感谢 Denny Britz, Anna Goldie, Derek Murray, 和 Cinjon Resnick 为 TensorFlow 和 seq2seq 库带来的新特性. 还要感谢 Lukasz Kaiser 在 seq2seq 代码库上最初的帮助; Quoc Le 提议复现一个 GNMT; Yonghui Wu 和 Zhifeng Chen 负责 GNMT 系统的细节; 同时还要感谢谷歌大脑 （Google Brain ）团队的支持和反馈!&lt;/p&gt;
&lt;h2 id="can kao"&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015.&lt;a href="https://arxiv.org/pdf/1409.0473.pdf"&gt; Neural machine translation by jointly learning to align and translate&lt;/a&gt;. ICLR.&lt;/li&gt;
&lt;li&gt;Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015.&lt;a href="https://arxiv.org/pdf/1508.04025.pdf"&gt; Effective approaches to attention-based neural machine translation&lt;/a&gt;. EMNLP.&lt;/li&gt;
&lt;li&gt;Ilya Sutskever, Oriol Vinyals, and Quoc
V. Le. 2014.&lt;a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"&gt; Sequence to sequence learning with neural networks&lt;/a&gt;. NIPS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="yi hou yu"&gt;译后语&lt;/h2&gt;
&lt;p&gt;这是我发表的第一篇译文，本来是自己想弄明白这份教程，进而把代码吃透，再顺便弄个自己的翻译系统当玩具。结果把原文读了几遍，发现仍然头晕脑胀，不知所云，毕竟专业术语太多，需要的背景知识也不少，索性一发狠就把它译了出来。人笨，大约花了5天时间。&lt;/p&gt;
&lt;p&gt;翻译是直接在原文上编辑，所以原文里的排版、链接、跳转都得以保留，而且还修改了原文中几个显示不正确的数学符号。文中的标点符号半角、全角混用，其实我倾向于全部用半角符号，因为如果有英文，采用全角的标点符号很丑。但大家知道的，中文输入法自动就出中文标点符号，于是索性不管了，遇到什么就是什么。发现很多术语如果翻出来，特别影响理解，比如本文中的&amp;ldquo;attention&amp;rdquo;、&amp;ldquo;embedding&amp;rdquo;，不管译成什么，放到句中一读，立即变成天书。思来想去，不如保留原文，你就把它当作一个标识，反而更容易理解。原文中还有些地方写得比较晦涩，我为了说清楚（其实是保证自己弄明白），要么意译，要么直接把人家一句扩成三句。哎，个中滋味，一把心酸泪。&lt;/p&gt;
&lt;p&gt;文中错误，再所难免，欢迎大家批评指正，我知错就改。我给本 blog 加了评论功能，方便大家讨论交流。该评论系统来自海外，如果你是天朝子民，可能需要翻墙。
(18.1.31补：把需要翻墙的评论关了，因为发现如果在墙内，评论代码会影响右侧目录的功能，而方便阅读才是我最想要的）&lt;/p&gt;
&lt;p&gt;最后应个景，如果有读者想用力鼓励我一下，欢迎&lt;a href="/pages/da-shang.html"&gt;打赏&lt;/a&gt; 。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry></feed>