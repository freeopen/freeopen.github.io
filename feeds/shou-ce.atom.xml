<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Freeopen - 手册</title><link href="https://freeopen.github.io/" rel="alternate"></link><link href="https://freeopen.github.io/feeds/shou-ce.atom.xml" rel="self"></link><id>https://freeopen.github.io/</id><updated>2018-02-26T00:00:00+08:00</updated><entry><title>机器学习术语表</title><link href="https://freeopen.github.io/posts/ji-qi-xue-xi-zhu-yu-biao" rel="alternate"></link><published>2018-02-26T00:00:00+08:00</published><updated>2018-02-26T00:00:00+08:00</updated><author><name>freeopen</name></author><id>tag:freeopen.github.io,2018-02-26:/posts/ji-qi-xue-xi-zhu-yu-biao</id><summary type="html">&lt;p&gt;&lt;a href="https://developers.google.com/machine-learning/glossary/"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为什么做双语版本？当你读论文发现陌生英文术语时，就知道它的好处了。
有不准确的地方请来信告知，我会即时更正。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本术语表定义了一般机器学习术语以及特定于 TensorFlow 的术语。&lt;/p&gt;
&lt;h2 id="a"&gt;A&lt;/h2&gt;
&lt;h3 id="a/b testing"&gt;A/B testing&lt;/h3&gt;
&lt;p&gt;A statistical way of comparing two (or more) techniques, typically an incumbent against a new rival. A/B testing aims to determine not only which technique performs better but also to understand whether the difference is statistically …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://developers.google.com/machine-learning/glossary/"&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为什么做双语版本？当你读论文发现陌生英文术语时，就知道它的好处了。
有不准确的地方请来信告知，我会即时更正。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本术语表定义了一般机器学习术语以及特定于 TensorFlow 的术语。&lt;/p&gt;
&lt;h2 id="a"&gt;A&lt;/h2&gt;
&lt;h3 id="a/b testing"&gt;A/B testing&lt;/h3&gt;
&lt;p&gt;A statistical way of comparing two (or more) techniques, typically an incumbent against a new rival. A/B testing aims to determine not only which technique performs better but also to understand whether the difference is statistically significant. A/B testing usually considers only two techniques using one measurement, but it can be applied to any finite number of techniques and measures.&lt;/p&gt;
&lt;p&gt;一种统计方法，用于将两种或多种技术进行比较，通常是将当前采用的技术与新技术进行比较。A/B 测试不仅旨在确定哪种技术的效果更好，而且还有助于了解相应差异是否具有显著的统计意义。A/B 测试通常是采用一种衡量方式对两种技术进行比较，但也适用于任意有限数量的技术和衡量方式。&lt;/p&gt;
&lt;h3 id="accuracy"&gt;accuracy&lt;/h3&gt;
&lt;p&gt;The fraction of predictions that a &lt;a href="#classification model"&gt;&lt;strong&gt;classification model&lt;/strong&gt;&lt;/a&gt; got right. In &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification&lt;/strong&gt;&lt;/a&gt;, accuracy is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$ \text{Accuracy} = \frac{\text{Correct Predictions}} {\text{Total Number Of Examples}} $$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt;, accuracy has the following definition:&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}} {\text{Total Number Of Examples}}$$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;See &lt;a href="#true positive (tp)"&gt;&lt;strong&gt;true positive&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#true negative (tn)"&gt;&lt;strong&gt;true negative&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;的正确预测所占的比例。在&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类&lt;/strong&gt;&lt;/a&gt;中，准确率的定义如下：&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{准确率} = \frac{\text{正确的预测数}} {\text{样本总数}}$$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;中，准确率的定义如下：&lt;/p&gt;
&lt;p&gt;&lt;mj&gt; &lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{准确率} = \frac{\text{真正例数} + \text{真负例数}} {\text{样本总数}}$$&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;请参阅&lt;a href="#true positive (tp)"&gt;&lt;strong&gt;真正例&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#true negative (tn)"&gt;&lt;strong&gt;真负例&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="activation function"&gt;activation function&lt;/h3&gt;
&lt;p&gt;A function (for example, &lt;a href="#rectified linear unit (relu)"&gt;&lt;strong&gt;ReLU&lt;/strong&gt;&lt;/a&gt; or &lt;a href="#sigmoid function"&gt;&lt;strong&gt;sigmoid&lt;/strong&gt;&lt;/a&gt;) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer.&lt;/p&gt;
&lt;p&gt;一种函数（例如 &lt;a href="#rectified linear unit (relu)"&gt;&lt;strong&gt;ReLU&lt;/strong&gt;&lt;/a&gt; 或 &lt;a href="#sigmoid function"&gt;&lt;strong&gt;S 型&lt;/strong&gt;&lt;/a&gt;函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。&lt;/p&gt;
&lt;h3 id="adagrad"&gt;AdaGrad&lt;/h3&gt;
&lt;p&gt;A sophisticated gradient descent algorithm that rescales the gradients of each parameter, effectively giving each parameter an independent &lt;a href="#learning rate"&gt;&lt;strong&gt;learning rate&lt;/strong&gt;&lt;/a&gt;. For a full explanation, see &lt;a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种先进的梯度下降法，用于重新调整每个参数的梯度，以便有效地为每个参数指定独立的&lt;a href="#learning rate"&gt;&lt;strong&gt;学习速率&lt;/strong&gt;&lt;/a&gt;。如需查看完整的解释，请参阅&lt;a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf"&gt;这篇论文&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="auc (area under the roc curve)"&gt;AUC (Area under the ROC Curve)&lt;/h3&gt;
&lt;p&gt;An evaluation metric that considers all possible &lt;a href="#classification threshold"&gt;&lt;strong&gt;classification thresholds&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Area Under the &lt;a href="#roc-receiver-operating-characteristic-curve"&gt;ROC curve&lt;/a&gt; is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.&lt;/p&gt;
&lt;p&gt;一种会考虑所有可能&lt;a href="#classification threshold"&gt;&lt;strong&gt;分类阈值&lt;/strong&gt;&lt;/a&gt;的评估指标。&lt;/p&gt;
&lt;p&gt;&lt;a href="#roc-receiver-operating-characteristic-curve"&gt;ROC 曲线&lt;/a&gt;下面积是，对于随机选择的正类别样本确实为正类别，以及随机选择的负类别样本为正类别，分类器更确信前者的概率。&lt;/p&gt;
&lt;h2 id="b_1"&gt;B&lt;/h2&gt;
&lt;h3 id="backpropagation"&gt;backpropagation&lt;/h3&gt;
&lt;p&gt;The primary algorithm for performing &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; on &lt;a href="#neural network"&gt;&lt;strong&gt;neural networks&lt;/strong&gt;&lt;/a&gt;. First, the output values of each node are calculated (and cached) in a forward pass. Then, the &lt;a href="https://en.wikipedia.org/wiki/Partial_derivative"&gt;partial derivative&lt;/a&gt; of the error with respect to each parameter is calculated in a backward pass through the graph.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;上执行&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的&lt;a href="https://en.wikipedia.org/wiki/Partial_derivative"&gt;偏导数&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="baseline"&gt;baseline&lt;/h3&gt;
&lt;p&gt;A simple &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; or heuristic used as reference point for comparing how well a model is performing. A baseline helps model developers quantify the minimal, expected performance on a particular problem.&lt;/p&gt;
&lt;p&gt;一种简单的&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;或启发法，用作比较模型效果时的参考点。基准有助于模型开发者针对特定问题量化最低预期效果。&lt;/p&gt;
&lt;h3 id="batch"&gt;batch&lt;/h3&gt;
&lt;p&gt;The set of examples used in one &lt;a href="#iteration"&gt;&lt;strong&gt;iteration&lt;/strong&gt;&lt;/a&gt; (that is, one &lt;a href="#gradient"&gt;&lt;strong&gt;gradient&lt;/strong&gt;&lt;/a&gt; update) of &lt;a href="#model training"&gt;&lt;strong&gt;model training&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See also &lt;a href="#batch size"&gt;&lt;strong&gt;batch size&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#model training"&gt;&lt;strong&gt;模型训练&lt;/strong&gt;&lt;/a&gt;的一次&lt;a href="#iteration"&gt;&lt;strong&gt;迭代&lt;/strong&gt;&lt;/a&gt;（即一次&lt;a href="#gradient"&gt;&lt;strong&gt;梯度&lt;/strong&gt;&lt;/a&gt;更新）中使用的样本集。&lt;/p&gt;
&lt;p&gt;另请参阅&lt;a href="#batch size"&gt;&lt;strong&gt;批次规模&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="batch size"&gt;batch size&lt;/h3&gt;
&lt;p&gt;The number of examples in a &lt;a href="#batch"&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/a&gt;. For example, the batch size of &lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;SGD&lt;/strong&gt;&lt;/a&gt; is 1, while the batch size of a &lt;a href="#mini-batch"&gt;&lt;strong&gt;mini-batch&lt;/strong&gt;&lt;/a&gt; is usually between 10 and 1000. Batch size is usually fixed during training and inference; however, TensorFlow does permit dynamic batch sizes.&lt;/p&gt;
&lt;p&gt;一个&lt;a href="#batch"&gt;&lt;strong&gt;批次&lt;/strong&gt;&lt;/a&gt;中的样本数。例如，&lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;SGD&lt;/strong&gt;&lt;/a&gt; 的批次规模为 1，而&lt;a href="#mini-batch"&gt;&lt;strong&gt;小批次&lt;/strong&gt;&lt;/a&gt;的规模通常介于 10 到 1000 之间。批次规模在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次规模。&lt;/p&gt;
&lt;h3 id="bias"&gt;bias&lt;/h3&gt;
&lt;p&gt;An intercept or offset from an origin. Bias (also known as the &lt;strong&gt;bias term&lt;/strong&gt;) is referred to as &lt;span class="math"&gt;\(b\)&lt;/span&gt; or &lt;span class="math"&gt;\(w_0\)&lt;/span&gt; in machine learning models. For example, bias is the &lt;em&gt;b&lt;/em&gt; in the following formula:&lt;/p&gt;
&lt;p&gt;距离原点的截距或偏移。偏差（也称为&lt;strong&gt;偏差项&lt;/strong&gt;）在机器学习模型中以 &lt;span class="math"&gt;\(b\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(w_0\)&lt;/span&gt; 表示。例如，在下面的公式中，偏差为 b：&lt;/p&gt;
&lt;div class="math"&gt;$$y' = b + w_1x_1 + w_2x_2 + &amp;hellip; w_nx_n$$&lt;/div&gt;
&lt;p&gt;Not to be confused with &lt;a href="#prediction bias"&gt;&lt;strong&gt;prediction bias&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;请勿与&lt;a href="#prediction bias"&gt;&lt;strong&gt;预测偏差&lt;/strong&gt;&lt;/a&gt;混淆。&lt;/p&gt;
&lt;h3 id="binary classification"&gt;binary classification&lt;/h3&gt;
&lt;p&gt;A type of classification task that outputs one of two mutually exclusive classes. For example, a machine learning model that evaluates email messages and outputs either "spam" or "not spam" is a binary classifier.&lt;/p&gt;
&lt;p&gt;一种分类任务，可输出两种互斥类别之一。例如，对电子邮件进行评估并输出&amp;ldquo;垃圾邮件&amp;rdquo;或&amp;ldquo;非垃圾邮件&amp;rdquo;的机器学习模型就是一个二元分类器。&lt;/p&gt;
&lt;h3 id="binning"&gt;binning&lt;/h3&gt;
&lt;p&gt;See &lt;a href="#bucketing"&gt;&lt;strong&gt;bucketing&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="bucketing"&gt;bucketing&lt;/h3&gt;
&lt;p&gt;Converting a (usually &lt;a href="#continuous feature"&gt;&lt;strong&gt;continuous&lt;/strong&gt;&lt;/a&gt;) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin.&lt;/p&gt;
&lt;p&gt;将一个特征（通常是&lt;a href="#continuous feature"&gt;&lt;strong&gt;连续&lt;/strong&gt;&lt;/a&gt;特征）转换成多个二元特征（称为桶或箱），通常是根据值区间进行转换。例如，您可以将温度区间分割为离散分箱，而不是将温度表示成单个连续的浮点特征。假设温度数据可精确到小数点后一位，则可以将介于 0.0 到 15.0 度之间的所有温度都归入一个分箱，将介于 15.1 到 30.0 度之间的所有温度归入第二个分箱，并将介于 30.1 到 50.0 度之间的所有温度归入第三个分箱。&lt;/p&gt;
&lt;h2 id="c_1"&gt;C&lt;/h2&gt;
&lt;h3 id="calibration layer"&gt;calibration layer&lt;/h3&gt;
&lt;p&gt;A post-prediction adjustment, typically to account for &lt;a href="#prediction bias"&gt;&lt;strong&gt;prediction bias&lt;/strong&gt;&lt;/a&gt;. The adjusted predictions and probabilities should match the distribution of an observed set of labels.&lt;/p&gt;
&lt;p&gt;一种预测后调整，通常是为了降低&lt;a href="#prediction bias"&gt;&lt;strong&gt;预测偏差&lt;/strong&gt;&lt;/a&gt;。调整后的预测和概率应与观察到的标签集的分布一致。&lt;/p&gt;
&lt;h3 id="candidate sampling"&gt;candidate sampling&lt;/h3&gt;
&lt;p&gt;A training-time optimization in which a probability is calculated for all the positive labels, using, for example, softmax, but only for a random sample of negative labels. For example, if we have an example labeled &lt;em&gt;beagle&lt;/em&gt; and &lt;em&gt;dog&lt;/em&gt; candidate sampling computes the predicted probabilities and corresponding loss terms for the &lt;em&gt;beagle&lt;/em&gt; and &lt;em&gt;dog&lt;/em&gt; class outputs in addition to a random subset of the remaining classes (&lt;em&gt;cat&lt;/em&gt;, &lt;em&gt;lollipop&lt;/em&gt;, &lt;em&gt;fence&lt;/em&gt;). The idea is that the &lt;a href="#negative class"&gt;&lt;strong&gt;negative classes&lt;/strong&gt;&lt;/a&gt; can learn from less frequent negative reinforcement as long as &lt;a href="#positive class"&gt;&lt;strong&gt;positive classes&lt;/strong&gt;&lt;/a&gt; always get proper positive reinforcement, and this is indeed observed empirically. The motivation for candidate sampling is a computational efficiency win from not computing predictions for all negatives.&lt;/p&gt;
&lt;p&gt;一种训练时进行的优化，会使用某种函数（例如 softmax）针对所有正类别标签计算概率，但对于负类别标签，则仅针对其随机样本计算概率。例如，如果某个样本的标签为&amp;ldquo;小猎犬&amp;rdquo;和&amp;ldquo;狗&amp;rdquo;，则候选采样将针对&amp;ldquo;小猎犬&amp;rdquo;和&amp;ldquo;狗&amp;rdquo;类别输出以及其他类别（猫、棒棒糖、栅栏）的随机子集计算预测概率和相应的损失项。这种采样基于的想法是，只要&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;始终得到适当的正增强，&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;就可以从频率较低的负增强中进行学习，这确实是在实际中观察到的情况。候选采样的目的是，通过不针对所有负类别计算预测结果来提高计算效率。&lt;/p&gt;
&lt;h3 id="categorical data"&gt;categorical data&lt;/h3&gt;
&lt;p&gt;&lt;a href="#feature"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/a&gt; having a discrete set of possible values. For example, consider a categorical feature named &lt;code&gt;house style&lt;/code&gt;, which has a discrete set of three possible values: &lt;code&gt;Tudor, ranch, colonial&lt;/code&gt;. By representing &lt;code&gt;house style&lt;/code&gt; as categorical data, the model can learn the separate impacts of &lt;code&gt;Tudor&lt;/code&gt;, &lt;code&gt;ranch&lt;/code&gt;, and &lt;code&gt;colonial&lt;/code&gt; on house price.&lt;/p&gt;
&lt;p&gt;Sometimes, values in the discrete set are mutually exclusive, and only one value can be applied to a given example. For example, a &lt;code&gt;car maker&lt;/code&gt; categorical feature would probably permit only a single value (&lt;code&gt;Toyota&lt;/code&gt;) per example. Other times, more than one value may be applicable. A single car could be painted more than one different color, so a &lt;code&gt;car color&lt;/code&gt; categorical feature would likely permit a single example to have multiple values (for example, &lt;code&gt;red&lt;/code&gt; and &lt;code&gt;white&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Categorical features are sometimes called &lt;a href="#discrete feature"&gt;&lt;strong&gt;discrete features&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#numerical data"&gt;&lt;strong&gt;numerical data&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，拥有一组离散的可能值。以某个名为 &lt;code&gt;house style&lt;/code&gt; 的分类特征为例，该特征拥有一组离散的可能值（共三个），即 &lt;code&gt;Tudor, ranch, colonial&lt;/code&gt;。通过将 &lt;code&gt;house style&lt;/code&gt; 表示成分类数据，相应模型可以学习 &lt;code&gt;Tudor&lt;/code&gt;、&lt;code&gt;ranch&lt;/code&gt; 和 &lt;code&gt;colonial&lt;/code&gt; 分别对房价的影响。&lt;/p&gt;
&lt;p&gt;有时，离散集中的值是互斥的，只能将其中一个值应用于指定样本。例如，&lt;code&gt;car maker&lt;/code&gt; 分类特征可能只允许一个样本有一个值 (&lt;code&gt;Toyota&lt;/code&gt;)。在其他情况下，则可以应用多个值。一辆车可能会被喷涂多种不同的颜色，因此，&lt;code&gt;car color&lt;/code&gt; 分类特征可能会允许单个样本具有多个值（例如 &lt;code&gt;red&lt;/code&gt; 和 &lt;code&gt;white&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;分类特征有时称为&lt;a href="#discrete feature"&gt;&lt;strong&gt;离散特征&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#numerical data"&gt;&lt;strong&gt;数值数据&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="checkpoint"&gt;checkpoint&lt;/h3&gt;
&lt;p&gt;Data that captures the state of the variables of a model at a particular time. Checkpoints enable exporting model &lt;a href="#weight"&gt;&lt;strong&gt;weights&lt;/strong&gt;&lt;/a&gt;, as well as performing training across multiple sessions. Checkpoints also enable training to continue past errors (for example, job preemption). Note that the &lt;a href="#graph"&gt;&lt;strong&gt;graph&lt;/strong&gt;&lt;/a&gt; itself is not included in a checkpoint.&lt;/p&gt;
&lt;p&gt;一种数据，用于捕获模型变量在特定时间的状态。借助检查点，可以导出模型&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;，跨多个会话执行训练，以及使训练在发生错误之后得以继续（例如作业抢占）。请注意，&lt;a href="#graph"&gt;&lt;strong&gt;图&lt;/strong&gt;&lt;/a&gt;本身不包含在检查点中。&lt;/p&gt;
&lt;h3 id="class"&gt;class&lt;/h3&gt;
&lt;p&gt;One of a set of enumerated target values for a label. For example, in a &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt; model that detects spam, the two classes are &lt;em&gt;spam&lt;/em&gt; and &lt;em&gt;not spam&lt;/em&gt;. In a &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification&lt;/strong&gt;&lt;/a&gt; model that identifies dog breeds, the classes would be &lt;em&gt;poodle&lt;/em&gt;, &lt;em&gt;beagle&lt;/em&gt;, &lt;em&gt;pug&lt;/em&gt;, and so on.&lt;/p&gt;
&lt;p&gt;为标签枚举的一组目标值中的一个。例如，在检测垃圾邮件的&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;模型中，两种类别分别是&amp;ldquo;垃圾邮件&amp;rdquo;和&amp;ldquo;非垃圾邮件&amp;rdquo;。在识别狗品种的&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类&lt;/strong&gt;&lt;/a&gt;模型中，类别可以是&amp;ldquo;贵宾犬&amp;rdquo;、&amp;ldquo;小猎犬&amp;rdquo;、&amp;ldquo;哈巴犬&amp;rdquo;等等。&lt;/p&gt;
&lt;h3 id="class-imbalanced data set"&gt;class-imbalanced data set&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt; problem in which the &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt; for the two classes have significantly different frequencies. For example, a disease data set in which 0.0001 of examples have positive labels and 0.9999 have negative labels is a class-imbalanced problem, but a football game predictor in which 0.51 of examples label one team winning and 0.49 label the other team winning is &lt;em&gt;not&lt;/em&gt; a class-imbalanced problem.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;问题问题中，两种类别的&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;在出现频率方面具有很大的差距。例如，在某个疾病数据集中，0.0001 的样本具有正类别标签，0.9999 的样本具有负类别标签，这就属于分类不平衡问题；但在某个足球比赛预测器中，0.51 的样本的标签为其中一个球队赢，0.49 的样本的标签为另一个球队赢，这就不属于分类不平衡问题。&lt;/p&gt;
&lt;h3 id="classification model"&gt;classification model&lt;/h3&gt;
&lt;p&gt;A type of machine learning model for distinguishing among two or more discrete classes. For example, a natural language processing classification model could determine whether an input sentence was in French, Spanish, or Italian. Compare with &lt;a href="#regression model"&gt;&lt;strong&gt;regression model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种机器学习模型，用于区分两种或多种离散类别。例如，某个自然语言处理分类模型可以确定输入的句子是法语、西班牙语还是意大利语。请与&lt;a href="#regression model"&gt;&lt;strong&gt;回归模型&lt;/strong&gt;&lt;/a&gt;进行比较。&lt;/p&gt;
&lt;h3 id="classification threshold"&gt;classification threshold&lt;/h3&gt;
&lt;p&gt;A scalar-value criterion that is applied to a model's predicted score in order to separate the &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt; from the &lt;a href="#negative class"&gt;&lt;strong&gt;negative class&lt;/strong&gt;&lt;/a&gt;. Used when mapping &lt;a href="#logistic regression"&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt; results to &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt;. For example, consider a logistic regression model that determines the probability of a given email message being spam. If the classification threshold is 0.9, then logistic regression values above 0.9 are classified as &lt;em&gt;spam&lt;/em&gt; and those below 0.9 are classified as &lt;em&gt;not spam&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;一种标量值条件，应用于模型预测的得分，旨在将&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;与&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;区分开。将&lt;a href="#logistic regression"&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/a&gt;结果映射到&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;时使用。以某个逻辑回归模型为例，该模型用于确定指定电子邮件是垃圾邮件的概率。如果分类阈值为 0.9，那么逻辑回归值高于 0.9 的电子邮件将被归类为&amp;ldquo;垃圾邮件&amp;rdquo;，低于 0.9 的则被归类为&amp;ldquo;非垃圾邮件&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="collaborative filtering"&gt;collaborative filtering&lt;/h3&gt;
&lt;p&gt;Making predictions about the interests of one user based on the interests of many other users. Collaborative filtering is often used in recommendation systems.&lt;/p&gt;
&lt;p&gt;根据很多其他用户的兴趣来预测某位用户的兴趣。协同过滤通常用在推荐系统中。&lt;/p&gt;
&lt;h3 id="confusion matrix"&gt;confusion matrix&lt;/h3&gt;
&lt;p&gt;An NxN table that summarizes how successful a &lt;a href="#classification model"&gt;&lt;strong&gt;classification model's&lt;/strong&gt;&lt;/a&gt; predictions were; that is, the correlation between the label and the model's classification. One axis of a confusion matrix is the label that the model predicted, and the other axis is the actual label. N represents the number of classes. In a &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt; problem, N=2. For example, here is a sample confusion matrix for a binary classification problem:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;Tumor (predicted)&lt;/th&gt;
&lt;th align="center"&gt;Non-Tumor (predicted)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Tumor (actual)&lt;/td&gt;
&lt;td align="center"&gt;18&lt;/td&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Non-Tumor (actual)&lt;/td&gt;
&lt;td align="center"&gt;6&lt;/td&gt;
&lt;td align="center"&gt;452&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The preceding confusion matrix shows that of the 19 samples that actually had tumors, the model correctly classified 18 as having tumors (18 true positives), and incorrectly classified 1 as not having a tumor (1 false negative). Similarly, of 458 samples that actually did not have tumors, 452 were correctly classified (452 true negatives) and 6 were incorrectly classified (6 false positives).&lt;/p&gt;
&lt;p&gt;The confusion matrix for a multi-class classification problem can help you determine mistake patterns. For example, a confusion matrix could reveal that a model trained to recognize handwritten digits tends to mistakenly predict 9 instead of 4, or 1 instead of 7.&lt;/p&gt;
&lt;p&gt;Confusion matrices contain sufficient information to calculate a variety of performance metrics, including &lt;a href="#precision"&gt;&lt;strong&gt;precision&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#recall"&gt;&lt;strong&gt;recall&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种 NxN 表格，用于总结&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;的预测成效；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;问题中，N=2。例如，下面显示了一个二元分类问题的混淆矩阵示例：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Systems&lt;/th&gt;
&lt;th align="center"&gt;肿瘤(预测)&lt;/th&gt;
&lt;th align="center"&gt;非肿瘤(预测)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;肿瘤  (实际)&lt;/td&gt;
&lt;td align="center"&gt;18&lt;/td&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;非肿瘤(实际)&lt;/td&gt;
&lt;td align="center"&gt;6&lt;/td&gt;
&lt;td align="center"&gt;452&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;上面的混淆矩阵显示，在 19 个实际有肿瘤的样本中，该模型正确地将 18 个归类为有肿瘤（18 个真正例），错误地将 1 个归类为没有肿瘤（1 个假负例）。同样，在 458 个实际没有肿瘤的样本中，模型归类正确的有 452 个（452 个真负例），归类错误的有 6 个（6 个假正例）。&lt;/p&gt;
&lt;p&gt;多类别分类问题的混淆矩阵有助于确定出错模式。例如，某个混淆矩阵可以揭示，某个经过训练以识别手写数字的模型往往会将 4 错误地预测为 9，将 7 错误地预测为 1。&lt;/p&gt;
&lt;p&gt;混淆矩阵包含计算各种效果指标（包括&lt;a href="#precision"&gt;&lt;strong&gt;精确率&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#recall"&gt;&lt;strong&gt;召回率&lt;/strong&gt;&lt;/a&gt;）所需的充足信息。&lt;/p&gt;
&lt;h3 id="continuous feature"&gt;continuous feature&lt;/h3&gt;
&lt;p&gt;A floating-point feature with an infinite range of possible values. Contrast with &lt;a href="#discrete feature"&gt;&lt;strong&gt;discrete feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种浮点特征，可能值的区间不受限制。与&lt;a href="#discrete feature"&gt;&lt;strong&gt;离散特征&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="convergence"&gt;convergence&lt;/h3&gt;
&lt;p&gt;Informally, often refers to a state reached during training in which training &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence.&lt;/p&gt;
&lt;p&gt;See also &lt;a href="#early stopping"&gt;&lt;strong&gt;early stopping&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See also Boyd and Vandenberghe, &lt;a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"&gt;Convex Optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。&lt;/p&gt;
&lt;p&gt;另请参阅&lt;a href="#early stopping"&gt;&lt;strong&gt;早停法&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另请参阅 Boyd 和 Vandenberghe 合著的 &lt;a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"&gt;Convex Optimization&lt;/a&gt;（《凸优化》）。&lt;/p&gt;
&lt;h3 id="convex function"&gt;convex function&lt;/h3&gt;
&lt;p&gt;A function in which the region above the graph of the function is a &lt;a href="#convex set"&gt;&lt;strong&gt;convex set&lt;/strong&gt;&lt;/a&gt;. The prototypical convex function is shaped something like the letter &lt;strong&gt;U&lt;/strong&gt;. For example, the following are all convex functions:&lt;/p&gt;
&lt;p&gt;一种函数，函数图像以上的区域为&lt;a href="#convex set"&gt;&lt;strong&gt;凸集&lt;/strong&gt;&lt;/a&gt;。典型凸函数的形状类似于字母 &lt;strong&gt;U&lt;/strong&gt;。例如，以下都是凸函数：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/convex_functions.png" width="90%"/&gt;
&lt;br/&gt;
A typical convex function is shaped like the letter 'U'.
&lt;/p&gt;
&lt;p&gt;By contrast, the following function is not convex. Notice how the region above the graph is not a convex set:&lt;/p&gt;
&lt;p&gt;相反，以下函数则不是凸函数。请注意图像上方的区域如何不是凸集：&lt;/p&gt;
&lt;p&gt;
&lt;svg height="299.19577" id="svg2" version="1.1" viewbox="0 0 379.11578 299.19577" width="379.11578" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg"&gt;
&lt;title&gt;Nonconvex function&lt;/title&gt;
&lt;desc&gt;A nonconvex function that looks like a curved "W" character, with two local minima&lt;/desc&gt;
&lt;defs id="defs6"&gt;&lt;clippath clippathunits="userSpaceOnUse" id="clipPath20"&gt;&lt;path d="M 0,0 H 365760 V 274320 H 0 Z" id="path18"&gt;&lt;/path&gt;&lt;/clippath&gt;&lt;/defs&gt;&lt;g id="g10" transform="matrix(1.3333333,0,0,-1.3333333,-345.28663,532.80533)"&gt;&lt;path d="M 0,540 H 719.99856 V 0.00108 H 0 Z" id="path28" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="m 278.33212,392.51998 c 12.48685,-27.90513 53.26204,-155.55219 74.92111,-167.43077 21.65907,-11.87858 37.99042,86.40436 55.03336,96.15926 17.04293,9.7549 33.05274,-49.79059 47.22431,-37.62985 14.17157,12.16073 31.50421,92.1619 37.80504,110.59427" id="path36" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="m 278.33212,392.51998 c 12.48685,-27.90513 53.26204,-155.55219 74.92111,-167.43077 21.65907,-11.87858 37.99042,86.40436 55.03336,96.15926 17.04293,9.7549 33.05274,-49.79059 47.22431,-37.62985 14.17157,12.16073 31.50421,92.1619 37.80504,110.59427" id="path38" style="fill:none;stroke:#4285f4;stroke-width:1.49999702;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 482.16811,278.04383 h 61.13372 v -37.91331 h -61.13372 z" id="path46" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text56" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="488.91809" y="-260.73386"&gt;&lt;tspan id="tspan54" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="488.91809 491.36011 497.47607 502.97607 509.09207" y="-260.73386"&gt;local&lt;/tspan&gt;&lt;/text&gt;
&lt;text id="text60" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="488.91809" y="-247.23389"&gt;&lt;tspan id="tspan58" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="488.91809 498.08109 500.52307 506.63907 509.08105 518.24402 524.36005" y="-247.23389"&gt;minimum&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 452.82959,279.16588 29.33852,-20.0787" id="path70" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="m 459.19298,274.8109 22.97513,-15.72372" id="path72" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 459.19298,274.8109 3.51718,0.65906 -7.16613,1.8382 4.308,-6.01443 z" id="path74" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 264.28807,179.58871 v 220.0153" id="path82" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 264.28807,179.58871 V 391.89309" id="path84" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 264.28807,391.89309 2.53033,-2.53033 -2.53033,6.95198 -2.53033,-6.95198 z" id="path86" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 263.27841,180.53025 H 543.2936" id="path94" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 263.27841,180.53025 H 535.58265" id="path96" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 535.58265,180.53025 -2.53033,-2.53031 6.95195,2.53031 -6.95195,2.53031 z" id="path98" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 382.99136,219.87466 H 444.1251 V 181.96135 H 382.99136 Z" id="path106" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text116" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="389.74133" y="-202.56468"&gt;&lt;tspan id="tspan114" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="389.74133 392.18335 398.29932 403.79932 409.91531 412.3573" y="-202.56468"&gt;local &lt;/tspan&gt;&lt;/text&gt;
&lt;text id="text120" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="389.74133" y="-189.06471"&gt;&lt;tspan id="tspan118" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="389.74133 398.90433 401.34631 407.46231 409.9043 419.06729 425.18326" y="-189.06471"&gt;minimum&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 358.85873,221.41495 388.19726,205.6827" id="path130" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 365.65429,217.77094 388.19726,205.6827" id="path132" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 365.65429,217.77094 3.42568,1.03417 -7.32245,1.05539 4.93094,-5.51524 z" id="path134" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 280.99156,219.87466 H 342.1253 V 181.96135 H 280.99156 Z" id="path142" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text152" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="287.74155" y="-202.56468"&gt;&lt;tspan id="tspan150" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="287.74155 293.85754 296.29953 302.41553 308.53149 314.64749 317.08948" y="-202.56468"&gt;global &lt;/tspan&gt;&lt;/text&gt;
&lt;text id="text156" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="287.74155" y="-189.06471"&gt;&lt;tspan id="tspan154" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="287.74155 296.90454 299.34653 305.46252 307.90451 317.0675 323.18347" y="-189.06471"&gt;minimum&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 354.92356,221.42398 324.80551,206.63661" id="path166" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 348.00192,218.02558 324.80551,206.63661" id="path168" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 348.00192,218.02558 -1.15616,-3.38646 5.12521,5.33518 -7.35556,-0.79256 z" id="path170" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;/g&gt;&lt;/svg&gt;
&lt;/p&gt;
&lt;p&gt;Nonconvex function A nonconvex function that looks like a curved "W" character, with two local minimum.&lt;/p&gt;
&lt;p&gt;非凸函数的形状类似于字母&lt;strong&gt;W&lt;/strong&gt;, 有两个局部最低点.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;strictly convex function&lt;/strong&gt; has exactly one local minimum point, which is also the global minimum point. The classic U-shaped functions are strictly convex functions. However, some convex functions (for example, straight lines) are not.&lt;/p&gt;
&lt;p&gt;A lot of the common &lt;strong&gt;loss functions&lt;/strong&gt;, including the following, are convex functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#l2 loss"&gt;&lt;strong&gt;L2 loss&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#log loss"&gt;&lt;strong&gt;Log Loss&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many variations of &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; are guaranteed to find a point close to the minimum of a strictly convex function. Similarly, many variations of &lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;stochastic gradient descent&lt;/strong&gt;&lt;/a&gt; have a high probability (though, not a guarantee) of finding a point close to the minimum of a strictly convex function.&lt;/p&gt;
&lt;p&gt;The sum of two convex functions (for example, L2 loss + L1 regularization) is a convex function.&lt;/p&gt;
&lt;p&gt;&lt;a href="#deep model"&gt;&lt;strong&gt;Deep models&lt;/strong&gt;&lt;/a&gt; are never convex functions. Remarkably, algorithms designed for &lt;a href="#convex optimization"&gt;&lt;strong&gt;convex optimization&lt;/strong&gt;&lt;/a&gt; tend to find reasonably good solutions on deep networks anyway, even though those solutions are not guaranteed to be a global minimum.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;严格凸函数&lt;/strong&gt;只有一个局部最低点，该点也是全局最低点。经典的 U 形函数都是严格凸函数。不过，有些凸函数（例如直线）则不是这样。&lt;/p&gt;
&lt;p&gt;很多常见的&lt;strong&gt;损失函数&lt;/strong&gt;（包括下列函数）都是凸函数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#l2 loss"&gt;&lt;strong&gt;L2 损失函数&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#log loss"&gt;&lt;strong&gt;对数损失函数&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;的很多变体都一定能找到一个接近严格凸函数最小值的点。同样，&lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;随机梯度下降法&lt;/strong&gt;&lt;/a&gt;的很多变体都有很高的可能性能够找到接近严格凸函数最小值的点（但并非一定能找到）。&lt;/p&gt;
&lt;p&gt;两个凸函数的和（例如 L2 损失函数 + L1 正则化）也是凸函数。&lt;/p&gt;
&lt;p&gt;&lt;a href="#deep model"&gt;&lt;strong&gt;深度模型&lt;/strong&gt;&lt;/a&gt;绝不会是凸函数。值得注意的是，专门针对&lt;a href="#convex optimization"&gt;&lt;strong&gt;凸优化&lt;/strong&gt;&lt;/a&gt;设计的算法往往总能在深度网络上找到非常好的解决方案，虽然这些解决方案并不一定对应于全局最小值。&lt;/p&gt;
&lt;h3 id="convex optimization"&gt;convex optimization&lt;/h3&gt;
&lt;p&gt;The process of using mathematical techniques such as &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; to find the minimum of a &lt;a href="#convex function"&gt;&lt;strong&gt;convex function&lt;/strong&gt;&lt;/a&gt;. A great deal of research in machine learning has focused on formulating various problems as convex optimization problems and in solving those problems more efficiently.&lt;/p&gt;
&lt;p&gt;For complete details, see Boyd and Vandenberghe, &lt;a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"&gt;Convex Optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;使用数学方法（例如&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;）寻找&lt;a href="#convex function"&gt;&lt;strong&gt;凸函数&lt;/strong&gt;&lt;/a&gt;最小值的过程。机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。&lt;/p&gt;
&lt;p&gt;如需完整的详细信息，请参阅 Boyd 和 Vandenberghe 合著的 &lt;a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"&gt;Convex Optimization&lt;/a&gt;（《凸优化》）。&lt;/p&gt;
&lt;h3 id="convex set"&gt;convex set&lt;/h3&gt;
&lt;p&gt;A subset of Euclidean space such that a line drawn between any two points in the subset remains completely within the subset. For instance, the following two shapes are convex sets:&lt;/p&gt;
&lt;p&gt;欧几里德空间的一个子集，其中任意两点之间的连线仍完全落在该子集内。例如，下面的两个图形都是凸集：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/convex_set.png" width="80%"/&gt;
&lt;br/&gt;
A rectangle and a semi-ellipse are both convex sets.
矩形和半椭圆形都是凸集
&lt;/p&gt;
&lt;p&gt;By contrast, the following two shapes are not convex sets:&lt;/p&gt;
&lt;p&gt;相反，下面的两个图形都不是凸集：&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="/images/nonconvex_set.png" width="80%"/&gt;
&lt;br/&gt;
A pie-chart with a missing slice and a firework are both nonconvex sets.
缺少一块的饼图以及烟花图都是非凸集
&lt;/p&gt;
&lt;h3 id="cost"&gt;cost&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="cross-entropy"&gt;cross-entropy&lt;/h3&gt;
&lt;p&gt;A generalization of &lt;a href="#log loss"&gt;&lt;strong&gt;Log Loss&lt;/strong&gt;&lt;/a&gt; to &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification problems&lt;/strong&gt;&lt;/a&gt;. Cross-entropy quantifies the difference between two probability distributions. See also &lt;a href="#perplexity"&gt;&lt;strong&gt;perplexity&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#log loss"&gt;&lt;strong&gt;对数损失函数&lt;/strong&gt;&lt;/a&gt;向&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类问题&lt;/strong&gt;&lt;/a&gt;进行的一种泛化。交叉熵可以量化两种概率分布之间的差异。另请参阅&lt;a href="#perplexity"&gt;&lt;strong&gt;困惑度&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="custom estimator"&gt;custom Estimator&lt;/h3&gt;
&lt;p&gt;An &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt; that you write yourself by following &lt;a href="https://www.tensorflow.org/extend/estimators"&gt;these directions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#pre-made estimator"&gt;&lt;strong&gt;pre-made Estimators&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="d_1"&gt;D&lt;/h2&gt;
&lt;h3 id="data set"&gt;data set&lt;/h3&gt;
&lt;p&gt;A collection of &lt;a href="#example"&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一组&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;的集合。&lt;/p&gt;
&lt;h3 id="dataset api (tf.data)"&gt;Dataset API (tf.data)&lt;/h3&gt;
&lt;p&gt;A high-level TensorFlow API for reading data and transforming it into a form that a machine learning algorithm requires. A &lt;code&gt;tf.data.Dataset&lt;/code&gt; object represents a sequence of elements, in which each element contains one or more &lt;a href="#tensor"&gt;&lt;strong&gt;Tensors&lt;/strong&gt;&lt;/a&gt;. A &lt;code&gt;tf.data.Iterator&lt;/code&gt; object provides access to the elements of a &lt;code&gt;Dataset&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For details about the Dataset API, see &lt;a href="https://www.tensorflow.org/programmers_guide/datasets"&gt;Importing Data&lt;/a&gt; in the TensorFlow Programmer's Guide.&lt;/p&gt;
&lt;p&gt;一种高级别的 TensorFlow API，用于读取数据并将其转换为机器学习算法所需的格式。&lt;code&gt;tf.data.Dataset&lt;/code&gt; 对象表示一系列元素，其中每个元素都包含一个或多个&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;。&lt;code&gt;tf.data.Iterator&lt;/code&gt; 对象可获取 &lt;code&gt;Dataset&lt;/code&gt; 中的元素。&lt;/p&gt;
&lt;p&gt;如需详细了解 Dataset API，请参阅《TensorFlow 编程人员指南》中的&lt;a href="https://www.tensorflow.org/programmers_guide/datasets"&gt;导入数据&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="decision boundary"&gt;decision boundary&lt;/h3&gt;
&lt;p&gt;The separator between classes learned by a model in a &lt;a href="#binary classification"&gt;&lt;strong&gt;binary class&lt;/strong&gt;&lt;/a&gt; or &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification problems&lt;/strong&gt;&lt;/a&gt;. For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange class and the blue class:&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;或&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类问题&lt;/strong&gt;&lt;/a&gt;中，模型学到的类别之间的分界线。例如，在以下表示某个二元分类问题的图片中，决策边界是橙色类别和蓝色类别之间的分界线：&lt;/p&gt;
&lt;p&gt;&lt;img alt="A
well-defined boundary between one class and another." src="/images/decision_boundary.png"/&gt;&lt;/p&gt;
&lt;h3 id="dense layer"&gt;dense layer&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#fully connected layer"&gt;&lt;strong&gt;fully connected layer&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#fully connected layer"&gt;&lt;strong&gt;全连接层&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="deep model"&gt;deep model&lt;/h3&gt;
&lt;p&gt;A type of &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt; containing multiple &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layers&lt;/strong&gt;&lt;/a&gt;. Deep models rely on trainable nonlinearities.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#wide model"&gt;&lt;strong&gt;wide model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;，其中包含多个&lt;a href="#hidden layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;。深度模型依赖于可训练的非线性关系。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#wide model"&gt;&lt;strong&gt;宽度模型&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="dense feature"&gt;dense feature&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#feature"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/a&gt; in which most values are non-zero, typically a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt; of floating-point values. Contrast with &lt;a href="#sparse feature"&gt;&lt;strong&gt;sparse feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种大部分数值是非零值的&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，通常是一个浮点值&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;。参照&lt;a href="#sparse features"&gt;&lt;strong&gt;稀疏特征&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="derived feature"&gt;derived feature&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#synthetic feature"&gt;&lt;strong&gt;synthetic feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#synthetic feature"&gt;&lt;strong&gt;合成特征&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="discrete feature"&gt;discrete feature&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#feature"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/a&gt; with a finite set of possible values. For example, a feature whose values may only be &lt;em&gt;animal&lt;/em&gt;, &lt;em&gt;vegetable&lt;/em&gt;, or &lt;em&gt;mineral&lt;/em&gt; is a discrete (or categorical) feature. Contrast with &lt;a href="#continuous feature"&gt;&lt;strong&gt;continuous feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，包含有限个可能值。例如，某个值只能是&amp;ldquo;动物&amp;rdquo;、&amp;ldquo;蔬菜&amp;rdquo;或&amp;ldquo;矿物&amp;rdquo;的特征便是一个离散特征（或分类特征）。与&lt;a href="#continuous feature"&gt;&lt;strong&gt;连续特征&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="dropout regularization"&gt;dropout regularization&lt;/h3&gt;
&lt;p&gt;A form of &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt; useful in training &lt;a href="#neural network"&gt;&lt;strong&gt;neural networks&lt;/strong&gt;&lt;/a&gt;. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. For full details, see &lt;a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf"&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种形式的&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;，在训练&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;方面非常有用。丢弃正则化的运作机制是，在神经网络层的一个梯度步长中移除随机选择的固定数量的单元。丢弃的单元越多，正则化效果就越强。这类似于训练神经网络以模拟较小网络的指数级规模集成学习。如需完整的详细信息，请参阅 &lt;a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf"&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;（《丢弃：一种防止神经网络过拟合的简单方法》）。&lt;/p&gt;
&lt;h3 id="dynamic model"&gt;dynamic model&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; that is trained online in a continuously updating fashion. That is, data is continuously entering the model.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;，以持续更新的方式在线接受训练。也就是说，数据会源源不断地进入这种模型。&lt;/p&gt;
&lt;h2 id="e_1"&gt;E&lt;/h2&gt;
&lt;h3 id="early stopping"&gt;early stopping&lt;/h3&gt;
&lt;p&gt;A method for &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt; that involves ending model training &lt;em&gt;before&lt;/em&gt; training loss finishes decreasing. In early stopping, you end model training when the loss on a &lt;a href="#validation set"&gt;&lt;strong&gt;validation data set&lt;/strong&gt;&lt;/a&gt; starts to increase, that is, when &lt;a href="#generalization"&gt;&lt;strong&gt;generalization&lt;/strong&gt;&lt;/a&gt; performance worsens.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;方法，涉及在训练损失仍可以继续减少之前结束模型训练。使用早停法时，您会在基于&lt;a href="#validation set"&gt;&lt;strong&gt;验证数据集&lt;/strong&gt;&lt;/a&gt;的损失开始增加（也就是&lt;a href="#generalization"&gt;&lt;strong&gt;泛化&lt;/strong&gt;&lt;/a&gt;效果变差）时结束模型训练。&lt;/p&gt;
&lt;h3 id="embeddings"&gt;embeddings&lt;/h3&gt;
&lt;p&gt;A categorical feature represented as a continuous-valued feature. Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. For example, you can represent the words in an English sentence in either of the following two ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As a million-element (high-dimensional) &lt;a href="#sparse feature"&gt;&lt;strong&gt;sparse vector&lt;/strong&gt;&lt;/a&gt; in which all elements are integers. Each cell in the vector represents a separate English word; the value in a cell represents the number of times that word appears in a sentence. Since a single English sentence is unlikely to contain more than 50 words, nearly every cell in the vector will contain a 0. The few cells that aren't 0 will contain a low integer (usually 1) representing the number of times that word appeared in the sentence.&lt;/li&gt;
&lt;li&gt;As a several-hundred-element (low-dimensional) &lt;a href="#dense feature"&gt;&lt;strong&gt;dense vector&lt;/strong&gt;&lt;/a&gt; in which each element holds a floating-point value between 0 and 1. This is an embedding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In TensorFlow, embeddings are trained by &lt;a href="#backpropagation"&gt;&lt;strong&gt;backpropagating&lt;/strong&gt;&lt;/a&gt; &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; just like any other parameter in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种分类特征，以连续值特征表示。通常，嵌套是指将高维度向量映射到低维度的空间。例如，您可以采用以下两种方式之一来表示英文句子中的单词：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;表示成包含百万个元素（高维度）的&lt;a href="#sparse feature"&gt;&lt;strong&gt;稀疏向量&lt;/strong&gt;&lt;/a&gt;，其中所有元素都是整数。向量中的每个单元格都表示一个单独的英文单词，单元格中的值表示相应单词在句子中出现的次数。由于单个英文句子包含的单词不太可能超过 50 个，因此向量中几乎每个单元格都包含 0。少数非 0 的单元格中将包含一个非常小的整数（通常为 1），该整数表示相应单词在句子中出现的次数。&lt;/li&gt;
&lt;li&gt;表示成包含数百个元素（低维度）的&lt;a href="#dense feature"&gt;&lt;strong&gt;密集向量&lt;/strong&gt;&lt;/a&gt;，其中每个元素都包含一个介于 0 到 1 之间的浮点值。这就是一种嵌套。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 TensorFlow 中，会按&lt;a href="#backpropagation"&gt;&lt;strong&gt;反向传播&lt;/strong&gt;&lt;/a&gt;&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;训练嵌套，和训练&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的任何其他参数时一样。&lt;/p&gt;
&lt;h3 id="empirical risk minimization (erm)"&gt;empirical risk minimization (ERM)&lt;/h3&gt;
&lt;p&gt;Choosing the model function that minimizes loss on the training set. Contrast with &lt;a href="#structural risk minimization (srm)"&gt;&lt;strong&gt;structural risk minimization&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;用于选择可以将基于训练集的损失降至最低的模型函数。与&lt;a href="#structural risk minimization (srm)"&gt;&lt;strong&gt;结构风险最小化&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="ensemble"&gt;ensemble&lt;/h3&gt;
&lt;p&gt;A merger of the predictions of multiple &lt;a href="#model"&gt;&lt;strong&gt;models&lt;/strong&gt;&lt;/a&gt;. You can create an ensemble via one or more of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;different initializations&lt;/li&gt;
&lt;li&gt;different &lt;a href="#hyperparameter"&gt;&lt;strong&gt;hyperparameters&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;different overall structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/tutorials/wide_and_deep"&gt;Deep and wide models&lt;/a&gt; are a kind of ensemble.&lt;/p&gt;
&lt;p&gt;多个&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;的预测结果的并集。您可以通过以下一项或多项来创建集成学习：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同的初始化&lt;/li&gt;
&lt;li&gt;不同的&lt;a href="#hyperparameter"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;不同的整体结构&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/tutorials/wide_and_deep"&gt;深度模型和宽度模型&lt;/a&gt;属于一种集成学习。&lt;/p&gt;
&lt;h3 id="epoch"&gt;epoch&lt;/h3&gt;
&lt;p&gt;A full training pass over the entire data set such that each example has been seen once. Thus, an epoch represents &lt;code&gt;N&lt;/code&gt;/&lt;a href="#batch size"&gt;&lt;strong&gt;batch size&lt;/strong&gt;&lt;/a&gt; training &lt;a href="#iteration"&gt;&lt;strong&gt;iterations&lt;/strong&gt;&lt;/a&gt;, where &lt;code&gt;N&lt;/code&gt; is the total number of examples.&lt;/p&gt;
&lt;p&gt;在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。因此，一个周期表示（&lt;code&gt;N&lt;/code&gt;/&lt;a href="#batch size"&gt;&lt;strong&gt;批次规模&lt;/strong&gt;&lt;/a&gt;）次训练&lt;a href="#iteration"&gt;&lt;strong&gt;迭代&lt;/strong&gt;&lt;/a&gt;，其中 &lt;code&gt;N&lt;/code&gt; 是样本总数。&lt;/p&gt;
&lt;h3 id="estimator"&gt;Estimator&lt;/h3&gt;
&lt;p&gt;An instance of the &lt;code&gt;tf.Estimator&lt;/code&gt; class, which encapsulates logic that builds a TensorFlow graph and runs a TensorFlow session. You may create your own &lt;a href="#custom estimator"&gt;&lt;strong&gt;custom Estimators&lt;/strong&gt;&lt;/a&gt; (as described &lt;a href="https://www.tensorflow.org/extend/estimators"&gt;here&lt;/a&gt;) or instantiate &lt;a href="#pre-made estimator"&gt;&lt;strong&gt;pre-made Estimators&lt;/strong&gt;&lt;/a&gt; created by others.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tf.Estimator&lt;/code&gt; 类的一个实例，用于封装负责构建 TensorFlow 图并运行 TensorFlow 会话的逻辑。您可以创建自己的&lt;a href="#custom estimator"&gt;&lt;strong&gt;自定义 Estimator&lt;/strong&gt;&lt;/a&gt;（如需相关介绍，请&lt;a href="https://www.tensorflow.org/extend/estimators"&gt;点击此处&lt;/a&gt;），也可以将其他人&lt;a href="#pre-made estimator"&gt;&lt;strong&gt;预创建的 Estimator&lt;/strong&gt;&lt;/a&gt; 实例化。&lt;/p&gt;
&lt;h3 id="example"&gt;example&lt;/h3&gt;
&lt;p&gt;One row of a data set. An example contains one or more &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; and possibly a &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. See also &lt;a href="#labeled example"&gt;&lt;strong&gt;labeled example&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#unlabeled example"&gt;&lt;strong&gt;unlabeled example&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;数据集的一行。一个样本包含一个或多个&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，此外还可能包含一个&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;。另请参阅&lt;a href="#labeled example"&gt;&lt;strong&gt;有标签样本&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#unlabeled example"&gt;&lt;strong&gt;无标签样本&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="f_1"&gt;F&lt;/h2&gt;
&lt;h3 id="false negative (fn)"&gt;false negative (FN)&lt;/h3&gt;
&lt;p&gt;An example in which the model mistakenly predicted the &lt;a href="#negative class"&gt;&lt;strong&gt;negative class&lt;/strong&gt;&lt;/a&gt;. For example, the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam.&lt;/p&gt;
&lt;p&gt;被模型错误地预测为&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;的样本。例如，模型推断出某封电子邮件不是垃圾邮件（负类别），但该电子邮件其实是垃圾邮件。&lt;/p&gt;
&lt;h3 id="false positive (fp)"&gt;false positive (FP)&lt;/h3&gt;
&lt;p&gt;An example in which the model mistakenly predicted the &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt;. For example, the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam.&lt;/p&gt;
&lt;p&gt;被模型错误地预测为&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;的样本。例如，模型推断出某封电子邮件是垃圾邮件（正类别），但该电子邮件其实不是垃圾邮件。&lt;/p&gt;
&lt;h3 id="false positive rate (fp rate)"&gt;false positive rate (FP rate)&lt;/h3&gt;
&lt;p&gt;The x-axis in an &lt;a href="#roc (receiver operating characteristic) curve"&gt;&lt;strong&gt;ROC curve&lt;/strong&gt;&lt;/a&gt;. The FP rate is defined as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$\text{False Positive Rate} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}$$&lt;/div&gt;
&lt;p&gt;&lt;a href="#roc (receiver operating characteristic) curve"&gt;&lt;strong&gt;ROC 曲线&lt;/strong&gt;&lt;/a&gt;中的 x 轴。FP 率的定义如下：&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{假正例率} = \frac{\text{假正例数}}{\text{假正例数} + \text{真负例数}}$$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 id="feature"&gt;feature&lt;/h3&gt;
&lt;p&gt;An input variable used in making &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在进行&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;时使用的输入变量。&lt;/p&gt;
&lt;h3 id="feature columns (featurecolumns)"&gt;feature columns (FeatureColumns)&lt;/h3&gt;
&lt;p&gt;A set of related features, such as the set of all possible countries in which users might live. An example may have one or more features present in a feature column.&lt;/p&gt;
&lt;p&gt;Feature columns in TensorFlow also encapsulate metadata such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the feature's data type&lt;/li&gt;
&lt;li&gt;whether a feature is fixed length or should be converted to an embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A feature column can contain a single feature.&lt;/p&gt;
&lt;p&gt;"Feature column" is Google-specific terminology. A feature column is referred to as a "namespace" in the &lt;a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit"&gt;VW&lt;/a&gt; system (at Yahoo/Microsoft), or a &lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libffm/"&gt;field&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一组相关特征，例如用户可能居住的所有国家/地区的集合。样本的特征列中可能包含一个或多个特征。&lt;/p&gt;
&lt;p&gt;TensorFlow 中的特征列内还封装了元数据，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征的数据类型&lt;/li&gt;
&lt;li&gt;特征是固定长度还是应转换为嵌套&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;特征列可以包含单个特征。&lt;/p&gt;
&lt;p&gt;&amp;ldquo;特征列&amp;rdquo;是 Google 专用的术语。特征列在 Yahoo/Microsoft 使用的 &lt;a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit"&gt;VW&lt;/a&gt; 系统中称为&amp;ldquo;命名空间&amp;rdquo;，也称为&lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libffm/"&gt;场&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="feature cross"&gt;feature cross&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#synthetic feature"&gt;&lt;strong&gt;synthetic feature&lt;/strong&gt;&lt;/a&gt; formed by crossing (multiplying or taking a Cartesian product of) individual features. Feature crosses help represent nonlinear relationships.&lt;/p&gt;
&lt;p&gt;通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的&lt;a href="#synthetic feature"&gt;&lt;strong&gt;合成特征&lt;/strong&gt;&lt;/a&gt;。特征组合有助于表示非线性关系。&lt;/p&gt;
&lt;h3 id="feature engineering"&gt;feature engineering&lt;/h3&gt;
&lt;p&gt;The process of determining which &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; might be useful in training a model, and then converting raw data from log files and other sources into said features. In TensorFlow, feature engineering often means converting raw log file entries to &lt;a href="#tf.example"&gt;&lt;strong&gt;tf.Example&lt;/strong&gt;&lt;/a&gt; protocol buffers. See also &lt;a href="https://github.com/tensorflow/transform"&gt;tf.Transform&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feature engineering is sometimes called &lt;strong&gt;feature extraction&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;指以下过程：确定哪些&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。在 TensorFlow 中，特征工程通常是指将原始日志文件条目转换为 &lt;a href="#tf.Example"&gt;&lt;strong&gt;tf.Example&lt;/strong&gt;&lt;/a&gt; proto buffer。另请参阅 &lt;a href="https://github.com/tensorflow/transform"&gt;tf.Transform&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;特征工程有时称为&lt;strong&gt;特征提取&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="feature set"&gt;feature set&lt;/h3&gt;
&lt;p&gt;The group of &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; your machine learning model trains on. For example, postal code, property size, and property condition might comprise a simple feature set for a model that predicts housing prices.&lt;/p&gt;
&lt;p&gt;训练机器学习模型时采用的一组&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;。例如，对于某个用于预测房价的模型，邮政编码、房屋面积以及房屋状况可以组成一个简单的特征集。&lt;/p&gt;
&lt;h3 id="feature spec"&gt;feature spec&lt;/h3&gt;
&lt;p&gt;Describes the information required to extract &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; data from the &lt;a href="#tf.example"&gt;&lt;strong&gt;tf.Example&lt;/strong&gt;&lt;/a&gt; protocol buffer. Because the tf.Example protocol buffer is just a container for data, you must specify the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the data to extract (that is, the keys for the features)&lt;/li&gt;
&lt;li&gt;the data type (for example, float or int)&lt;/li&gt;
&lt;li&gt;The length (fixed or variable)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator API&lt;/strong&gt;&lt;/a&gt; provides facilities for producing a feature spec from a list of &lt;a href="#feature columns (featurecolumns)"&gt;&lt;strong&gt;FeatureColumns&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;用于描述如何从 &lt;a href="#tf.example"&gt;&lt;strong&gt;tf.Example&lt;/strong&gt;&lt;/a&gt; proto buffer 提取&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;数据。由于 tf.Example proto buffer 只是一个数据容器，因此您必须指定以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;要提取的数据（即特征的键）&lt;/li&gt;
&lt;li&gt;数据类型（例如 float 或 int）&lt;/li&gt;
&lt;li&gt;长度（固定或可变）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#estimator"&gt;&lt;strong&gt;Estimator API&lt;/strong&gt;&lt;/a&gt; 提供了一些可用来根据给定 &lt;a href="#feature columns (featurecolumns)"&gt;&lt;strong&gt;FeatureColumns&lt;/strong&gt;&lt;/a&gt; 列表生成特征规范的工具。&lt;/p&gt;
&lt;h3 id="full softmax"&gt;full softmax&lt;/h3&gt;
&lt;p&gt;See &lt;a href="#softmax"&gt;&lt;strong&gt;softmax&lt;/strong&gt;&lt;/a&gt;. Contrast with &lt;a href="#candidate sampling"&gt;&lt;strong&gt;candidate sampling&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;请参阅 &lt;a href="#softmax"&gt;&lt;strong&gt;softmax&lt;/strong&gt;&lt;/a&gt;。与&lt;a href="#candidate sampling"&gt;&lt;strong&gt;候选采样&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="fully connected layer"&gt;fully connected layer&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layer&lt;/strong&gt;&lt;/a&gt; in which each &lt;a href="#node"&gt;&lt;strong&gt;node&lt;/strong&gt;&lt;/a&gt; is connected to &lt;em&gt;every&lt;/em&gt; node in the subsequent hidden layer.&lt;/p&gt;
&lt;p&gt;A fully connected layer is also known as a &lt;a href="#dense layer"&gt;&lt;strong&gt;dense layer&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#hidden layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;，其中的每个&lt;a href="#node"&gt;&lt;strong&gt;节点&lt;/strong&gt;&lt;/a&gt;均与下一个隐藏层中的每个节点相连。&lt;/p&gt;
&lt;p&gt;全连接层又称为&lt;a href="#dense layer"&gt;&lt;strong&gt;密集层&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="g_1"&gt;G&lt;/h2&gt;
&lt;h3 id="generalization"&gt;generalization&lt;/h3&gt;
&lt;p&gt;Refers to your model's ability to make correct predictions on new, previously unseen data as opposed to the data used to train the model.&lt;/p&gt;
&lt;p&gt;指的是模型依据训练时采用的数据，针对以前未见过的新数据做出正确预测的能力。&lt;/p&gt;
&lt;h3 id="generalized linear model"&gt;generalized linear model&lt;/h3&gt;
&lt;p&gt;A generalization of &lt;a href="https://developers.google.com/machine-learning/glossary/least_squares_regression"&gt;&lt;strong&gt;least squares regression&lt;/strong&gt;&lt;/a&gt; models, which are based on &lt;a href="https://en.wikipedia.org/wiki/Gaussian_noise"&gt;Gaussian noise&lt;/a&gt;, to other types of models based on other types of noise, such as &lt;a href="https://en.wikipedia.org/wiki/Shot_noise"&gt;Poisson noise&lt;/a&gt; or categorical noise. Examples of generalized linear models include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#logistic regression"&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;multi-class regression&lt;/li&gt;
&lt;li&gt;least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The parameters of a generalized linear model can be found through &lt;a href="https://en.wikipedia.org/wiki/Convex_optimization"&gt;convex optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Generalized linear models exhibit the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The average prediction of the optimal least squares regression model is equal to the average label on the training data.&lt;/li&gt;
&lt;li&gt;The average probability predicted by the optimal logistic regression model is equal to the average label on the training data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The power of a generalized linear model is limited by its features. Unlike a deep model, a generalized linear model cannot "learn new features."&lt;/p&gt;
&lt;p&gt;&lt;a href="https://developers.google.com/machine-learning/glossary/least_squares_regression"&gt;&lt;strong&gt;最小二乘回归&lt;/strong&gt;&lt;/a&gt;模型（基于&lt;a href="https://en.wikipedia.org/wiki/Gaussian_noise"&gt;高斯噪声&lt;/a&gt;）向其他类型的模型（基于其他类型的噪声，例如&lt;a href="https://en.wikipedia.org/wiki/Shot_noise"&gt;泊松噪声&lt;/a&gt;或分类噪声）进行的一种泛化。广义线性模型的示例包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#logistic regression"&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;多类别回归&lt;/li&gt;
&lt;li&gt;最小二乘回归&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以通过&lt;a href="https://en.wikipedia.org/wiki/Convex_optimization"&gt;凸优化&lt;/a&gt;找到广义线性模型的参数。&lt;/p&gt;
&lt;p&gt;广义线性模型具有以下特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最优的最小二乘回归模型的平均预测结果等于训练数据的平均标签。&lt;/li&gt;
&lt;li&gt;最优的逻辑回归模型预测的平均概率等于训练数据的平均标签。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;广义线性模型的功能受其特征的限制。与深度模型不同，广义线性模型无法&amp;ldquo;学习新特征&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="gradient"&gt;gradient&lt;/h3&gt;
&lt;p&gt;The vector of &lt;a href="#partial derivative"&gt;&lt;strong&gt;partial derivatives&lt;/strong&gt;&lt;/a&gt; with respect to all of the independent variables. In machine learning, the gradient is the the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent.&lt;/p&gt;
&lt;p&gt;&lt;a href="#partial derivative"&gt;&lt;strong&gt;偏导数&lt;/strong&gt;&lt;/a&gt;相对于所有自变量的向量。在机器学习中，梯度是模型函数偏导数的向量。梯度指向最速上升的方向。&lt;/p&gt;
&lt;h3 id="gradient clipping"&gt;gradient clipping&lt;/h3&gt;
&lt;p&gt;Capping &lt;a href="#gradient"&gt;&lt;strong&gt;gradient&lt;/strong&gt;&lt;/a&gt; values before applying them. Gradient clipping helps ensure numerical stability and prevents &lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf"&gt;exploding gradients&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在应用&lt;a href="#gradient"&gt;&lt;strong&gt;梯度&lt;/strong&gt;&lt;/a&gt;值之前先设置其上限。梯度裁剪有助于确保数值稳定性以及防止&lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf"&gt;梯度爆炸&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="gradient descent"&gt;gradient descent&lt;/h3&gt;
&lt;p&gt;A technique to minimize &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of &lt;a href="#weight"&gt;&lt;strong&gt;weights&lt;/strong&gt;&lt;/a&gt; and bias to minimize loss.&lt;/p&gt;
&lt;p&gt;一种通过计算并且减小梯度将&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;和偏差的最佳组合，从而将损失降至最低。&lt;/p&gt;
&lt;h3 id="graph"&gt;graph&lt;/h3&gt;
&lt;p&gt;In TensorFlow, a computation specification. Nodes in the graph represent operations. Edges are directed and represent passing the result of an operation (a &lt;a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"&gt;Tensor&lt;/a&gt;) as an operand to another operation. Use &lt;a href="#tensorboard"&gt;&lt;strong&gt;TensorBoard&lt;/strong&gt;&lt;/a&gt; to visualize a graph.&lt;/p&gt;
&lt;p&gt;TensorFlow 中的一种计算规范。图中的节点表示操作。边缘具有方向，表示将某项操作的结果（一个&lt;a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"&gt;张量&lt;/a&gt;）作为一个操作数传递给另一项操作。可以使用 &lt;a href="#tensorboard"&gt;&lt;strong&gt;TensorBoard&lt;/strong&gt;&lt;/a&gt; 直观呈现图。&lt;/p&gt;
&lt;h2 id="h_1"&gt;H&lt;/h2&gt;
&lt;h3 id="heuristic"&gt;heuristic&lt;/h3&gt;
&lt;p&gt;A practical and nonoptimal solution to a problem, which is sufficient for making progress or for learning from.&lt;/p&gt;
&lt;p&gt;一种非最优但实用的问题解决方案，足以用于进行改进或从中学习。&lt;/p&gt;
&lt;h3 id="hidden layer"&gt;hidden layer&lt;/h3&gt;
&lt;p&gt;A synthetic layer in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt; between the &lt;a href="#input layer"&gt;&lt;strong&gt;input layer&lt;/strong&gt;&lt;/a&gt; (that is, the features) and the &lt;a href="#output layer"&gt;&lt;strong&gt;output layer&lt;/strong&gt;&lt;/a&gt; (the prediction). A neural network contains one or more hidden layers.&lt;/p&gt;
&lt;p&gt;&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的合成层，介于&lt;a href="#input layer"&gt;&lt;strong&gt;输入层&lt;/strong&gt;&lt;/a&gt;（即特征）和&lt;a href="#output layer"&gt;&lt;strong&gt;输出层&lt;/strong&gt;&lt;/a&gt;（即预测）之间。神经网络包含一个或多个隐藏层。&lt;/p&gt;
&lt;h3 id="hinge loss"&gt;hinge loss&lt;/h3&gt;
&lt;p&gt;A family of &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; functions for &lt;a href="#classification model"&gt;&lt;strong&gt;classification&lt;/strong&gt;&lt;/a&gt; designed to find the &lt;a href="#decision boundary"&gt;&lt;strong&gt;decision boundary&lt;/strong&gt;&lt;/a&gt; as distant as possible from each training example, thus maximizing the margin between examples and the boundary. &lt;a href="#kernel support vector machines (ksvms)"&gt;&lt;strong&gt;KSVMs&lt;/strong&gt;&lt;/a&gt; use hinge loss (or a related function, such as squared hinge loss). For binary classification, the hinge loss function is defined as follows:&lt;/p&gt;
&lt;p&gt;一系列用于&lt;a href="#classification model"&gt;&lt;strong&gt;分类&lt;/strong&gt;&lt;/a&gt;的&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;函数，旨在找到距离每个训练样本都尽可能远的&lt;a href="#decision boundary"&gt;&lt;strong&gt;决策边界&lt;/strong&gt;&lt;/a&gt;，从而使样本和边界之间的裕度最大化。 &lt;a href="#kernel support vector machines (ksvms)"&gt;&lt;strong&gt;KSVM&lt;/strong&gt;&lt;/a&gt; 使用合页损失函数（或相关函数，例如平方合页损失函数）。对于二元分类，合页损失函数的定义如下：&lt;/p&gt;
&lt;div class="math"&gt;$$\text{loss} = \text{max}(0, 1 - (y' * y))$$&lt;/div&gt;
&lt;p&gt;where &lt;em&gt;y'&lt;/em&gt; is the raw output of the classifier model:&lt;/p&gt;
&lt;p&gt;其中&amp;ldquo;y'&amp;rdquo;表示分类器模型的原始输出：&lt;/p&gt;
&lt;div class="math"&gt;$$y' = b + w_1x_1 + w_2x_2 + &amp;hellip; w_nx_n$$&lt;/div&gt;
&lt;p&gt;and &lt;em&gt;y&lt;/em&gt; is the true label, either -1 or +1.&lt;/p&gt;
&lt;p&gt;Consequently, a plot of hinge loss vs. (y * y') looks as follows:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;y&amp;rdquo;表示真标签，值为 -1 或 +1。&lt;/p&gt;
&lt;p&gt;因此，合页损失与 (y * y') 的关系图如下所示：&lt;/p&gt;
&lt;p&gt;
&lt;svg height="566.00629" id="svg2" version="1.1" viewbox="0 0 659.45264 566.00629" width="659.45264" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg"&gt;
&lt;title&gt;Hinge loss vs. (y * y')&lt;/title&gt;
&lt;desc&gt;A plot of hinge loss vs. raw classifier score shows a distinct hinge at the
coordinate (1,0).&lt;/desc&gt;
&lt;defs id="defs6"&gt;&lt;clippath clippathunits="userSpaceOnUse" id="clipPath20"&gt;&lt;path d="M 0,0 H 365760 V 274320 H 0 Z" id="path18"&gt;&lt;/path&gt;&lt;/clippath&gt;&lt;/defs&gt;&lt;g id="g10" transform="matrix(1.3333333,0,0,-1.3333333,-22.422343,676.03376)"&gt;&lt;path d="M 0,540 H 719.99856 V 0.00108 H 0 Z" id="path28" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 95.419101,498.0867 V 157.52832" id="path36" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 95.419101,498.0867 V 157.52832" id="path38" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 95.419101,157.76328 H 503.46553" id="path46" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 95.419101,157.76328 H 503.46553" id="path48" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 294.60847,173.49483 V 142.03032" id="path56" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 294.60847,173.49483 V 142.03032" id="path58" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 362.51227,173.49483 V 142.03032" id="path66" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 362.51227,173.49483 V 142.03032" id="path68" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 430.41607,173.49483 V 142.03032" id="path76" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 430.41607,173.49483 V 142.03032" id="path78" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 226.70466,173.49483 V 142.03032" id="path86" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 226.70466,173.49483 V 142.03032" id="path88" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 158.80086,173.49483 V 142.03032" id="path96" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 158.80086,173.49483 V 142.03032" id="path98" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 498.31987,173.49483 V 142.03032" id="path106" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 498.31987,173.49483 V 142.03032" id="path108" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="m 282.09479,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path116" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text126" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="292.508" y="-122.12723"&gt;&lt;tspan id="tspan124" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="292.508" y="-122.12723"&gt;0&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 124.85634,138.47718 h 66.992 v -31.4645 h -66.992 z" id="path136" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text146" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="153.90829" y="-122.12723"&gt;&lt;tspan id="tspan144" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="153.90829 157.23828" y="-122.12723"&gt;-2&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 198.28295,138.47718 h 56.50382 v -31.4645 h -56.50382 z" id="path156" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text166" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="222.09081" y="-122.12723"&gt;&lt;tspan id="tspan164" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="222.09081 225.42079" y="-122.12723"&gt;-1&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 350.12473,138.47718 h 26.38578 v -31.4645 h -26.38578 z" id="path176" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text186" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="360.53796" y="-122.12723"&gt;&lt;tspan id="tspan184" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="360.53796" y="-122.12723"&gt;1&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 417.22358,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path196" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text206" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="427.63678" y="-122.12723"&gt;&lt;tspan id="tspan204" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="427.63678" y="-122.12723"&gt;2&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="m 485.02047,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path216" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text226" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="495.43369" y="-122.12723"&gt;&lt;tspan id="tspan224" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="495.43369" y="-122.12723"&gt;3&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 111.32608,415.76206 H 79.861573" id="path236" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,415.76206 H 79.861573" id="path238" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 111.32608,347.85826 H 79.861573" id="path246" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,347.85826 H 79.861573" id="path248" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 111.32608,279.95446 H 79.861573" id="path256" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,279.95446 H 79.861573" id="path258" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 111.32608,483.66586 H 79.861573" id="path266" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,483.66586 H 79.861573" id="path268" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 50.757773,302.93551 H 77.143547 V 255.21907 H 50.757773 Z" id="path276" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text286" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="61.170982" y="-275.47729"&gt;&lt;tspan id="tspan284" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="61.170982" y="-275.47729"&gt;1&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 53.622428,371.98048 H 80.008206 V 324.26403 H 53.622428 Z" id="path296" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text306" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-344.52228"&gt;&lt;tspan id="tspan304" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-344.52228"&gt;2&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 53.622428,438.99539 H 80.008206 V 391.27895 H 53.622428 Z" id="path316" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text326" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-411.53717"&gt;&lt;tspan id="tspan324" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-411.53717"&gt;3&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 53.622428,507.02533 H 80.008206 V 459.30889 H 53.622428 Z" id="path336" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text346" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-479.56711"&gt;&lt;tspan id="tspan344" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-479.56711"&gt;4&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 95.053282,483.16711 360.44646,217.77393" id="path356" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 95.053282,483.16711 360.44646,217.77393" id="path358" style="fill:none;stroke:#ff0000;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 359.32928,218.08535 H 501.43923" id="path366" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 359.32928,218.08535 H 501.43923" id="path368" style="fill:none;stroke:#ff0000;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 50.757773,241.9206 H 77.143547 V 194.20416 H 50.757773 Z" id="path376" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text386" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="61.170982" y="-214.46239"&gt;&lt;tspan id="tspan384" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="61.170982" y="-214.46239"&gt;0&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 111.32608,217.92452 H 79.861573" id="path396" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="M 111.32608,217.92452 H 79.861573" id="path398" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"&gt;&lt;/path&gt;&lt;path d="M 9.299194,394.88415 H 62.614048 V 344.4748 H 9.299194 Z" id="path406" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;text id="text416" style="font-variant:normal;font-weight:normal;font-size:11.99997616px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="16.049181" y="-376.6142"&gt;&lt;tspan id="tspan414" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="16.049181 22.721169 25.385162 32.057148 38.729134 45.401123" y="-376.6142"&gt;hinge &lt;/tspan&gt;&lt;/text&gt;
&lt;text id="text420" style="font-variant:normal;font-weight:normal;font-size:11.99997616px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="16.049181" y="-362.36423"&gt;&lt;tspan id="tspan418" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="16.049181 18.713175 25.385162 31.385151" y="-362.36423"&gt;loss&lt;/tspan&gt;&lt;/text&gt;
&lt;path d="M 95.419101,110.41818 H 498.31593 V 82.5206 H 95.419101 Z" id="path430" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"&gt;&lt;/path&gt;&lt;path d="m 266.48095,103.66819 h 60.77311 V 88.068225 h -60.77311 z" id="path438" style="fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851"&gt;&lt;/path&gt;&lt;text id="text442" style="font-variant:normal;font-weight:normal;font-size:12.99997425px;font-family:Consolas;-inkscape-font-specification:Consolas;writing-mode:lr-tb;fill:#222222;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="270.09167" y="-91.188217"&gt;&lt;tspan id="tspan440" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="270.09167 277.22867 284.36563 291.50262 298.63962 305.77661 312.91357 320.05057" y="-91.188217"&gt;(y * y')&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;/svg&gt;
&lt;/p&gt;
&lt;p&gt;Hinge loss vs. (y * y') A plot of hinge loss vs. raw classifier score shows a distinct hinge at the coordinate (1,0). &lt;/p&gt;
&lt;h3 id="holdout data"&gt;holdout data&lt;/h3&gt;
&lt;p&gt;&lt;a href="#example"&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; intentionally not used ("held out") during training. The &lt;a href="#validation set"&gt;&lt;strong&gt;validation data set&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#test set"&gt;&lt;strong&gt;test data set&lt;/strong&gt;&lt;/a&gt; are examples of holdout data. Holdout data helps evaluate your model's ability to generalize to data other than the data it was trained on. The loss on the holdout set provides a better estimate of the loss on an unseen data set than does the loss on the training set.&lt;/p&gt;
&lt;p&gt;训练期间故意不使用（&amp;ldquo;维持&amp;rdquo;）的&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;。&lt;a href="#validation set"&gt;&lt;strong&gt;验证数据集&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#test set"&gt;&lt;strong&gt;测试数据集&lt;/strong&gt;&lt;/a&gt;都属于维持数据。维持数据有助于评估模型向训练时所用数据之外的数据进行泛化的能力。与基于训练数据集的损失相比，基于维持数据集的损失有助于更好地估算基于未见过的数据集的损失。&lt;/p&gt;
&lt;h3 id="hyperparameter"&gt;hyperparameter&lt;/h3&gt;
&lt;p&gt;The "knobs" that you tweak during successive runs of training a model. For example, &lt;a href="#learning rate"&gt;&lt;strong&gt;learning rate&lt;/strong&gt;&lt;/a&gt; is a hyperparameter.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#parameter"&gt;&lt;strong&gt;parameter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在模型训练的连续过程中，您调节的&amp;ldquo;旋钮&amp;rdquo;。例如，&lt;a href="#learning rate"&gt;&lt;strong&gt;学习速率&lt;/strong&gt;&lt;/a&gt;就是一种超参数。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#parameter"&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="hyperplane"&gt;hyperplane&lt;/h3&gt;
&lt;p&gt;A boundary that separates a space into two subspaces. For example, a line is a hyperplane in two dimensions and a plane is a hyperplane in three dimensions. More typically in machine learning, a hyperplane is the boundary separating a high-dimensional space. &lt;a href="#kernel support vector machines (ksvms)"&gt;&lt;strong&gt;Kernel Support Vector Machines&lt;/strong&gt;&lt;/a&gt; use hyperplanes to separate positive classes from negative classes, often in a very high-dimensional space.&lt;/p&gt;
&lt;p&gt;将一个空间划分为两个子空间的边界。例如，在二维空间中，直线就是一个超平面，在三维空间中，平面则是一个超平面。在机器学习中更典型的是：超平面是分隔高维度空间的边界。&lt;a href="#kernel support vector machines (ksvms)"&gt;&lt;strong&gt;核支持向量机&lt;/strong&gt;&lt;/a&gt;利用超平面将正类别和负类别区分开来（通常是在极高维度空间中）。&lt;/p&gt;
&lt;h2 id="i_1"&gt;I&lt;/h2&gt;
&lt;h3 id="independently and identically distributed (i.i.d)"&gt;independently and identically distributed (i.i.d)&lt;/h3&gt;
&lt;p&gt;Data drawn from a distribution that doesn't change, and where each value drawn doesn't depend on values that have been drawn previously. An i.i.d. is the &lt;a href="https://en.wikipedia.org/wiki/Ideal_gas"&gt;ideal gas&lt;/a&gt; of machine learning&amp;mdash;a useful mathematical construct but almost never exactly found in the real world. For example, the distribution of visitors to a web page may be i.i.d. over a brief window of time; that is, the distribution doesn't change during that brief window and one person's visit is generally independent of another's visit. However, if you expand that window of time, seasonal differences in the web page's visitors may appear.&lt;/p&gt;
&lt;p&gt;从不会改变的分布中提取的数据，其中提取的每个值都不依赖于之前提取的值。i.i.d. 是机器学习的&lt;a href="https://en.wikipedia.org/wiki/Ideal_gas"&gt;理想气体&lt;/a&gt; - 一种实用的数学结构，但在现实世界中几乎从未发现过。例如，某个网页的访问者在短时间内的分布可能为 i.i.d.，即分布在该短时间内没有变化，且一位用户的访问行为通常与另一位用户的访问行为无关。不过，如果将时间窗口扩大，网页访问者的分布可能呈现出季节性变化。&lt;/p&gt;
&lt;h3 id="inference"&gt;inference&lt;/h3&gt;
&lt;p&gt;In machine learning, often refers to the process of making predictions by applying the trained model to &lt;a href="#unlabeled example"&gt;&lt;strong&gt;unlabeled examples&lt;/strong&gt;&lt;/a&gt;. In statistics, inference refers to the process of fitting the parameters of a distribution conditioned on some observed data. (See the &lt;a href="https://en.wikipedia.org/wiki/Statistical_inference"&gt;Wikipedia article on statistical inference&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;在机器学习中，推断通常指以下过程：通过将训练过的模型应用于&lt;a href="#unlabeled example"&gt;&lt;strong&gt;无标签样本&lt;/strong&gt;&lt;/a&gt;来做出预测。在统计学中，推断是指在某些观测数据条件下拟合分布参数的过程。（请参阅&lt;a href="https://en.wikipedia.org/wiki/Statistical_inference"&gt;维基百科中有关统计学推断的文章&lt;/a&gt;。）&lt;/p&gt;
&lt;h3 id="input function"&gt;input function&lt;/h3&gt;
&lt;p&gt;In TensorFlow, a function that returns input data to the training, evaluation, or prediction method of an &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt;. For example, the training input function returns a &lt;a href="#batch"&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/a&gt; of features and labels from the &lt;a href="#training set"&gt;&lt;strong&gt;training set&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在 TensorFlow 中，用于将输入数据返回到 &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt; 的训练、评估或预测方法的函数。例如，训练输入函数用于返回&lt;a href="#training set"&gt;&lt;strong&gt;训练集&lt;/strong&gt;&lt;/a&gt;中的&lt;a href="#batch"&gt;&lt;strong&gt;批次&lt;/strong&gt;&lt;/a&gt;特征和标签。&lt;/p&gt;
&lt;h3 id="input layer"&gt;input layer&lt;/h3&gt;
&lt;p&gt;The first layer (the one that receives the input data) in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的第一层（接收输入数据的层）。&lt;/p&gt;
&lt;h3 id="instance"&gt;instance&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#example"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="interpretability"&gt;interpretability&lt;/h3&gt;
&lt;p&gt;The degree to which a model's predictions can be readily explained. Deep models are often non-interpretable; that is, a deep model's different layers can be hard to decipher. By contrast, linear regression models and &lt;a href="#wide model"&gt;&lt;strong&gt;wide models&lt;/strong&gt;&lt;/a&gt; are typically far more interpretable.&lt;/p&gt;
&lt;p&gt;模型的预测可解释的难易程度。深度模型通常不可解释，也就是说，很难对深度模型的不同层进行解释。相比之下，线性回归模型和&lt;a href="#wide model"&gt;&lt;strong&gt;宽度模型&lt;/strong&gt;&lt;/a&gt;的可解释性通常要好得多。&lt;/p&gt;
&lt;h3 id="inter-rater agreement"&gt;inter-rater agreement&lt;/h3&gt;
&lt;p&gt;A measurement of how often human raters agree when doing a task. If raters disagree, the task instructions may need to be improved. Also sometimes called &lt;strong&gt;inter-annotator agreement&lt;/strong&gt; or &lt;strong&gt;inter-rater reliability&lt;/strong&gt;. See also &lt;a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa"&gt;Cohen's kappa&lt;/a&gt;, which is one of the most popular inter-rater agreement measurements.&lt;/p&gt;
&lt;p&gt;一种衡量指标，用于衡量在执行某项任务时评分者达成一致的频率。如果评分者未达成一致，则可能需要改进任务说明。有时也称为&lt;strong&gt;注释者间一致性信度&lt;/strong&gt;或&lt;strong&gt;评分者间可靠性信度&lt;/strong&gt;。另请参阅 &lt;a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa"&gt;Cohen's kappa&lt;/a&gt;（最热门的评分者间一致性信度衡量指标之一）。&lt;/p&gt;
&lt;h3 id="iteration"&gt;iteration&lt;/h3&gt;
&lt;p&gt;A single update of a model's weights during training. An iteration consists of computing the gradients of the parameters with respect to the loss on a single &lt;a href="#batch"&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/a&gt; of data.&lt;/p&gt;
&lt;p&gt;模型的权重在训练期间的一次更新。迭代包含计算参数在单个&lt;a href="#batch"&gt;&lt;strong&gt;批量&lt;/strong&gt;&lt;/a&gt;数据上的梯度损失。&lt;/p&gt;
&lt;h2 id="k_1"&gt;K&lt;/h2&gt;
&lt;h3 id="keras"&gt;Keras&lt;/h3&gt;
&lt;p&gt;A popular Python machine learning API. &lt;a href="https://keras.io"&gt;Keras&lt;/a&gt; runs on several deep learning frameworks, including TensorFlow, where it is made available as &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras"&gt;&lt;strong&gt;tf.keras&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种热门的 Python 机器学习 API。&lt;a href="https://keras.io"&gt;Keras&lt;/a&gt; 能够在多种深度学习框架上运行，其中包括 TensorFlow（在该框架上，Keras 作为 &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras"&gt;&lt;strong&gt;tf.keras&lt;/strong&gt;&lt;/a&gt; 提供）。&lt;/p&gt;
&lt;h3 id="kernel support vector machines (ksvms)"&gt;Kernel Support Vector Machines (KSVMs)&lt;/h3&gt;
&lt;p&gt;A classification algorithm that seeks to maximize the margin between &lt;a href="#positive class"&gt;&lt;strong&gt;positive&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#negative class"&gt;&lt;strong&gt;negative classes&lt;/strong&gt;&lt;/a&gt; by mapping input data vectors to a higher dimensional space. For example, consider a classification problem in which the input data set consists of a hundred features. In order to maximize the margin between positive and negative classes, KSVMs could internally map those features into a million-dimension space. KSVMs uses a loss function called &lt;a href="#hinge-loss"&gt;hinge loss&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种分类算法，旨在通过将输入数据向量映射到更高维度的空间，来最大化&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;之间的裕度。以某个输入数据集包含一百个特征的分类问题为例。为了最大化正类别和负类别之间的裕度，KSVM 可以在内部将这些特征映射到百万维度的空间。KSVM 使用&lt;a href="#hinge loss"&gt;&lt;strong&gt;合页损失函数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="l_1"&gt;L&lt;/h2&gt;
&lt;h3 id="l1 loss"&gt;L1 loss&lt;/h3&gt;
&lt;p&gt;&lt;a href="#loss"&gt;&lt;strong&gt;Loss&lt;/strong&gt;&lt;/a&gt; function based on the absolute value of the difference between the values that a model is predicting and the actual values of the &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt;. L1 loss is less sensitive to outliers than &lt;a href="#squared loss"&gt;&lt;strong&gt;L2 loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;函数，基于模型预测的值与&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的实际值之差的绝对值。与 &lt;a href="#squared loss"&gt;&lt;strong&gt;L2 损失函数&lt;/strong&gt;&lt;/a&gt;相比，L1 损失函数对离群值的敏感性弱一些。&lt;/p&gt;
&lt;h3 id="l1 regularization"&gt;L1 regularization&lt;/h3&gt;
&lt;p&gt;A type of &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt; that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on &lt;a href="#sparse feature"&gt;&lt;strong&gt;sparse features&lt;/strong&gt;&lt;/a&gt;, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with &lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;，根据权重的绝对值的总和来惩罚权重。在依赖&lt;a href="#sparse feature"&gt;&lt;strong&gt;稀疏特征&lt;/strong&gt;&lt;/a&gt;的模型中，L1 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 &lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 正则化&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="l2 loss"&gt;L2 loss&lt;/h3&gt;
&lt;p&gt;See &lt;a href="#squared loss"&gt;&lt;strong&gt;squared loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;请参阅&lt;a href="#squared loss"&gt;&lt;strong&gt;平方损失函数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="l2 regularization"&gt;L2 regularization&lt;/h3&gt;
&lt;p&gt;A type of &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt; that penalizes weights in proportion to the sum of the &lt;em&gt;squares&lt;/em&gt; of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with &lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 regularization&lt;/strong&gt;&lt;/a&gt;.) L2 regularization always improves generalization in linear models.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;，根据权重的平方和来惩罚权重。L2 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 &lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 正则化&lt;/strong&gt;&lt;/a&gt;相对。）在线性模型中，L2 正则化始终可以改进泛化。&lt;/p&gt;
&lt;h3 id="label"&gt;label&lt;/h3&gt;
&lt;p&gt;In supervised learning, the "answer" or "result" portion of an &lt;a href="#example"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;. Each example in a labeled data set consists of one or more features and a label. For instance, in a housing data set, the features might include the number of bedrooms, the number of bathrooms, and the age of the house, while the label might be the house's price. in a spam detection dataset, the features might include the subject line, the sender, and the email message itself, while the label would probably be either "spam" or "not spam."&lt;/p&gt;
&lt;p&gt;在监督式学习中，标签指&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;的&amp;ldquo;答案&amp;rdquo;或&amp;ldquo;结果&amp;rdquo;部分。有标签数据集中的每个样本都包含一个或多个特征以及一个标签。例如，在房屋数据集中，特征可以包括卧室数、卫生间数以及房龄，而标签则可以是房价。在垃圾邮件检测数据集中，特征可以包括主题行、发件人以及电子邮件本身，而标签则可以是&amp;ldquo;垃圾邮件&amp;rdquo;或&amp;ldquo;非垃圾邮件&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="labeled example"&gt;labeled example&lt;/h3&gt;
&lt;p&gt;An example that contains &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; and a &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. In supervised training, models learn from labeled examples.&lt;/p&gt;
&lt;p&gt;包含&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的样本。在监督式训练中，模型从有标签样本中进行学习。&lt;/p&gt;
&lt;h3 id="lambda"&gt;lambda&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#regularization rate"&gt;&lt;strong&gt;regularization rate&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(This is an overloaded term. Here we're focusing on the term's definition within &lt;a href="#regularization"&gt;&lt;strong&gt;regularization&lt;/strong&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;是&lt;a href="#regularization rate"&gt;&lt;strong&gt;正则化率&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;p&gt;（多含义术语，我们在此关注的是该术语在&lt;a href="#regularization"&gt;&lt;strong&gt;正则化&lt;/strong&gt;&lt;/a&gt;中的定义。）&lt;/p&gt;
&lt;h3 id="layer"&gt;layer&lt;/h3&gt;
&lt;p&gt;A set of &lt;a href="#neuron"&gt;&lt;strong&gt;neurons&lt;/strong&gt;&lt;/a&gt; in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt; that process a set of input features, or the output of those neurons.&lt;/p&gt;
&lt;p&gt;Also, an abstraction in TensorFlow. Layers are Python functions that take &lt;a href="#tensor"&gt;&lt;strong&gt;Tensors&lt;/strong&gt;&lt;/a&gt; and configuration options as input and produce other tensors as output. Once the necessary Tensors have been composed, the user can convert the result into an &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt; via a model function.&lt;/p&gt;
&lt;p&gt;&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的一组&lt;a href="#neuron"&gt;&lt;strong&gt;神经元&lt;/strong&gt;&lt;/a&gt;，处理一组输入特征，或一组神经元的输出。&lt;/p&gt;
&lt;p&gt;此外还指 TensorFlow 中的抽象层。层是 Python 函数，以&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;和配置选项作为输入，然后生成其他张量作为输出。当必要的张量组合起来，用户便可以通过模型函数将结果转换为 &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="layers api (tf.layers)"&gt;Layers API (tf.layers)&lt;/h3&gt;
&lt;p&gt;A TensorFlow API for constructing a &lt;a href="#deep model"&gt;&lt;strong&gt;deep&lt;/strong&gt;&lt;/a&gt; neural network as a composition of layers. The Layers API enables you to build different types of &lt;a href="#layer"&gt;&lt;strong&gt;layers&lt;/strong&gt;&lt;/a&gt;, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.layers.Dense&lt;/code&gt; for a &lt;a href="#fully connected layer"&gt;&lt;strong&gt;fully-connected layer&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.layers.Conv2D&lt;/code&gt; for a convolutional layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When writing a &lt;a href="#custom estimator"&gt;&lt;strong&gt;custom Estimator&lt;/strong&gt;&lt;/a&gt;, you compose Layers objects to define the characteristics of all the &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Layers API follows the &lt;a href="#keras"&gt;&lt;strong&gt;Keras&lt;/strong&gt;&lt;/a&gt; layers API conventions. That is, aside from a different prefix, all functions in the Layers API have the same names and signatures as their counterparts in the Keras layers API.&lt;/p&gt;
&lt;p&gt;一种 TensorFlow API，用于以层组合的方式构建&lt;a href="#deep model"&gt;&lt;strong&gt;深度&lt;/strong&gt;&lt;/a&gt;神经网络。通过 Layers API，您可以构建不同类型的&lt;a href="#layer"&gt;&lt;strong&gt;层&lt;/strong&gt;&lt;/a&gt;，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过 &lt;code&gt;tf.layers.Dense&lt;/code&gt; 构建&lt;a href="#fully connected layer"&gt;&lt;strong&gt;全连接层&lt;/strong&gt;&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;通过 &lt;code&gt;tf.layers.Conv2D&lt;/code&gt; 构建卷积层。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在编写&lt;a href="#custom estimator"&gt;&lt;strong&gt;自定义 Estimator&lt;/strong&gt;&lt;/a&gt; 时，您可以编写&amp;ldquo;层&amp;rdquo;对象来定义所有&lt;a href="#hidden layers"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;的特征。&lt;/p&gt;
&lt;p&gt;Layers API 遵循 &lt;a href="#keras"&gt;&lt;strong&gt;Keras&lt;/strong&gt;&lt;/a&gt; layers API 规范。也就是说，除了前缀不同以外，Layers API 中的所有函数均与 Keras layers API 中的对应函数具有相同的名称和签名。&lt;/p&gt;
&lt;h3 id="learning rate"&gt;learning rate&lt;/h3&gt;
&lt;p&gt;A scalar used to train a model via gradient descent. During each iteration, the &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; algorithm multiplies the learning rate by the gradient. The resulting product is called the &lt;strong&gt;gradient step&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Learning rate is a key &lt;a href="#hyperparameter"&gt;&lt;strong&gt;hyperparameter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在训练模型时用于梯度下降的一个变量。在每次迭代期间，&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;都会将学习速率与梯度相乘。得出的乘积称为&lt;strong&gt;梯度步长&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;学习速率是一个重要的&lt;a href="#hyperparameter"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="least squares regression"&gt;least squares regression&lt;/h3&gt;
&lt;p&gt;A linear regression model trained by minimizing &lt;a href="#l2 loss"&gt;&lt;strong&gt;L2 Loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种通过最小化 &lt;a href="#l2 loss"&gt;&lt;strong&gt;L2 损失&lt;/strong&gt;&lt;/a&gt;训练出的线性回归模型。&lt;/p&gt;
&lt;h3 id="linear regression"&gt;linear regression&lt;/h3&gt;
&lt;p&gt;A type of &lt;a href="#regression model"&gt;&lt;strong&gt;regression model&lt;/strong&gt;&lt;/a&gt; that outputs a continuous value from a linear combination of input features.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#regression model"&gt;&lt;strong&gt;回归模型&lt;/strong&gt;&lt;/a&gt;，通过将输入特征进行线性组合，以连续值作为输出。&lt;/p&gt;
&lt;h3 id="logistic regression"&gt;logistic regression&lt;/h3&gt;
&lt;p&gt;A model that generates a probability for each possible discrete label value in classification problems by applying a &lt;a href="#sigmoid function"&gt;&lt;strong&gt;sigmoid function&lt;/strong&gt;&lt;/a&gt; to a linear prediction. Although logistic regression is often used in &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt; problems, it can also be used in &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class&lt;/strong&gt;&lt;/a&gt; classification problems (where it becomes called &lt;strong&gt;multi-class logistic regression&lt;/strong&gt; or &lt;strong&gt;multinomial regression&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;一种模型，通过将 &lt;a href="#sigmoid function"&gt;&lt;strong&gt;S 型函数&lt;/strong&gt;&lt;/a&gt;应用于线性预测，生成分类问题中每个可能的离散标签值的概率。虽然逻辑回归经常用于&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;问题，但也可用于&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别&lt;/strong&gt;&lt;/a&gt;分类问题（其叫法变为&lt;strong&gt;多类别逻辑回归&lt;/strong&gt;或&lt;strong&gt;多项回归&lt;/strong&gt;）。&lt;/p&gt;
&lt;h3 id="log loss"&gt;Log Loss&lt;/h3&gt;
&lt;p&gt;The &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; function used in binary &lt;a href="#logistic regression"&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;二元&lt;a href="#logistic regression"&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/a&gt;中使用的&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;函数。&lt;/p&gt;
&lt;h3 id="loss"&gt;loss&lt;/h3&gt;
&lt;p&gt;A measure of how far a model's &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt; are from its &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use &lt;a href="#mean squared error (mse)"&gt;&lt;strong&gt;mean squared error&lt;/strong&gt;&lt;/a&gt; for a loss function, while logistic regression models use &lt;a href="#log loss"&gt;&lt;strong&gt;Log Loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种衡量指标，用于衡量模型的&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;偏离其&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将&lt;a href="#mean squared error (mse)"&gt;&lt;strong&gt;均方误差&lt;/strong&gt;&lt;/a&gt;用于损失函数，而逻辑回归模型则使用&lt;a href="#log loss"&gt;&lt;strong&gt;对数损失函数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="m_1"&gt;M&lt;/h2&gt;
&lt;h3 id="machine learning"&gt;machine learning&lt;/h3&gt;
&lt;p&gt;A program or system that builds (trains) a predictive model from input data. The system uses the learned model to make useful predictions from new (never-before-seen) data drawn from the same distribution as the one used to train the model. Machine learning also refers to the field of study concerned with these programs or systems.&lt;/p&gt;
&lt;p&gt;一种程序或系统，用于根据输入数据构建（训练）预测模型。这种系统会利用学到的模型根据从分布（训练该模型时使用的同一分布）中提取的新数据（以前从未见过的数据）进行实用的预测。机器学习还指与这些程序或系统相关的研究领域。&lt;/p&gt;
&lt;h3 id="mean squared error (mse)"&gt;Mean Squared Error (MSE)&lt;/h3&gt;
&lt;p&gt;The average squared loss per example. MSE is calculated by dividing the &lt;a href="#squared loss"&gt;&lt;strong&gt;squared loss&lt;/strong&gt;&lt;/a&gt; by the number of &lt;a href="#example"&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt;. The values that &lt;a href="#tensorflow playground"&gt;&lt;strong&gt;TensorFlow Playground&lt;/strong&gt;&lt;/a&gt; displays for "Training loss" and "Test loss" are MSE.&lt;/p&gt;
&lt;p&gt;每个样本的平均平方损失。MSE 的计算方法是&lt;a href="#squared loss"&gt;&lt;strong&gt;平方损失&lt;/strong&gt;&lt;/a&gt;除以&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;数。&lt;a href="#tensorflow playground"&gt;&lt;strong&gt;TensorFlow Playground&lt;/strong&gt;&lt;/a&gt; 显示的&amp;ldquo;训练损失&amp;rdquo;值和&amp;ldquo;测试损失&amp;rdquo;值都是 MSE。&lt;/p&gt;
&lt;h3 id="metric"&gt;metric&lt;/h3&gt;
&lt;p&gt;A number that you care about. May or may not be directly optimized in a machine-learning system. A metric that your system tries to optimize is called an &lt;a href="#objective"&gt;&lt;strong&gt;objective&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;您关心的一个数值。可能可以也可能不可以直接在机器学习系统中得到优化。您的系统尝试优化的指标称为&lt;a href="#objective"&gt;&lt;strong&gt;目标&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="metrics api (tf.metrics)"&gt;Metrics API (tf.metrics)&lt;/h3&gt;
&lt;p&gt;A TensorFlow API for evaluating models. For example, &lt;code&gt;tf.metrics.accuracy&lt;/code&gt; determines how often a model's predictions match labels. When writing a &lt;a href="#custom estimator"&gt;&lt;strong&gt;custom Estimator&lt;/strong&gt;&lt;/a&gt;, you invoke Metrics API functions to specify how your model should be evaluated.&lt;/p&gt;
&lt;p&gt;一种用于评估模型的 TensorFlow API。例如，&lt;code&gt;tf.metrics.accuracy&lt;/code&gt; 用于确定模型的预测与标签匹配的频率。在编写&lt;a href="#custom estimator"&gt;&lt;strong&gt;自定义 Estimator&lt;/strong&gt;&lt;/a&gt; 时，您可以调用 Metrics API 函数来指定应如何评估您的模型。&lt;/p&gt;
&lt;h3 id="mini-batch"&gt;mini-batch&lt;/h3&gt;
&lt;p&gt;A small, randomly selected subset of the entire batch of &lt;a href="#example"&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt; run together in a single iteration of training or inference. The &lt;a href="#batch size"&gt;&lt;strong&gt;batch size&lt;/strong&gt;&lt;/a&gt; of a mini-batch is usually between 10 and 1,000. It is much more efficient to calculate the loss on a mini-batch than on the full training data.&lt;/p&gt;
&lt;p&gt;从训练或推断过程的一次迭代中一起运行的整批&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;内随机选择的一小部分。小批次的&lt;a href="#batch size"&gt;&lt;strong&gt;规模&lt;/strong&gt;&lt;/a&gt;通常介于 10 到 1000 之间。与基于完整的训练数据计算损失相比，基于小批次数据计算损失要高效得多。&lt;/p&gt;
&lt;h3 id="mini-batch stochastic gradient descent (sgd)"&gt;mini-batch stochastic gradient descent (SGD)&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; algorithm that uses &lt;a href="#mini-batch"&gt;&lt;strong&gt;mini-batches&lt;/strong&gt;&lt;/a&gt;. In other words, mini-batch SGD estimates the gradient based on a small subset of the training data. &lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;Vanilla SGD&lt;/strong&gt;&lt;/a&gt; uses a mini-batch of size 1.&lt;/p&gt;
&lt;p&gt;一种采用&lt;a href="#mini-batch"&gt;&lt;strong&gt;小批次&lt;/strong&gt;&lt;/a&gt;样本的&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;。也就是说，小批次 SGD 会根据一小部分训练数据来估算梯度。&lt;a href="#stochastic gradient descent (sgd)"&gt;&lt;strong&gt;Vanilla SGD&lt;/strong&gt;&lt;/a&gt; 使用的小批次的规模为 1。&lt;/p&gt;
&lt;h3 id="ml"&gt;ML&lt;/h3&gt;
&lt;p&gt;Abbreviation for &lt;a href="#machine learning"&gt;&lt;strong&gt;machine learning&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#machine learning"&gt;&lt;strong&gt;机器学习&lt;/strong&gt;&lt;/a&gt;的缩写。&lt;/p&gt;
&lt;h3 id="model"&gt;model&lt;/h3&gt;
&lt;p&gt;The representation of what an ML system has learned from the training data. This is an overloaded term, which can have either of the following two related meanings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href="#tensorflow"&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/a&gt; graph that expresses the structure of how a prediction will be computed.&lt;/li&gt;
&lt;li&gt;The particular weights and biases of that TensorFlow graph, which are determined by &lt;a href="#model training"&gt;&lt;strong&gt;training&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;机器学习系统从训练数据学到的内容的表示形式。多含义术语，可以理解为下列两种相关含义之一：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一种 &lt;a href="#tensorflow"&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/a&gt; 图，用于表示预测计算结构。&lt;/li&gt;
&lt;li&gt;该 TensorFlow 图的特定权重和偏差，通过&lt;a href="#model training"&gt;&lt;strong&gt;训练&lt;/strong&gt;&lt;/a&gt;决定。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="model training"&gt;model training&lt;/h3&gt;
&lt;p&gt;The process of determining the best &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;确定最佳&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;的过程。&lt;/p&gt;
&lt;h3 id="momentum"&gt;Momentum&lt;/h3&gt;
&lt;p&gt;A sophisticated gradient descent algorithm in which a learning step depends not only on the derivative in the current step, but also on the derivatives of the step(s) that immediately preceded it. Momentum involves computing an exponentially weighted moving average of the gradients over time, analogous to momentum in physics. Momentum sometimes prevents learning from getting stuck in local minima.&lt;/p&gt;
&lt;p&gt;一种先进的梯度下降法，其中学习步长不仅取决于当前步长的导数，还取决于之前一步或多步的步长的导数。动量涉及计算梯度随时间而变化的指数级加权移动平均值，与物理学中的动量类似。动量有时可以防止学习过程被卡在局部最小的情况。&lt;/p&gt;
&lt;h3 id="multi-class classification"&gt;multi-class classification&lt;/h3&gt;
&lt;p&gt;Classification problems that distinguish among more than two classes. For example, there are approximately 128 species of maple trees, so a model that categorized maple tree species would be multi-class. Conversely, a model that divided emails into only two categories (&lt;em&gt;spam&lt;/em&gt; and &lt;em&gt;not spam&lt;/em&gt;) would be a &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;区分两种以上类别的分类问题。例如，枫树大约有 128 种，因此，确定枫树种类的模型就属于多类别模型。反之，仅将电子邮件分为两类（&amp;ldquo;垃圾邮件&amp;rdquo;和&amp;ldquo;非垃圾邮件&amp;rdquo;）的模型属于&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类模型&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="multinomial classification"&gt;multinomial classification&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h2 id="n_1"&gt;N&lt;/h2&gt;
&lt;h3 id="nan trap"&gt;NaN trap&lt;/h3&gt;
&lt;p&gt;When one number in your model becomes a &lt;a href="https://en.wikipedia.org/wiki/NaN"&gt;NaN&lt;/a&gt; during training, which causes many or all other numbers in your model to eventually become a NaN.&lt;/p&gt;
&lt;p&gt;NaN is an abbreviation for "Not a Number."&lt;/p&gt;
&lt;p&gt;模型中的一个数字在训练期间变成 &lt;a href="https://en.wikipedia.org/wiki/NaN"&gt;NaN&lt;/a&gt;，这会导致模型中的很多或所有其他数字最终也会变成 NaN。&lt;/p&gt;
&lt;p&gt;NaN 是&amp;ldquo;非数字&amp;rdquo;的缩写。&lt;/p&gt;
&lt;h3 id="negative class"&gt;negative class&lt;/h3&gt;
&lt;p&gt;In &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt;, one class is termed positive and the other is termed negative. The positive class is the thing we're looking for and the negative class is the other possibility. For example, the negative class in a medical test might be "not tumor." The negative class in an email classifier might be "not spam." See also &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;中，一种类别称为正类别，另一种类别称为负类别。正类别是我们要寻找的类别，负类别则是另一种可能性。例如，在医学检查中，负类别可以是&amp;ldquo;非肿瘤&amp;rdquo;。在电子邮件分类器中，负类别可以是&amp;ldquo;非垃圾邮件&amp;rdquo;。另请参阅&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="neural network"&gt;neural network&lt;/h3&gt;
&lt;p&gt;A model that, taking inspiration from the brain, is composed of layers (at least one of which is &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden&lt;/strong&gt;&lt;/a&gt;) consisting of simple connected units or &lt;a href="#neuron"&gt;&lt;strong&gt;neurons&lt;/strong&gt;&lt;/a&gt; followed by nonlinearities.&lt;/p&gt;
&lt;p&gt;一种模型，灵感来源于脑部结构，由多个层构成（至少有一个是&lt;a href="#hidden layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;），每个层都包含简单相连的单元或&lt;a href="#neuron"&gt;&lt;strong&gt;神经元&lt;/strong&gt;&lt;/a&gt;（具有非线性关系）。&lt;/p&gt;
&lt;h3 id="neuron"&gt;neuron&lt;/h3&gt;
&lt;p&gt;A node in a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt;, typically taking in multiple input values and generating one output value. The neuron calculates the output value by applying an &lt;a href="#activation function"&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/a&gt; (nonlinear transformation) to a weighted sum of input values.&lt;/p&gt;
&lt;p&gt;&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中的节点，通常是接收多个输入值并生成一个输出值。神经元通过将&lt;a href="#activation function"&gt;&lt;strong&gt;激活函数&lt;/strong&gt;&lt;/a&gt;（非线性转换）应用于输入值的加权和来计算输出值。&lt;/p&gt;
&lt;h3 id="node"&gt;node&lt;/h3&gt;
&lt;p&gt;An overloaded term that means either of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A neuron in a &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layer&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;An operation in a TensorFlow &lt;a href="#graph"&gt;&lt;strong&gt;graph&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;多含义术语，可以理解为下列两种含义之一：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hidden layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;中的神经元。&lt;/li&gt;
&lt;li&gt;TensorFlow &lt;a href="#graph"&gt;&lt;strong&gt;图&lt;/strong&gt;&lt;/a&gt;中的操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="normalization"&gt;normalization&lt;/h3&gt;
&lt;p&gt;The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. For example, suppose the natural range of a certain feature is 800 to 6,000. Through subtraction and division, you can normalize those values into the range -1 to +1.&lt;/p&gt;
&lt;p&gt;See also &lt;a href="#scaling"&gt;&lt;strong&gt;scaling&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;将实际的值区间转换为标准的值区间（通常为 -1 到 +1 或 0 到 1）的过程。例如，假设某个特征的自然区间是 800 到 6000。通过减法和除法运算，您可以将这些值标准化为位于 -1 到 +1 区间内。&lt;/p&gt;
&lt;p&gt;另请参阅&lt;a href="#scaling"&gt;&lt;strong&gt;缩放&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="numerical data"&gt;numerical data&lt;/h3&gt;
&lt;p&gt;&lt;a href="#feature"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/a&gt; represented as integers or real-valued numbers. For example, in a real estate model, you would probably represent the size of a house (in square feet or square meters) as numerical data. Representing a feature as numerical data indicates that the feature's values have a &lt;em&gt;mathematical&lt;/em&gt; relationship to each other and possibly to the label. For example, representing the size of a house as numerical data indicates that a 200 square-meter house is twice as large as a 100 square-meter house. Furthermore, the number of square meters in a house probably has some mathematical relationship to the price of the house.&lt;/p&gt;
&lt;p&gt;Not all integer data should be represented as numerical data. For example, postal codes in some parts of the world are integers; however, integer postal codes should not be represented as numerical data in models. That's because a postal code of &lt;code&gt;20000&lt;/code&gt; is not twice (or half) as potent as a postal code of 10000. Furthermore, although different postal codes &lt;em&gt;do&lt;/em&gt; correlate to different real estate values, we can't assume that real estate values at postal code 20000 are twice as valuable as real estate values at postal code 10000. Postal codes should be represented as &lt;a href="#categorical data"&gt;&lt;strong&gt;categorical data&lt;/strong&gt;&lt;/a&gt; instead.&lt;/p&gt;
&lt;p&gt;Numerical features are sometimes called &lt;a href="#continuous feature"&gt;&lt;strong&gt;continuous features&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;用整数或实数表示的&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;。例如，在房地产模型中，您可能会用数值数据表示房子大小（以平方英尺或平方米为单位）。如果用数值数据表示特征，则可以表明特征的值相互之间具有数学关系，并且与标签可能也有数学关系。例如，如果用数值数据表示房子大小，则可以表明面积为 200 平方米的房子是面积为 100 平方米的房子的两倍。此外，房子面积的平方米数可能与房价存在一定的数学关系。&lt;/p&gt;
&lt;p&gt;并非所有整数数据都应表示成数值数据。例如，世界上某些地区的邮政编码是整数，但在模型中，不应将整数邮政编码表示成数值数据。这是因为邮政编码 &lt;code&gt;20000&lt;/code&gt; 在效力上并不是邮政编码 10000 的两倍（或一半）。此外，虽然不同的邮政编码确实与不同的房地产价值有关，但我们也不能假设邮政编码为 20000 的房地产在价值上是邮政编码为 10000 的房地产的两倍。邮政编码应表示成&lt;a href="#categorical data"&gt;&lt;strong&gt;分类数据&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;数值特征有时称为&lt;a href="#continuous feature"&gt;&lt;strong&gt;连续特征&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="numpy"&gt;numpy&lt;/h3&gt;
&lt;p&gt;An &lt;a href="http://www.numpy.org/"&gt;open-source math library&lt;/a&gt; that provides efficient array operations in Python. &lt;a href="#pandas"&gt;&lt;strong&gt;pandas&lt;/strong&gt;&lt;/a&gt; is built on numpy.&lt;/p&gt;
&lt;p&gt;一个&lt;a href="http://www.numpy.org/"&gt;开放源代码数学库&lt;/a&gt;，在 Python 中提供高效的数组操作。&lt;a href="#pandas"&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/a&gt; 就建立在 Numpy 之上。&lt;/p&gt;
&lt;h2 id="o_1"&gt;O&lt;/h2&gt;
&lt;h3 id="objective"&gt;objective&lt;/h3&gt;
&lt;p&gt;A metric that your algorithm is trying to optimize.&lt;/p&gt;
&lt;p&gt;算法尝试优化的指标。&lt;/p&gt;
&lt;h3 id="offline inference"&gt;offline inference&lt;/h3&gt;
&lt;p&gt;Generating a group of &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt;, storing those predictions, and then retrieving those predictions on demand. Contrast with &lt;a href="#online inference"&gt;&lt;strong&gt;online inference&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;生成一组&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;，存储这些预测，然后根据需求检索这些预测。与&lt;a href="#online inference"&gt;&lt;strong&gt;在线推断&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="one-hot encoding"&gt;one-hot encoding&lt;/h3&gt;
&lt;p&gt;A sparse vector in which:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One element is set to 1.&lt;/li&gt;
&lt;li&gt;All other elements are set to 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One-hot encoding is commonly used to represent strings or identifiers that have a finite set of possible values. For example, suppose a given botany data set chronicles 15,000 different species, each denoted with a unique string identifier. As part of feature engineering, you'll probably encode those string identifiers as one-hot vectors in which the vector has a size of 15,000.&lt;/p&gt;
&lt;p&gt;一种稀疏向量，其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个元素设为 1。&lt;/li&gt;
&lt;li&gt;所有其他元素均设为 0。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;one-hot 编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为 one-hot 向量，向量的大小为 15000。&lt;/p&gt;
&lt;h3 id="one-vs.-all"&gt;one-vs.-all&lt;/h3&gt;
&lt;p&gt;Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classifiers&lt;/strong&gt;&lt;/a&gt;&amp;mdash;one binary classifier for each possible outcome. For example, given a model that classifies examples as animal, vegetable, or mineral, a one-vs.-all solution would provide the following three separate binary classifiers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;animal vs. not animal&lt;/li&gt;
&lt;li&gt;vegetable vs. not vegetable&lt;/li&gt;
&lt;li&gt;mineral vs. not mineral&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设某个分类问题有 N 种可能的解决方案，一对多解决方案将包含 N 个单独的&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类器&lt;/strong&gt;&lt;/a&gt; - 一个二元分类器对应一种可能的结果。例如，假设某个模型用于区分样本属于动物、蔬菜还是矿物，一对多解决方案将提供下列三个单独的二元分类器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;动物和非动物&lt;/li&gt;
&lt;li&gt;蔬菜和非蔬菜&lt;/li&gt;
&lt;li&gt;矿物和非矿物&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="online inference"&gt;online inference&lt;/h3&gt;
&lt;p&gt;Generating &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt; on demand. Contrast with &lt;a href="#offline inference"&gt;&lt;strong&gt;offline inference&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;根据需求生成&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;。与&lt;a href="#offline inference"&gt;&lt;strong&gt;离线推断&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="operation (op)"&gt;Operation (op)&lt;/h3&gt;
&lt;p&gt;A node in the TensorFlow graph. In TensorFlow, any procedure that creates, manipulates, or destroys a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt; is an operation. For example, a matrix multiply is an operation that takes two Tensors as input and generates one Tensor as output.&lt;/p&gt;
&lt;p&gt;TensorFlow 图中的节点。在 TensorFlow 中，任何创建、操纵或销毁&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;的过程都属于操作。例如，矩阵相乘就是一种操作，该操作以两个张量作为输入，并生成一个张量作为输出。&lt;/p&gt;
&lt;h3 id="optimizer"&gt;optimizer&lt;/h3&gt;
&lt;p&gt;A specific implementation of the &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; algorithm. TensorFlow's base class for optimizers is &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"&gt;tf.train.Optimizer&lt;/a&gt;. Different optimizers may leverage one or more of the following concepts to enhance the effectiveness of gradient descent on a given &lt;a href="#training set"&gt;&lt;strong&gt;training set&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer"&gt;momentum&lt;/a&gt; (Momentum)&lt;/li&gt;
&lt;li&gt;update frequency (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer"&gt;AdaGrad&lt;/a&gt; = ADAptive GRADient descent; &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer"&gt;Adam&lt;/a&gt; = ADAptive with Momentum; RMSProp)&lt;/li&gt;
&lt;li&gt;sparsity/regularization (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer"&gt;Ftrl&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;more complex math (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer"&gt;Proximal&lt;/a&gt;, and others)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might even imagine an &lt;a href="https://arxiv.org/abs/1606.04474"&gt;NN-driven optimizer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;的一种具体实现。TensorFlow 的优化器基类是 &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"&gt;tf.train.Optimizer&lt;/a&gt;。不同的优化器（&lt;code&gt;tf.train.Optimizer&lt;/code&gt; 的子类）会考虑如下概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer"&gt;动量&lt;/a&gt; (Momentum)&lt;/li&gt;
&lt;li&gt;更新频率 （&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer"&gt;AdaGrad&lt;/a&gt; = ADAptive GRADient descent； &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer"&gt;Adam&lt;/a&gt; = ADAptive with Momentum；RMSProp）&lt;/li&gt;
&lt;li&gt;稀疏性/正则化 (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer"&gt;Ftrl&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;更复杂的计算方法 （&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer"&gt;Proximal&lt;/a&gt;， 等等）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;甚至还包括 &lt;a href="https://arxiv.org/abs/1606.04474"&gt;NN 驱动的优化器&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="outliers"&gt;outliers&lt;/h3&gt;
&lt;p&gt;Values distant from most other values. In machine learning, any of the following are outliers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#weight"&gt;&lt;strong&gt;Weights&lt;/strong&gt;&lt;/a&gt; with high absolute values.&lt;/li&gt;
&lt;li&gt;Predicted values relatively far away from the actual values.&lt;/li&gt;
&lt;li&gt;Input data whose values are more than roughly 3 standard deviations from the mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Outliers often cause problems in model training.&lt;/p&gt;
&lt;p&gt;与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;绝对值很高的&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;与实际值相差很大的预测值。&lt;/li&gt;
&lt;li&gt;值比平均值高大约 3 个标准偏差的输入数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;离群值常常会导致模型训练出现问题。&lt;/p&gt;
&lt;h3 id="output layer"&gt;output layer&lt;/h3&gt;
&lt;p&gt;The "final" layer of a neural network. The layer containing the answer(s).&lt;/p&gt;
&lt;p&gt;神经网络的&amp;ldquo;最后&amp;rdquo;一层，也是包含答案的层。&lt;/p&gt;
&lt;h3 id="overfitting"&gt;overfitting&lt;/h3&gt;
&lt;p&gt;Creating a model that matches the &lt;a href="#training set"&gt;&lt;strong&gt;training data&lt;/strong&gt;&lt;/a&gt; so closely that the model fails to make correct predictions on new data.&lt;/p&gt;
&lt;p&gt;创建的模型与&lt;a href="#training set"&gt;&lt;strong&gt;训练数据&lt;/strong&gt;&lt;/a&gt;过于匹配，以致于模型无法根据新数据做出正确的预测。&lt;/p&gt;
&lt;h2 id="p_1"&gt;P&lt;/h2&gt;
&lt;h3 id="pandas"&gt;pandas&lt;/h3&gt;
&lt;p&gt;A column-oriented data analysis API. Many ML frameworks, including TensorFlow, support pandas data structures as input. See &lt;a href="http://pandas.pydata.org/"&gt;pandas documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;面向列的数据分析 API。很多机器学习框架（包括 TensorFlow）都支持将 Pandas 数据结构作为输入。请参阅 &lt;a href="http://pandas.pydata.org/"&gt;Pandas 文档&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="parameter"&gt;parameter&lt;/h3&gt;
&lt;p&gt;A variable of a model that the ML system trains on its own. For example, &lt;a href="#weight"&gt;&lt;strong&gt;weights&lt;/strong&gt;&lt;/a&gt; are parameters whose values the ML system gradually learns through successive training iterations. Contrast with &lt;a href="#hyperparameter"&gt;&lt;strong&gt;hyperparameter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;机器学习系统自行训练的模型的变量。例如，&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;就是一种参数，它们的值是机器学习系统通过连续的训练迭代逐渐学习到的。与&lt;a href="#hyperparameter"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="parameter server (ps)"&gt;Parameter Server (PS)&lt;/h3&gt;
&lt;p&gt;A job that keeps track of a model's &lt;a href="#parameter"&gt;&lt;strong&gt;parameters&lt;/strong&gt;&lt;/a&gt; in a distributed setting.&lt;/p&gt;
&lt;p&gt;一种作业，负责在分布式设置中跟踪模型&lt;a href="#parameter"&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="parameter update"&gt;parameter update&lt;/h3&gt;
&lt;p&gt;The operation of adjusting a model's &lt;a href="#parameter"&gt;&lt;strong&gt;parameters&lt;/strong&gt;&lt;/a&gt; during training, typically within a single iteration of &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在训练期间（通常是在&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;的单次迭代中）调整模型&lt;a href="#parameter"&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/a&gt;的操作。&lt;/p&gt;
&lt;h3 id="partial derivative"&gt;partial derivative&lt;/h3&gt;
&lt;p&gt;A derivative in which all but one of the variables is considered a constant. For example, the partial derivative of &lt;em&gt;f(x, y)&lt;/em&gt; with respect to &lt;em&gt;x&lt;/em&gt; is the derivative of &lt;em&gt;f&lt;/em&gt; considered as a function of &lt;em&gt;x&lt;/em&gt; alone (that is, keeping &lt;em&gt;y&lt;/em&gt; constant). The partial derivative of &lt;em&gt;f&lt;/em&gt; with respect to &lt;em&gt;x&lt;/em&gt; focuses only on how &lt;em&gt;x&lt;/em&gt; is changing and ignores all other variables in the equation.&lt;/p&gt;
&lt;p&gt;一种导数，除一个变量之外的所有变量都被视为常量。例如，f(x, y) 对 x 的偏导数就是 f(x) 的导数（即，使 y 保持恒定）。f 对 x 的偏导数仅关注 x 如何变化，而忽略公式中的所有其他变量。&lt;/p&gt;
&lt;h3 id="partitioning strategy"&gt;partitioning strategy&lt;/h3&gt;
&lt;p&gt;The algorithm by which variables are divided across &lt;a href="#parameter server (ps)"&gt;&lt;strong&gt;parameter servers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#parameter server (ps)"&gt;&lt;strong&gt;参数服务器&lt;/strong&gt;&lt;/a&gt;中分割变量的算法。&lt;/p&gt;
&lt;h3 id="performance"&gt;performance&lt;/h3&gt;
&lt;p&gt;Overloaded term with the following meanings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The traditional meaning within software engineering. Namely: How fast (or efficiently) does this piece of software run?&lt;/li&gt;
&lt;li&gt;The meaning within ML. Here, performance answers the following question: How correct is this &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt;? That is, how good are the model's predictions?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;多含义术语，具有以下含义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在软件工程中的传统含义。即：相应软件的运行速度有多快（或有多高效）？&lt;/li&gt;
&lt;li&gt;在机器学习中的含义。在机器学习领域，性能旨在回答以下问题：相应&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;的准确度有多高？即模型在预测方面的表现有多好？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="perplexity"&gt;perplexity&lt;/h3&gt;
&lt;p&gt;One measure of how well a &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; is accomplishing its task. For example, suppose your task is to read the first few letters of a word a user is typing on a smartphone keyboard, and to offer a list of possible completion words. Perplexity, P, for this task is approximately the number of guesses you need to offer in order for your list to contain the actual word the user is trying to type.&lt;/p&gt;
&lt;p&gt;Perplexity is related to &lt;a href="#cross-entropy"&gt;&lt;strong&gt;cross-entropy&lt;/strong&gt;&lt;/a&gt; as follows:&lt;/p&gt;
&lt;p&gt;一种衡量指标，用于衡量&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;能够多好地完成任务。例如，假设任务是读取用户使用智能手机键盘输入字词时输入的前几个字母，然后列出一组可能的完整字词。此任务的困惑度 (P) 是：为了使列出的字词中包含用户尝试输入的实际字词，您需要提供的猜测项的个数。&lt;/p&gt;
&lt;p&gt;困惑度与&lt;a href="#cross-entropy"&gt;&lt;strong&gt;交叉熵&lt;/strong&gt;&lt;/a&gt;的关系如下：&lt;/p&gt;
&lt;div class="math"&gt;$$P= 2^{-\text{cross entropy}}$$&lt;/div&gt;
&lt;h3 id="pipeline"&gt;pipeline&lt;/h3&gt;
&lt;p&gt;The infrastructure surrounding a machine learning algorithm. A pipeline includes gathering the data, putting the data into training data files, training one or more models, and exporting the models to production.&lt;/p&gt;
&lt;p&gt;机器学习算法的基础架构。流水线包括收集数据、将数据放入训练数据文件、训练一个或多个模型，以及将模型导出到生产环境。&lt;/p&gt;
&lt;h3 id="positive class"&gt;positive class&lt;/h3&gt;
&lt;p&gt;In &lt;a href="#binary classification"&gt;&lt;strong&gt;binary classification&lt;/strong&gt;&lt;/a&gt;, the two possible classes are labeled as positive and negative. The positive outcome is the thing we're testing for. (Admittedly, we're simultaneously testing for both outcomes, but play along.) For example, the positive class in a medical test might be "tumor." The positive class in an email classifier might be "spam."&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#negative class"&gt;&lt;strong&gt;negative class&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#binary classification"&gt;&lt;strong&gt;二元分类&lt;/strong&gt;&lt;/a&gt;中，两种可能的类别分别被标记为正类别和负类别。正类别结果是我们要测试的对象。（不可否认的是，我们会同时测试这两种结果，但只关注正类别结果。）例如，在医学检查中，正类别可以是&amp;ldquo;肿瘤&amp;rdquo;。在电子邮件分类器中，正类别可以是&amp;ldquo;垃圾邮件&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="precision"&gt;precision&lt;/h3&gt;
&lt;p&gt;A metric for &lt;a href="#classification model"&gt;&lt;strong&gt;classification models&lt;/strong&gt;&lt;/a&gt;. Precision identifies the frequency with which a model was correct when predicting the &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt;. That is:&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;指标。精确率指模型正确预测&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;的频率, 即：&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Precision} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Positives}}$$&lt;/div&gt;
&lt;h3 id="prediction"&gt;prediction&lt;/h3&gt;
&lt;p&gt;A model's output when provided with an input &lt;a href="#example"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;模型在收到输入的&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;后的输出。&lt;/p&gt;
&lt;h3 id="prediction bias"&gt;prediction bias&lt;/h3&gt;
&lt;p&gt;A value indicating how far apart the average of &lt;a href="#prediction"&gt;&lt;strong&gt;predictions&lt;/strong&gt;&lt;/a&gt; is from the average of &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt; in the data set.&lt;/p&gt;
&lt;p&gt;一个值，用于表明&lt;a href="#prediction"&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/a&gt;平均值与数据集中&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的平均值相差有多大。&lt;/p&gt;
&lt;h3 id="pre-made estimator"&gt;pre-made Estimator&lt;/h3&gt;
&lt;p&gt;An &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt; that someone has already built. TensorFlow provides several pre-made Estimators, including &lt;code&gt;DNNClassifier&lt;/code&gt;, &lt;code&gt;DNNRegressor&lt;/code&gt;, and &lt;code&gt;LinearClassifier&lt;/code&gt;. You may build your own pre-made Estimators by following &lt;a href="https://www.tensorflow.org/extend/estimators"&gt;these instructions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;其他人已建好的 &lt;a href="#estimator"&gt;&lt;strong&gt;Estimator&lt;/strong&gt;&lt;/a&gt;。TensorFlow 提供了一些预创建的 Estimator，包括 &lt;code&gt;DNNClassifier&lt;/code&gt;、&lt;code&gt;DNNRegressor&lt;/code&gt; 和 &lt;code&gt;LinearClassifier&lt;/code&gt;。您可以按照&lt;a href="https://www.tensorflow.org/extend/estimators"&gt;这些说明&lt;/a&gt;构建自己预创建的 Estimator。&lt;/p&gt;
&lt;h3 id="pre-trained model"&gt;pre-trained model&lt;/h3&gt;
&lt;p&gt;Models or model components (such as &lt;a href="#embeddings"&gt;&lt;strong&gt;embeddings&lt;/strong&gt;&lt;/a&gt;) that have been already been trained. Sometimes, you'll feed pre-trained embeddings into a &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt;. Other times, your model will train the embeddings itself rather than rely on the pre-trained embeddings.&lt;/p&gt;
&lt;p&gt;已经过训练的模型或模型组件（例如&lt;a href="#embeddings"&gt;&lt;strong&gt;嵌套&lt;/strong&gt;&lt;/a&gt;）。有时，您需要将预训练的嵌套馈送到&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;。在其他时候，您的模型将自行训练嵌套，而不依赖于预训练的嵌套。&lt;/p&gt;
&lt;h3 id="prior belief"&gt;prior belief&lt;/h3&gt;
&lt;p&gt;What you believe about the data before you begin training on it. For example, &lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/a&gt; relies on a prior belief that &lt;a href="#weight"&gt;&lt;strong&gt;weights&lt;/strong&gt;&lt;/a&gt; should be small and normally distributed around zero.&lt;/p&gt;
&lt;p&gt;在开始采用相应数据进行训练之前，您对这些数据抱有的信念。例如，&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 正则化&lt;/strong&gt;&lt;/a&gt;依赖的先验信念是&lt;a href="#weight"&gt;&lt;strong&gt;权重&lt;/strong&gt;&lt;/a&gt;应该很小且应以 0 为中心呈正态分布。&lt;/p&gt;
&lt;h2 id="q_1"&gt;Q&lt;/h2&gt;
&lt;h3 id="queue"&gt;queue&lt;/h3&gt;
&lt;p&gt;A TensorFlow &lt;a href="#operation (op)"&gt;&lt;strong&gt;Operation&lt;/strong&gt;&lt;/a&gt; that implements a queue data structure. Typically used in I/O.&lt;/p&gt;
&lt;p&gt;一种 TensorFlow &lt;a href="#operation (op)"&gt;&lt;strong&gt;操作&lt;/strong&gt;&lt;/a&gt;，用于实现队列数据结构。通常用于 I/O 中。&lt;/p&gt;
&lt;h2 id="r_1"&gt;R&lt;/h2&gt;
&lt;h3 id="rank"&gt;rank&lt;/h3&gt;
&lt;p&gt;Overloaded term in ML that can mean either of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of dimensions in a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt;. For instance, a scalar has rank 0, a vector has rank 1, and a matrix has rank 2.&lt;/li&gt;
&lt;li&gt;The ordinal position of a class in an ML problem that categorizes classes from highest to lowest. For example, a behavior ranking system could rank a dog's rewards from highest (a steak) to lowest (wilted kale).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;机器学习中的一个多含义术语，可以理解为下列含义之一：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;中的维度数量。例如，标量等级为 0，向量等级为 1，矩阵等级为 2。&lt;/li&gt;
&lt;li&gt;在将类别从最高到最低进行排序的机器学习问题中，类别的顺序位置。例如，行为排序系统可以将狗狗的奖励从最高（牛排）到最低（枯萎的羽衣甘蓝）进行排序。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rater"&gt;rater&lt;/h3&gt;
&lt;p&gt;A human who provides &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt; in &lt;a href="#example"&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt;. Sometimes called an "annotator."&lt;/p&gt;
&lt;p&gt;为&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;提供&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的人。有时称为&amp;ldquo;注释者&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="recall"&gt;recall&lt;/h3&gt;
&lt;p&gt;A metric for &lt;a href="#classification model"&gt;&lt;strong&gt;classification models&lt;/strong&gt;&lt;/a&gt; that answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is:&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Recall} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Negatives}} $$&lt;/div&gt;
&lt;p&gt;一种&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;指标，用于回答以下问题：在所有可能的正类别标签中，模型正确地识别出了多少个？即：&lt;/p&gt;
&lt;p&gt;&lt;mj&gt;&lt;/mj&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\text{召回率} = \frac{\text{真正例数}} {\text{真正例数} + \text{假负例数}} $$&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 id="rectified linear unit (relu)"&gt;Rectified Linear Unit (ReLU)&lt;/h3&gt;
&lt;p&gt;An &lt;a href="#activation function"&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/a&gt; with the following rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If input is negative or zero, output is 0.&lt;/li&gt;
&lt;li&gt;If input is positive, output is equal to input.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一种&lt;a href="#activation function"&gt;&lt;strong&gt;激活函数&lt;/strong&gt;&lt;/a&gt;，其规则如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果输入为负数或 0，则输出 0。&lt;/li&gt;
&lt;li&gt;如果输入为正数，则输出等于输入。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="regression model"&gt;regression model&lt;/h3&gt;
&lt;p&gt;A type of model that outputs continuous (typically, floating-point) values. Compare with &lt;a href="#classification model"&gt;&lt;strong&gt;classification models&lt;/strong&gt;&lt;/a&gt;, which output discrete values, such as "day lily" or "tiger lily."&lt;/p&gt;
&lt;p&gt;一种模型，能够输出连续的值（通常为浮点值）。请与&lt;a href="#classification model"&gt;&lt;strong&gt;分类模型&lt;/strong&gt;&lt;/a&gt;进行比较，分类模型输出离散值，例如&amp;ldquo;黄花菜&amp;rdquo;或&amp;ldquo;虎皮百合&amp;rdquo;。&lt;/p&gt;
&lt;h3 id="regularization"&gt;regularization&lt;/h3&gt;
&lt;p&gt;The penalty on a model's complexity. Regularization helps prevent &lt;a href="#overfitting"&gt;&lt;strong&gt;overfitting&lt;/strong&gt;&lt;/a&gt;. Different kinds of regularization include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dropout regularization"&gt;&lt;strong&gt;dropout regularization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#early stopping"&gt;&lt;strong&gt;early stopping&lt;/strong&gt;&lt;/a&gt; (this is not a formal regularization method, but can effectively limit overfitting)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对模型复杂度的惩罚。正则化有助于防止出现&lt;a href="#overfitting"&gt;&lt;strong&gt;过拟合&lt;/strong&gt;&lt;/a&gt;，包含以下类型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#l1 regularization"&gt;&lt;strong&gt;L1 正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#l2 regularization"&gt;&lt;strong&gt;L2 正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dropout regularization"&gt;&lt;strong&gt;丢弃正则化&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#early stopping"&gt;&lt;strong&gt;早停法&lt;/strong&gt;&lt;/a&gt;（这不是正式的正则化方法，但可以有效限制过拟合）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="regularization rate"&gt;regularization rate&lt;/h3&gt;
&lt;p&gt;A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; equation shows the regularization rate's influence:&lt;/p&gt;
&lt;p&gt;一种标量值，以 lambda 表示，用于指定正则化函数的相对重要性。从下面简化的&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;公式中可以看出正则化率的影响：&lt;/p&gt;
&lt;div class="math"&gt;$$\text{minimize(loss function + }\lambda\text{(regularization function))}$$&lt;/div&gt;
&lt;p&gt;Raising the regularization rate reduces &lt;a href="#overfitting"&gt;&lt;strong&gt;overfitting&lt;/strong&gt;&lt;/a&gt; but may make the model less &lt;a href="#accuracy"&gt;&lt;strong&gt;accurate&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;提高正则化率可以减少&lt;a href="#overfitting"&gt;&lt;strong&gt;过拟合&lt;/strong&gt;&lt;/a&gt;，但可能会使模型的&lt;a href="#accuracy"&gt;&lt;strong&gt;准确率&lt;/strong&gt;&lt;/a&gt;降低。&lt;/p&gt;
&lt;h3 id="representation"&gt;representation&lt;/h3&gt;
&lt;p&gt;The process of mapping data to useful &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;将数据映射到实用&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;的过程。&lt;/p&gt;
&lt;h3 id="roc (receiver operating characteristic) curve"&gt;ROC (receiver operating characteristic) Curve&lt;/h3&gt;
&lt;p&gt;A curve of &lt;a href="#true positive rate (tp rate)"&gt;&lt;strong&gt;true positive rate&lt;/strong&gt;&lt;/a&gt; vs. &lt;a href="#false positive rate (fp rate)"&gt;&lt;strong&gt;false positive rate&lt;/strong&gt;&lt;/a&gt; at different &lt;a href="#classification threshold"&gt;&lt;strong&gt;classification thresholds&lt;/strong&gt;&lt;/a&gt;. See also &lt;a href="#auc (area under the roc curve)"&gt;&lt;strong&gt;AUC&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;不同&lt;a href="#classification threshold"&gt;&lt;strong&gt;分类阈值&lt;/strong&gt;&lt;/a&gt;下的&lt;a href="#true positive rate (tp rate)"&gt;&lt;strong&gt;真正例率&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#false positive rate (fp rate)"&gt;&lt;strong&gt;假正例率&lt;/strong&gt;&lt;/a&gt;构成的曲线。另请参阅&lt;a href="#auc (area under the roc curve)"&gt;&lt;strong&gt;曲线下面积&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="root directory"&gt;root directory&lt;/h3&gt;
&lt;p&gt;The directory you specify for hosting subdirectories of the TensorFlow checkpoint and events files of multiple models.&lt;/p&gt;
&lt;p&gt;您指定的目录，用于托管多个模型的 TensorFlow 检查点和事件文件的子目录。&lt;/p&gt;
&lt;h3 id="root mean squared error (rmse)"&gt;Root Mean Squared Error (RMSE)&lt;/h3&gt;
&lt;p&gt;The square root of the &lt;a href="#mean squared error (mse)"&gt;&lt;strong&gt;Mean Squared Error&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#mean squared error (mse)"&gt;&lt;strong&gt;均方误差&lt;/strong&gt;&lt;/a&gt;的平方根。&lt;/p&gt;
&lt;h2 id="s_1"&gt;S&lt;/h2&gt;
&lt;h3 id="savedmodel"&gt;SavedModel&lt;/h3&gt;
&lt;p&gt;The recommended format for saving and recovering TensorFlow models. SavedModel is a language-neutral, recoverable serialization format, which enables higher-level systems and tools to produce, consume, and transform TensorFlow models.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://www.tensorflow.org/programmers_guide/saved_model"&gt;Saving and Restoring&lt;/a&gt; in the TensorFlow Programmer's Guide for complete details.&lt;/p&gt;
&lt;p&gt;保存和恢复 TensorFlow 模型时建议使用的格式。SavedModel 是一种独立于语言且可恢复的序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型。&lt;/p&gt;
&lt;p&gt;如需完整的详细信息，请参阅《TensorFlow 编程人员指南》中的&lt;a href="https://www.tensorflow.org/programmers_guide/saved_model"&gt;保存和恢复&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="saver"&gt;Saver&lt;/h3&gt;
&lt;p&gt;A &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver"&gt;TensorFlow object&lt;/a&gt; responsible for saving model checkpoints.&lt;/p&gt;
&lt;p&gt;一种 &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver"&gt;TensorFlow 对象&lt;/a&gt;，负责保存模型检查点。&lt;/p&gt;
&lt;h3 id="scaling"&gt;scaling&lt;/h3&gt;
&lt;p&gt;A commonly used practice in &lt;a href="#feature engineering"&gt;&lt;strong&gt;feature engineering&lt;/strong&gt;&lt;/a&gt; to tame a feature's range of values to match the range of other features in the data set. For example, suppose that you want all floating-point features in the data set to have a range of 0 to 1. Given a particular feature's range of 0 to 500, you could scale that feature by dividing each value by 500.&lt;/p&gt;
&lt;p&gt;See also &lt;a href="#normalization"&gt;&lt;strong&gt;normalization&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="#feature engineering"&gt;&lt;strong&gt;特征工程&lt;/strong&gt;&lt;/a&gt;中的一种常用做法，是对某个特征的值区间进行调整，使之与数据集中其他特征的值区间一致。例如，假设您希望数据集中所有浮点特征的值都位于 0 到 1 区间内，如果某个特征的值位于 0 到 500 区间内，您就可以通过将每个值除以 500 来缩放该特征。&lt;/p&gt;
&lt;p&gt;另请参阅&lt;a href="#normalization"&gt;&lt;strong&gt;标准化&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="scikit-learn"&gt;scikit-learn&lt;/h3&gt;
&lt;p&gt;A popular open-source ML platform. See &lt;a href="http://www.scikit-learn.org/"&gt;www.scikit-learn.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一个热门的开放源代码机器学习平台。请访问 &lt;a href="http://www.scikit-learn.org/"&gt;www.scikit-learn.org&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="semi-supervised learning"&gt;semi-supervised learning&lt;/h3&gt;
&lt;p&gt;Training a model on data where some of the training examples have labels but others don&amp;rsquo;t. One technique for semi-supervised learning is to infer labels for the unlabeled examples, and then to train on the inferred labels to create a new model. Semi-supervised learning can be useful if labels are expensive to obtain but unlabeled examples are plentiful.&lt;/p&gt;
&lt;p&gt;训练模型时采用的数据中，某些训练样本有标签，而其他样本则没有标签。半监督式学习采用的一种技术是推断无标签样本的标签，然后使用推断出的标签进行训练，以创建新模型。如果获得有标签样本需要高昂的成本，而无标签样本则有很多，那么半监督式学习将非常有用。&lt;/p&gt;
&lt;h3 id="sequence model"&gt;sequence model&lt;/h3&gt;
&lt;p&gt;A model whose inputs have a sequential dependence. For example, predicting the next video watched from a sequence of previously watched videos.&lt;/p&gt;
&lt;p&gt;一种模型，其输入具有序列依赖性。例如，根据之前观看过的一系列视频对观看的下一个视频进行预测。&lt;/p&gt;
&lt;h3 id="session"&gt;session&lt;/h3&gt;
&lt;p&gt;Maintains state (for example, variables) within a TensorFlow program.&lt;/p&gt;
&lt;p&gt;维持 TensorFlow 程序中的状态（例如变量）。&lt;/p&gt;
&lt;h3 id="sigmoid function"&gt;sigmoid function&lt;/h3&gt;
&lt;p&gt;A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1. The sigmoid function has the following formula:&lt;/p&gt;
&lt;p&gt;一种函数，可将逻辑回归输出或多项回归输出（对数几率）映射到概率，以返回介于 0 到 1 之间的值。S 型函数的公式如下：&lt;/p&gt;
&lt;div class="math"&gt;$$y = \frac{1}{1 + e^{-\sigma}}$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; in &lt;a href="#logistic regression"&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt; problems is simply:&lt;/p&gt;
&lt;p&gt;在&lt;a href="#logistic regression"&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;&lt;/a&gt;问题中，&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; 非常简单：&lt;/p&gt;
&lt;div class="math"&gt;$$\\sigma = b + w_1x_1 + w_2x_2 + &amp;hellip; w_nx_n$$&lt;/div&gt;
&lt;p&gt;In other words, the sigmoid function converts &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; into a probability between 0 and 1.&lt;/p&gt;
&lt;p&gt;换句话说，S 型函数可将 &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; 转换为介于 0 到 1 之间的概率。&lt;/p&gt;
&lt;p&gt;In some &lt;a href="#neural network"&gt;&lt;strong&gt;neural networks&lt;/strong&gt;&lt;/a&gt;, the sigmoid function acts as the &lt;a href="#activation function"&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在某些&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;中，S 型函数可作为&lt;a href="#activation function"&gt;&lt;strong&gt;激活函数&lt;/strong&gt;&lt;/a&gt;使用。&lt;/p&gt;
&lt;h3 id="softmax"&gt;softmax&lt;/h3&gt;
&lt;p&gt;A function that provides probabilities for each possible class in a &lt;a href="#multi-class classification"&gt;&lt;strong&gt;multi-class classification model&lt;/strong&gt;&lt;/a&gt;. The probabilities add up to exactly 1.0. For example, softmax might determine that the probability of a particular image being a dog at 0.9, a cat at 0.08, and a horse at 0.02. (Also called &lt;strong&gt;full softmax&lt;/strong&gt;.)&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#candidate sampling"&gt;&lt;strong&gt;candidate sampling&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种函数，可提供&lt;a href="#multi-class classification"&gt;&lt;strong&gt;多类别分类模型&lt;/strong&gt;&lt;/a&gt;中每个可能类别的概率。这些概率的总和正好为 1.0。例如，softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为&lt;strong&gt;完整 softmax&lt;/strong&gt;。）&lt;/p&gt;
&lt;p&gt;与&lt;a href="#candidate sampling"&gt;&lt;strong&gt;候选采样&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="sparse feature"&gt;sparse feature&lt;/h3&gt;
&lt;p&gt;&lt;a href="#feature"&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/a&gt; vector whose values are predominately zero or empty. For example, a vector containing a single 1 value and a million 0 values is sparse. As another example, words in a search query could also be a sparse feature&amp;mdash;there are many possible words in a given language, but only a few of them occur in a given query.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#dense feature"&gt;&lt;strong&gt;dense feature&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;向量，其中的大多数值都为 0 或为空。例如，某个向量包含一个为 1 的值和一百万个为 0 的值，则该向量就属于稀疏向量。再举一个例子，搜索查询中的单词也可能属于稀疏特征 - 在某种指定语言中有很多可能的单词，但在某个指定的查询中仅包含其中几个。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#dense feature"&gt;&lt;strong&gt;密集特征&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="squared hinge loss"&gt;squared hinge loss&lt;/h3&gt;
&lt;p&gt;The square of the &lt;a href="#hinge loss"&gt;&lt;strong&gt;hinge loss&lt;/strong&gt;&lt;/a&gt;. Squared hinge loss penalizes outliers more harshly than regular hinge loss.&lt;/p&gt;
&lt;p&gt;&lt;a href="#hinge loss"&gt;&lt;strong&gt;合页损失函数&lt;/strong&gt;&lt;/a&gt;的平方。与常规合页损失函数相比，平方合页损失函数对离群值的惩罚更严厉。&lt;/p&gt;
&lt;h3 id="squared loss"&gt;squared loss&lt;/h3&gt;
&lt;p&gt;The &lt;a href="#loss"&gt;&lt;strong&gt;loss&lt;/strong&gt;&lt;/a&gt; function used in &lt;a href="#linear regression"&gt;&lt;strong&gt;linear regression&lt;/strong&gt;&lt;/a&gt;. (Also known as &lt;strong&gt;L2 Loss&lt;/strong&gt;.) This function calculates the squares of the difference between a model's predicted value for a labeled &lt;a href="#example"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt; and the actual value of the &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. Due to squaring, this loss function amplifies the influence of bad predictions. That is, squared loss reacts more strongly to outliers than &lt;a href="#l1 loss"&gt;&lt;strong&gt;L1 loss&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在&lt;a href="#linear regression"&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/a&gt;中使用的&lt;a href="#loss"&gt;&lt;strong&gt;损失&lt;/strong&gt;&lt;/a&gt;函数（也称为 &lt;strong&gt;L2 损失函数&lt;/strong&gt;）。该函数可计算模型为有标签&lt;a href="#example"&gt;&lt;strong&gt;样本&lt;/strong&gt;&lt;/a&gt;预测的值和&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的实际值之差的平方。由于取平方值，因此该损失函数会放大不佳预测的影响。也就是说，与 &lt;a href="#l1 loss"&gt;&lt;strong&gt;L1 损失函数&lt;/strong&gt;&lt;/a&gt;相比，平方损失函数对离群值的反应更强烈。&lt;/p&gt;
&lt;h3 id="static model"&gt;static model&lt;/h3&gt;
&lt;p&gt;A model that is trained offline.&lt;/p&gt;
&lt;p&gt;离线训练的一种模型。&lt;/p&gt;
&lt;h3 id="stationarity"&gt;stationarity&lt;/h3&gt;
&lt;p&gt;A property of data in a data set, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December.&lt;/p&gt;
&lt;p&gt;数据集中数据的一种属性，表示数据分布在一个或多个维度保持不变。这种维度最常见的是时间，即表明平稳性的数据不随时间而变化。例如，从 9 月到 12 月，表明平稳性的数据没有发生变化。&lt;/p&gt;
&lt;h3 id="step"&gt;step&lt;/h3&gt;
&lt;p&gt;A forward and backward evaluation of one &lt;a href="#batch"&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;对一个&lt;a href="#batch"&gt;&lt;strong&gt;批次&lt;/strong&gt;&lt;/a&gt;的向前和向后评估。&lt;/p&gt;
&lt;h3 id="step size"&gt;step size&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#learning rate"&gt;&lt;strong&gt;learning rate&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#learning rate"&gt;&lt;strong&gt;学习速率&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="stochastic gradient descent (sgd)"&gt;stochastic gradient descent (SGD)&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#gradient descent"&gt;&lt;strong&gt;gradient descent&lt;/strong&gt;&lt;/a&gt; algorithm in which the batch size is one. In other words, SGD relies on a single example chosen uniformly at random from a data set to calculate an estimate of the gradient at each step.&lt;/p&gt;
&lt;p&gt;批次规模为 1 的一种&lt;a href="#gradient descent"&gt;&lt;strong&gt;梯度下降法&lt;/strong&gt;&lt;/a&gt;。换句话说，SGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。&lt;/p&gt;
&lt;h3 id="structural risk minimization (srm)"&gt;structural risk minimization (SRM)&lt;/h3&gt;
&lt;p&gt;An algorithm that balances two goals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The desire to build the most predictive model (for example, lowest loss).&lt;/li&gt;
&lt;li&gt;The desire to keep the model as simple as possible (for example, strong regularization).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, a model function that minimizes loss+regularization on the training set is a structural risk minimization algorithm.&lt;/p&gt;
&lt;p&gt;For more information, see &lt;a href="http://www.svms.org/srm/"&gt;http://www.svms.org/srm/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#empirical risk minimization (erm)"&gt;&lt;strong&gt;empirical risk minimization&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种算法，用于平衡以下两个目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;期望构建最具预测性的模型（例如损失最低）。&lt;/li&gt;
&lt;li&gt;期望使模型尽可能简单（例如强大的正则化）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如，旨在将基于训练集的损失和正则化降至最低的模型函数就是一种结构风险最小化算法。&lt;/p&gt;
&lt;p&gt;如需更多信息，请参阅 &lt;a href="http://www.svms.org/srm/"&gt;http://www.svms.org/srm/&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#empirical risk minimization (erm)"&gt;&lt;strong&gt;经验风险最小化&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="summary"&gt;summary&lt;/h3&gt;
&lt;p&gt;In TensorFlow, a value or set of values calculated at a particular &lt;a href="#step"&gt;&lt;strong&gt;step&lt;/strong&gt;&lt;/a&gt;, usually used for tracking model metrics during training.&lt;/p&gt;
&lt;p&gt;在 TensorFlow 中的某一&lt;a href="#step"&gt;&lt;strong&gt;步&lt;/strong&gt;&lt;/a&gt;计算出的一个值或一组值，通常用于在训练期间跟踪模型指标。&lt;/p&gt;
&lt;h3 id="supervised machine learning"&gt;supervised machine learning&lt;/h3&gt;
&lt;p&gt;Training a &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; from input data and its corresponding &lt;a href="#label"&gt;&lt;strong&gt;labels&lt;/strong&gt;&lt;/a&gt;. Supervised machine learning is analogous to a student learning a subject by studying a set of questions and their corresponding answers. After mastering the mapping between questions and answers, the student can then provide answers to new (never-before-seen) questions on the same topic. Compare with &lt;a href="#unsupervised machine learning"&gt;&lt;strong&gt;unsupervised machine learning&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;根据输入数据及其对应的&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;来训练&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;。监督式机器学习类似于学生通过研究一系列问题及其对应的答案来学习某个主题。在掌握了问题和答案之间的对应关系后，学生便可以回答关于同一主题的新问题（以前从未见过的问题）。请与&lt;a href="#unsupervised machine learning"&gt;&lt;strong&gt;非监督式机器学习&lt;/strong&gt;&lt;/a&gt;进行比较。&lt;/p&gt;
&lt;h3 id="synthetic feature"&gt;synthetic feature&lt;/h3&gt;
&lt;p&gt;A &lt;a href="#feature"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/a&gt; that is not present among the input features, but is derived from one or more of them. Kinds of synthetic features include the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiplying one feature by itself or by other feature(s). (These are termed &lt;a href="#feature cross"&gt;&lt;strong&gt;feature crosses&lt;/strong&gt;&lt;/a&gt;.)&lt;/li&gt;
&lt;li&gt;Dividing one feature by a second feature.&lt;/li&gt;
&lt;li&gt;&lt;a href="#bucketing"&gt;&lt;strong&gt;Bucketing&lt;/strong&gt;&lt;/a&gt; a continuous feature into range bins.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Features created by &lt;a href="#normalization"&gt;&lt;strong&gt;normalizing&lt;/strong&gt;&lt;/a&gt; or &lt;a href="#scaling"&gt;&lt;strong&gt;scaling&lt;/strong&gt;&lt;/a&gt; alone are not considered synthetic features.&lt;/p&gt;
&lt;p&gt;一种&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;，不在输入特征之列，而是从一个或多个输入特征衍生而来。合成特征包括以下类型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将一个特征与其本身或其他特征相乘（称为&lt;a href="#feature cross"&gt;&lt;strong&gt;特征组合&lt;/strong&gt;&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;两个特征相除。&lt;/li&gt;
&lt;li&gt;对连续特征进行&lt;a href="#bucketing"&gt;&lt;strong&gt;分桶&lt;/strong&gt;&lt;/a&gt;，以分为多个区间分箱。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过&lt;a href="#normalization"&gt;&lt;strong&gt;标准化&lt;/strong&gt;&lt;/a&gt;或&lt;a href="#scaling"&gt;&lt;strong&gt;缩放&lt;/strong&gt;&lt;/a&gt;单独创建的特征不属于合成特征。&lt;/p&gt;
&lt;h2 id="t_1"&gt;T&lt;/h2&gt;
&lt;h3 id="target"&gt;target&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;是&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的同义词。&lt;/p&gt;
&lt;h3 id="temporal data"&gt;temporal data&lt;/h3&gt;
&lt;p&gt;Data recorded at different points in time. For example, winter coat sales recorded for each day of the year would be temporal data.&lt;/p&gt;
&lt;p&gt;在不同时间点记录的数据。例如，记录的一年中每一天的冬外套销量就属于时态数据。&lt;/p&gt;
&lt;h3 id="tensor"&gt;Tensor&lt;/h3&gt;
&lt;p&gt;The primary data structure in TensorFlow programs. Tensors are N-dimensional (where N could be very large) data structures, most commonly scalars, vectors, or matrices. The elements of a Tensor can hold integer, floating-point, or string values.&lt;/p&gt;
&lt;p&gt;TensorFlow 程序中的主要数据结构。张量是 N 维（其中 N 可能非常大）数据结构，最常见的是标量、向量或矩阵。张量的元素可以包含整数值、浮点值或字符串值。&lt;/p&gt;
&lt;h3 id="tensor processing unit (tpu)"&gt;Tensor Processing Unit (TPU)&lt;/h3&gt;
&lt;p&gt;An ASIC (application-specific integrated circuit) that optimizes the performance of TensorFlow programs.&lt;/p&gt;
&lt;p&gt;一种 ASIC（应用专用集成电路），用于优化 TensorFlow 程序的性能。&lt;/p&gt;
&lt;h3 id="tensor rank"&gt;Tensor rank&lt;/h3&gt;
&lt;p&gt;See &lt;a href="#rank"&gt;&lt;strong&gt;rank&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="tensor shape"&gt;Tensor shape&lt;/h3&gt;
&lt;p&gt;The number of elements a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt; contains in various dimensions. For example, a [5, 10] Tensor has a shape of 5 in one dimension and 10 in another.&lt;/p&gt;
&lt;p&gt;&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;在各种维度中包含的元素数。例如，张量 [5, 10] 在一个维度中的形状为 5，在另一个维度中的形状为 10。&lt;/p&gt;
&lt;h3 id="tensor size"&gt;Tensor size&lt;/h3&gt;
&lt;p&gt;The total number of scalars a &lt;a href="#tensor"&gt;&lt;strong&gt;Tensor&lt;/strong&gt;&lt;/a&gt; contains. For example, a [5, 10] Tensor has a size of 50.&lt;/p&gt;
&lt;p&gt;&lt;a href="#tensor"&gt;&lt;strong&gt;张量&lt;/strong&gt;&lt;/a&gt;包含的标量总数。例如，张量 [5, 10] 的大小为 50。&lt;/p&gt;
&lt;h3 id="tensorboard"&gt;TensorBoard&lt;/h3&gt;
&lt;p&gt;The dashboard that displays the summaries saved during the execution of one or more TensorFlow programs.&lt;/p&gt;
&lt;p&gt;一个信息中心，用于显示在执行一个或多个 TensorFlow 程序期间保存的摘要信息。&lt;/p&gt;
&lt;h3 id="tensorflow"&gt;TensorFlow&lt;/h3&gt;
&lt;p&gt;A large-scale, distributed, machine learning platform. The term also refers to the base API layer in the TensorFlow stack, which supports general computation on dataflow graphs.&lt;/p&gt;
&lt;p&gt;Although TensorFlow is primarily used for machine learning, you may also use TensorFlow for non-ML tasks that require numerical computation using dataflow graphs.&lt;/p&gt;
&lt;p&gt;一个大型的分布式机器学习平台。该术语还指 TensorFlow 堆栈中的基本 API 层，该层支持对数据流图进行一般计算。&lt;/p&gt;
&lt;p&gt;虽然 TensorFlow 主要应用于机器学习领域，但也可用于需要使用数据流图进行数值计算的非机器学习任务。&lt;/p&gt;
&lt;h3 id="tensorflow playground"&gt;TensorFlow Playground&lt;/h3&gt;
&lt;p&gt;A program that visualizes how different &lt;a href="#hyperparameters"&gt;&lt;strong&gt;hyperparameters&lt;/strong&gt;&lt;/a&gt; influence model (primarily neural network) training. Go to &lt;a href="http://playground.tensorflow.org"&gt;http://playground.tensorflow.org&lt;/a&gt; to experiment with TensorFlow Playground.&lt;/p&gt;
&lt;p&gt;一款用于直观呈现不同的&lt;a href="#hyperparameters"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;对模型（主要是神经网络）训练的影响的程序。要试用 TensorFlow Playground，请前往 &lt;a href="http://playground.tensorflow.org"&gt;http://playground.tensorflow.org&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="tensorflow serving"&gt;TensorFlow Serving&lt;/h3&gt;
&lt;p&gt;A platform to deploy trained models in production.&lt;/p&gt;
&lt;p&gt;一个平台，用于将训练过的模型部署到生产环境。&lt;/p&gt;
&lt;h3 id="test set"&gt;test set&lt;/h3&gt;
&lt;p&gt;The subset of the data set that you use to test your &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; after the model has gone through initial vetting by the validation set.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#training set"&gt;&lt;strong&gt;training set&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#validation set"&gt;&lt;strong&gt;validation set&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;数据集的子集，用于在&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;经由验证集的初步验证之后测试模型。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#training set"&gt;&lt;strong&gt;训练集&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#validation set"&gt;&lt;strong&gt;验证集&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="tf.example"&gt;tf.Example&lt;/h3&gt;
&lt;p&gt;A standard &lt;a href="https://developers.google.com/protocol-buffers/"&gt;protocol buffer&lt;/a&gt; for describing input data for machine learning model training or inference.&lt;/p&gt;
&lt;p&gt;一种标准的 &lt;a href="https://developers.google.com/protocol-buffers/"&gt;proto buffer&lt;/a&gt;，旨在描述用于机器学习模型训练或推断的输入数据。&lt;/p&gt;
&lt;h3 id="time series analysis"&gt;time series analysis&lt;/h3&gt;
&lt;p&gt;A subfield of machine learning and statistics that analyzes &lt;a href="#temporal data"&gt;&lt;strong&gt;temporal data&lt;/strong&gt;&lt;/a&gt;. Many types of machine learning problems require time series analysis, including classification, clustering, forecasting, and anomaly detection. For example, you could use time series analysis to forecast the future sales of winter coats by month based on historical sales data.&lt;/p&gt;
&lt;p&gt;机器学习和统计学的一个子领域，旨在分析&lt;a href="#temporal data"&gt;&lt;strong&gt;时态数据&lt;/strong&gt;&lt;/a&gt;。很多类型的机器学习问题都需要时间序列分析，其中包括分类、聚类、预测和异常检测。例如，您可以利用时间序列分析根据历史销量数据预测未来每月的冬外套销量。&lt;/p&gt;
&lt;h3 id="training"&gt;training&lt;/h3&gt;
&lt;p&gt;The process of determining the ideal &lt;a href="#parameter"&gt;&lt;strong&gt;parameters&lt;/strong&gt;&lt;/a&gt; comprising a model.&lt;/p&gt;
&lt;p&gt;确定构成模型的理想&lt;a href="#parameter"&gt;&lt;strong&gt;参数&lt;/strong&gt;&lt;/a&gt;的过程。&lt;/p&gt;
&lt;h3 id="training set"&gt;training set&lt;/h3&gt;
&lt;p&gt;The subset of the data set used to train a model.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#validation set"&gt;&lt;strong&gt;validation set&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#test set"&gt;&lt;strong&gt;test set&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;数据集的子集，用于训练模型。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#validation set"&gt;&lt;strong&gt;验证集&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#test set"&gt;&lt;strong&gt;测试集&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h3 id="transfer learning"&gt;transfer learning&lt;/h3&gt;
&lt;p&gt;Transferring information from one machine learning task to another. For example, in multi-task learning, a single model solves multiple tasks, such as a &lt;a href="#deep model"&gt;&lt;strong&gt;deep model&lt;/strong&gt;&lt;/a&gt; that has different output nodes for different tasks. Transfer learning might involve transferring knowledge from the solution of a simpler task to a more complex one, or involve transferring knowledge from a task where there is more data to one where there is less data.&lt;/p&gt;
&lt;p&gt;Most machine learning systems solve a &lt;em&gt;single&lt;/em&gt; task. Transfer learning is a baby step towards artificial intelligence in which a single program can solve &lt;em&gt;multiple&lt;/em&gt; tasks.&lt;/p&gt;
&lt;p&gt;将信息从一个机器学习任务转移到另一个机器学习任务。例如，在多任务学习中，一个模型可以完成多项任务，例如针对不同任务具有不同输出节点的&lt;a href="#deep model"&gt;&lt;strong&gt;深度模型&lt;/strong&gt;&lt;/a&gt;。转移学习可能涉及将知识从较简单任务的解决方案转移到较复杂的任务，或者将知识从数据较多的任务转移到数据较少的任务。&lt;/p&gt;
&lt;p&gt;大多数机器学习系统都只能完成一项任务。转移学习是迈向人工智能的一小步；在人工智能中，单个程序可以完成多项任务。&lt;/p&gt;
&lt;h3 id="true negative (tn)"&gt;true negative (TN)&lt;/h3&gt;
&lt;p&gt;An example in which the model &lt;em&gt;correctly&lt;/em&gt; predicted the &lt;a href="#negative class"&gt;&lt;strong&gt;negative class&lt;/strong&gt;&lt;/a&gt;. For example, the model inferred that a particular email message was not spam, and that email message really was not spam.&lt;/p&gt;
&lt;p&gt;被模型正确地预测为&lt;a href="#negative class"&gt;&lt;strong&gt;负类别&lt;/strong&gt;&lt;/a&gt;的样本。例如，模型推断出某封电子邮件不是垃圾邮件，而该电子邮件确实不是垃圾邮件。&lt;/p&gt;
&lt;h3 id="true positive (tp)"&gt;true positive (TP)&lt;/h3&gt;
&lt;p&gt;An example in which the model &lt;em&gt;correctly&lt;/em&gt; predicted the &lt;a href="#positive class"&gt;&lt;strong&gt;positive class&lt;/strong&gt;&lt;/a&gt;. For example, the model inferred that a particular email message was spam, and that email message really was spam.&lt;/p&gt;
&lt;p&gt;被模型正确地预测为&lt;a href="#positive class"&gt;&lt;strong&gt;正类别&lt;/strong&gt;&lt;/a&gt;的样本。例如，模型推断出某封电子邮件是垃圾邮件，而该电子邮件确实是垃圾邮件。&lt;/p&gt;
&lt;h3 id="true positive rate (tp rate)"&gt;true positive rate (TP rate)&lt;/h3&gt;
&lt;p&gt;Synonym for &lt;a href="#recall"&gt;&lt;strong&gt;recall&lt;/strong&gt;&lt;/a&gt;. That is:&lt;/p&gt;
&lt;p&gt;是&lt;a href="#recall"&gt;&lt;strong&gt;召回率&lt;/strong&gt;&lt;/a&gt;的同义词，即：&lt;/p&gt;
&lt;div class="math"&gt;$$\text{True Positive Rate} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Negatives}}$$&lt;/div&gt;
&lt;p&gt;True positive rate is the y-axis in an &lt;a href="#roc (receiver operating characteristic) curve"&gt;&lt;strong&gt;ROC curve&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;真正例率是 &lt;a href="#roc (receiver operating characteristic) curve"&gt;&lt;strong&gt;ROC 曲线&lt;/strong&gt;&lt;/a&gt;的 y 轴。&lt;/p&gt;
&lt;h2 id="u_1"&gt;U&lt;/h2&gt;
&lt;h3 id="unlabeled example"&gt;unlabeled example&lt;/h3&gt;
&lt;p&gt;An example that contains &lt;a href="#feature"&gt;&lt;strong&gt;features&lt;/strong&gt;&lt;/a&gt; but no &lt;a href="#label"&gt;&lt;strong&gt;label&lt;/strong&gt;&lt;/a&gt;. Unlabeled examples are the input to &lt;a href="#inference"&gt;&lt;strong&gt;inference&lt;/strong&gt;&lt;/a&gt;. In &lt;a href="#semi-supervised learning"&gt;&lt;strong&gt;semi-supervised&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#unsupervised machine learning"&gt;&lt;strong&gt;unsupervised&lt;/strong&gt;&lt;/a&gt; learning, unlabeled examples are used during training.&lt;/p&gt;
&lt;p&gt;包含&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;但没有&lt;a href="#label"&gt;&lt;strong&gt;标签&lt;/strong&gt;&lt;/a&gt;的样本。无标签样本是用于进行&lt;a href="#inference"&gt;&lt;strong&gt;推断&lt;/strong&gt;&lt;/a&gt;的输入内容。在&lt;a href="#semi-supervised learning"&gt;&lt;strong&gt;半监督式&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#unsupervised machine learning"&gt;&lt;strong&gt;非监督式&lt;/strong&gt;&lt;/a&gt;学习中，无标签样本在训练期间被使用。&lt;/p&gt;
&lt;h3 id="unsupervised machine learning"&gt;unsupervised machine learning&lt;/h3&gt;
&lt;p&gt;Training a &lt;a href="#model"&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/a&gt; to find patterns in a data set, typically an unlabeled data set.&lt;/p&gt;
&lt;p&gt;The most common use of unsupervised machine learning is to cluster data into groups of similar examples. For example, an unsupervised machine learning algorithm can cluster songs together based on various properties of the music. The resulting clusters can become an input to other machine learning algorithms (for example, to a music recommendation service). Clustering can be helpful in domains where true labels are hard to obtain. For example, in domains such as anti-abuse and fraud, clusters can help humans better understand the data.&lt;/p&gt;
&lt;p&gt;Another example of unsupervised machine learning is &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;&lt;strong&gt;principal component analysis (PCA)&lt;/strong&gt;&lt;/a&gt;. For example, applying PCA on a data set containing the contents of millions of shopping carts might reveal that shopping carts containing lemons frequently also contain antacids.&lt;/p&gt;
&lt;p&gt;Compare with &lt;a href="#supervised machine learning"&gt;&lt;strong&gt;supervised machine learning&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;训练&lt;a href="#model"&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/a&gt;，以找出数据集（通常是无标签数据集）中的模式。&lt;/p&gt;
&lt;p&gt;非监督式机器学习最常见的用途是将数据分为不同的聚类，使相似的样本位于同一组中。例如，非监督式机器学习算法可以根据音乐的各种属性将歌曲分为不同的聚类。所得聚类可以作为其他机器学习算法（例如音乐推荐服务）的输入。在很难获取真标签的领域，聚类可能会非常有用。例如，在反滥用和反欺诈等领域，聚类有助于人们更好地了解相关数据。&lt;/p&gt;
&lt;p&gt;非监督式机器学习的另一个例子是&lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;&lt;strong&gt;主成分分析 (PCA)&lt;/strong&gt;&lt;/a&gt;。例如，通过对包含数百万购物车中物品的数据集进行主成分分析，可能会发现有柠檬的购物车中往往也有抗酸药。&lt;/p&gt;
&lt;p&gt;请与&lt;a href="#supervised machine learning"&gt;&lt;strong&gt;监督式机器学习&lt;/strong&gt;&lt;/a&gt;进行比较。&lt;/p&gt;
&lt;h2 id="v_1"&gt;V&lt;/h2&gt;
&lt;h3 id="validation set"&gt;validation set&lt;/h3&gt;
&lt;p&gt;A subset of the data set&amp;mdash;disjunct from the training set&amp;mdash;that you use to adjust &lt;a href="#hyperparameter"&gt;&lt;strong&gt;hyperparameters&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#training set"&gt;&lt;strong&gt;training set&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#test set"&gt;&lt;strong&gt;test set&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;数据集的一个子集，从训练集分离而来，用于调整&lt;a href="#hyperparameter"&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#training set"&gt;&lt;strong&gt;训练集&lt;/strong&gt;&lt;/a&gt;和&lt;a href="#test set"&gt;&lt;strong&gt;测试集&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;h2 id="w_1"&gt;W&lt;/h2&gt;
&lt;h3 id="weight"&gt;weight&lt;/h3&gt;
&lt;p&gt;A coefficient for a &lt;a href="#feature"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/a&gt; in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0, then its corresponding feature does not contribute to the model.&lt;/p&gt;
&lt;p&gt;线性模型中&lt;a href="#feature"&gt;&lt;strong&gt;特征&lt;/strong&gt;&lt;/a&gt;的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。&lt;/p&gt;
&lt;h3 id="wide model"&gt;wide model&lt;/h3&gt;
&lt;p&gt;A linear model that typically has many &lt;a href="#sparse feature"&gt;&lt;strong&gt;sparse input features&lt;/strong&gt;&lt;/a&gt;. We refer to it as "wide" since such a model is a special type of &lt;a href="#neural network"&gt;&lt;strong&gt;neural network&lt;/strong&gt;&lt;/a&gt; with a large number of inputs that connect directly to the output node. Wide models are often easier to debug and inspect than deep models. Although wide models cannot express nonlinearities through &lt;a href="#hidden layer"&gt;&lt;strong&gt;hidden layers&lt;/strong&gt;&lt;/a&gt;, they can use transformations such as &lt;a href="#feature cross"&gt;&lt;strong&gt;feature crossing&lt;/strong&gt;&lt;/a&gt; and &lt;a href="#bucketing"&gt;&lt;strong&gt;bucketization&lt;/strong&gt;&lt;/a&gt; to model nonlinearities in different ways.&lt;/p&gt;
&lt;p&gt;Contrast with &lt;a href="#deep model"&gt;&lt;strong&gt;deep model&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;一种线性模型，通常有很多&lt;a href="#sparse feature"&gt;&lt;strong&gt;稀疏输入特征&lt;/strong&gt;&lt;/a&gt;。我们之所以称之为&amp;ldquo;宽度模型&amp;rdquo;，是因为这是一种特殊类型的&lt;a href="#neural network"&gt;&lt;strong&gt;神经网络&lt;/strong&gt;&lt;/a&gt;，其大量输入均直接与输出节点相连。与深度模型相比，宽度模型通常更易于调试和检查。虽然宽度模型无法通过&lt;a href="#hidden-layer"&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;&lt;/a&gt;来表示非线性关系，但可以利用&lt;a href="#feature cross"&gt;&lt;strong&gt;特征组合&lt;/strong&gt;&lt;/a&gt;、&lt;a href="#bucketing"&gt;&lt;strong&gt;分桶&lt;/strong&gt;&lt;/a&gt;等转换以不同的方式为非线性关系建模。&lt;/p&gt;
&lt;p&gt;与&lt;a href="#deep model"&gt;&lt;strong&gt;深度模型&lt;/strong&gt;&lt;/a&gt;相对。&lt;/p&gt;
&lt;p&gt;上次更新日期：二月 27, 2018&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Markdown 数学公式指导手册</title><link href="https://freeopen.github.io/posts/markdown-shu-xue-gong-shi-zhi-dao-shou-ce" rel="alternate"></link><published>2017-05-11T00:00:00+08:00</published><updated>2017-05-11T00:00:00+08:00</updated><author><name>潘嘉豪(编译), freeopen(修订)</name></author><id>tag:freeopen.github.io,2017-05-11:/posts/markdown-shu-xue-gong-shi-zhi-dao-shou-ce</id><summary type="html">&lt;p&gt;本手册的主要内容来自 &lt;a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference"&gt;mathjax-basic-tutorial-and-quick-reference&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: 鼠标右键单击公式表达式, 选择 "Show Math As &amp;gt; TeX Commands", 即可查看公式的LaTex代码。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="yi , gong shi shi yong can kao"&gt;一、公式使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he cha ru gong shi"&gt;1．如何插入公式&lt;/h3&gt;
&lt;p&gt;LaTex 的数学公式有两种：行中公式和独立公式。行中公式放在文中与其它文字混编，独立公式单独成行。&lt;/p&gt;
&lt;p&gt;行中公式示例: &lt;code&gt;$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$&lt;/code&gt; &lt;span class="math"&gt;\(\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;独立公式示例：&lt;code&gt;$$\sum_{i …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;本手册的主要内容来自 &lt;a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference"&gt;mathjax-basic-tutorial-and-quick-reference&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: 鼠标右键单击公式表达式, 选择 "Show Math As &amp;gt; TeX Commands", 即可查看公式的LaTex代码。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="yi , gong shi shi yong can kao"&gt;一、公式使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he cha ru gong shi"&gt;1．如何插入公式&lt;/h3&gt;
&lt;p&gt;LaTex 的数学公式有两种：行中公式和独立公式。行中公式放在文中与其它文字混编，独立公式单独成行。&lt;/p&gt;
&lt;p&gt;行中公式示例: &lt;code&gt;$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$&lt;/code&gt; &lt;span class="math"&gt;\(\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;独立公式示例：&lt;code&gt;$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$&lt;/code&gt; &lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: 以下代码示例，为排版方便，将省略公式插入符号 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;自动编号的公式可以用如下方法表示：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若需要手动编号，参见 &lt;a href="#大括号和行标的使用"&gt;大括号和行标的使用&lt;/a&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{equation}
数学公式
\label{eq:当前公式名}
\end{equation}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;自动编号后的公式可在全文任意处使用&lt;code&gt;\eqref{eq:公式名}&lt;/code&gt; 语句引用。&lt;/p&gt;
&lt;h3 id="2.ru he shu ru shang xia biao"&gt;2．如何输入上下标&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;^&lt;/code&gt; 表示上标,&lt;code&gt;_&lt;/code&gt; 表示下标。如果上下标的内容多于一个字符，需要用 &lt;code&gt;{}&lt;/code&gt; 将这些内容括成一个整体。上下标可以嵌套，也可以同时使用。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x^{y^z}=(1+{\rm e}^x)^{-2xy^w} 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ x^{y^z}=(1+{\rm e}^x)^{-2xy^w} $$&lt;/div&gt;
&lt;p&gt;另外，如果要在左右两边都有上下标，可以用 &lt;code&gt;\sideset&lt;/code&gt; 命令。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\sideset{^1_2}{^3_4}\bigotimes 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \sideset{^1_2}{^3_4}\bigotimes $$&lt;/div&gt;
&lt;h3 id="3.ru he shu ru gua hao he fen ge fu"&gt;3．如何输入括号和分隔符&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;()&lt;/code&gt;、&lt;code&gt;[]&lt;/code&gt; 和 &lt;code&gt;|&lt;/code&gt; 表示符号本身，使用 &lt;code&gt;\{\}&lt;/code&gt; 来表示 &lt;code&gt;{}&lt;/code&gt; 。当要显示大号的括号或分隔符时，要用 &lt;code&gt;\left&lt;/code&gt; 和 &lt;code&gt;\right&lt;/code&gt; 命令。&lt;/p&gt;
&lt;p&gt;一些特殊的括号：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示}  \\ 
\hline
\text{\langle} &amp;amp; \langle &amp;amp; \text{\rangle} &amp;amp; \rangle \\  
\text{\lceil } &amp;amp; \lceil  &amp;amp; \text{\rceil} &amp;amp;  \rceil  \\  
\text{\lfloor} &amp;amp; \lfloor &amp;amp; \text{\rfloor} &amp;amp; \rfloor \\
\text{\lbrace} &amp;amp; \lbrace &amp;amp; \text{\rbrace} &amp;amp; \rbrace \\
\hline 
\end{array}
$$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right) 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right) $$&lt;/div&gt;
&lt;p&gt;有时候要用 &lt;code&gt;\left.&lt;/code&gt; 或 &lt;code&gt;\right.&lt;/code&gt; 进行匹配而不显示本身。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\left. \frac{{\rm d}u}{{\rm d}x} \right| _{x=0} 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \left. \frac{{\rm d}u}{{\rm d}x} \right| _{x=0} $$&lt;/div&gt;
&lt;h3 id="4.ru he shu ru fen shu"&gt;4．如何输入分数&lt;/h3&gt;
&lt;p&gt;通常使用 &lt;code&gt;\frac {分子} {分母}&lt;/code&gt; 命令产生一个分数，分数可嵌套。 
便捷情况可直接输入 \frac ab 来快速生成一个  。 
如果分式很复杂，亦可使用 &lt;code&gt;分子 \over 分母&lt;/code&gt; 命令，此时分数仅有一层。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\frac{a-1}{b-1} \quad and \quad {a+1\over b+1}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\frac{a-1}{b-1} \quad and \quad {a+1\over b+1}$$&lt;/div&gt;
&lt;h3 id="5.ru he shu ru kai fang"&gt;5．如何输入开方&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\sqrt [根指数，省略时为2] {被开方数}&lt;/code&gt; 命令输入开方。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\sqrt{2} \quad and \quad \sqrt[n]{3}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\sqrt{2} \quad and \quad \sqrt[n]{3}$$&lt;/div&gt;
&lt;h3 id="6.ru he shu ru sheng lue hao"&gt;6．如何输入省略号&lt;/h3&gt;
&lt;p&gt;数学公式中常见的省略号有两种，&lt;code&gt;\ldots&lt;/code&gt; 表示与文本底线对齐的省略号，&lt;code&gt;\cdots&lt;/code&gt; 表示与文本中线对齐的省略号。&lt;/p&gt;
&lt;p&gt;```f(x_1,x_2,\underbrace{\ldots}&lt;em _rm="\rm" cdots=""&gt;{\rm ldots} ,x_n) = x_1^2 + x_2^2 + \underbrace{\cdots}&lt;/em&gt; + x_n^2&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$$f(x_1,x_2,\underbrace{\ldots}_{\rm ldots} ,x_n) = x_1^2 + x_2^2 + \underbrace{\cdots}_{\rm cdots} + x_n^2$$

### 7．如何输入矢量

使用 `\vec{矢量}` 来自动产生一个矢量。也可以使用 `\overrightarrow` 等命令自定义字母上方的符号。


```\vec{a} \cdot \vec{b}=0
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\vec{a} \cdot \vec{b}=0$$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\overleftarrow{xy} \quad and \quad \overleftrightarrow{xy} \quad and \quad \overrightarrow{xy}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\overleftarrow{xy} \quad and \quad \overleftrightarrow{xy} \quad and \quad \overrightarrow{xy}$$&lt;/div&gt;
&lt;h3 id="8.ru he shu ru ji fen"&gt;8．如何输入积分&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\int_积分下限^积分上限 {被积表达式}&lt;/code&gt; 来输入一个积分。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\int_0^1 {x^2} \,{\rm d}x
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\int_0^1 {x^2} \,{\rm d}x$$&lt;/div&gt;
&lt;p&gt;本例中 &lt;code&gt;\,&lt;/code&gt; 和 &lt;code&gt;{\rm d}&lt;/code&gt; 部分可省略，但建议加入，能使式子更美观。&lt;/p&gt;
&lt;h3 id="9.ru he shu ru ji xian yun suan"&gt;9．如何输入极限运算&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\lim_{变量 \to 表达式}&lt;/code&gt; 表达式 来输入一个极限。如有需求，可以更改 &lt;code&gt;\to&lt;/code&gt; 符号至任意符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\lim_{n \to +\infty} \frac{1}{n(n+1)} \quad and \quad \lim_{x\leftarrow{示例}} \frac{1}{n(n+1)} 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \lim_{n \to +\infty} \frac{1}{n(n+1)} \quad and \quad \lim_{x\leftarrow{示例}} \frac{1}{n(n+1)} $$&lt;/div&gt;
&lt;h3 id="10.ru he shu ru lei jia , lei cheng yun suan"&gt;10．如何输入累加、累乘运算&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\sum_{下标表达式}^{上标表达式} {累加表达式}&lt;/code&gt; 来输入一个累加。 
与之类似，使用 &lt;code&gt;\prod \bigcup \bigcap&lt;/code&gt; 来分别输入累乘、并集和交集。 
此类符号在行内显示时上下标表达式将会移至右上角和右下角。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sum_{i=1}^n \frac{1}{i^2} \quad and \quad \prod_{i=1}^n \frac{1}{i^2} \quad and \quad \bigcup_{i=1}^{2} R
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^n \frac{1}{i^2} \quad and \quad \prod_{i=1}^n \frac{1}{i^2} \quad and \quad \bigcup_{i=1}^{2} R$$&lt;/div&gt;
&lt;h3 id="11.ru he shu ru xi la zi mu"&gt;11．如何输入希腊字母&lt;/h3&gt;
&lt;p&gt;输入 &lt;code&gt;\小写希腊字母英文全称&lt;/code&gt; 和 &lt;code&gt;\首字母大写希腊字母英文全称&lt;/code&gt; 来分别输入小写和大写希腊字母。 
对于大写希腊字母与现有字母相同的，直接输入大写字母即可。&lt;/p&gt;
&lt;div class="math"&gt;\begin{array}{cccc|cccc|cccc}
\hline
\text{大写} &amp;amp; \text{小写} &amp;amp; \text{名称} &amp;amp; \text{音标} &amp;amp; \text{大写} &amp;amp; \text{小写} &amp;amp; \text{名称} &amp;amp; \text{音标} \\ 
\hline
A &amp;amp; \alpha &amp;amp; alpha &amp;amp; \text{/ˈ&amp;aelig;lfə/} &amp;amp; N   &amp;amp; \nu &amp;amp; nu &amp;amp; \text{/nju:/}  \\
B &amp;amp; \beta  &amp;amp; beta  &amp;amp; \text{/ˈbeɪtə/} &amp;amp; \Xi &amp;amp; \xi &amp;amp; xi &amp;amp; \text{/ˈzaɪ/ or /ˈksaɪ/}  \\
\Gamma &amp;amp; \gamma &amp;amp; gamma &amp;amp; \text{/ˈg&amp;aelig;mə/} &amp;amp; O   &amp;amp; o &amp;amp; omicron &amp;amp; \text{/əuˈmaikrən/ or /ˈɑmɪˌkrɑn/} \\
\Delta &amp;amp; \delta &amp;amp; delta &amp;amp; \text{/ˈdeltə/} &amp;amp; \Pi &amp;amp; \pi &amp;amp; pi &amp;amp; \text{/paɪ/}  \\
E &amp;amp; \epsilon &amp;amp; epsilon &amp;amp; \text{/ˈepsɪlɒn/} &amp;amp; P      &amp;amp; \rho  &amp;amp; rho &amp;amp; \text{/rəʊ/}  \\
Z &amp;amp; \zeta    &amp;amp; zeta &amp;amp; \text{/ˈzi:tə/}  &amp;amp; \Sigma &amp;amp; \sigma &amp;amp; sigma &amp;amp; \text{/ˈsɪɡmə/}   \\
H &amp;amp; \eta     &amp;amp; eta  &amp;amp; \text{/ˈi:tə/}   &amp;amp; T &amp;amp; \tau &amp;amp; tau &amp;amp; \text{/tɔ:/ or /taʊ/}   \\
\Theta &amp;amp; \theta &amp;amp; theta &amp;amp; \text{/ˈ&amp;theta;i:tə/} &amp;amp; \Upsilon &amp;amp; \upsilon &amp;amp; upsilon  &amp;amp; \text{/ˈipsilon/ or /ˈʌpsɨlɒn/} \\
I &amp;amp; \iota  &amp;amp; iota &amp;amp; \text{/aɪˈəʊtə/} &amp;amp; \Phi &amp;amp; \phi &amp;amp; phi &amp;amp; \text{/faɪ/} \\
K &amp;amp; \kappa &amp;amp; kappa &amp;amp; \text{/ˈk&amp;aelig;pə/} &amp;amp; X    &amp;amp; \chi &amp;amp; chi &amp;amp; \text{/kaɪ/} \\
\Lambda &amp;amp; \lambda &amp;amp; lambda &amp;amp; \text{/ˈl&amp;aelig;mdə/} &amp;amp; \Psi &amp;amp; \psi &amp;amp; psi &amp;amp; \text{/psaɪ/} \\
M  &amp;amp; \mu &amp;amp; mu &amp;amp; \text{/mju:/} &amp;amp; \Omega &amp;amp; \omega &amp;amp; omega &amp;amp; \text{/ˈəʊmɪɡə/ or /oʊˈmeɡə/} \\
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;输入法对照表&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{cc|cc|cc}
\hline
\text{大写输入} &amp;amp; \text{小写输入}  &amp;amp; \text{大写输入} &amp;amp; \text{小写输入} &amp;amp;  \text{大写输入} &amp;amp; \text{小写输入}  \\ 
\hline
\text{A} &amp;amp; \text{\alpha} &amp;amp;  \text{I} &amp;amp; \text{\iota}  &amp;amp;  \text{P}      &amp;amp; \text{\rho}  \\
\text{B} &amp;amp; \text{\beta}  &amp;amp; \text{K} &amp;amp; \text{\kappa} &amp;amp; \text{\Sigma} &amp;amp; \text{\sigma} \\
\text{\Gamma} &amp;amp; \text{\gamma}  &amp;amp; \text{\Lambda} &amp;amp; \text{\lambda} &amp;amp; \text{T} &amp;amp; \text{\tau}  \\
\text{\Delta} &amp;amp; \text{\delta}  &amp;amp; \text{M}  &amp;amp; \text{\mu}  &amp;amp; \text{\Upsilon} &amp;amp; \text{\upsilon}  \\
\text{E} &amp;amp; \text{\epsilon} &amp;amp; \text{N}   &amp;amp; \text{\nu} &amp;amp;  \text{\Phi} &amp;amp; \text{\phi}  \\
\text{Z} &amp;amp; \text{\zeta}    &amp;amp; \text{\Xi} &amp;amp; \text{\xi} &amp;amp; \text{X}    &amp;amp; \text{\chi}  \\
\text{H} &amp;amp; \text{\eta}     &amp;amp; \text{O}   &amp;amp; \text{o} &amp;amp; \text{\Psi} &amp;amp; \text{\psi}  \\
\text{\Theta} &amp;amp; \text{\theta}  &amp;amp; \text{\Pi} &amp;amp; \text{\pi} &amp;amp; \text{\Omega} &amp;amp; \text{\omega}  \\
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;部分字母有变量专用形式，以 &lt;code&gt;\var&lt;/code&gt; 开头。 &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{c|c|c|l}
\hline
\text{小写} &amp;amp; \text{大写}  &amp;amp; \text{变量形式} &amp;amp; \text{输入}  \\ 
\hline
\epsilon &amp;amp;  E &amp;amp; \varepsilon &amp;amp; \text{\varepsilon} \\ 
\theta &amp;amp;    \Theta &amp;amp;    \vartheta &amp;amp;  \text{\vartheta} \\    
\rho &amp;amp;  P &amp;amp; \varrho &amp;amp;  \text{\varrho} \\    
\sigma &amp;amp;    \Sigma &amp;amp;    \varsigma &amp;amp; \text{\varsigma} \\     
\phi &amp;amp;  \Phi &amp;amp;  \varphi &amp;amp;  \text{\varphi} \\
\hline
\end{array}&lt;/div&gt;
&lt;h3 id="12.ru he shu ru qi ta te shu zi fu"&gt;12．如何输入其它特殊字符&lt;/h3&gt;
&lt;p&gt;若需要显示更大或更小的字符，在符号前插入 &lt;code&gt;\large&lt;/code&gt; 或 &lt;code&gt;\small&lt;/code&gt; 命令。&lt;/p&gt;
&lt;p&gt;若找不到需要的符号，使用 &lt;a href="http://detexify.kirelabs.org/classify.html"&gt;Detexify&lt;/a&gt; 来画出想要的符号。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1)．关系运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{\pm} &amp;amp; \pm &amp;amp; \text{\times} &amp;amp; \times &amp;amp; \text{\div} &amp;amp; \div &amp;amp; \text{\mid} &amp;amp; \mid \\  
\text{\nmid} &amp;amp; \nmid &amp;amp; \text{\cdot} &amp;amp; \cdot &amp;amp; \text{\circ} &amp;amp; \circ &amp;amp; \text{\ast} &amp;amp; \ast \\  
\text{\bigodot} &amp;amp; \bigodot &amp;amp; \text{\bigotimes} &amp;amp; \bigotimes &amp;amp; \text{\bigoplus} &amp;amp; \bigoplus &amp;amp; \text{\leq} &amp;amp; \leq \\  
\text{\geq} &amp;amp; \geq &amp;amp; \text{\neq} &amp;amp; \neq &amp;amp; \text{\approx} &amp;amp; \approx &amp;amp; \text{\equiv} &amp;amp; \equiv \\  
\text{\sum} &amp;amp; \sum &amp;amp; \text{\prod} &amp;amp; \prod &amp;amp; \text{\coprod} &amp;amp; \coprod &amp;amp; \text{\backslash} &amp;amp; \backslash \\    
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(2)．集合运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{\emptyset} &amp;amp; \emptyset &amp;amp; \text{\in} &amp;amp; \in &amp;amp; \text{\notin} &amp;amp; \notin \\ 
\text{\subset} &amp;amp; \subset &amp;amp; \text{\supset} &amp;amp; \supset &amp;amp; \text{\subseteq} &amp;amp; \subseteq \\   
\text{\supseteq} &amp;amp; \supseteq &amp;amp; \text{\bigcap} &amp;amp; \bigcap &amp;amp; \text{\bigcup} &amp;amp; \bigcup \\   
\text{\bigvee} &amp;amp; \bigvee &amp;amp; \text{\bigwedge} &amp;amp; \bigwedge &amp;amp; \text{\biguplus} &amp;amp; \biguplus \\   
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(3)．对数运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{\log} &amp;amp; \log &amp;amp; \text{\lg} &amp;amp; \lg &amp;amp; \text{\ln} &amp;amp; \ln \\ 
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(4)．三角运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{30^\circ} &amp;amp; 30^\circ    &amp;amp; \text{\bot} &amp;amp; \bot &amp;amp; \text{\angle A} &amp;amp; \angle A \\  
\text{\sin} &amp;amp; \sin    &amp;amp; \text{\cos} &amp;amp; \cos &amp;amp; \text{\tan} &amp;amp; \tan \\  
\text{\csc} &amp;amp; \csc  &amp;amp; \text{\sec} &amp;amp; \sec  &amp;amp; \text{\cot} &amp;amp; \cot \\   
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(5)．微积分运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示} \\ 
\hline
\text{\int} &amp;amp; \int    &amp;amp; \text{\iint} &amp;amp; \iint &amp;amp; \text{\iiint} &amp;amp; \iiint \\    
\text{\iiiint} &amp;amp; \iiiint  &amp;amp; \text{\oint} &amp;amp; \oint &amp;amp; \text{\prime} &amp;amp; \prime \\    
\text{\lim} &amp;amp; \lim  &amp;amp; \text{\infty} &amp;amp; \infty  &amp;amp; \text{\nabla} &amp;amp; \nabla \\   
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(6)．逻辑运算符&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示} &amp;amp; \text{输入} &amp;amp; \text{显示}  \\ 
\hline
\text{\because} &amp;amp; \because &amp;amp; \text{\therefore} &amp;amp; \therefore \\  
\text{\forall} &amp;amp; \forall &amp;amp; \text{\exists} &amp;amp; \exists \\  
\text{\not\subset} &amp;amp; \not\subset &amp;amp; \text{\not&amp;lt;} &amp;amp; \not&amp;lt;  \\ 
\text{\not&amp;gt;} &amp;amp; \not&amp;gt; &amp;amp;  \text{\not=} &amp;amp; \not=  \\    
\hline
\end{array}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(7)．戴帽符号&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示}  \\ 
\hline
\text{\hat{xy}} &amp;amp; \hat{xy}    &amp;amp; \text{\widehat{xyz}} &amp;amp; \widehat{xyz} \\ 
\text{\tilde{xy}} &amp;amp; \tilde{xy}    &amp;amp; \text{\widetilde{xyz}} &amp;amp; \widetilde{xyz} \\ 
\text{\check{x}} &amp;amp; \check{x}    &amp;amp; \text{\breve{y}} &amp;amp; \breve{y}  \\  
\text{\grave{x}} &amp;amp; \grave{x} &amp;amp;  \text{\acute{y}} &amp;amp; \acute{y}   \\   
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(8)．连线符号&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c}
\hline
\text{输入} &amp;amp; \text{显示}    \\ 
\hline
\text{\fbox{a+b+c+d}} &amp;amp; \fbox{a+b+c+d}  \\     
\hline
\text{\overleftarrow{a+b+c+d}} &amp;amp; \overleftarrow{a+b+c+d} \\ 
\hline
\text{\overrightarrow{a+b+c+d}} &amp;amp; \overrightarrow{a+b+c+d}  \\  
\hline
\text{\overleftrightarrow{a+b+c+d}} &amp;amp; \overleftrightarrow{a+b+c+d} \\   
\hline
\text{\underleftarrow{a+b+c+d}} &amp;amp; \underleftarrow{a+b+c+d}  \\ 
\hline
\text{\underrightarrow{a+b+c+d}} &amp;amp; \underrightarrow{a+b+c+d}  \\    
\hline
\text{\underleftrightarrow{a+b+c+d}} &amp;amp; \underleftrightarrow{a+b+c+d} \\ 
\hline
\text{\overline{a+b+c+d}} &amp;amp; \overline{a+b+c+d}   \\ 
\hline
\text{\underline{a+b+c+d}} &amp;amp; \underline{a+b+c+d}    \\ 
\hline
\text{\overbrace{a+b+c+d}^{Sample}} &amp;amp; \overbrace{a+b+c+d}^{Sample}  \\  
\hline
\text{\underbrace{a+b+c+d}_{Sample}} &amp;amp; \underbrace{a+b+c+d}_{Sample} \\  
\hline
\text{\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}} &amp;amp; \overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}  \\  
\hline
\text{\underbrace{a\cdot a\cdots a}_{b\text{ times}}} &amp;amp; \underbrace{a\cdot a\cdots a}_{b\text{ times}}  \\ 
\hline
\end{array}&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(9)．箭头符号&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{显示}  \\ 
\hline
\text{\uparrow} &amp;amp; \uparrow    &amp;amp; \text{\Uparrow} &amp;amp; \Uparrow \\   
\text{\downarrow} &amp;amp; \downarrow    &amp;amp; \text{\Downarrow} &amp;amp; \Downarrow \\   
\text{\leftarrow} &amp;amp; \leftarrow  &amp;amp; \text{\Leftarrow} &amp;amp; \Leftarrow  \\    
\text{\rightarrow or \to} &amp;amp; \rightarrow &amp;amp;   \text{\Rightarrow} &amp;amp; \Rightarrow   \\   
\text{\leftrightarrow} &amp;amp; \leftrightarrow    &amp;amp; \text{\Leftrightarrow} &amp;amp; \Leftrightarrow  \\  
\text{\longleftarrow} &amp;amp; \longleftarrow  &amp;amp; \text{\Longleftarrow or \impliedby} &amp;amp; \Longleftarrow  \\  
\text{\longrightarrow} &amp;amp; \longrightarrow    &amp;amp; \text{\Longrightarrow or \implies} &amp;amp; \Longrightarrow  \\  
\text{\longleftrightarrow} &amp;amp; \longleftrightarrow    &amp;amp; \text{\Longleftrightarrow or \iff} &amp;amp; \Longleftrightarrow  \\  
\text{\mapsto} &amp;amp; \mapsto  &amp;amp; \text{ } &amp;amp; \text{ } \\  
\hline
\end{array}&lt;/div&gt;
&lt;h3 id="13.ru he jin xing zi ti zhuan huan"&gt;13．如何进行字体转换&lt;/h3&gt;
&lt;p&gt;若要对公式的某一部分字符进行字体转换，可以用 &lt;code&gt;{\字体 {需转换的部分字符}}&lt;/code&gt; 命令，其中 &lt;code&gt;\字体&lt;/code&gt; 部分可以参照下表选择合适的字体。一般情况下，公式默认为意大利体  。&lt;/p&gt;
&lt;p&gt;示例中 &lt;strong&gt;全部大写&lt;/strong&gt; 的字体仅大写可用。&lt;/p&gt;
&lt;div class="math"&gt;\begin{array}{l|c|l|c}
\hline
\text{输入} &amp;amp; \text{说明} &amp;amp; \text{显示}  &amp;amp; \text{输入} &amp;amp; \text{说明} &amp;amp; \text{显示}  \\ 
\hline
\text{\rm} &amp;amp; \text{罗马体} &amp;amp; \rm{Sample} &amp;amp; \text{\cal}  &amp;amp; \text{花体} &amp;amp; \cal{SAMPLE} \\  
\text{\it} &amp;amp; \text{意大利体} &amp;amp; \it{Sample} &amp;amp; \text{\Bbb}  &amp;amp; \text{黑板粗体} &amp;amp; \Bbb{SAMPLE} \\   
\text{\bf} &amp;amp; \text{粗体} &amp;amp; \rm{Sample} &amp;amp; \text{\mit}  &amp;amp; \text{数字斜体} &amp;amp; \mit{SAMPLE} \\ 
\text{\sf} &amp;amp; \text{等线体} &amp;amp; \sf{Sample} &amp;amp; \text{\scr}  &amp;amp; \text{手写体} &amp;amp; \scr{SAMPLE} \\ 
\text{\tt} &amp;amp; \text{打字机体} &amp;amp; \tt{Sample} &amp;amp; \text{ }  &amp;amp; \text{ } &amp;amp;  \\ 
\text{\frak} &amp;amp; \text{旧德式体} &amp;amp; \frak{Sample} &amp;amp; \text{ }  &amp;amp; \text{ } &amp;amp;  \\ 
\hline
\end{array}&lt;/div&gt;
&lt;p&gt;转换字体十分常用，例如在积分中：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c | c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\int_0^1 x^2 dx &amp;amp; \int_0^1 x^2 \,{\rm d}x
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c | c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\int_0^1 x^2 dx &amp;amp; \int_0^1 x^2 \,{\rm d}x
\end{array}
$$&lt;/div&gt;
&lt;p&gt;注意比较两个式子间 &lt;code&gt;dx&lt;/code&gt; 的不同。 
使用 &lt;code&gt;\operatorname&lt;/code&gt; 命令也可以达到相同的效果，详见 [定义新的符号 &lt;code&gt;\operatorname&lt;/code&gt;] 。&lt;/p&gt;
&lt;h3 id="14.da gua hao he xing biao de shi yong"&gt;14．大括号和行标的使用&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\left&lt;/code&gt; 和 &lt;code&gt;\right&lt;/code&gt; 来创建自动匹配高度的 (圆括号)，[方括号] 和 {花括号} 。 
在每个公式末尾前使用 &lt;code&gt;\tag{行标}&lt;/code&gt; 来实现行标。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f\left(
   \left[ 
     \frac{
       1+\left\{x,y\right\}
     }{
       \left(
          \frac{x}{y}+\frac{y}{x}
       \right)
       \left(u+1\right)
     }+a
   \right]^{3/2}
\right)
\tag{行标}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
f\left(
   \left[ 
     \frac{
       1+\left\{x,y\right\}
     }{
       \left(
          \frac{x}{y}+\frac{y}{x}
       \right)
       \left(u+1\right)
     }+a
   \right]^{3/2}
\right)
\tag{行标}
$$&lt;/div&gt;
&lt;p&gt;如果你需要在不同的行显示对应括号，可以在每一行对应处使用 &lt;code&gt;\left.&lt;/code&gt; 或 &lt;code&gt;\right.&lt;/code&gt; 来放一个"影子"括号：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{aligned}
a=&amp;amp;\left(1+2+3+  \cdots \right. \\
&amp;amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)
\end{aligned}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
a=&amp;amp;\left(1+2+3+  \cdots \right. \\
&amp;amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;如果你需要将行内显示的分隔符也变大，可以使用 &lt;code&gt;\middle&lt;/code&gt; 命令：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\left\langle  
  q
\middle\|
  \frac{\frac{x}{y}}{\frac{u}{v}}
\middle| 
   p 
\right\rangle
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\left\langle  
  q
\middle\|
  \frac{\frac{x}{y}}{\frac{u}{v}}
\middle| 
   p 
\right\rangle
$$&lt;/div&gt;
&lt;h3 id="15.qi ta ming ling"&gt;15．其它命令&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;(1)．定义新的符号 &lt;code&gt;\operatorname&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;查询 &lt;a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference/15077#15077"&gt;关于此命令的定义&lt;/a&gt; 和 &lt;a href="http://meta.math.stackexchange.com/search?q=operatorname"&gt;关于此命令的讨论&lt;/a&gt; 来进一步了解此命令。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\operatorname{Symbol} A 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \operatorname{Symbol} A $$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(2)．添加注释文字 &lt;code&gt;\text&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 &lt;code&gt;\text {文字}&lt;/code&gt; 中仍可以使用 &lt;code&gt;$公式$&lt;/code&gt; 插入其它公式。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(n)= \begin{cases} n/2, &amp;amp; \text {if $n$ is even} \\ 3n+1, &amp;amp; \text{if $n$ is odd} \end{cases} 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ f(n)= \begin{cases} n/2, &amp;amp; \text {if $n$ is even} \\ 3n+1, &amp;amp; \text{if $n$ is odd} \end{cases} $$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(3)．在字符间加入空格&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有四种宽度的空格可以使用： &lt;code&gt;\,&lt;/code&gt;、&lt;code&gt;\;&lt;/code&gt;、&lt;code&gt;\quad&lt;/code&gt; 和 &lt;code&gt;\qquad&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;a \, b \mid a \; b \mid a \quad b \mid a \qquad b 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ a \, b \mid a \; b \mid a \quad b \mid a \qquad b $$&lt;/div&gt;
&lt;p&gt;当然，使用 &lt;code&gt;\text {n个空格}&lt;/code&gt; 也可以达到同样效果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(4)．更改文字颜色&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用 &lt;code&gt;\color{颜色}{文字}&lt;/code&gt; 来更改特定的文字颜色。 
更改文字颜色 &lt;em&gt;需要浏览器支持&lt;/em&gt; ，如果浏览器不知道你所需的颜色，那么文字将被渲染为黑色。&lt;/p&gt;
&lt;p&gt;对于较旧的浏览器（HTML4与CSS2），支持的颜色较少;
对于较新的浏览器（HTML5与CSS3），额外的124种颜色将被支持
输入 &lt;code&gt;\color {#rgb} {text}&lt;/code&gt; 来自定义更多的颜色，其中 &lt;code&gt;#rgb&lt;/code&gt; 的 &lt;code&gt;r g b&lt;/code&gt; 可输入 &lt;code&gt;0-9&lt;/code&gt; 和 &lt;code&gt;a-f&lt;/code&gt; 来表示红色、绿色和蓝色的纯度（饱和度）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{|rrrrrrrr|}
\hline
\verb+#000+ &amp;amp; \color{#000}{text} &amp;amp; \verb+#005+ &amp;amp; \color{#005}{text} &amp;amp; \verb+#00A+ &amp;amp; \color{#00A}{text} &amp;amp; \verb+#00F+ &amp;amp; \color{#00F}{text}  \\
\verb+#500+ &amp;amp; \color{#500}{text} &amp;amp; \verb+#505+ &amp;amp; \color{#505}{text} &amp;amp; \verb+#50A+ &amp;amp; \color{#50A}{text} &amp;amp; \verb+#50F+ &amp;amp; \color{#50F}{text}  \\
\verb+#A00+ &amp;amp; \color{#A00}{text} &amp;amp; \verb+#A05+ &amp;amp; \color{#A05}{text} &amp;amp; \verb+#A0A+ &amp;amp; \color{#A0A}{text} &amp;amp; \verb+#A0F+ &amp;amp; \color{#A0F}{text}  \\
\verb+#F00+ &amp;amp; \color{#F00}{text} &amp;amp; \verb+#F05+ &amp;amp; \color{#F05}{text} &amp;amp; \verb+#F0A+ &amp;amp; \color{#F0A}{text} &amp;amp; \verb+#F0F+ &amp;amp; \color{#F0F}{text}  \\
\hline
\verb+#080+ &amp;amp; \color{#080}{text} &amp;amp; \verb+#085+ &amp;amp; \color{#085}{text} &amp;amp; \verb+#08A+ &amp;amp; \color{#08A}{text} &amp;amp; \verb+#08F+ &amp;amp; \color{#08F}{text}  \\
\verb+#580+ &amp;amp; \color{#580}{text} &amp;amp; \verb+#585+ &amp;amp; \color{#585}{text} &amp;amp; \verb+#58A+ &amp;amp; \color{#58A}{text} &amp;amp; \verb+#58F+ &amp;amp; \color{#58F}{text}  \\
\verb+#A80+ &amp;amp; \color{#A80}{text} &amp;amp; \verb+#A85+ &amp;amp; \color{#A85}{text} &amp;amp; \verb+#A8A+ &amp;amp; \color{#A8A}{text} &amp;amp; \verb+#A8F+ &amp;amp; \color{#A8F}{text}  \\
\verb+#F80+ &amp;amp; \color{#F80}{text} &amp;amp; \verb+#F85+ &amp;amp; \color{#F85}{text} &amp;amp; \verb+#F8A+ &amp;amp; \color{#F8A}{text} &amp;amp; \verb+#F8F+ &amp;amp; \color{#F8F}{text}  \\
\hline
\verb+#0F0+ &amp;amp; \color{#0F0}{text} &amp;amp; \verb+#0F5+ &amp;amp; \color{#0F5}{text} &amp;amp; \verb+#0FA+ &amp;amp; \color{#0FA}{text} &amp;amp; \verb+#0FF+ &amp;amp; \color{#0FF}{text}  \\
\verb+#5F0+ &amp;amp; \color{#5F0}{text} &amp;amp; \verb+#5F5+ &amp;amp; \color{#5F5}{text} &amp;amp; \verb+#5FA+ &amp;amp; \color{#5FA}{text} &amp;amp; \verb+#5FF+ &amp;amp; \color{#5FF}{text}  \\
\verb+#AF0+ &amp;amp; \color{#AF0}{text} &amp;amp; \verb+#AF5+ &amp;amp; \color{#AF5}{text} &amp;amp; \verb+#AFA+ &amp;amp; \color{#AFA}{text} &amp;amp; \verb+#AFF+ &amp;amp; \color{#AFF}{text}  \\
\verb+#FF0+ &amp;amp; \color{#FF0}{text} &amp;amp; \verb+#FF5+ &amp;amp; \color{#FF5}{text} &amp;amp; \verb+#FFA+ &amp;amp; \color{#FFA}{text} &amp;amp; \verb+#FFF+ &amp;amp; \color{#FFF}{text}  \\
\hline
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{|rrrrrrrr|}
\hline
\verb+#000+ &amp;amp; \color{#000}{text} &amp;amp; \verb+#005+ &amp;amp; \color{#005}{text} &amp;amp; \verb+#00A+ &amp;amp; \color{#00A}{text} &amp;amp; \verb+#00F+ &amp;amp; \color{#00F}{text}  \\
\verb+#500+ &amp;amp; \color{#500}{text} &amp;amp; \verb+#505+ &amp;amp; \color{#505}{text} &amp;amp; \verb+#50A+ &amp;amp; \color{#50A}{text} &amp;amp; \verb+#50F+ &amp;amp; \color{#50F}{text}  \\
\verb+#A00+ &amp;amp; \color{#A00}{text} &amp;amp; \verb+#A05+ &amp;amp; \color{#A05}{text} &amp;amp; \verb+#A0A+ &amp;amp; \color{#A0A}{text} &amp;amp; \verb+#A0F+ &amp;amp; \color{#A0F}{text}  \\
\verb+#F00+ &amp;amp; \color{#F00}{text} &amp;amp; \verb+#F05+ &amp;amp; \color{#F05}{text} &amp;amp; \verb+#F0A+ &amp;amp; \color{#F0A}{text} &amp;amp; \verb+#F0F+ &amp;amp; \color{#F0F}{text}  \\
\hline
\verb+#080+ &amp;amp; \color{#080}{text} &amp;amp; \verb+#085+ &amp;amp; \color{#085}{text} &amp;amp; \verb+#08A+ &amp;amp; \color{#08A}{text} &amp;amp; \verb+#08F+ &amp;amp; \color{#08F}{text}  \\
\verb+#580+ &amp;amp; \color{#580}{text} &amp;amp; \verb+#585+ &amp;amp; \color{#585}{text} &amp;amp; \verb+#58A+ &amp;amp; \color{#58A}{text} &amp;amp; \verb+#58F+ &amp;amp; \color{#58F}{text}  \\
\verb+#A80+ &amp;amp; \color{#A80}{text} &amp;amp; \verb+#A85+ &amp;amp; \color{#A85}{text} &amp;amp; \verb+#A8A+ &amp;amp; \color{#A8A}{text} &amp;amp; \verb+#A8F+ &amp;amp; \color{#A8F}{text}  \\
\verb+#F80+ &amp;amp; \color{#F80}{text} &amp;amp; \verb+#F85+ &amp;amp; \color{#F85}{text} &amp;amp; \verb+#F8A+ &amp;amp; \color{#F8A}{text} &amp;amp; \verb+#F8F+ &amp;amp; \color{#F8F}{text}  \\
\hline
\verb+#0F0+ &amp;amp; \color{#0F0}{text} &amp;amp; \verb+#0F5+ &amp;amp; \color{#0F5}{text} &amp;amp; \verb+#0FA+ &amp;amp; \color{#0FA}{text} &amp;amp; \verb+#0FF+ &amp;amp; \color{#0FF}{text}  \\
\verb+#5F0+ &amp;amp; \color{#5F0}{text} &amp;amp; \verb+#5F5+ &amp;amp; \color{#5F5}{text} &amp;amp; \verb+#5FA+ &amp;amp; \color{#5FA}{text} &amp;amp; \verb+#5FF+ &amp;amp; \color{#5FF}{text}  \\
\verb+#AF0+ &amp;amp; \color{#AF0}{text} &amp;amp; \verb+#AF5+ &amp;amp; \color{#AF5}{text} &amp;amp; \verb+#AFA+ &amp;amp; \color{#AFA}{text} &amp;amp; \verb+#AFF+ &amp;amp; \color{#AFF}{text}  \\
\verb+#FF0+ &amp;amp; \color{#FF0}{text} &amp;amp; \verb+#FF5+ &amp;amp; \color{#FF5}{text} &amp;amp; \verb+#FFA+ &amp;amp; \color{#FFA}{text} &amp;amp; \verb+#FFF+ &amp;amp; \color{#FFF}{text}  \\
\hline
\end{array}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(5)．添加删除线&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用删除线功能必须声明 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;p&gt;在公式内使用 &lt;code&gt;\require{cancel}&lt;/code&gt; 来允许 &lt;em&gt;片段删除线&lt;/em&gt; 的显示。 
声明片段删除线后，使用 &lt;code&gt;\cancel{字符}&lt;/code&gt;、&lt;code&gt;\bcancel{字符}&lt;/code&gt;、&lt;code&gt;\xcancel{字符}&lt;/code&gt; 和 &lt;code&gt;\cancelto{字符}&lt;/code&gt; 来实现各种片段删除线效果。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\require{cancel}\begin{array}{rl}
\verb|y+\cancel{x}| &amp;amp; y+\cancel{x}\\
\verb|\cancel{y+x}| &amp;amp; \cancel{y+x}\\
\verb|y+\bcancel{x}| &amp;amp; y+\bcancel{x}\\
\verb|y+\xcancel{x}| &amp;amp; y+\xcancel{x}\\
\verb|y+\cancelto{0}{x}| &amp;amp; y+\cancelto{0}{x}\\
\verb+\frac{1\cancel9}{\cancel95} = \frac15+&amp;amp; \frac{1\cancel9}{\cancel95} = \frac15 \\
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\require{cancel}\begin{array}{rl}
\verb|y+\cancel{x}| &amp;amp; y+\cancel{x}\\
\verb|\cancel{y+x}| &amp;amp; \cancel{y+x}\\
\verb|y+\bcancel{x}| &amp;amp; y+\bcancel{x}\\
\verb|y+\xcancel{x}| &amp;amp; y+\xcancel{x}\\
\verb|y+\cancelto{0}{x}| &amp;amp; y+\cancelto{0}{x}\\
\verb+\frac{1\cancel9}{\cancel95} = \frac15+&amp;amp; \frac{1\cancel9}{\cancel95} = \frac15 \\
\end{array}
$$&lt;/div&gt;
&lt;p&gt;使用 &lt;code&gt;\require{enclose}&lt;/code&gt; 来允许 整段删除线 的显示。 
声明整段删除线后，使用 &lt;code&gt;\enclose{删除线效果}{字符}&lt;/code&gt; 来实现各种整段删除线效果。 
其中，删除线效果有 &lt;code&gt;horizontalstrike&lt;/code&gt;、&lt;code&gt;verticalstrike&lt;/code&gt;、&lt;code&gt;updiagonalstrike&lt;/code&gt; 和 &lt;code&gt;downdiagonalstrike&lt;/code&gt;，可叠加使用。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\require{enclose}\begin{array}{rl}
\verb|\enclose{horizontalstrike}{x+y}| &amp;amp; \enclose{horizontalstrike}{x+y}\\
\verb|\enclose{verticalstrike}{\frac xy}| &amp;amp; \enclose{verticalstrike}{\frac xy}\\
\verb|\enclose{updiagonalstrike}{x+y}| &amp;amp; \enclose{updiagonalstrike}{x+y}\\
\verb|\enclose{downdiagonalstrike}{x+y}| &amp;amp; \enclose{downdiagonalstrike}{x+y}\\
\verb|\enclose{horizontalstrike,updiagonalstrike}{x+y}| &amp;amp; \enclose{horizontalstrike,updiagonalstrike}{x+y}\\
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\require{enclose}\begin{array}{rl}
\verb|\enclose{horizontalstrike}{x+y}| &amp;amp; \enclose{horizontalstrike}{x+y}\\
\verb|\enclose{verticalstrike}{\frac xy}| &amp;amp; \enclose{verticalstrike}{\frac xy}\\
\verb|\enclose{updiagonalstrike}{x+y}| &amp;amp; \enclose{updiagonalstrike}{x+y}\\
\verb|\enclose{downdiagonalstrike}{x+y}| &amp;amp; \enclose{downdiagonalstrike}{x+y}\\
\verb|\enclose{horizontalstrike,updiagonalstrike}{x+y}| &amp;amp; \enclose{horizontalstrike,updiagonalstrike}{x+y}\\
\end{array}
$$&lt;/div&gt;
&lt;p&gt;此外，&lt;code&gt;\enclose&lt;/code&gt; 命令还可以产生包围的边框和圆等，参见 &lt;a href="https://developer.mozilla.org/en-US/docs/Web/MathML/Element/menclose"&gt;MathML Menclose Documentation&lt;/a&gt; 以查看更多效果。&lt;/p&gt;
&lt;h2 id="er , ju zhen shi yong can kao_1"&gt;二、矩阵使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru wu kuang ju zhen"&gt;1．如何输入无框矩阵&lt;/h3&gt;
&lt;p&gt;在开头使用 &lt;code&gt;begin{matrix}&lt;/code&gt;，在结尾使用 &lt;code&gt;end{matrix}&lt;/code&gt;，在中间插入矩阵元素，每个元素之间插入 &lt;code&gt;&amp;amp;&lt;/code&gt; ，并在每行结尾处使用 \ 。 
使用矩阵时必须声明 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        \begin{matrix}
        1 &amp;amp; x &amp;amp; x^2 \\
        1 &amp;amp; y &amp;amp; y^2 \\
        1 &amp;amp; z &amp;amp; z^2 \\
        \end{matrix}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
        \begin{matrix}
        1 &amp;amp; x &amp;amp; x^2 \\
        1 &amp;amp; y &amp;amp; y^2 \\
        1 &amp;amp; z &amp;amp; z^2 \\
        \end{matrix}
$$&lt;/div&gt;
&lt;h3 id="2.ru he shu ru dai bian kuang de ju zhen"&gt;2．如何输入带边框的矩阵&lt;/h3&gt;
&lt;p&gt;在开头将 &lt;code&gt;matrix&lt;/code&gt; 替换为 &lt;code&gt;pmatrix bmatrix Bmatrix vmatrix Vmatrix&lt;/code&gt; 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;pmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{pmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{pmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;bmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{bmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{bmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Bmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{Bmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{Bmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;vmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{vmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{vmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Vmatrix&lt;/code&gt;
&lt;div class="math"&gt;$$\begin{Vmatrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{Vmatrix}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="3.ru he shu ru dai sheng lue fu hao de ju zhen"&gt;3．如何输入带省略符号的矩阵&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\cdots&lt;/code&gt;  , &lt;code&gt;\ddots&lt;/code&gt;  , &lt;code&gt;\vdots&lt;/code&gt;  来输入省略符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        \begin{pmatrix}
        1 &amp;amp; a_1 &amp;amp; a_1^2 &amp;amp; \cdots &amp;amp; a_1^n \\
        1 &amp;amp; a_2 &amp;amp; a_2^2 &amp;amp; \cdots &amp;amp; a_2^n \\
        \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
        1 &amp;amp; a_m &amp;amp; a_m^2 &amp;amp; \cdots &amp;amp; a_m^n \\
        \end{pmatrix}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
        \begin{pmatrix}
        1 &amp;amp; a_1 &amp;amp; a_1^2 &amp;amp; \cdots &amp;amp; a_1^n \\
        1 &amp;amp; a_2 &amp;amp; a_2^2 &amp;amp; \cdots &amp;amp; a_2^n \\
        \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
        1 &amp;amp; a_m &amp;amp; a_m^2 &amp;amp; \cdots &amp;amp; a_m^n \\
        \end{pmatrix}
$$&lt;/div&gt;
&lt;h3 id="4.ru he shu ru dai fen ge fu hao de ju zhen"&gt;4．如何输入带分割符号的矩阵&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        \left[
            \begin{array}{cc|c}
              1&amp;amp;2&amp;amp;3\\
              4&amp;amp;5&amp;amp;6
            \end{array}
        \right]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中 &lt;code&gt;cc|c&lt;/code&gt; 代表在一个三列矩阵中的第二和第三列之间插入分割线。&lt;/p&gt;
&lt;div class="math"&gt;$$
\left[
    \begin{array}{cc|c}
      1&amp;amp;2&amp;amp;3\\
      4&amp;amp;5&amp;amp;6
    \end{array}
\right]
$$&lt;/div&gt;
&lt;h3 id="5.ru he shu ru xing zhong ju zhen"&gt;5．如何输入行中矩阵&lt;/h3&gt;
&lt;p&gt;若想在一行内显示矩阵，使用&lt;code&gt;\bigl(\begin{smallmatrix} ... \end{smallmatrix}\bigr)&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;这是一个行中矩阵的示例 $\bigl( \begin{smallmatrix} a &amp;amp; b \\ c &amp;amp; d \end{smallmatrix} \bigr)$ 。
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这是一个行中矩阵的示例&lt;/p&gt;
&lt;div class="math"&gt;$$\bigl(\begin{smallmatrix} a &amp;amp; b \\ c &amp;amp; d \end{smallmatrix}\bigr)$$&lt;/div&gt;
&lt;p&gt; 。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 这里的页面生成有bug, 左右两边均为两个$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="san , fang cheng shi xu lie shi yong can kao_1"&gt;三、方程式序列使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge fang cheng shi xu lie"&gt;1．如何输入一个方程式序列&lt;/h3&gt;
&lt;p&gt;人们经常想要一列整齐且居中的方程式序列。使用 &lt;code&gt;\begin{align}&amp;hellip;\end{align}&lt;/code&gt; 来创造一列方程式，其中在每行结尾处使用 &lt;code&gt;\\&lt;/code&gt; 。 
使用方程式序列无需声明公式符号 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;请注意 &lt;code&gt;{align}&lt;/code&gt; 语句是 &lt;strong&gt;自动编号&lt;/strong&gt; 的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{align}
\sqrt{37} &amp;amp; = \sqrt{\frac{73^2-1}{12^2}}  \\
 &amp;amp; = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}}  \\ 
 &amp;amp; = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}}  \\
 &amp;amp; = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}}  \\ 
 &amp;amp; \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right)
\end{align}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;\begin{align}
\sqrt{37} &amp;amp; = \sqrt{\frac{73^2-1}{12^2}} \tag{1} \\
 &amp;amp; = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \tag{2} \\ 
 &amp;amp; = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \tag{3} \\
 &amp;amp; = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \tag{4} \\ 
 &amp;amp; \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right) \tag{5}
\end{align}&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: 本blog不能自动编号, 示例效果采用手动编号&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="2.zai yi ge fang cheng shi xu lie de mei yi xing zhong zhu ming yuan yin"&gt;2．在一个方程式序列的每一行中注明原因&lt;/h3&gt;
&lt;p&gt;在 &lt;code&gt;{align}&lt;/code&gt; 中灵活组合 &lt;code&gt;\text&lt;/code&gt; 和 &lt;code&gt;\tag&lt;/code&gt; 语句。&lt;code&gt;\tag&lt;/code&gt; 语句编号优先级高于自动编号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{align}
   v + w &amp;amp; = 0  &amp;amp;\text{Given} \tag 1\\
   -w &amp;amp; = -w + 0 &amp;amp; \text{additive identity} \tag 2\\
   -w + 0 &amp;amp; = -w + (v + w) &amp;amp; \text{equations $(1)$ and $(2)$}
\end{align}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align}
   v + w &amp;amp; = 0  &amp;amp;\text{Given} \tag 1\\
   -w &amp;amp; = -w + 0 &amp;amp; \text{additive identity} \tag 2\\
   -w + 0 &amp;amp; = -w + (v + w) &amp;amp; \text{equations $(1)$ and $(2)$}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;本例中第一、第二行的自动编号被 &lt;code&gt;\tag&lt;/code&gt; 语句覆盖，第三行的编号为自动编号。&lt;/p&gt;
&lt;h2 id="si , tiao jian biao da shi shi yong can kao_1"&gt;四、条件表达式使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge tiao jian biao da shi"&gt;1．如何输入一个条件表达式&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;begin{cases}&lt;/code&gt; 来创造一组条件表达式，在每一行条件中插入 &lt;code&gt;&amp;amp;&lt;/code&gt; 来指定需要对齐的内容，并在每一行结尾处使用 &lt;code&gt;\\&lt;/code&gt;，以 &lt;code&gt;end{cases}&lt;/code&gt; 结束。 
条件表达式无需声明 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        f(n) =
        \begin{cases}
        n/2,  &amp;amp; \text{if $n$ is even} \\
        3n+1, &amp;amp; \text{if $n$ is odd}
        \end{cases}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
        f(n) =
        \begin{cases}
        n/2,  &amp;amp; \text{if $n$ is even} \\
        3n+1, &amp;amp; \text{if $n$ is odd}
        \end{cases}
$$&lt;/div&gt;
&lt;h3 id="2.ru he shu ru yi ge zuo ce dui qi de tiao jian biao da shi"&gt;2．如何输入一个左侧对齐的条件表达式&lt;/h3&gt;
&lt;p&gt;若想让文字在 左侧对齐显示 ，则有如下方式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        \left.
        \begin{array}{l}
        \text{if $n$ is even:}&amp;amp;n/2\\
        \text{if $n$ is odd:}&amp;amp;3n+1
        \end{array}
        \right\}
        =f(n)
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
        \left.
        \begin{array}{l}
        \text{if $n$ is even:}&amp;amp;n/2\\
        \text{if $n$ is odd:}&amp;amp;3n+1
        \end{array}
        \right\}
        =f(n)
$$&lt;/div&gt;
&lt;h3 id="3.ru he shi tiao jian biao da shi gua pei xing gao"&gt;3．如何使条件表达式适配行高&lt;/h3&gt;
&lt;p&gt;在一些情况下，条件表达式中某些行的行高为非标准高度，此时使用 &lt;code&gt;\\[2ex]&lt;/code&gt; 语句代替该行末尾的 &lt;code&gt;\\&lt;/code&gt; 来让编辑器适配。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不适配[2ex]&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(n) = 
\begin{cases}
\frac{n}{2},  &amp;amp; \text{if $n$ is even} \\
3n+1, &amp;amp; \text{if $n$ is odd}
\end{cases}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
f(n) = 
\begin{cases}
\frac{n}{2},  &amp;amp; \text{if $n$ is even} \\
3n+1, &amp;amp; \text{if $n$ is odd}
\end{cases}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;适配[2ex]&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(n) = 
\begin{cases}
\frac{n}{2},  &amp;amp; \text{if $n$ is even} \\[2ex]
3n+1, &amp;amp; \text{if $n$ is odd}
\end{cases}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
f(n) = 
\begin{cases}
\frac{n}{2},  &amp;amp; \text{if $n$ is even} \\[2ex]
3n+1, &amp;amp; \text{if $n$ is odd}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;一个 &lt;code&gt;[ex]&lt;/code&gt; 指一个 "X-Height"，即&lt;code&gt;x&lt;/code&gt;字母高度。可以根据情况指定多个 &lt;code&gt;[ex]&lt;/code&gt;，如 &lt;code&gt;[3ex]&lt;/code&gt;、&lt;code&gt;[4ex]&lt;/code&gt; 等。 
其实可以在任何地方使用 &lt;code&gt;\\[2ex]&lt;/code&gt; 语句，只要你觉得合适。&lt;/p&gt;
&lt;h2 id="wu , shu zu yu biao ge shi yong can kao_1"&gt;五、数组与表格使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge shu zu huo biao ge"&gt;1．如何输入一个数组或表格&lt;/h3&gt;
&lt;p&gt;通常，一个格式化后的表格比单纯的文字或排版后的文字更具有可读性。数组和表格均以 &lt;code&gt;begin{array}&lt;/code&gt; 开头，并在其后定义列数及每一列的文本对齐属性，&lt;code&gt;c l r&lt;/code&gt; 分别代表居中、左对齐及右对齐。若需要插入垂直分割线，在定义式中插入 &lt;code&gt;|&lt;/code&gt; ，若要插入水平分割线，在下一行输入前插入 &lt;code&gt;\hline&lt;/code&gt; 。与矩阵相似，每行元素间均须要插入 &lt;code&gt;&amp;amp;&lt;/code&gt; ，每行元素以 &lt;code&gt;\\&lt;/code&gt; 结尾，最后以 &lt;code&gt;end{array}&lt;/code&gt; 结束数组。 
使用单个数组或表格时无需声明 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|lcr}
n &amp;amp; \text{左对齐} &amp;amp; \text{居中对齐} &amp;amp; \text{右对齐} \\
\hline
1 &amp;amp; 0.24 &amp;amp; 1 &amp;amp; 125 \\
2 &amp;amp; -1 &amp;amp; 189 &amp;amp; -8 \\
3 &amp;amp; -20 &amp;amp; 2000 &amp;amp; 1+10i
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|lcr}
n &amp;amp; \text{左对齐} &amp;amp; \text{居中对齐} &amp;amp; \text{右对齐} \\
\hline
1 &amp;amp; 0.24 &amp;amp; 1 &amp;amp; 125 \\
2 &amp;amp; -1 &amp;amp; 189 &amp;amp; -8 \\
3 &amp;amp; -20 &amp;amp; 2000 &amp;amp; 1+10i
\end{array}
$$&lt;/div&gt;
&lt;h3 id="2.ru he shu ru yi ge qian tao de shu zu huo biao ge"&gt;2．如何输入一个嵌套的数组或表格&lt;/h3&gt;
&lt;p&gt;多个数组/表格可&lt;strong&gt;互相嵌套&lt;/strong&gt; 并组成一组数组/一组表格。 
使用嵌套前必须声明 &lt;code&gt;$$&lt;/code&gt; 符号。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% outer vertical array of arrays 外层垂直表格&lt;/span&gt;
&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="c"&gt;% inner horizontal array of arrays 内层水平表格&lt;/span&gt;
    &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;cc&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c"&gt;% inner array of minimum values 内层"最小值"数组&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;cccc&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;hline&lt;/span&gt;
        &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;
        &lt;span class="c"&gt;% inner array of maximum values 内层"最大值"数组&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;cccc&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;hline&lt;/span&gt;
        &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="c"&gt;% 内层第一行表格组结束&lt;/span&gt;
    &lt;span class="o"&gt;\\&lt;/span&gt;
    &lt;span class="c"&gt;% inner array of delta values 内层第二行Delta值数组&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;cccc&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Delta&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="n"&gt;hline&lt;/span&gt;
        &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;\\&lt;/span&gt;
        &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c"&gt;% 内层第二行表格组结束&lt;/span&gt;
&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
% outer vertical array of arrays 外层垂直表格
\begin{array}{c}
    % inner horizontal array of arrays 内层水平表格
    \begin{array}{cc}
        % inner array of minimum values 内层"最小值"数组
        \begin{array}{c|cccc}
        \text{min} &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\\
        \hline
        0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
        1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\\
        2 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 2\\
        3 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3
        \end{array}
    &amp;amp;
        % inner array of maximum values 内层"最大值"数组
        \begin{array}{c|cccc}
        \text{max}&amp;amp;0&amp;amp;1&amp;amp;2&amp;amp;3\\
        \hline
        0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\\
        1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\\
        2 &amp;amp; 2 &amp;amp; 2 &amp;amp; 2 &amp;amp; 3\\
        3 &amp;amp; 3 &amp;amp; 3 &amp;amp; 3 &amp;amp; 3
        \end{array}
    \end{array}
    % 内层第一行表格组结束
    \\
    % inner array of delta values 内层第二行Delta值数组
        \begin{array}{c|cccc}
        \Delta&amp;amp;0&amp;amp;1&amp;amp;2&amp;amp;3\\
        \hline
        0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\\
        1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2\\
        2 &amp;amp; 2 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1\\
        3 &amp;amp; 3 &amp;amp; 2 &amp;amp; 1 &amp;amp; 0
        \end{array}
        % 内层第二行表格组结束
\end{array}
$$&lt;/div&gt;
&lt;h3 id="3.ru he shu ru yi ge fang cheng zu"&gt;3．如何输入一个方程组&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;\begin{array}&amp;hellip;\end{array}&lt;/code&gt; 和 &lt;code&gt;\left\{&amp;hellip;\right.&lt;/code&gt; 来创建一个方程组。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\left\{ 
\begin{array}{c}
a_1x+b_1y+c_1z=d_1 \\ 
a_2x+b_2y+c_2z=d_2 \\ 
a_3x+b_3y+c_3z=d_3
\end{array}
\right. 
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\left\{ 
\begin{array}{c}
a_1x+b_1y+c_1z=d_1 \\ 
a_2x+b_2y+c_2z=d_2 \\ 
a_3x+b_3y+c_3z=d_3
\end{array}
\right. 
$$&lt;/div&gt;
&lt;p&gt;或者使用条件表达式组 &lt;code&gt;\begin{cases}&amp;hellip;\end{cases}&lt;/code&gt; 来实现相同效果：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{cases}
a_1x+b_1y+c_1z=d_1 \\ 
a_2x+b_2y+c_2z=d_2 \\ 
a_3x+b_3y+c_3z=d_3
\end{cases}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{cases}
a_1x+b_1y+c_1z=d_1 \\ 
a_2x+b_2y+c_2z=d_2 \\ 
a_3x+b_3y+c_3z=d_3
\end{cases}
$$&lt;/div&gt;
&lt;h2 id="liu , lian fen shu shi yong can kao_1"&gt;六、连分数使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge lian fen shi"&gt;1．如何输入一个连分式&lt;/h3&gt;
&lt;p&gt;就像输入分式时使用 &lt;code&gt;\frac&lt;/code&gt; 一样，使用 &lt;code&gt;\cfrac&lt;/code&gt; 来创建一个连分数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x = a_0 + \cfrac{1^2}{a_1
          + \cfrac{2^2}{a_2
          + \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
x = a_0 + \cfrac{1^2}{a_1
          + \cfrac{2^2}{a_2
          + \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}}
$$&lt;/div&gt;
&lt;p&gt;不要使用普通的 &lt;code&gt;\frac&lt;/code&gt; 或 &lt;code&gt;\over&lt;/code&gt; 来创建，否则会看起来 &lt;strong&gt;很恶心&lt;/strong&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x = a_0 + \frac{1^2}{a_1
          + \frac{2^2}{a_2
          + \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
x = a_0 + \frac{1^2}{a_1
          + \frac{2^2}{a_2
          + \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}}
$$&lt;/div&gt;
&lt;p&gt;当然，你可以使用 &lt;code&gt;\frac&lt;/code&gt; 来表达连分数的 &lt;strong&gt;紧缩记法&lt;/strong&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x = a_0 + \frac{1^2}{a_1+}
          \frac{2^2}{a_2+}
          \frac{3^2}{a_3 +} \frac{4^4}{a_4 +} \cdots
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
x = a_0 + \frac{1^2}{a_1+}
          \frac{2^2}{a_2+}
          \frac{3^2}{a_3 +} \frac{4^4}{a_4 +} \cdots
$$&lt;/div&gt;
&lt;p&gt;连分数通常都太大以至于不易排版，所以建议在连分数前后声明 &lt;code&gt;$$&lt;/code&gt; 符号，或使用像 &lt;code&gt;[a0;a1,a2,a3,&amp;hellip;]&lt;/code&gt; 一样的紧缩记法。&lt;/p&gt;
&lt;h2 id="qi , jiao huan tu biao shi yong can kao_1"&gt;七、交换图表使用参考&lt;/h2&gt;
&lt;h3 id="1.ru he shu ru yi ge jiao huan tu biao"&gt;1．如何输入一个交换图表&lt;/h3&gt;
&lt;p&gt;使用一行 &lt;code&gt;$ \require{AMScd} $&lt;/code&gt; 语句来允许交换图表的显示。 
声明交换图表后，语法与矩阵相似，在开头使用 &lt;code&gt;begin{CD}&lt;/code&gt;，在结尾使用 &lt;code&gt;end{CD}&lt;/code&gt;，在中间插入图表元素，每个元素之间插入 &lt;code&gt;&amp;amp;&lt;/code&gt; ，并在每行结尾处使用&lt;code&gt;\\&lt;/code&gt; 。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$\require{AMScd}$
\begin{CD}
    A @&amp;gt;a&amp;gt;&amp;gt; B\\
    @V b V V\# @VV c V\\
    C @&amp;gt;&amp;gt;d&amp;gt; D
\end{CD}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
$\require{AMScd}$
\begin{CD}
    A @&amp;gt;a&amp;gt;&amp;gt; B\\
    @V b V V\# @VV c V\\
    C @&amp;gt;&amp;gt;d&amp;gt; D
\end{CD}
$$&lt;/div&gt;
&lt;p&gt;其中，&lt;code&gt;@&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 代表右箭头、&lt;code&gt;@&amp;lt;&amp;lt;&amp;lt;&lt;/code&gt; 代表左箭头、&lt;code&gt;@VVV&lt;/code&gt; 代表下箭头、&lt;code&gt;@AAA&lt;/code&gt; 代表上箭头、&lt;code&gt;@=&lt;/code&gt; 代表水平双实线、&lt;code&gt;@|&lt;/code&gt; 代表竖直双实线、&lt;code&gt;@.&lt;/code&gt;代表没有箭头。 
在 &lt;code&gt;@&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 的 &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 之间任意插入文字即代表该箭头的注释文字。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{CD}
    A @&amp;gt;&amp;gt;&amp;gt; B @&amp;gt;{\text{very long label}}&amp;gt;&amp;gt; C \\
    @. @AAA @| \\
    D @= E @&amp;lt;&amp;lt;&amp;lt; F
\end{CD}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{CD}
    A @&amp;gt;&amp;gt;&amp;gt; B @&amp;gt;{\text{very long label}}&amp;gt;&amp;gt; C \\
    @. @AAA @| \\
    D @= E @&amp;lt;&amp;lt;&amp;lt; F
\end{CD}
$$&lt;/div&gt;
&lt;p&gt;在本例中， "very long label"自动延长了它所在箭头以及对应箭头的长度。&lt;/p&gt;
&lt;h2 id="ba , yi xie te shu de zhu yi shi xiang_1"&gt;八、一些特殊的注意事项&lt;/h2&gt;
&lt;p&gt;有些小窍门会让数学公式显得更好看，强迫症和完美主义者会喜欢下面的内容。&lt;/p&gt;
&lt;h3 id="1. mou xie fen shu de xian shi wen ti"&gt;1. 某些分数的显示问题&lt;/h3&gt;
&lt;p&gt;在以&lt;code&gt;e&lt;/code&gt;为底的指数函数、极限和积分中尽量不要使用 &lt;code&gt;\frac&lt;/code&gt; 符号：它会使整段函数看起来很怪，而且可能产生歧义。也正是因此它在专业数学排版中几乎从不出现。 
横着写这些分式，中间使用斜线间隔 &lt;code&gt;/&lt;/code&gt; （用斜线代替分数线）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
e^{i\frac{\pi}2} \quad e^{\frac{i\pi}2}&amp;amp; e^{i\pi/2} \\
\int_{-\frac\pi2}^\frac\pi2 \sin x\,dx &amp;amp; \int_{-\pi/2}^{\pi/2}\sin x\,dx \\
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
e^{i\frac{\pi}2} \quad e^{\frac{i\pi}2}&amp;amp; e^{i\pi/2} \\
\int_{-\frac\pi2}^\frac\pi2 \sin x\,dx &amp;amp; \int_{-\pi/2}^{\pi/2}\sin x\,dx \\
\end{array}
$$&lt;/div&gt;
&lt;h3 id="2. liu chu he li de jian ge"&gt;2. 留出合理的间隔&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;|&lt;/code&gt; 符号在被当作分隔符时会产生过于狭窄的间隔，因此在需要分隔时最好使用 &lt;code&gt;\mid&lt;/code&gt; 来代替它。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\{x|x^2\in\Bbb Z\} &amp;amp; \{x\mid x^2\in\Bbb Z\} \\
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\{x|x^2\in\Bbb Z\} &amp;amp; \{x\mid x^2\in\Bbb Z\} \\
\end{array}
$$&lt;/div&gt;
&lt;h3 id="3. duo zhong ji fen fu hao de xian shi"&gt;3. 多重积分符号的显示&lt;/h3&gt;
&lt;p&gt;使用多重积分符号时，不要多次使用 &lt;code&gt;\int&lt;/code&gt; 来声明，直接使用 &lt;code&gt;\iint&lt;/code&gt; 来表示 二重积分 ，使用 &lt;code&gt;\iiint&lt;/code&gt; 来表示 三重积分 等。对于无限次积分，可以用 &lt;code&gt;\int \cdots \int&lt;/code&gt; 表示。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\int\int_S f(x)\,dy\,dx &amp;amp; \iint_S f(x)\,dy\,dx \\
\int\int\int_V f(x)\,dz\,dy\,dx &amp;amp; \iiint_V f(x)\,dz\,dy\,dx
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\int\int_S f(x)\,dy\,dx &amp;amp; \iint_S f(x)\,dy\,dx \\
\int\int\int_V f(x)\,dz\,dy\,dx &amp;amp; \iiint_V f(x)\,dz\,dy\,dx
\end{array}
$$&lt;/div&gt;
&lt;h3 id="4. duo ge wei fen fu hao de xian shi"&gt;4. 多个微分符号的显示&lt;/h3&gt;
&lt;p&gt;在微分符号前加入 &lt;code&gt;\&lt;/code&gt;, 来插入一个小的间隔空隙；没有 &lt;code&gt;\&lt;/code&gt;, 符号的话， 将会把不同的微分符号堆在一起。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\iiint_V f(x){\rm d}z {\rm d}y {\rm d}x &amp;amp; \iiint_V f(x)\,{\rm d}z\,{\rm d}y\,{\rm d}x
\end{array}
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{array}{c|c}
\mathrm{Bad} &amp;amp; \mathrm{Better} \\
\hline \\
\iiint_V f(x){\rm d}z {\rm d}y {\rm d}x &amp;amp; \iiint_V f(x)\,{\rm d}z\,{\rm d}y\,{\rm d}x
\end{array}
$$&lt;/div&gt;
&lt;p&gt;感谢您花费时间阅读这份指导手册，本手册内容可能有疏漏之处，欢迎更改指正。 &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Pandoc's Markdown 語法中文翻譯</title><link href="https://freeopen.github.io/posts/pandocs-markdown-yu-fa-zhong-wen-fan-yi" rel="alternate"></link><published>2017-05-11T00:00:00+08:00</published><updated>2017-05-11T00:00:00+08:00</updated><author><name>John MacFarlane; Tzeng Yuxio(translated)</name></author><id>tag:freeopen.github.io,2017-05-11:/posts/pandocs-markdown-yu-fa-zhong-wen-fan-yi</id><summary type="html">&lt;h2 id="qian yan"&gt;前言&lt;/h2&gt;
&lt;p&gt;這份文件是 &lt;a href="http://johnmacfarlane.net/pandoc/"&gt;Pandoc&lt;/a&gt; 版本 Markdown 語法的中文翻譯。Pandoc 本身是由 &lt;a href="http://johnmacfarlane.net/"&gt;John MacFarlane&lt;/a&gt; 所開發的文件轉換工具，可以在 HTML, Markdown, PDF, TeX...等等格式之間進行轉換。有許多喜歡純文字編輯的人，利用 Pandoc 來進行論文的撰寫或投影片製作。但除了轉換的功能外，Pandoc 所定義的 Markdown 擴充語法也是這套工具的一大亮點，在 Pandoc 的官方使用說明文件中，光是其針對 Markdown 格式的擴充就佔了整整一半左右的篇幅。 &lt;/p&gt;
&lt;p&gt;本文件翻譯自 &lt;a href="http://johnmacfarlane.net/pandoc/README.html#pandocs-markdown"&gt;Pandoc - Pandoc User&amp;rsquo;s Guide&lt;/a&gt; 中的 "Pandoc's markdown" 一節。你可以看看&lt;a href="https://raw.github.com/tzengyuxio/pages/gh-pages/pandoc/pandoc.markdown"&gt;這份文件的原始檔&lt;/a&gt;、產生文件&lt;a href="https://github.com/tzengyuxio/pages/blob/gh-pages/pandoc/pm-template.html5"&gt;所使用的 HTML 範本&lt;/a&gt;，以及&lt;a href="https://github.com/tzengyuxio/pages/blob/gh-pages/pandoc/convert.sh"&gt;轉換時的命令參數 …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h2 id="qian yan"&gt;前言&lt;/h2&gt;
&lt;p&gt;這份文件是 &lt;a href="http://johnmacfarlane.net/pandoc/"&gt;Pandoc&lt;/a&gt; 版本 Markdown 語法的中文翻譯。Pandoc 本身是由 &lt;a href="http://johnmacfarlane.net/"&gt;John MacFarlane&lt;/a&gt; 所開發的文件轉換工具，可以在 HTML, Markdown, PDF, TeX...等等格式之間進行轉換。有許多喜歡純文字編輯的人，利用 Pandoc 來進行論文的撰寫或投影片製作。但除了轉換的功能外，Pandoc 所定義的 Markdown 擴充語法也是這套工具的一大亮點，在 Pandoc 的官方使用說明文件中，光是其針對 Markdown 格式的擴充就佔了整整一半左右的篇幅。 &lt;/p&gt;
&lt;p&gt;本文件翻譯自 &lt;a href="http://johnmacfarlane.net/pandoc/README.html#pandocs-markdown"&gt;Pandoc - Pandoc User&amp;rsquo;s Guide&lt;/a&gt; 中的 "Pandoc's markdown" 一節。你可以看看&lt;a href="https://raw.github.com/tzengyuxio/pages/gh-pages/pandoc/pandoc.markdown"&gt;這份文件的原始檔&lt;/a&gt;、產生文件&lt;a href="https://github.com/tzengyuxio/pages/blob/gh-pages/pandoc/pm-template.html5"&gt;所使用的 HTML 範本&lt;/a&gt;，以及&lt;a href="https://github.com/tzengyuxio/pages/blob/gh-pages/pandoc/convert.sh"&gt;轉換時的命令參數&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;以下翻譯開始。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="pandoc's markdown"&gt;Pandoc's markdown&lt;/h2&gt;
&lt;p&gt;與 John Gruber 的 原始 &lt;a href="http://daringfireball.net/projects/markdown/"&gt;markdown&lt;/a&gt; 相比，Pandoc 版本的 markdown 在語法上有額外的擴充與些許的修正。這份文件解釋了這些語法，並指出其與原始 markdown 的差異所在。除非特別提到，不然這些差異均可藉由使用 &lt;code&gt;markdown_strict&lt;/code&gt; 而非 &lt;code&gt;markdown&lt;/code&gt; 的格式來關閉。單獨一項擴充也可透過 &lt;code&gt;+EXTENSION&lt;/code&gt; 或 &lt;code&gt;-EXTENSION&lt;/code&gt; 的方式來開啟或關閉。例如，&lt;code&gt;markdown_strict+footnotes&lt;/code&gt; 表示加上腳註擴充的原始 markdown，而 &lt;code&gt;markdown-footnotes-pipe_tables&lt;/code&gt; 則是拿掉了腳註與管線表格擴充的 pandoc markdown。&lt;/p&gt;
&lt;h2 id="zhe xue"&gt;哲學&lt;/h2&gt;
&lt;p&gt;Markdown 是針對易於書寫與閱讀的目標而設計的，特別是在易於閱讀這點上尤為重要：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一份 Markdown 格式的文件應該要能以純文字形式直接發表，並且一眼看過去不存在任何標記用的標籤或格式指令。
&lt;small&gt;&lt;a href="http://daringfireball.net/projects/markdown/syntax#philosophy"&gt;John Gruber&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這項原則同樣也是 pandoc 在制訂表格、腳註以及其他擴充的語法時，所依循的規範。&lt;/p&gt;
&lt;p&gt;然而，pandoc 的目標與原始 markdown 的最初目標有著方向性的不同。在 markdown 原本的設計中，HTML 是其主要輸出對象；然而 pandoc 則是針對多種輸出格式而設計。因此，雖然 pandoc 同樣也允許直接嵌入 HTML 標籤，但並不鼓勵這樣的作法，取而代之的是 pandoc 提供了許多非 HTML 的方式，來讓使用者輸入像是定義清單、表格、數學公式以及腳註等諸如此類的重要文件元素。&lt;/p&gt;
&lt;h2 id="duan luo"&gt;段落&lt;/h2&gt;
&lt;p&gt;一個段落指的是一行以上的文字，跟在一行以上的空白行之後。換行字元會被當作是空白處理，因此你可以依自己喜好排列段落文字。如果你需要強制換行，在行尾放上兩個以上的空白字元即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;escaped_line_breaks&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一個反斜線後跟著一個換行字元，同樣也有強制換行的效果。&lt;/p&gt;
&lt;h2 id="biao ti"&gt;標題&lt;/h2&gt;
&lt;p&gt;有兩種不同形式的標題語法，Setext 以及 atx。&lt;/p&gt;
&lt;h3 id="setext feng ge biao ti"&gt;Setext 風格標題&lt;/h3&gt;
&lt;p&gt;Setext 風格的標題是由一行文字底下接著一行 &lt;code&gt;=&lt;/code&gt; 符號（用於一階標題）或 &lt;code&gt;-&lt;/code&gt; 符號（用於二階標題）所構成：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gh"&gt;A level-one header&lt;/span&gt;
&lt;span class="gh"&gt;==================&lt;/span&gt;

&lt;span class="gh"&gt;A level-two header&lt;/span&gt;
&lt;span class="gh"&gt;------------------&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;標題的文字可以包含行內格式，例如強調（見下方 [行內格式] 一節）。&lt;/p&gt;
&lt;h3 id="atx feng ge biao ti"&gt;Atx 風格標題&lt;/h3&gt;
&lt;p&gt;Atx 風格的標題是由一到六個 &lt;code&gt;#&lt;/code&gt; 符號以及一行文字所組成，你可以在文字後面加上任意數量的 &lt;code&gt;#&lt;/code&gt; 符號。由行首起算的 &lt;code&gt;#&lt;/code&gt; 符號數量決定了標題的階層：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## A level-two header

### A level-three header ###
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如同 setext 風格標題，這裡的標題文字同樣可包含行內格式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# A level-one header with a [link](/url) and *emphasis*
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;blank_before_header&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原始 markdown 語法在標題之前並不需要預留空白行。Pandoc 則需要（除非標題位於文件最開始的地方）。這是因為以 &lt;code&gt;#&lt;/code&gt; 符號開頭的情況在一般文字段落中相當常見，這會導致非預期的標題。例如下面的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;I like several of their flavors of ice cream:
#22, for example, and #5.
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="html, latex yu  context de biao ti shi bie fu"&gt;HTML, LaTeX 與 ConTeXt 的標題識別符&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;header_attributes&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在標題文字所在行的行尾，可以使用以下語法為標題加上屬性：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{#identifier .class .class key=value key=value}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;雖然這個語法也包含加入類別 (class) 以及鍵／值形式的屬性 (attribute)，但目前只有識別符 (identifier/ID) 在輸出時有實際作用（且只在部分格式的輸出，包括：HTML, LaTeX, ConTeXt, Textile, AsciiDoc）。舉例來說，下面是將標題加上 &lt;code&gt;foo&lt;/code&gt; 識別符的幾種方法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# My header {#foo}

## My header ##    {#foo}

My other header   {#foo}
---------------
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;（此語法與 &lt;a href="http://www.michelf.com/projects/php-markdown/extra/"&gt;PHP Markdown Extra&lt;/a&gt; 相容。）&lt;/p&gt;
&lt;p&gt;具有 &lt;code&gt;unnumbered&lt;/code&gt; 類別的標題將不會被編號，即使 &lt;code&gt;--number-sections&lt;/code&gt; 的選項是開啟的。單一連字符號 (&lt;code&gt;-&lt;/code&gt;) 等同於 &lt;code&gt;.unnumbered&lt;/code&gt;，且更適用於非英文文件中。因此，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# My header {-}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;與下面這行是等價的&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# My header {.unnumbered}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;auto_identifiers&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;沒有明確指定 ID（識別符）的標題將會依據其標題文字，自動指派一個獨一無二的 ID。由標題文字推導 ID 的規則如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;移除所有格式，連結等。&lt;/li&gt;
&lt;li&gt;移除所有標點符號，除了底線、連字符號與句號。&lt;/li&gt;
&lt;li&gt;以連字符號取代所有空白與換行字元。&lt;/li&gt;
&lt;li&gt;將所有英文字母轉為小寫。&lt;/li&gt;
&lt;li&gt;移除第一個字元前的所有內容（ID 不能以數字或標點符號開頭）。&lt;/li&gt;
&lt;li&gt;如果剩下為空字串，則使用 &lt;code&gt;section&lt;/code&gt; 作為 ID。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下是一些範例，&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Header&lt;/th&gt;
&lt;th&gt;Identifier&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Header identifiers in HTML&lt;/td&gt;
&lt;td&gt;&lt;code&gt;header-identifiers-in-html&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Dogs&lt;/em&gt;?--in &lt;em&gt;my&lt;/em&gt; house?&lt;/td&gt;
&lt;td&gt;&lt;code&gt;dogs--in-my-house&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[HTML], [S5], or [RTF]?&lt;/td&gt;
&lt;td&gt;&lt;code&gt;html-s5-or-rtf&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3. Applications&lt;/td&gt;
&lt;td&gt;&lt;code&gt;applications&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;&lt;code&gt;section&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在大多數情況下，這些規則應該讓人能夠直接從標題文字推導出 ID。唯一的例外是當有多個標題具有同樣文字的情況；在這情況下，第一個標題的 ID 仍舊是透過以上規則推導而得；第二個則是在同樣 ID 後加上 &lt;code&gt;-1&lt;/code&gt;；第三個加上 &lt;code&gt;-2&lt;/code&gt;；以此類推。&lt;/p&gt;
&lt;p&gt;在開啟 &lt;code&gt;--toc|--table-of-contents&lt;/code&gt; 的選項時，這些 ID 是用來產生目錄 (Table of Contents) 所需的頁面連結。此外，這些 ID 也提供了一個簡便的方式來輸入跳到指定章節的連結。一個以 ID 產生的連結，其使用的語法看起來就像下面的例子：
    :::md
    See the section on
    &lt;a href="#header-identifiers-in-html-latex-and-context"&gt;header identifiers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;然而要注意的一點是，只有在以 HTML、LaTeX 與 ConTeXt 格式輸出時，才能以這種方式產生對應的章節連結。&lt;/p&gt;
&lt;p&gt;如果指定了 &lt;code&gt;--section-divs&lt;/code&gt; 選項，則每一個小節都會以 &lt;code&gt;div&lt;/code&gt; 標籤包住（或是 &lt;code&gt;section&lt;/code&gt; 標籤，如果有指定 &lt;code&gt;--html5&lt;/code&gt; 選項的話），並且 ID 會被附加在用來包住小節的 &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt;（或是 &lt;code&gt;&amp;lt;section&amp;gt;&lt;/code&gt;）標籤，而非附加在標題上。這使得整個小節都可以透過 javascript 來操作，或是採用不同的 CSS 設定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;implicit_header_references&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc 假設每個標題都定義了其參考連結，因此，相較於以下的連結語法&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你也可以單純只寫&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;][]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;section&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果有多個標題具有同樣文字，對應的參考只會連結到第一個符合的標題，這時若要連結到其他符合的標題，就必須以先前提到的方式，明確指定連結到該標題的 ID。&lt;/p&gt;
&lt;p&gt;與其他一般參考連結不同的是，這些參考連結是大小寫有別的。&lt;/p&gt;
&lt;p&gt;注意：如果你有明確定義了任何一個標題的標示符，那麼選項 &lt;code&gt;implicit_header_references&lt;/code&gt; 就沒有作用。&lt;/p&gt;
&lt;h2 id="qu kuai yin yan_1"&gt;區塊引言&lt;/h2&gt;
&lt;p&gt;Markdown 使用 email 的習慣來建立引言區塊。一個引言區塊可以由一或多個段落或其他的區塊元素（如清單或標題）組成，並且其行首均是由一個 &lt;code&gt;&amp;gt;&lt;/code&gt; 符號加上一個空白作為開頭。（&lt;code&gt;&amp;gt;&lt;/code&gt; 符號不一定要位在該行最左邊，但也不能縮進超過三個空白）。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; This is a block quote. This
&amp;amp;gt; paragraph has two lines.
&amp;amp;gt;
&amp;amp;gt; 1. This is a list inside a block quote.
&amp;amp;gt; 2. Second item.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;有一個「偷懶」的形式：你只需要在引言區塊的第一行行首輸入 &lt;code&gt;&amp;gt;&lt;/code&gt; 即可，後面的行首可以省略符號：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; This is a block quote. This
paragraph has two lines.

&amp;amp;gt; 1. This is a list inside a block quote.
2. Second item.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;由於區塊引言可包含其他區塊元素，而區塊引言本身也是區塊元素，所以，引言是可以嵌套入其他引言的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; This is a block quote.
&amp;amp;gt;
&amp;amp;gt; &amp;amp;gt; A block quote within a block quote.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;blank_before_blockquote&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原始 markdown 語法在區塊引言之前並不需要預留空白行。Pandoc 則需要（除非區塊引言位於文件最開始的地方）。這是因為以 &lt;code&gt;&amp;gt;&lt;/code&gt; 符號開頭的情況在一般文字段落中相當常見（也許由於斷行所致），這會導致非預期的格式。因此，除非是指定為 &lt;code&gt;markdown_strict&lt;/code&gt; 格式，不然以下的語法在 pandoc 中將不會產生出嵌套區塊引言：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; This is a block quote.
&amp;amp;gt;&amp;amp;gt; Nested.
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="zi mian (dai ma )qu kuai"&gt;字面（代碼）區塊&lt;/h2&gt;
&lt;h3 id="suo jin dai ma qu kuai"&gt;縮進代碼區塊&lt;/h3&gt;
&lt;p&gt;一段以四個空白（或一個 tab）縮進的文字區塊會被視為字面區塊 (Verbatim Block)：換句話說，特殊字元並不會轉換為任何格式，單純只以字面形式呈現，而所有的空白與換行也都會被保留。例如，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    if (a &amp;amp;gt; 3) {
      moveShip(5 * gravity, DOWN);
    }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;位於行首的縮排（四個空白或一個 tab）並不會被視為字面區塊的一部分，因此在輸出時會被移除掉。&lt;/p&gt;
&lt;p&gt;注意：在字面文字之間的空白行並不需要也以四個空白字元做開頭。&lt;/p&gt;
&lt;h3 id="wei lan dai ma qu kuai"&gt;圍欄代碼區塊&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;fenced_code_blocks&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了標準的縮進代碼區塊外，Pandoc 也支援了&lt;strong&gt;圍欄&lt;/strong&gt; (&lt;em&gt;fenced&lt;/em&gt;) 代碼區塊的語法。這區塊需以包含三個以上波浪線 (&lt;code&gt;~&lt;/code&gt;) 或反引號 (&lt;code&gt;`&lt;/code&gt;) 的一行作為開始，並以同樣符號且至少同樣長度的一行作為結束。所有介於開始與結束之間的文字行都會視為代碼。不需要額外的縮進：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~~~~~~~
if (a &amp;amp;gt; 3) {
  moveShip(5 * gravity, DOWN);
}
~~~~~~~
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如同一般的代碼區塊，圍欄代碼區塊與其前後的文字之間必須以空白行作間隔。&lt;/p&gt;
&lt;p&gt;如果代碼本身也包含了一整行的波浪線或反引號，那麼只要在區塊首尾處使用更長的波浪線或反引號即可：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~~~~~~~~~~~~~~~~
~~~~~~~~~~
code including tildes
~~~~~~~~~~
~~~~~~~~~~~~~~~~
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你也可以選擇性地使用以下語法附加屬性到代碼區塊上：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;~~~~&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;#mycode&lt;/span&gt; &lt;span class="err"&gt;.haskell&lt;/span&gt; &lt;span class="err"&gt;.numberLines&lt;/span&gt; &lt;span class="err"&gt;startFrom="100"&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="nt"&gt;qsort&lt;/span&gt; &lt;span class="cp"&gt;[]&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[]&lt;/span&gt;
&lt;span class="nt"&gt;qsort&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;qsort&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;
               &lt;span class="nt"&gt;qsort&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;=&lt;/span&gt; &lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這裡的 &lt;code&gt;mycode&lt;/code&gt; 為 ID，&lt;code&gt;haskell&lt;/code&gt; 與 &lt;code&gt;numberLines&lt;/code&gt; 是類別，而 &lt;code&gt;startsFrom&lt;/code&gt; 則是值為 &lt;code&gt;100&lt;/code&gt; 的屬性。有些輸出格式可以利用這些資訊來作語法高亮。目前有使用到這些資訊的輸出格式僅有 HTML 與 LaTeX。如果指定的輸出格式及語言類別有支援語法高亮，那麼上面那段代碼區塊將會以高亮並帶有行號的方式呈現。（要查詢支援的程式語言清單，可在命令列輸入 &lt;code&gt;pandoc --version&lt;/code&gt;。）反之若無支援，則上面那段代碼區塊則會以下面的形式呈現：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;lt;pre id="mycode" class="haskell numberLines" startFrom="100"&amp;amp;gt;
  &amp;amp;lt;code&amp;amp;gt;
  ...
  &amp;amp;lt;/code&amp;amp;gt;
&amp;amp;lt;/pre&amp;amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;下面這個是針對代碼區塊只有指定程式語言屬性的簡便形式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;```haskell
qsort [] = []
```
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這與下面這行的效果是相同的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;``` {.haskell}
qsort [] = []
```
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;要取消所有語法高亮，使用 &lt;code&gt;--no-highlight&lt;/code&gt; 選項。要設定語法高亮的配色，則使用 &lt;code&gt;--highlight-style&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id="xing qu kuai_1"&gt;行區塊&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;line_blocks&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;行區塊是一連串以豎線 (&lt;code&gt;|&lt;/code&gt;) 加上一個空格所構成的連續行。行與行間的區隔在輸出時將會以原樣保留，行首的空白字元數目也一樣會被保留；反之，這些行將會以 markdown 的格式處理。這個語法在輸入詩句或地址時很有幫助。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;| The limerick packs laughs anatomical
| In space that is quite economical.
|    But the good ones I've seen
|    So seldom are clean
| And the clean ones so seldom are comical

| 200 Main St.
| Berkeley, CA 94718
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果有需要的話，書寫時也可以將完整一行拆成多行，但後續行必須以空白作為開始。下面範例的前兩行在輸出時會被視為一整行：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;| The Right Honorable Most Venerable and Righteous Samuel L.
  Constable, Jr.
| 200 Main St.
| Berkeley, CA 94718
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這是從 &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/introduction.html"&gt;reStructuredText&lt;/a&gt; 借來的語法。&lt;/p&gt;
&lt;h2 id="qing dan"&gt;清單&lt;/h2&gt;
&lt;h3 id="wu xu qing dan"&gt;無序清單&lt;/h3&gt;
&lt;p&gt;無序清單是以項目符號作列舉的清單。每條項目都以項目符號 (&lt;code&gt;*&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt; 或 &lt;code&gt;-&lt;/code&gt;) 作開頭。下面是個簡單的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* one
* two
* three
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這會產生一個「緊湊」清單。如果你想要一個「寬鬆」清單，也就是說以段落格式處理每個項目內的文字內容，那麼只要在每個項目間加上空白行即可：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* one

* two

* three
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;項目符號不能直接從行首最左邊處輸入，而必須以一至三個空白字元作縮進。項目符號後必須跟著一個空白字元。&lt;/p&gt;
&lt;p&gt;清單項目中的接續行，若與該項目的第一行文字對齊（在項目符號之後），看上去會較為美觀：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* here is my first
  list item.
* and my second.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;但 markdown 也允許以下「偷懶」的格式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* here is my first
list item.
* and my second.
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="si ge kong bai gui ze"&gt;四個空白規則&lt;/h3&gt;
&lt;p&gt;一個清單項目可以包含多個段落以及其他區塊等級的內容。然而，後續的段落必須接在空白行之後，並且以四個空白或一個 tab 作縮進。因此，如果項目裡第一個段落與後面段落對齊的話（也就是項目符號前置入兩個空白），看上去會比較整齊美觀：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  * First paragraph.

    Continued.

  * Second paragraph. With a code block, which must be indented
    eight spaces:

        { code }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;清單項目也可以包含其他清單。在這情況下前置的空白行是可有可無的。嵌套清單必須以四個空白或一個 tab 作縮進：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* fruits
    + apples
        - macintosh
        - red delicious
    + pears
    + peaches
* vegetables
    + brocolli
    + chard
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上一節提到，markdown 允許你以「偷懶」的方式書寫，項目的接續行可以不和第一行對齊。不過，如果一個清單項目中包含了多個段落或是其他區塊元素，那麼每個元素的第一行都必須縮進對齊。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+ A lazy, lazy, list
item.

+ Another one; this looks
bad but is legal.

    Second paragraph of second
list item.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;儘管針對接續段落的「四個空白規則」是出自於官方的 &lt;a href=""&gt;markdown syntax guide&lt;/a&gt;，但是作為對應參考用的 &lt;code&gt;Markdown.pl&lt;/code&gt; 實作版本中並未遵循此一規則。所以當輸入時若接續段落的縮進少於四個空白時，pandoc 所輸出的結果會與 &lt;code&gt;Markdown.pl&lt;/code&gt; 的輸出有所出入。&lt;/p&gt;
&lt;p&gt;在 &lt;a href=""&gt;markdown syntax guide&lt;/a&gt; 中並未明確表示「四個空白規則」是否一體適用於 &lt;strong&gt;所有&lt;/strong&gt; 位於清單項目裡的區塊元素上；規範文件中只提及了段落與代碼區塊。但文件暗示了此規則適用於所有區塊等級的內容（包含嵌套清單），並且 pandoc 以此方向進行解讀與實作。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;http://daringfireball.net/projects/markdown/syntax#list
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="you xu qing dan"&gt;有序清單&lt;/h3&gt;
&lt;p&gt;有序清單與無序清單相類似，唯一的差別在於清單項目是以列舉編號作開頭，而不是項目符號。&lt;/p&gt;
&lt;p&gt;在原始 markdown 中，列舉編號是阿拉伯數字後面接著一個句點與空白。數字本身代表的數值會被忽略，因此下面兩個清單並無差別：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1.  one
2.  two
3.  three
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上下兩個清單的輸出是相同的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;5.  one
7.  two
1.  three
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;fancy_lists&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;與原始 markdown 不同的是，Pandoc 除了使用阿拉伯數字作為有序清單的編號外，也可以使用大寫或小寫的英文字母，以及羅馬數字。清單標記可以用括號包住，也可以單獨一個右括號，抑或是句號。如果清單標記是大寫字母接著一個句號，句號後請使用至少兩個空白字元。&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;startnum&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了清單標記外，Pandoc 也能判讀清單的起始編號，這兩項資訊都會保留於輸出格式中。舉例來說，下面的輸入可以產生一個從編號 9 開始，以單括號為編號標記的清單，底下還跟著一個小寫羅馬數字的子清單：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; 9)  Ninth
10)  Tenth
11)  Eleventh
       i. subone
      ii. subtwo
     iii. subthree
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;當遇到不同形式的清單標記時，Pandoc 會重新開始一個新的清單。所以，以下的輸入會產生三份清單：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(2) Two
(5) Three
1.  Four
*   Five
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果需要預設的有序清單標記符號，可以使用 &lt;code&gt;#.&lt;/code&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#.  one
#.  two
#.  three
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="ding yi qing dan"&gt;定義清單&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;definition_lists&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc 支援定義清單，其語法的靈感來自於 &lt;a href="http://www.michelf.com/projects/php-markdown/extra/"&gt;PHP Markdown Extra&lt;/a&gt; 以及 &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/introduction.html"&gt;reStructuredText&lt;/a&gt;：&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Term 1

:   Definition 1

Term 2 with *inline markup*

:   Definition 2

        { some code, part of Definition 2 }

    Third paragraph of definition 2.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;每個專有名詞 (term) 都必須單獨存在於一行，後面可以接著一個空白行，也可以省略，但一定要接上一或多筆定義內容。一筆定義需由一個冒號或波浪線作開頭，可以接上一或兩個空白作為縮進。定義本身的內容主體（包括接在冒號或波浪線後的第一行）應該以四個空白縮進。一個專有名詞可以有多個定義，而每個定義可以包含一或多個區塊元素（段落、代碼區塊、清單等），每個區塊元素都要縮進四個空白或一個 tab。&lt;/p&gt;
&lt;p&gt;如果你在定義內容後面留下空白行（如同上面的範例），那麼該段定義會被當作段落處理。在某些輸出格式中，這意謂著成對的專有名詞與定義內容間會有較大的空白間距。在定義與定義之間，以及定義與下個專有名詞間不要留空白行，即可產生一個比較緊湊的定義清單：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Term 1
  ~ Definition 1
Term 2
  ~ Definition 2a
  ~ Definition 2b
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="bian hao fan li qing dan"&gt;編號範例清單&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;example_lists&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;這個特別的清單標記 &lt;code&gt;@&lt;/code&gt; 可以用來產生連續編號的範例清單。清單中第一個以 &lt;code&gt;@&lt;/code&gt; 標記的項目會被編號為 '1'，接著編號為 '2'，依此類推，直到文件結束。範例項目的編號不會侷限於單一清單中，而是文件中所有以 &lt;code&gt;@&lt;/code&gt; 為標記的項目均會次序遞增其編號，直到最後一個。舉例如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(@)  My first example will be numbered (1).
(@)  My second example will be numbered (2).

Explanation of examples.

(@)  My third example will be numbered (3).
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;編號範例可以加上標籤，並且在文件的其他地方作參照：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(@good)  This is a good example.

As (@good) illustrates, ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;標籤可以是由任何英文字母、底線或是連字符號所組成的字串。&lt;/p&gt;
&lt;h3 id="jin cou yu kuan song qing dan"&gt;緊湊與寬鬆清單&lt;/h3&gt;
&lt;p&gt;在與清單相關的「邊界處理」上，Pandoc 與 &lt;code&gt;Markdown.pl&lt;/code&gt; 有著不同的處理結果。考慮如下代碼：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+   First
+   Second:
    -   Fee
    -   Fie
    -   Foe

+   Third
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Pandoc 會將以上清單轉換為「緊湊清單」（在 "First", "Second" 或 "Third" 之中沒有 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 標籤），而 markdown 則會在 "Second" 與 "Third" （但不包含 "First"）裡面置入 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 標籤，這是因為 "Third" 之前的空白行而造成的結果。Pandoc 依循著一個簡單規則：如果文字後面跟著空白行，那麼就會被視為段落。既然 "Second" 後面是跟著一個清單，而非空白行，那麼就不會被視為段落了。至於子清單的後面是不是跟著空白行，那就無關緊要了。（注意：即使是設定為 &lt;code&gt;markdown_strict&lt;/code&gt; 格式，Pandoc 仍是依以上方式處理清單項目是否為段落的判定。這個處理方式與 markdown 官方語法規範裡的描述一致，然而卻與 &lt;code&gt;Markdown.pl&lt;/code&gt; 的處理不同。）&lt;/p&gt;
&lt;h3 id="jie shu yi ge qing dan"&gt;結束一個清單&lt;/h3&gt;
&lt;p&gt;如果你在清單之後放入一個縮排的代碼區塊，會有什麼結果？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-   item one
-   item two

    { my code block }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;問題大了！這邊 pandoc（其他的 markdown 實作也是如此）會將 &lt;code&gt;{ my code block }&lt;/code&gt; 視為 &lt;code&gt;item two&lt;/code&gt; 這個清單項目的第二個段落來處理，而不會將其視為一個代碼區塊。&lt;/p&gt;
&lt;p&gt;要在 &lt;code&gt;item two&lt;/code&gt; 之後「切斷」清單，你可以插入一些沒有縮排、輸出時也不可見的內容，例如 HTML 的註解：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-   item one
-   item two

&amp;amp;lt;!-- end of list --&amp;amp;gt;

    { my code block }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;當你想要兩個各自獨立的清單，而非一個大且連續的清單時，也可以運用同樣的技巧：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1.  one
2.  two
3.  three

&amp;amp;lt;!-- --&amp;amp;gt;

1.  uno
2.  dos
3.  tres
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="fen ge xian_1"&gt;分隔線&lt;/h2&gt;
&lt;p&gt;一行中若包含三個以上的 &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 符號（中間可以以空白字元分隔），則會產生一條分隔線：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;*  *  *  *

---------------
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="biao ge"&gt;表格&lt;/h2&gt;
&lt;p&gt;有四種表格的形式可以使用。前三種適用於等寬字型的編輯環境，例如 Courier。第四種則不需要直行的對齊，因此可以在比例字型的環境下使用。&lt;/p&gt;
&lt;h3 id="jian dan biao ge"&gt;簡單表格&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;simple_tables&lt;/code&gt;, &lt;code&gt;table_captions&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;簡單表格看起來像這樣子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  Right     Left     Center     Default
-------     ------ ----------   -------
     12     12        12            12
    123     123       123          123
      1     1          1             1

Table:  Demonstration of simple table syntax.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;表頭與資料列分別以一行為單位。直行的對齊則依照表頭的文字和其底下虛線的相對位置來決定：&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果虛線與表頭文字的右側有切齊，而左側比表頭文字還長，則該直行為靠右對齊。&lt;/li&gt;
&lt;li&gt;如果虛線與表頭文字的左側有切齊，而右側比表頭文字還長，則該直行為靠左對齊。&lt;/li&gt;
&lt;li&gt;如果虛線的兩側都比表頭文字長，則該直行為置中對齊。&lt;/li&gt;
&lt;li&gt;如果虛線與表頭文字的兩側都有切齊，則會套用預設的對齊方式（在大多數情況下，這將會是靠左對齊）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;表格底下必須接著一個空白行，或是一行虛線後再一個空白行。表格標題為可選的（上面的範例中有出現）。標題需是一個以 &lt;code&gt;Table:&lt;/code&gt; （或單純只有 &lt;code&gt;:&lt;/code&gt;）開頭作為前綴的段落，輸出時前綴的這部份會被去除掉。表格標題可以放在表格之前或之後。&lt;/p&gt;
&lt;p&gt;表頭也可以省略，在省略表頭的情況下，表格下方必須加上一行虛線以清楚標明表格的範圍。例如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------     ------ ----------   -------
     12     12        12             12
    123     123       123           123
      1     1          1              1
-------     ------ ----------   -------
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;當省略表頭時，直行的對齊會以表格內容的第一行資料列決定。所以，以上面的表格為例，各直行的對齊依序會是靠右、靠左、置中以及靠右對齊。&lt;/p&gt;
&lt;h3 id="duo xing biao ge"&gt;多行表格&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;multiline_tables&lt;/code&gt;, &lt;code&gt;table_captions&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;多行表格允許表頭與表格資料格的文字能以複數行呈現（但不支援橫跨多欄或縱跨多列的資料格）。以下為範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------------------------------------------------------------
 Centered   Default           Right Left
  Header    Aligned         Aligned Aligned
----------- ------- --------------- -------------------------
   First    row                12.0 Example of a row that
                                    spans multiple lines.

  Second    row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.
-------------------------------------------------------------

Table: Here's the caption. It, too, may span
multiple lines.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;看起來很像簡單表格，但兩者間有以下差別：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在表頭文字之前，必須以一列虛線作為開頭（除非有省略表頭）。&lt;/li&gt;
&lt;li&gt;必須以一列虛線作為表格結尾，之後接一個空白行。&lt;/li&gt;
&lt;li&gt;資料列與資料列之間以空白行隔開。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在多行表格中，表格分析器會計算各直行的欄寬，並在輸出時盡可能維持各直行在原始文件中的相對比例。因此，要是你覺得某些欄位在輸出時不夠寬，你可以在 markdown 的原始檔中加寬一點。&lt;/p&gt;
&lt;p&gt;和簡單表格一樣，表頭在多行表格中也是可以省略的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;----------- ------- --------------- -------------------------
   First    row                12.0 Example of a row that
                                    spans multiple lines.

  Second    row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.
----------- ------- --------------- -------------------------

: Here's a multiline table without headers.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;多行表格中可以單只包含一個資料列，但該資料列之後必須接著一個空白行（然後才是標示表格結尾的一行虛線）。如果沒有此空白行，此表格將會被解讀成簡單表格。&lt;/p&gt;
&lt;h3 id="ge kuang biao ge"&gt;格框表格&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;grid_tables&lt;/code&gt;, &lt;code&gt;table_captions&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;格框表格看起來像這樣：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;: Sample grid table.

+---------------+---------------+--------------------+
| Fruit         | Price         | Advantages         |
+===============+===============+====================+
| Bananas       | $1.34         | - built-in wrapper |
|               |               | - bright color     |
+---------------+---------------+--------------------+
| Oranges       | $2.10         | - cures scurvy     |
|               |               | - tasty            |
+---------------+---------------+--------------------+
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;以 &lt;code&gt;=&lt;/code&gt; 串成的一行區分了表頭與表格本體，這在沒有表頭的表格中也是可以省略的。在格框表格中的資料格可以包含任意的區塊元素（複數段落、代碼區塊、清單等等）。不支援對齊，也不支援橫跨多欄或縱跨多列的資料格。格框表格可以在 &lt;a href="http://table.sourceforge.net/"&gt;Emacs table mode&lt;/a&gt; 下輕鬆建立。&lt;/p&gt;
&lt;h3 id="guan xian biao ge"&gt;管線表格&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;pipe_tables&lt;/code&gt;, &lt;code&gt;table_captions&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;管線表格看起來像這樣：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;Right&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;Left&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;Default&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;Center&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="s s-Atom"&gt;------:&lt;/span&gt;&lt;span class="p"&gt;|:-&lt;/span&gt;&lt;span class="s s-Atom"&gt;----&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="s s-Atom"&gt;---------&lt;/span&gt;&lt;span class="p"&gt;|:-&lt;/span&gt;&lt;span class="s s-Atom"&gt;-----:&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;    &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;    &lt;span class="mi"&gt;12&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="mi"&gt;123&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="mi"&gt;123&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="mi"&gt;123&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="mi"&gt;123&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt;

  &lt;span class="s s-Atom"&gt;:&lt;/span&gt; &lt;span class="nv"&gt;Demonstration&lt;/span&gt; &lt;span class="s s-Atom"&gt;of&lt;/span&gt; &lt;span class="s s-Atom"&gt;simple&lt;/span&gt; &lt;span class="s s-Atom"&gt;table&lt;/span&gt; &lt;span class="s s-Atom"&gt;syntax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這個語法與 &lt;a href=""&gt;PHP markdown extra 中的表格語法&lt;/a&gt; 相同。開始與結尾的管線字元是可選的，但各直行間則必須以管線區隔。上面範例中的冒號表明了對齊方式。表頭可以省略，但表頭下的水平虛線必須保留，因為虛線上定義了資料欄的對齊方式。&lt;/p&gt;
&lt;p&gt;因為管線界定了各欄之間的邊界，表格的原始碼並不需要像上面例子中各欄之間保持直行對齊。所以，底下一樣是個完全合法（雖然醜陋）的管線表格：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fruit| price
-----|-----:
apple|2.05
pear|1.37
orange|3.09
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;管線表格的資料格不能包含如段落、清單之類的區塊元素，也不能包含複數行文字。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;http://michelf.ca/projects/php-markdown/extra/#table
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意：Pandoc 也可以看得懂以下形式的管線表格，這是由 Emacs 的 orgtbl-mod 所繪製：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;| One | Two   |
|-----+-------|
| my  | table |
| is  | nice  |
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;主要的差別在於以 &lt;code&gt;+&lt;/code&gt; 取代了部分的 &lt;code&gt;|&lt;/code&gt;。其他的 orgtbl 功能並未支援。如果要指定非預設的直行對齊形式，你仍然需要在上面的表格中自行加入冒號。&lt;/p&gt;
&lt;h2 id="wen jian biao ti qu kuai_1"&gt;文件標題區塊&lt;/h2&gt;
&lt;p&gt;（譯註：本節中提到的「標題」均指 Title，而非 Headers）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;pandoc_title_block&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果檔案以文件標題（Title）區塊開頭&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% title&lt;/span&gt;
&lt;span class="c"&gt;% author(s) (separated by semicolons)&lt;/span&gt;
&lt;span class="c"&gt;% date&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這部份將不會作為一般文字處理，而會以書目資訊的方式解析。（這可用在像是單一 LaTeX 或是 HTML 輸出文件的書名上。）這個區塊僅能包含標題，或是標題與作者，或是標題、作者與日期。如果你只想包含作者卻不想包含標題，或是只有標題與日期而沒有作者，你得利用空白行：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;%&lt;/span&gt;
&lt;span class="c"&gt;% Author&lt;/span&gt;

&lt;span class="c"&gt;% My title&lt;/span&gt;
&lt;span class="c"&gt;%&lt;/span&gt;
&lt;span class="c"&gt;% June 15, 2006&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;標題可以包含多行文字，但接續行必須以空白字元開頭，像是：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% My title&lt;/span&gt;
  &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;multiple&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果文件有多個作者，作者也可以分列在不同行並以空白字元作開頭，或是以分號間隔，或是兩者並行。所以，下列各種寫法得到的結果都是相同的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% Author One&lt;/span&gt;
  &lt;span class="n"&gt;Author&lt;/span&gt; &lt;span class="n"&gt;Two&lt;/span&gt;

&lt;span class="c"&gt;% Author One; Author Two&lt;/span&gt;

&lt;span class="c"&gt;% Author One;&lt;/span&gt;
  &lt;span class="n"&gt;Author&lt;/span&gt; &lt;span class="n"&gt;Two&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;日期就只能寫在一行之內。&lt;/p&gt;
&lt;p&gt;所有這三個 metadata 欄位都可以包含標準的行內格式（斜體、連結、腳註等等）。&lt;/p&gt;
&lt;p&gt;文件標題區塊一定會被分析處理，但只有在 &lt;code&gt;--standaline&lt;/code&gt; (&lt;code&gt;-s&lt;/code&gt;) 選項被設定時才會影響輸出內容。在輸出 HTML 時，文件標題會出現的地方有兩個：一個是在文件的 &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; 區塊裡－－這會顯示在瀏覽器的視窗標題上－－另外一個是文件的 &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; 區塊最前面。位於 &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; 裡的文件標題可以選擇性地加上前綴文字（透過 &lt;code&gt;--title-prefix&lt;/code&gt; 或 &lt;code&gt;-T&lt;/code&gt; 選項）。而在 &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; 裡的文件標題會以 H1 元素呈現，並附帶 "title" 類別 (class)，這樣就能藉由 CSS 來隱藏顯示或重新定義格式。如果以 &lt;code&gt;-T&lt;/code&gt; 選項指定了標題前綴文字，卻沒有設定文件標題區塊裡的標題，那麼前綴文字本身就會被當作是 HTML 的文件標題。&lt;/p&gt;
&lt;p&gt;而 man page 的輸出器會分析文件標題區塊的標題行，以解出標題、man page section number，以及其他頁眉 (header) 頁腳 (footer) 所需要的資訊。一般會假設標題行的第一個單字為標題，標題後也許會緊接著一個以括號包住的單一數字，代表 section number（標題與括號之間沒有空白）。在此之後的其他文字則為頁腳與頁眉文字。頁腳與頁眉文字之間是以單獨的一個管線符號 (&lt;code&gt;|&lt;/code&gt;) 作為區隔。所以，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% PANDOC(1)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;將會產生一份標題為 &lt;code&gt;PANDOC&lt;/code&gt; 且 section 為 1 的 man page。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% PANDOC(1) Pandoc User Manuals&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;產生的 man page 會再加上 "Pandoc User Manuals" 在頁腳處。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;% PANDOC(1) Pandoc User Manuals | Version 4.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;產生的 man page 會再加上 "Version 4.0" 在頁眉處。&lt;/p&gt;
&lt;h2 id="fan xie xian tiao tuo zi yuan"&gt;反斜線跳脫字元&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;all_symbols_escapable&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了在代碼區塊或行內代碼之外，任何標點符號或空白字元前面只要加上一個反斜線，都能使其保留字面原義，而不會進行格式的轉義解讀。因此，舉例來說，下面的寫法&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;*\*hello\**
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;輸出後會得到&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;lt;em&amp;amp;gt;*hello*&amp;amp;lt;/em&amp;amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;而不是&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;lt;strong&amp;amp;gt;hello&amp;amp;lt;/strong&amp;amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這條規則比原始的 markdown 規則來得好記許多，原始規則中，只有以下字元才有支援反斜線跳脫，不作進一步轉義：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\`*_{}[]()&amp;amp;gt;#+-.!
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;（然而，如果使用了 &lt;code&gt;markdown_strict&lt;/code&gt; 格式，那麼就會採用原始的 markdown 規則）&lt;/p&gt;
&lt;p&gt;一個反斜線之後的空白字元會被解釋為不斷行的空白 (nonbreaking space)。這在 TeX 的輸出中會顯示為 &lt;code&gt;~&lt;/code&gt;，而在 HTML 與 XML 則是顯示為 &lt;code&gt;\&amp;amp;#160;&lt;/code&gt; 或 &lt;code&gt;\&amp;amp;nbsp;&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;一個反斜線之後的換行字元（例如反斜線符號出現在一行的最尾端）則會被解釋為強制換行。這在 TeX 的輸出中會顯示為 &lt;code&gt;\\&lt;/code&gt;，而在 HTML 裡則是 &lt;code&gt;&amp;lt;br /&amp;gt;&lt;/code&gt;。相對於原始 markdown 是以在行尾加上兩個空白字元這種「看不見」的方式進行強制換行，反斜線接換行字元會是比較好的替代方案。&lt;/p&gt;
&lt;p&gt;反斜線跳脫字元在代碼上下文中不起任何作用。&lt;/p&gt;
&lt;h2 id="zhi hui xing biao dian fu hao"&gt;智慧型標點符號&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果指定了 &lt;code&gt;--smart&lt;/code&gt; 選項，pandoc 將會輸出正式印刷用的標點符號，像是將 straight quotes 轉換為 curly quotes&lt;sup id="fnref:T1"&gt;&lt;a class="footnote-ref" href="#fn:T1" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;、&lt;code&gt;---&lt;/code&gt; 轉為破折號 (em-dashes)，&lt;code&gt;--&lt;/code&gt; 轉為連接號 (en-dashes)，以及將 &lt;code&gt;...&lt;/code&gt; 轉為刪節號。不斷行空格 (Nonbreaking spaces) 將會插入某些縮寫詞之後，例如 "Mr."。&lt;/p&gt;
&lt;p&gt;注意：如果你的 LaTeX template 使用了 &lt;code&gt;csquotes&lt;/code&gt; 套件，pandoc 會自動偵測並且使用 &lt;code&gt;\enquote{...}&lt;/code&gt; 在引言文字上。&lt;/p&gt;
&lt;h2 id="xing nei ge shi"&gt;行內格式&lt;/h2&gt;
&lt;h3 id="qiang diao"&gt;強調&lt;/h3&gt;
&lt;p&gt;要 &lt;em&gt;強調&lt;/em&gt; 某些文字，只要以 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 符號前後包住即可，像這樣：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This text is _emphasized with underscores_, and this
is *emphasized with asterisks*.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;重複兩個 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 符號以產生 &lt;strong&gt;更強烈的強調&lt;/strong&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This is **strong emphasis** and __with underscores__.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;一個前後以空白字元包住，或是前面加上反斜線的 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 符號，都不會轉換為強調格式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This is * not emphasized *, and \*neither is this\*.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;intraword_underscores&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因為 &lt;code&gt;_&lt;/code&gt; 字元有時會使用在單字或是 ID 之中，所以 pandoc 不會把被字母包住的 &lt;code&gt;_&lt;/code&gt; 解讀為強調標記。如果有需要特別強調單字中的一部分，就用 &lt;code&gt;*&lt;/code&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;feas*ible*, not feas*able*.
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="shan chu xian"&gt;刪除線&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension:  &lt;code&gt;strikeout&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要將一段文字加上水平線作為刪除效果，將該段文字前後以 &lt;code&gt;~~&lt;/code&gt; 包住即可。例如，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This ~~is deleted text.~~
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="shang biao yu xia biao"&gt;上標與下標&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;superscript&lt;/code&gt;, &lt;code&gt;subscript&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要輸入上標可以用 &lt;code&gt;^&lt;/code&gt; 字元將要上標的文字包起來；要輸入下標可以用 &lt;code&gt;~&lt;/code&gt; 字元將要下標的文字包起來。直接看範例，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;H~2~O is a liquid.  2^10^ is 1024.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果要上標或下標的文字中包含了空白，那麼這個空白字元之前必須加上反斜線。（這是為了避免一般使用下的 &lt;code&gt;~&lt;/code&gt; 和 &lt;code&gt;^&lt;/code&gt; 在非預期的情況下產生出意外的上標或下標。）所以，如果你想要讓字母 P 後面跟著下標文字 'a cat'，那麼就要輸入 &lt;code&gt;P~a\ cat~&lt;/code&gt;，而不是 &lt;code&gt;P~a cat~&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id="zi mian wen zi"&gt;字面文字&lt;/h3&gt;
&lt;p&gt;要讓一小段文字直接以其字面形式呈現，可以用反引號將其包住：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;What is the difference between `&amp;amp;gt;&amp;amp;gt;=` and `&amp;amp;gt;&amp;amp;gt;`?
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果字面文字中也包含了反引號，那就使用雙重反引號包住：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Here is a literal backtick `` ` ``.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;（在起始反引號後的空白以及結束反引號前的空白都會被忽略。）&lt;/p&gt;
&lt;p&gt;一般性的規則如下，字面文字區段是以連續的反引號字元作為開始（反引號後的空白字元為可選），一直到同樣數目的反引號字元出現才結束（反引號前的空白字元也為可選）。&lt;/p&gt;
&lt;p&gt;要注意的是，反斜線跳脫字元（以及其他 markdown 結構）在字面文字的上下文中是沒有效果的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This is a backslash followed by an asterisk: `\*`.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;inline_code_attributes&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;與[圍欄代碼區塊]一樣，字面文字也可以附加屬性：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;`&amp;amp;lt;$&amp;amp;gt;`{.haskell}
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="shu xue_1"&gt;數學&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;tex_math_dollars&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所有介於兩個 &lt;code&gt;$&lt;/code&gt; 字元之間的內容將會被視為 TeX 數學公式處理。開頭的 &lt;code&gt;$&lt;/code&gt; 右側必須立刻接上任意文字，而結尾 &lt;code&gt;$&lt;/code&gt; 的左側同樣也必須緊挨著文字。這樣一來，&lt;code&gt;$20,000 and $30,000&lt;/code&gt; 就不會被當作數學公式處理了。如果基於某些原因，有必須使用 &lt;code&gt;$&lt;/code&gt; 符號將其他文字括住的需求時，那麼可以在 &lt;code&gt;$&lt;/code&gt; 前使用反斜線跳脫字元，這樣 &lt;code&gt;$&lt;/code&gt; 就不會被當作數學公式的分隔符。&lt;/p&gt;
&lt;p&gt;TeX 數學公式會在所有輸出格式中印出。至於會以什麼方式演算編排 (render) 則取決於輸出的格式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Markdown, LaTeX, Org-Mode, ConTeXt
  ~ 公式會以字面文字呈現在兩個 &lt;code&gt;$&lt;/code&gt; 符號之間。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;reStructuredText
  ~ 公式會使用 &lt;a href="http://www.american.edu/econ/itex2mml/mathhack.rst"&gt;此處&lt;/a&gt; 所描述的 &lt;code&gt;:math:&lt;/code&gt; 這個 "interpreted text role" 來進行演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AsciiDoc
  ~ 公式會以 &lt;code&gt;latexmath:[...]&lt;/code&gt; 演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Texinfo
  ~ 公式會在 &lt;code&gt;@math&lt;/code&gt; 指令中演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;groff man
  ~ 公式會以去掉 &lt;code&gt;$&lt;/code&gt; 後的字面文字演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MediaWiki
  ~ 公式會在 &lt;code&gt;&amp;lt;math&amp;gt;&lt;/code&gt; 標籤中演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Textile
  ~ 公式會在 &lt;code&gt;&amp;lt;span class="math"&amp;gt;&lt;/code&gt; 標籤中演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RTF, OpenDocument, ODT
  ~ 如果可以的話，公式會以 unicode 字元演算編排，不然就直接使用字面字元。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Docbook
  ~ 如果使用了 &lt;code&gt;--mathml&lt;/code&gt; 旗標，公式就會在 &lt;code&gt;inlineequation&lt;/code&gt; 或 &lt;code&gt;informalequation&lt;/code&gt; 標籤中使用 mathml 演算編排。否則就會盡可能使用 unicode 字元演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Docx
  ~ 公式會以 OMML 數學標記的方式演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FictionBook2
  ~ 如果有使用 &lt;code&gt;--webtex&lt;/code&gt; 選項，公式會以 Google Charts 或其他相容的網路服務演算編排為圖片，並下載嵌入於電子書中。否則就會以字面文字顯示。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;HTML, Slidy, DZSlides, S5, EPUB
  ~ 公式會依照以下命令列選項的設置，以不同的方法演算編排為 HTML 代碼。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;預設方式是將 TeX 數學公式盡可能地以 unicode 字元演算編排，如同 RTF、DocBook 以及 OpenDocument 的輸出。公式會被放在附有屬性 &lt;code&gt;class="math"&lt;/code&gt; 的 &lt;code&gt;span&lt;/code&gt; 標籤內，所以可以在需要時給予不同的樣式，使其突出於周遭的文字內容。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--latexmathml&lt;/code&gt; 選項，TeX 數學公式會被顯示於 &lt;code&gt;$&lt;/code&gt; 或 &lt;code&gt;$$&lt;/code&gt; 字元中，並放在附帶 &lt;code&gt;LaTeX&lt;/code&gt; 類別的 &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; 標籤裡。這段內容會用 &lt;a href="http://math.etsu.edu/LaTeXMathML/"&gt;LaTeXMathML&lt;/a&gt; script 演算編排為數學公式。（這個方法無法適用於所有瀏覽器，但在 Firefox 中是有效的。在不支援 LaTeXMathML 的瀏覽器中，TeX 數學公式會單純的以兩個 &lt;code&gt;$&lt;/code&gt; 字元間的字面文字呈現。）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--jsmath&lt;/code&gt; 選項，TeX數學公式會放在 &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; 標籤（用於行內數學公式）或 &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; 標籤（用於區塊數學公式）中，並附帶類別屬性 &lt;code&gt;math&lt;/code&gt;。這段內容會使用 &lt;a href="http://www.math.union.edu/~dpvc/jsmath/"&gt;jsMath&lt;/a&gt; script 來演算編排。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--mimetex&lt;/code&gt; 選項，&lt;a href="http://www.forkosh.com/mimetex.html"&gt;mimeTeX&lt;/a&gt; CGI script 會被呼叫來產生每個 TeX 數學公式的圖片。這適用於所有瀏覽器。&lt;code&gt;--mimetex&lt;/code&gt; 選項有一個可選的 URL 參數。如果沒有指定 URL，它會假設 mimeTeX CGI script 的位置在 &lt;code&gt;/cgi-bin/mimetex.cig&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--gladtex&lt;/code&gt; 選項，TeX 數學公式在 HTML 的輸出中會被 &lt;code&gt;&amp;lt;eq&amp;gt;&lt;/code&gt; 標籤包住。產生的 &lt;code&gt;htex&lt;/code&gt; 檔案之後可以透過 &lt;a href="http://ans.hsh.no/home/mgg/gladtex/"&gt;gladTeX&lt;/a&gt; 處理，這會針對每個數學公式生成圖片，並於最後生成一個包含這些圖片連結的 &lt;code&gt;html&lt;/code&gt; 檔案。所以，整個處理流程如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pandoc -s --gladtex myfile.txt -o myfile.htex
gladtex -d myfile-images myfile.htex
# produces myfile.html and images in myfile-images
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--webtex&lt;/code&gt; 選項，TeX 數學公式會被轉換為 &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; 標籤並連結到一個用以轉換公式為圖片的外部 script。公式將會編碼為 URL 可接受格式並且與指定的 URL 參數串接。如果沒有指定 URL，那麼將會使用 Google Chart API (&lt;code&gt;http://chart.apis.google.com/chart?cht=tx&amp;amp;chl=&lt;/code&gt;)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用了 &lt;code&gt;--mathjax&lt;/code&gt; 選項，TeX 數學公式將會被包在 &lt;code&gt;\(...\)&lt;/code&gt;（用於行內數學公式）或 &lt;code&gt;\[...\]&lt;/code&gt;（用於區塊數學公式）之間顯示，並且放在附帶類別 &lt;code&gt;math&lt;/code&gt; 的 &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; 標籤之中。這段內容會使用 &lt;a href="http://www.mathjax.org/"&gt;MathJax&lt;/a&gt; script 演算編排為頁面上的數學公式。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="raw html"&gt;Raw HTML&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;raw_html&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Markdown 允許你在文件中的任何地方插入原始 HTML（或 DocBook）指令（除了在字面文字上下文處，此時的 &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt; 和 &lt;code&gt;&amp;amp;&lt;/code&gt; 都會按其字面意義顯示）。（技術上而言這不算擴充功能，因為原始 markdown 本身就有提供此功能，但做成擴充形式便可以在有特殊需要的時候關閉此功能。）&lt;/p&gt;
&lt;p&gt;輸出 HTML, S5, Slidy, Slideous, DZSlides, EPUB, Markdown 以及 Textile 等格式時，原始 HTML 代碼會不作修改地保留至輸出檔案中；而其他格式的輸出內容則會將原始 HTML 代碼去除掉。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;markdown_in_html_blocks&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原始 markdown 允許你插入 HTML「區塊」：所謂的 HTML 區塊是指，上下各由一個空白行所隔開，開始與結尾均由所在行最左側開始的一連串對稱均衡的 HTML 標籤。在這個區塊中，任何內容都會當作是 HTML 來分析，而不再視為 markdown；所以（舉例來說），&lt;code&gt;*&lt;/code&gt; 符號就不再代表強調。&lt;/p&gt;
&lt;p&gt;當指定格式為 &lt;code&gt;markdown_strict&lt;/code&gt; 時，Pandoc 會以上述方式處理；但預設情況下，Pandoc 能夠以 markdown 語法解讀 HTML 區塊標籤中的內容。舉例說明，Pandoc 能夠將底下這段&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;tr&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;td&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;*&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;td&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;td&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//google.com)&amp;amp;lt;/td&amp;amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;tr&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;轉換為&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;table&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;tr&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;td&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;em&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;one&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;em&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;td&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;td&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"http://google.com"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;a link&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;td&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;tr&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;table&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;而 &lt;code&gt;Markdown.pl&lt;/code&gt; 則是保留該段原樣。&lt;/p&gt;
&lt;p&gt;這個規則只有一個例外：那就是介於 &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; 與 &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; 之間的文字都不會被拿來當作 markdown 解讀。&lt;/p&gt;
&lt;p&gt;這邊與原始 markdown 的分歧，主要是為了讓 markdown 能夠更便利地混入 HTML 區塊元素。比方說，一段 markdown 文字可以用 &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; 標籤將其前後包住來進行樣式指定，而不用擔心裡面的 markdown 不會被解譯到。&lt;/p&gt;
&lt;h2 id="raw tex"&gt;Raw TeX&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;raw_tex&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了 HTML 之外，pandoc 也接受文件中嵌入原始 LaTeX, TeX 以及 ConTeXt 代碼。行內 TeX 指令會被保留並不作修改地輸出至 LaTeX 與 ConTeXt 格式中。所以，舉例來說，你可以使用 LaTeX 來導入 BibTeX 的引用文獻：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This result was proved in \cite{jones.1967}.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;請注意在 LaTeX 環境下時，像是底下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\begin{tabular}{|l|l|}\hline
Age &amp;amp;amp; Frequency \\ \hline
18--25  &amp;amp;amp; 15 \\
26--35  &amp;amp;amp; 33 \\
36--45  &amp;amp;amp; 22 \\ \hline
\end{tabular}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;位在 &lt;code&gt;begin&lt;/code&gt; 與 &lt;code&gt;end&lt;/code&gt; 標籤之間的內容，都會被當作是原始 LaTeX 資料解讀，而不會視為 markdown。&lt;/p&gt;
&lt;p&gt;行內 LaTeX 在輸出至 Markdown, LaTeX 及 ConTeXt 之外的格式時會被忽略掉。&lt;/p&gt;
&lt;h2 id="latex ju ji"&gt;LaTeX 巨集&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;latex_macros&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;當輸出格式不是 LaTeX 時，pandoc 會分析 LaTeX 的 &lt;code&gt;\newcommand&lt;/code&gt; 和 &lt;code&gt;\renewcommand&lt;/code&gt; 定義，並套用其產生的巨集到所有 LaTeX 數學公式中。所以，舉例來說，下列指令對於所有的輸出格式均有作用，而非僅僅作用於 LaTeX 格式： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\newcommand{\tuple}[1]{\langle #1 \rangle}

$\tuple{a, b, c}$
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在 LaTeX 的輸出中，&lt;code&gt;\newcommand&lt;/code&gt; 定義會單純不作修改地保留至輸出結果。&lt;/p&gt;
&lt;h2 id="lian jie"&gt;連結&lt;/h2&gt;
&lt;p&gt;Markdown 接受以下數種指定連結的方式。&lt;/p&gt;
&lt;h3 id="zi dong lian jie"&gt;自動連結&lt;/h3&gt;
&lt;p&gt;如果你用角括號將一段 URL 或是 email 位址包起來，它會自動轉換成連結：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="nt"&gt;google&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;com&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;sam&lt;/span&gt;&lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="k"&gt;green&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;eggs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;ham&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="xing nei lian jie"&gt;行內連結&lt;/h3&gt;
&lt;p&gt;一個行內連結包含了位在方括號中的連結文字，以及方括號後以圓括號包起來的 URL。（你可以選擇性地在 URL 後面加入連結標題，標題文字要放在引號之中。）&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;one&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//fsf.org "click here for a good time!").&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;方括號與圓括號之間不能有空白。連結文字可以包含格式（例如強調），但連結標題則否。&lt;/p&gt;
&lt;h3 id="can kao lian jie"&gt;參考連結&lt;/h3&gt;
&lt;p&gt;一個 &lt;strong&gt;明確&lt;/strong&gt; 的參考連結包含兩個部分，連結本身以及連結定義，其中連結定義可以放在文件的任何地方（不論是放在連結所在處之前或之後）。&lt;/p&gt;
&lt;p&gt;連結本身是由兩組方括號所組成，第一組方括號中為連結文字，第二組為連結標籤。（在兩個方括號間可以有空白。）連結定義則是以方括號框住的連結標籤作開頭，後面跟著一個冒號一個空白，再接著一個 URL，最後可以選擇性地（在一個空白之後）加入由引號或是圓括號包住的連結標題。&lt;/p&gt;
&lt;p&gt;以下是一些範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[my label 1]: /foo/bar.html  "My title, optional"
[my label 2]: /foo
[my label 3]: http://fsf.org (The free software foundation)
[my label 4]: /bar#special  'A title in single quotes'
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;連結的 URL 也可以選擇性地以角括號包住：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;my&lt;/span&gt; &lt;span class="nx"&gt;label&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="nt"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;baz&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;連結標題可以放在第二行：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[my label 3]: http://fsf.org
  "The free software foundation"
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;需注意連結標籤並不區分大小寫。所以下面的例子會建立合法的連結：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Here&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;FOO&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;baz&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在一個 &lt;strong&gt;隱性&lt;/strong&gt; 參考連結中，第二組方括號的內容是空的，甚至可以完全地略去：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;See&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;website&lt;/span&gt;&lt;span class="p"&gt;][],&lt;/span&gt; &lt;span class="n"&gt;or&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;website&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;website&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//foo.bar.baz&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意：在 &lt;code&gt;Markdown.pl&lt;/code&gt; 以及大多數其他 markdown 實作中，參考連結的定義不能存在於嵌套結構中，例如清單項目或是區塊引言。Pandoc lifts this arbitrary seeming restriction。所以雖然下面的語法在幾乎所有其他實作中都是錯誤的，但在 pandoc 中可以正確處理：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;amp;gt; My block [quote].
&amp;amp;gt;
&amp;amp;gt; [quote]: /foo
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="nei bu lian jie"&gt;內部連結&lt;/h3&gt;
&lt;p&gt;要連結到同一份文件的其他章節，可使用自動產生的 ID（參見 [HTML, LaTeX 與 ConTeXt 的標題識別符] 一節後半）。例如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;See the [Introduction](#introduction).
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;或是&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;See the [Introduction].

[Introduction]: #introduction
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;內部連結目前支援的格式有 HTML（包括 HTML slide shows 與 EPUB）、LaTeX 以及 ConTeXt。&lt;/p&gt;
&lt;h2 id="tu pian_1"&gt;圖片&lt;/h2&gt;
&lt;p&gt;在連結語法的前面加上一個 &lt;code&gt;!&lt;/code&gt; 就是圖片的語法了。連結文字將會作為圖片的替代文字（alt text）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;la&lt;/span&gt; &lt;span class="n"&gt;lune&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;lalune&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="s"&gt;"Voyage to the moon"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;movie&lt;/span&gt; &lt;span class="n"&gt;reel&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;movie&lt;/span&gt; &lt;span class="n"&gt;reel&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gif&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="fu shang shuo ming de tu pian"&gt;附上說明的圖片&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;implicit_figures&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一個圖片若自身單獨存在一個段落中，那麼將會以附上圖片說明 (caption) 的圖表 (figure) 形式呈現。&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;（在 LaTeX 中，會使用圖表環境；在 HTML 中，圖片會被放在具有 &lt;code&gt;figure&lt;/code&gt; 類別的 &lt;code&gt;div&lt;/code&gt; 元素中，並會附上一個具有 &lt;code&gt;caption&lt;/code&gt; 類別的 &lt;code&gt;p&lt;/code&gt; 元素。）圖片的替代文字同時也會用來作為圖片說明。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;![This is the caption](/url/of/image.png)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果你只是想要個一般的行內圖片，那麼只要讓圖片不是段落裡唯一的元素即可。一個簡單的方法是在圖片後面插入一個不斷行空格：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;![This image won't be a figure](/url/of/image.png)\
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="jiao zhu_1"&gt;腳註&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;footnotes&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc's markdown 支援腳註功能，使用以下的語法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Here is a footnote reference,[^1] and another.[^longnote]

[^1]: Here is the footnote.

[^longnote]: Here's one with multiple blocks.

    Subsequent paragraphs are indented to show that they
belong to the previous footnote.

        { some.code }

    The whole paragraph can be indented, or just the first
    line.  In this way, multi-paragraph footnotes work like
    multi-paragraph list items.

This paragraph won't be part of the note, because it
isn't indented.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;腳註參考用的 ID 不得包含空白、tabs 或換行字元。這些 ID 只會用來建立腳註位置與腳註文字的對應關連；在輸出時，腳註將會依序遞增編號。&lt;/p&gt;
&lt;p&gt;腳註本身不需要放在文件的最後面。它們可以放在文件裡的任何地方，但不能被放入區塊元素（清單、區塊引言、表格等）之中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;inline_notes&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc 也支援了行內腳註（儘管，與一般腳註不同，行內腳註不能包含多個段落）。其語法如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Here is an inline note.^[Inlines notes are easier to write, since
you don't have to pick an identifier and move down to type the
note.]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;行內與一般腳註可以自由交錯使用。&lt;/p&gt;
&lt;h2 id="yin yong"&gt;引用&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Extension: &lt;code&gt;citations&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandoc 能夠以數種形式自動產生引用與參考書目（使用 Andrea Rossato 的 &lt;code&gt;hs-citeproc&lt;/code&gt;）。為了使用這項功能，你需要一個下列其中一種格式的參考書目資料庫：&lt;/p&gt;
&lt;p&gt;Format            File extension
  ------------      --------------
  MODS              .mods
  BibLaTeX          .bib
  BibTeX            .bibtex
  RIS               .ris
  EndNote           .enl
  EndNote XML       .xml
  ISI               .wos
  MEDLINE           .medline
  Copac             .copac
  JSON citeproc     .json&lt;/p&gt;
&lt;p&gt;需注意的是副檔名 &lt;code&gt;.bib&lt;/code&gt; 一般而言同時適用於 BibTeX 與 BibLaTeX 的檔案，不過你可以使用 &lt;code&gt;.bibtex&lt;/code&gt; 來強制指定 BibTeX。&lt;/p&gt;
&lt;p&gt;你需要使用命令列選項 &lt;code&gt;--bibliography&lt;/code&gt; 來指定參考書目檔案（如果有多個書目檔就得反覆指定）。&lt;/p&gt;
&lt;p&gt;預設情況下，pandoc 會在引用文獻與參考書目中使用芝加哥「作者－日期」格式。要使用其他的格式，你需要用 &lt;code&gt;--csl&lt;/code&gt; 選項來指定一個 &lt;a href="http://CitationStyles.org"&gt;CSL&lt;/a&gt; 1.0 格式的檔案。關於建立與修改 CSL 格式的入門可以在 &lt;a href="http://citationstyles.org/downloads/primer.html"&gt;http://citationstyles.org/downloads/primer.html&lt;/a&gt; 這邊找到。&lt;a href="https://github.com/citation-style-language/styles"&gt;https://github.com/citation-style-language/styles&lt;/a&gt; 是 CSL 格式的檔案庫。也可以在 &lt;a href="http://zotero.org/styles"&gt;http://zotero.org/styles&lt;/a&gt; 以簡單的方式瀏覽。&lt;/p&gt;
&lt;p&gt;引用資訊放在方括號中，以分號區隔。每一條引用都會有個 key，由 &lt;code&gt;@&lt;/code&gt; 加上資料庫中的引用 ID 組成，並且可以選擇性地包含前綴、定位以及後綴。以下是一些範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Blah blah [see @doe99, pp. 33-35; also @smith04, ch. 1].

Blah blah [@doe99, pp. 33-35, 38-39 and *passim*].

Blah blah [@smith04; @doe99].
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在 &lt;code&gt;@&lt;/code&gt; 前面的減號 (&lt;code&gt;-&lt;/code&gt;) 將會避免作者名字在引用中出現。這可以用在已經提及作者的文章場合中：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Smith says blah [-@smith04].
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你也可以在文字中直接插入引用資訊，方式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;@smith04 says blah.

@smith04 [p. 33] says blah.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果引用格式檔需要產生一份引用作品的清單，這份清單會被放在文件的最後面。一般而言，你需要以一個適當的標題結束你的文件：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;last paragraph...

# References
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如此一來參考書目就會被放在這個標題後面了。&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;之所以有這條規則，主要是要避免以人名頭文字縮寫作為開頭的段落所帶來的混淆，像是&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;B. Russell was an English philosopher.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這樣就不會被當作清單項目了。&lt;/p&gt;
&lt;p&gt;這條規則並不會避免以下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(C) 2007 Joe Smith
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;這樣的敘述被解釋成清單項目。在這情形下，可以使用反斜線：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(C\) 2007 Joe Smith
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="http://www.justatheory.com/computers/markup/modest-markdown-proposal.html"&gt;David Wheeler&lt;/a&gt; 對於 markdown 的建議也同時影響了我。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;這個方案是由 Michel Fortin 在 &lt;a href="http://six.pairlist.net/pipermail/markdown-discuss/2005-March/001097.html"&gt;Markdown discussion list&lt;/a&gt; 的討論中所提出。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:T1"&gt;
&lt;p&gt;譯註：straight quotes 指的是左右兩側都長得一樣的引號，例如我們直接在鍵盤上打出來的單引號或雙引號；curly quotes 則是左右兩側不同，有從兩側向內包夾視覺效果的引號。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:T1" rev="footnote" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;這項功能尚未在 RTF, OpenDocument 或 ODT 格式上實現。在這些格式中，你會得到一個在段落中只包含自己的圖片，而無圖片說明。&amp;nbsp;&lt;a class="footnote-backref" href="#fnref:5" rev="footnote" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content></entry></feed>