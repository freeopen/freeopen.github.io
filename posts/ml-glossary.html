<!DOCTYPE html>
<html lang="en">
<head>
          <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta http-equiv="content-type" content="text/html; charset=utf-8">
        <!-- Enable responsiveness on mobile devices-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

        <title>Freeopen - 机器学习术语表</title>
        <link rel="stylesheet" href="https://freeopen.github.io/theme/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://freeopen.github.io/theme/css/main.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/academicons/css/academicons.min.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/font-awesome/css/font-awesome.min.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/fonts-googleapis/css/googleapi-fonts.css" />

        






</head>

<body id="index" class="theme-myblue" data-spy="scroll" data-target="#post_toc">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 col-lg">
        <nav class="navbar navbar-expand-lg navbar-light sidebar" role="navigation">
          <a class="sidebar-about" href="https://freeopen.github.io/">Freeopen</a>
          <button class="navbar-toggler " type="button" data-toggle="collapse" data-target="#navbarNav">
            <span class="navbar-toggler-icon"></span>
          </button>
          <nav class="collapse navbar-collapse sidebar-nav flex-column" id="navbarNav">

              <a class="nav-link" href="https://freeopen.github.io/pages/guan-yu-zhe-li.html">关于这里</a>
              <a class="nav-link" href="https://freeopen.github.io/pages/zou-guo-lu-guo.html">走过路过</a>

              <a class="nav-link" href="https://freeopen.github.io/category/bian-cheng-zhi-hui.html">编程智慧</a>
              <a class="nav-link" href="https://freeopen.github.io/category/ji-qi-xue-xi.html">机器学习</a>
              <a class="nav-link" href="https://freeopen.github.io/category/shu-xue-za-tan.html">数学杂谈</a>
              <a class="nav-link" href="https://freeopen.github.io/category/su-cha-shou-ce.html">速查手册</a>
            <div class="sidebar-icons">
              <hr>
                <a href="" title="Atom feed" target="_blank" 
                    style="display: inline; padding: 0px 0px 0px 0; margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;">
                  <i class="fa fa-rss-square"></i>
                </a>
                <a href="https://github.com/freeopen" title="Software I released on Github" 
                      target="_blank" style="display: inline; padding: 0px 0px 0px 0; margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;"><i class="fa fa-github-square"></i> </a>
                <a href="Mailto:freeopen@163.com" title="My email" 
                    target="_blank" style="display: inline; padding: 0px 0px 0px 0;
                    margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;"><i
                  class="fa fa-envelope-square"></i> </a>

            </div>
          </nav>

        </nav>

      </div>

      <div class="col-12 col-lg-7" role="main">
        <div id="content" class="content">
          <div class="post">
<section id="content" class="body">
  <header>
    <h1 class="entry-title">机器学习术语表</h2>
 
  </header>
  <div class="post-info">

    <span>2018-02-26</span>


    <span>| By             <a class="url fn" href="https://freeopen.github.io/author/freeopen.html">freeopen</a>
    </span>

    <span>
      | 分类于 <a href="https://freeopen.github.io/category/su-cha-shou-ce.html">速查手册</a>
    </span>

  </div><!-- /.post-info -->
  <div class="post content">
    <blockquote>
<p>为方便读论文时查询术语，只做英文目录，我会不时把新遇到的术语增补到这里。
<a href="https://developers.google.com/machine-learning/glossary/">初版</a>来自 Google Developers 网站。
有不准确的地方请来信告知，我会即时更正。</p>
</blockquote>
<p>本术语表定义了一般机器学习术语以及特定于 TensorFlow 的术语。</p>
<p><font color="red">常用评价指标导航</font></p>
<ul>
<li><a href="#accuracy">accuracy</a></li>
<li><a href="#precision">precision</a></li>
<li><a href="#recall">recall</a></li>
<li><a href="#f-score">F1-Score</a></li>
<li><a href="#ap">AP</a></li>
<li><a href="#map">mAP</a></li>
<li><a href="#auc-area-under-the-roc-curve">AUC</a></li>
<li><a href="#roc-receiver-operating-characteristic-curve">ROC</a></li>
<li><a href="#bleu">BLEU</a></li>
<li><a href="#iou">IoU</a></li>
</ul>
<h2 id="a">A</h2>
<h3 id="ab-testing">A/B testing</h3>
<p>A statistical way of comparing two (or more) techniques, typically an incumbent against a new rival. A/B testing aims to determine not only which technique performs better but also to understand whether the difference is statistically significant. A/B testing usually considers only two techniques using one measurement, but it can be applied to any finite number of techniques and measures.</p>
<p>一种统计方法，用于将两种或多种技术进行比较，通常是将当前采用的技术与新技术进行比较。A/B 测试不仅旨在确定哪种技术的效果更好，而且还有助于了解相应差异是否具有显著的统计意义。A/B 测试通常是采用一种衡量方式对两种技术进行比较，但也适用于任意有限数量的技术和衡量方式。</p>
<h3 id="accuracy">accuracy</h3>
<p>The fraction of predictions that a <a href="#classification-model"><strong>classification model</strong></a> got right. In <a href="#multi-class-classification"><strong>multi-class classification</strong></a>, accuracy is defined as follows:</p>
<p><mj></mj></p>
<div class="math">$$ \text{Accuracy} = \frac{\text{Correct Predictions}} {\text{Total Number Of Examples}} $$</div>
<p></p>
<p>In <a href="#binary-classification"><strong>binary classification</strong></a>, accuracy has the following definition:</p>
<p><mj></mj></p>
<div class="math">$$\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}} {\text{Total Number Of Examples}}$$</div>
<p></p>
<p>See <a href="#true-positive-tp"><strong>true positive</strong></a>) and <a href="#true-negative-tn"><strong>true negative</strong></a>.</p>
<p><a href="#classification-model"><strong>分类模型</strong></a>的正确预测所占的比例。在<a href="#multi-class-classification"><strong>多类别分类</strong></a>中，准确率的定义如下：</p>
<p><mj></mj></p>
<div class="math">$$\text{准确率} = \frac{\text{正确的预测数}} {\text{样本总数}}$$</div>
<p></p>
<p>在<a href="#binary-classification"><strong>二元分类</strong></a>中，准确率的定义如下：</p>
<p><mj> </mj></p>
<div class="math">$$\text{准确率} = \frac{\text{真正例数} + \text{真负例数}} {\text{样本总数}}$$</div>
<p> </p>
<p>请参阅<a href="#true-positive-tp"><strong>真正例</strong></a>和<a href="#true-negative-tn"><strong>真负例</strong></a>。</p>
<h3 id="activation-function">activation function</h3>
<p>A function (for example, <a href="#rectified-linear-unit-relu"><strong>ReLU</strong></a> or <a href="#sigmoid-function"><strong>sigmoid</strong></a>) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer.</p>
<p>一种函数（例如 <a href="#rectified-linear-unit-relu"><strong>ReLU</strong></a> 或 <a href="#sigmoid-function"><strong>S 型</strong></a>函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。</p>
<h3 id="adagrad">AdaGrad</h3>
<p>A sophisticated gradient descent algorithm that rescales the gradients of each parameter, effectively giving each parameter an independent <a href="#learning-rate"><strong>learning rate</strong></a>. For a full explanation, see <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">this paper</a>.</p>
<p>一种先进的梯度下降法，用于重新调整每个参数的梯度，以便有效地为每个参数指定独立的<a href="#learning-rate"><strong>学习速率</strong></a>。如需查看完整的解释，请参阅<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">这篇论文</a>。</p>
<h3 id="ap">AP</h3>
<p>平均精度，衡量在单个类别上的平均精度, 所有类别上的平均精度称为平均精度均值<a href="#map">mAP</a>。以2010年为界，前后有两种算法。</p>
<p>第一种算法，首先设定一组recall阈值，[0, 0.1, 0.2, &hellip;, 1]。然后对每个查全率或召回率<a href="#recall">recall</a>阈值从小到大取值，同时计算当取大于该recall阈值时top-n所对应的最大查准率<a href="#precision">precision</a>。这样，我们就计算出了11个precision。AP 即为这 11 个 precision 的平均值。这种方法英文叫做 11-point interpolated average precision。 </p>
<p>第二种算法，假设 N 个样本中有 M 个正例，那么我们会得到 M 个recall值（1/M, 2/M, &hellip;, M/M）,对于每个 recall 值 r，该recall阈值时top-n所对应的最大precision，然后对这 M 个 precision 值取平均即得到最后的AP值。</p>
<h3 id="auc-area-under-the-roc-curve">AUC (Area under the ROC Curve)</h3>
<p>An evaluation metric that considers all possible <a href="#classification-threshold"><strong>classification thresholds</strong></a>.</p>
<p>The Area Under the <a href="#roc-receiver-operating-characteristic-curve">ROC curve</a> is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.</p>
<p>一种会考虑所有可能<a href="#classification-threshold"><strong>分类阈值</strong></a>的评估指标。</p>
<p><a href="#roc-receiver-operating-characteristic-curve">ROC 曲线</a>下面积是，对于随机选择的正类别样本确实为正类别，以及随机选择的负类别样本为正类别，分类器更确信前者的概率。</p>
<h2 id="b_1">B</h2>
<h3 id="backpropagation">backpropagation</h3>
<p>The primary algorithm for performing <a href="#gradient-descent"><strong>gradient descent</strong></a> on <a href="#neural-network"><strong>neural networks</strong></a>. First, the output values of each node are calculated (and cached) in a forward pass. Then, the <a href="https://en.wikipedia.org/wiki/Partial_derivative">partial derivative</a> of the error with respect to each parameter is calculated in a backward pass through the graph.</p>
<p>在<a href="#neural-network"><strong>神经网络</strong></a>上执行<a href="#gradient-descent"><strong>梯度下降法</strong></a>的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的<a href="https://en.wikipedia.org/wiki/Partial_derivative">偏导数</a>。</p>
<h3 id="baseline">baseline</h3>
<p>A simple <a href="#model"><strong>model</strong></a> or heuristic used as reference point for comparing how well a model is performing. A baseline helps model developers quantify the minimal, expected performance on a particular problem.</p>
<p>一种简单的<a href="#model"><strong>模型</strong></a>或启发法，用作比较模型效果时的参考点。基准有助于模型开发者针对特定问题量化最低预期效果。</p>
<h3 id="batch">batch</h3>
<p>The set of examples used in one <a href="#iteration"><strong>iteration</strong></a> (that is, one <a href="#gradient"><strong>gradient</strong></a> update) of <a href="#model-training"><strong>model training</strong></a>.</p>
<p>See also <a href="#batch-size"><strong>batch size</strong></a>.</p>
<p><a href="#model-training"><strong>模型训练</strong></a>的一次<a href="#iteration"><strong>迭代</strong></a>（即一次<a href="#gradient"><strong>梯度</strong></a>更新）中使用的样本集。</p>
<p>另请参阅<a href="#batch-size"><strong>批次规模</strong></a>。</p>
<h3 id="batch-size">batch size</h3>
<p>The number of examples in a <a href="#batch"><strong>batch</strong></a>. For example, the batch size of <a href="#stochastic-gradient-descent-sgd"><strong>SGD</strong></a> is 1, while the batch size of a <a href="#mini-batch"><strong>mini-batch</strong></a> is usually between 10 and 1000. Batch size is usually fixed during training and inference; however, TensorFlow does permit dynamic batch sizes.</p>
<p>一个<a href="#batch"><strong>批次</strong></a>中的样本数。例如，<a href="#stochastic-gradient-descent-sgd"><strong>SGD</strong></a> 的批次规模为 1，而<a href="#mini-batch"><strong>小批次</strong></a>的规模通常介于 10 到 1000 之间。批次规模在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次规模。</p>
<h3 id="bias">bias</h3>
<p>An intercept or offset from an origin. Bias (also known as the <strong>bias term</strong>) is referred to as <span class="math">\(b\)</span> or <span class="math">\(w_0\)</span> in machine learning models. For example, bias is the <em>b</em> in the following formula:</p>
<p>距离原点的截距或偏移。偏差（也称为<strong>偏差项</strong>）在机器学习模型中以 <span class="math">\(b\)</span> 或 <span class="math">\(w_0\)</span> 表示。例如，在下面的公式中，偏差为 b：</p>
<div class="math">$$y' = b + w_1x_1 + w_2x_2 + &hellip; w_nx_n$$</div>
<p>Not to be confused with <a href="#prediction-bias"><strong>prediction bias</strong></a>.</p>
<p>请勿与<a href="#prediction-bias"><strong>预测偏差</strong></a>混淆。</p>
<h3 id="binary-classification">binary classification</h3>
<p>A type of classification task that outputs one of two mutually exclusive classes. For example, a machine learning model that evaluates email messages and outputs either "spam" or "not spam" is a binary classifier.</p>
<p>一种分类任务，可输出两种互斥类别之一。例如，对电子邮件进行评估并输出&ldquo;垃圾邮件&rdquo;或&ldquo;非垃圾邮件&rdquo;的机器学习模型就是一个二元分类器。</p>
<h3 id="binning">binning</h3>
<p>See <a href="#bucketing"><strong>bucketing</strong></a>.</p>
<h3 id="bleu">BLEU</h3>
<p>BLEU( bilingual evaluation understudy ) 双语评估替换, 是一种文本评估算法，它是用来评估机器翻译跟专业人工翻译之间的对应关系，核心思想就是机器翻译越接近专业人工翻译，质量就越好，经过bleu算法得出的分数可以作为机器翻译质量的其中一个指标。</p>
<p>整个算法的公式较复杂，列示如下：</p>
<div class="math">$$BLEU = BP\cdot exp(\sum_{n=1}^N w_n\log p_n)  \tag{1}$$</div>
<div class="math">$$p_n = {\sum_{C\in \{Candidates\}} \sum_{\text{n-gram} \in C} Count_{clip}(\text{n-gram}) \over \sum_{C^{'}\in \{Candidates\}} \sum_{\text{n-gram}^{'} \in C^{'}} Count(\text{n-gram}^{'}) } \tag{2}$$</div>
<div class="math">$$
BP = 
\begin{cases}
1,  &amp; \text{if $c$ &gt; $r$} \\
e^{(1-r/c)}, &amp; \text{if $c \leq r$}
\end{cases}   
\tag{3}$$</div>
<div class="math">$$Count_{clip} = min(Count, Max_Ref_Count) \tag{4}$$</div>
<p>其中，(1)式为总公式，(2)(3)式是对(1)式的说明，(4)式是对(2)式的说明。</p>
<ul>
<li>(2)式称为<a href="#n-gram">N-gram</a>(注：N 的取值范围通常为1至4)距离计算公式，在评测机器翻译任务时，用机器译文去对比参考译文，
分子部分表示取n-gram在机器译文和参考译文中出现的最小次数, 分母部分表示取n-gram在机器译文中出现次数。</li>
<li>(3)式称为惩罚因子，c是机器译文的词数，r是参考译文的词数。</li>
<li>(4)式的意思是取机器译文N-gram的出现次数和参考译文中N-gram最大出现次数中的最小值，
  它是对原始N-gram算法统计匹配次数的修正。</li>
</ul>
<p>优点：方便、快速，结果比较接近人类评分。</p>
<p>缺点：
  * 不考虑语言表达（语法）上的准确性；
  * 测评精度会受常用词的干扰；
  * 短译句的测评精度有时会较高；
  * 没有考虑同义词或相似表达的情况，可能会导致合理翻译被否定；</p>
<p>BLEU本身就不追求百分之百的准确性，也不可能做到百分之百，它的目标只是给出一个快且不差的自动评估解决方案。</p>
<h3 id="bucketing">bucketing</h3>
<p>Converting a (usually <a href="#continuous-feature"><strong>continuous</strong></a>) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin.</p>
<p>将一个特征（通常是<a href="#continuous-feature"><strong>连续</strong></a>特征）转换成多个二元特征（称为桶或箱），通常是根据值区间进行转换。例如，您可以将温度区间分割为离散分箱，而不是将温度表示成单个连续的浮点特征。假设温度数据可精确到小数点后一位，则可以将介于 0.0 到 15.0 度之间的所有温度都归入一个分箱，将介于 15.1 到 30.0 度之间的所有温度归入第二个分箱，并将介于 30.1 到 50.0 度之间的所有温度归入第三个分箱。</p>
<h2 id="c_1">C</h2>
<h3 id="calibration-layer">calibration layer</h3>
<p>A post-prediction adjustment, typically to account for <a href="#prediction-bias"><strong>prediction bias</strong></a>. The adjusted predictions and probabilities should match the distribution of an observed set of labels.</p>
<p>一种预测后调整，通常是为了降低<a href="#prediction-bias"><strong>预测偏差</strong></a>。调整后的预测和概率应与观察到的标签集的分布一致。</p>
<h3 id="candidate-sampling">candidate sampling</h3>
<p>A training-time optimization in which a probability is calculated for all the positive labels, using, for example, softmax, but only for a random sample of negative labels. For example, if we have an example labeled <em>beagle</em> and <em>dog</em> candidate sampling computes the predicted probabilities and corresponding loss terms for the <em>beagle</em> and <em>dog</em> class outputs in addition to a random subset of the remaining classes (<em>cat</em>, <em>lollipop</em>, <em>fence</em>). The idea is that the <a href="#negative-class"><strong>negative classes</strong></a> can learn from less frequent negative reinforcement as long as <a href="#positive-class"><strong>positive classes</strong></a> always get proper positive reinforcement, and this is indeed observed empirically. The motivation for candidate sampling is a computational efficiency win from not computing predictions for all negatives.</p>
<p>一种训练时进行的优化，会使用某种函数（例如 softmax）针对所有正类别标签计算概率，但对于负类别标签，则仅针对其随机样本计算概率。例如，如果某个样本的标签为&ldquo;小猎犬&rdquo;和&ldquo;狗&rdquo;，则候选采样将针对&ldquo;小猎犬&rdquo;和&ldquo;狗&rdquo;类别输出以及其他类别（猫、棒棒糖、栅栏）的随机子集计算预测概率和相应的损失项。这种采样基于的想法是，只要<a href="#positive-class"><strong>正类别</strong></a>始终得到适当的正增强，<a href="#negative-class"><strong>负类别</strong></a>就可以从频率较低的负增强中进行学习，这确实是在实际中观察到的情况。候选采样的目的是，通过不针对所有负类别计算预测结果来提高计算效率。</p>
<h3 id="categorical-data">categorical data</h3>
<p><a href="#feature"><strong>Features</strong></a> having a discrete set of possible values. For example, consider a categorical feature named <code>house style</code>, which has a discrete set of three possible values: <code>Tudor, ranch, colonial</code>. By representing <code>house style</code> as categorical data, the model can learn the separate impacts of <code>Tudor</code>, <code>ranch</code>, and <code>colonial</code> on house price.</p>
<p>Sometimes, values in the discrete set are mutually exclusive, and only one value can be applied to a given example. For example, a <code>car maker</code> categorical feature would probably permit only a single value (<code>Toyota</code>) per example. Other times, more than one value may be applicable. A single car could be painted more than one different color, so a <code>car color</code> categorical feature would likely permit a single example to have multiple values (for example, <code>red</code> and <code>white</code>).</p>
<p>Categorical features are sometimes called <a href="#discrete-feature"><strong>discrete features</strong></a>.</p>
<p>Contrast with <a href="#numerical-data"><strong>numerical data</strong></a>.</p>
<p>一种<a href="#feature"><strong>特征</strong></a>，拥有一组离散的可能值。以某个名为 <code>house style</code> 的分类特征为例，该特征拥有一组离散的可能值（共三个），即 <code>Tudor, ranch, colonial</code>。通过将 <code>house style</code> 表示成分类数据，相应模型可以学习 <code>Tudor</code>、<code>ranch</code> 和 <code>colonial</code> 分别对房价的影响。</p>
<p>有时，离散集中的值是互斥的，只能将其中一个值应用于指定样本。例如，<code>car maker</code> 分类特征可能只允许一个样本有一个值 (<code>Toyota</code>)。在其他情况下，则可以应用多个值。一辆车可能会被喷涂多种不同的颜色，因此，<code>car color</code> 分类特征可能会允许单个样本具有多个值（例如 <code>red</code> 和 <code>white</code>）。</p>
<p>分类特征有时称为<a href="#discrete-feature"><strong>离散特征</strong></a>。</p>
<p>与<a href="#numerical-data"><strong>数值数据</strong></a>相对。</p>
<h3 id="checkpoint">checkpoint</h3>
<p>Data that captures the state of the variables of a model at a particular time. Checkpoints enable exporting model <a href="#weight"><strong>weights</strong></a>, as well as performing training across multiple sessions. Checkpoints also enable training to continue past errors (for example, job preemption). Note that the <a href="#graph"><strong>graph</strong></a> itself is not included in a checkpoint.</p>
<p>一种数据，用于捕获模型变量在特定时间的状态。借助检查点，可以导出模型<a href="#weight"><strong>权重</strong></a>，跨多个会话执行训练，以及使训练在发生错误之后得以继续（例如作业抢占）。请注意，<a href="#graph"><strong>图</strong></a>本身不包含在检查点中。</p>
<h3 id="class">class</h3>
<p>One of a set of enumerated target values for a label. For example, in a <a href="#binary-classification"><strong>binary classification</strong></a> model that detects spam, the two classes are <em>spam</em> and <em>not spam</em>. In a <a href="#multi-class-classification"><strong>multi-class classification</strong></a> model that identifies dog breeds, the classes would be <em>poodle</em>, <em>beagle</em>, <em>pug</em>, and so on.</p>
<p>为标签枚举的一组目标值中的一个。例如，在检测垃圾邮件的<a href="#binary-classification"><strong>二元分类</strong></a>模型中，两种类别分别是&ldquo;垃圾邮件&rdquo;和&ldquo;非垃圾邮件&rdquo;。在识别狗品种的<a href="#multi-class-classification"><strong>多类别分类</strong></a>模型中，类别可以是&ldquo;贵宾犬&rdquo;、&ldquo;小猎犬&rdquo;、&ldquo;哈巴犬&rdquo;等等。</p>
<h3 id="class-imbalanced-data-set">class-imbalanced data set</h3>
<p>A <a href="#binary-classification"><strong>binary classification</strong></a> problem in which the <a href="#label"><strong>labels</strong></a> for the two classes have significantly different frequencies. For example, a disease data set in which 0.0001 of examples have positive labels and 0.9999 have negative labels is a class-imbalanced problem, but a football game predictor in which 0.51 of examples label one team winning and 0.49 label the other team winning is <em>not</em> a class-imbalanced problem.</p>
<p>在<a href="#binary-classification"><strong>二元分类</strong></a>问题问题中，两种类别的<a href="#label"><strong>标签</strong></a>在出现频率方面具有很大的差距。例如，在某个疾病数据集中，0.0001 的样本具有正类别标签，0.9999 的样本具有负类别标签，这就属于分类不平衡问题；但在某个足球比赛预测器中，0.51 的样本的标签为其中一个球队赢，0.49 的样本的标签为另一个球队赢，这就不属于分类不平衡问题。</p>
<h3 id="classification-model">classification model</h3>
<p>A type of machine learning model for distinguishing among two or more discrete classes. For example, a natural language processing classification model could determine whether an input sentence was in French, Spanish, or Italian. Compare with <a href="#regression-model"><strong>regression model</strong></a>.</p>
<p>一种机器学习模型，用于区分两种或多种离散类别。例如，某个自然语言处理分类模型可以确定输入的句子是法语、西班牙语还是意大利语。请与<a href="#regression-model"><strong>回归模型</strong></a>进行比较。</p>
<h3 id="classification-threshold">classification threshold</h3>
<p>A scalar-value criterion that is applied to a model's predicted score in order to separate the <a href="#positive-class"><strong>positive class</strong></a> from the <a href="#negative-class"><strong>negative class</strong></a>. Used when mapping <a href="#logistic-regression"><strong>logistic regression</strong></a> results to <a href="#binary-classification"><strong>binary classification</strong></a>. For example, consider a logistic regression model that determines the probability of a given email message being spam. If the classification threshold is 0.9, then logistic regression values above 0.9 are classified as <em>spam</em> and those below 0.9 are classified as <em>not spam</em>.</p>
<p>一种标量值条件，应用于模型预测的得分，旨在将<a href="#positive-class"><strong>正类别</strong></a>与<a href="#negative-class"><strong>负类别</strong></a>区分开。将<a href="#logistic-regression"><strong>逻辑回归</strong></a>结果映射到<a href="#binary-classification"><strong>二元分类</strong></a>时使用。以某个逻辑回归模型为例，该模型用于确定指定电子邮件是垃圾邮件的概率。如果分类阈值为 0.9，那么逻辑回归值高于 0.9 的电子邮件将被归类为&ldquo;垃圾邮件&rdquo;，低于 0.9 的则被归类为&ldquo;非垃圾邮件&rdquo;。</p>
<h3 id="collaborative-filtering">collaborative filtering</h3>
<p>Making predictions about the interests of one user based on the interests of many other users. Collaborative filtering is often used in recommendation systems.</p>
<p>根据很多其他用户的兴趣来预测某位用户的兴趣。协同过滤通常用在推荐系统中。</p>
<h3 id="confusion-matrix">confusion matrix</h3>
<p>An NxN table that summarizes how successful a <a href="#classification-model"><strong>classification model's</strong></a> predictions were; that is, the correlation between the label and the model's classification. One axis of a confusion matrix is the label that the model predicted, and the other axis is the actual label. N represents the number of classes. In a <a href="#binary-classification"><strong>binary classification</strong></a> problem, N=2. For example, here is a sample confusion matrix for a binary classification problem:</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th align="center">Tumor (predicted)</th>
<th align="center">Non-Tumor (predicted)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tumor (actual)</td>
<td align="center">18</td>
<td align="center">1</td>
</tr>
<tr>
<td>Non-Tumor (actual)</td>
<td align="center">6</td>
<td align="center">452</td>
</tr>
</tbody>
</table>
<p>The preceding confusion matrix shows that of the 19 samples that actually had tumors, the model correctly classified 18 as having tumors (18 true positives), and incorrectly classified 1 as not having a tumor (1 false negative). Similarly, of 458 samples that actually did not have tumors, 452 were correctly classified (452 true negatives) and 6 were incorrectly classified (6 false positives).</p>
<p>The confusion matrix for a multi-class classification problem can help you determine mistake patterns. For example, a confusion matrix could reveal that a model trained to recognize handwritten digits tends to mistakenly predict 9 instead of 4, or 1 instead of 7.</p>
<p>Confusion matrices contain sufficient information to calculate a variety of performance metrics, including <a href="#precision"><strong>precision</strong></a> and <a href="#recall"><strong>recall</strong></a>.</p>
<p>一种 NxN 表格，用于总结<a href="#classification-model"><strong>分类模型</strong></a>的预测成效；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在<a href="#binary-classification"><strong>二元分类</strong></a>问题中，N=2。例如，下面显示了一个二元分类问题的混淆矩阵示例：</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th align="center">肿瘤(预测)</th>
<th align="center">非肿瘤(预测)</th>
</tr>
</thead>
<tbody>
<tr>
<td>肿瘤  (实际)</td>
<td align="center">18</td>
<td align="center">1</td>
</tr>
<tr>
<td>非肿瘤(实际)</td>
<td align="center">6</td>
<td align="center">452</td>
</tr>
</tbody>
</table>
<p>上面的混淆矩阵显示，在 19 个实际有肿瘤的样本中，该模型正确地将 18 个归类为有肿瘤（18 个真正例），错误地将 1 个归类为没有肿瘤（1 个假负例）。同样，在 458 个实际没有肿瘤的样本中，模型归类正确的有 452 个（452 个真负例），归类错误的有 6 个（6 个假正例）。</p>
<p>多类别分类问题的混淆矩阵有助于确定出错模式。例如，某个混淆矩阵可以揭示，某个经过训练以识别手写数字的模型往往会将 4 错误地预测为 9，将 7 错误地预测为 1。</p>
<p>混淆矩阵包含计算各种效果指标（包括<a href="#precision"><strong>精确率</strong></a>和<a href="#recall"><strong>召回率</strong></a>）所需的充足信息。</p>
<h3 id="continuous-feature">continuous feature</h3>
<p>A floating-point feature with an infinite range of possible values. Contrast with <a href="#discrete-feature"><strong>discrete feature</strong></a>.</p>
<p>一种浮点特征，可能值的区间不受限制。与<a href="#discrete-feature"><strong>离散特征</strong></a>相对。</p>
<h3 id="convergence">convergence</h3>
<p>Informally, often refers to a state reached during training in which training <a href="#loss"><strong>loss</strong></a> and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence.</p>
<p>See also <a href="#early-stopping"><strong>early stopping</strong></a>.</p>
<p>See also Boyd and Vandenberghe, <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>.</p>
<p>通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练<a href="#loss"><strong>损失</strong></a>和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。</p>
<p>另请参阅<a href="#early-stopping"><strong>早停法</strong></a>。</p>
<p>另请参阅 Boyd 和 Vandenberghe 合著的 <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>（《凸优化》）。</p>
<h3 id="convex-function">convex function</h3>
<p>A function in which the region above the graph of the function is a <a href="#convex-set"><strong>convex set</strong></a>. The prototypical convex function is shaped something like the letter <strong>U</strong>. For example, the following are all convex functions:</p>
<p>一种函数，函数图像以上的区域为<a href="#convex-set"><strong>凸集</strong></a>。典型凸函数的形状类似于字母 <strong>U</strong>。例如，以下都是凸函数：</p>
<p align="center">
<img src="/images/convex_functions.png" width="90%"/>
<figcaption>
A typical convex function is shaped like the letter 'U'.
</figcaption>
</p>
<p>By contrast, the following function is not convex. Notice how the region above the graph is not a convex set:</p>
<p>相反，以下函数则不是凸函数。请注意图像上方的区域如何不是凸集：</p>
<p>
<svg height="299.19577" id="svg2" version="1.1" viewbox="0 0 379.11578 299.19577" width="379.11578" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg">
<title>Nonconvex function</title>
<desc>A nonconvex function that looks like a curved "W" character, with two local minima</desc>
<defs id="defs6"><clippath clippathunits="userSpaceOnUse" id="clipPath20"><path d="M 0,0 H 365760 V 274320 H 0 Z" id="path18"></path></clippath></defs><g id="g10" transform="matrix(1.3333333,0,0,-1.3333333,-345.28663,532.80533)"><path d="M 0,540 H 719.99856 V 0.00108 H 0 Z" id="path28" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="m 278.33212,392.51998 c 12.48685,-27.90513 53.26204,-155.55219 74.92111,-167.43077 21.65907,-11.87858 37.99042,86.40436 55.03336,96.15926 17.04293,9.7549 33.05274,-49.79059 47.22431,-37.62985 14.17157,12.16073 31.50421,92.1619 37.80504,110.59427" id="path36" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="m 278.33212,392.51998 c 12.48685,-27.90513 53.26204,-155.55219 74.92111,-167.43077 21.65907,-11.87858 37.99042,86.40436 55.03336,96.15926 17.04293,9.7549 33.05274,-49.79059 47.22431,-37.62985 14.17157,12.16073 31.50421,92.1619 37.80504,110.59427" id="path38" style="fill:none;stroke:#4285f4;stroke-width:1.49999702;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="m 482.16811,278.04383 h 61.13372 v -37.91331 h -61.13372 z" id="path46" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text56" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="488.91809" y="-260.73386"><tspan id="tspan54" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="488.91809 491.36011 497.47607 502.97607 509.09207" y="-260.73386">local</tspan></text>
<text id="text60" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="488.91809" y="-247.23389"><tspan id="tspan58" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="488.91809 498.08109 500.52307 506.63907 509.08105 518.24402 524.36005" y="-247.23389">minimum</tspan></text>
<path d="m 452.82959,279.16588 29.33852,-20.0787" id="path70" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="m 459.19298,274.8109 22.97513,-15.72372" id="path72" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="m 459.19298,274.8109 3.51718,0.65906 -7.16613,1.8382 4.308,-6.01443 z" id="path74" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"></path><path d="m 264.28807,179.58871 v 220.0153" id="path82" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 264.28807,179.58871 V 391.89309" id="path84" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="m 264.28807,391.89309 2.53033,-2.53033 -2.53033,6.95198 -2.53033,-6.95198 z" id="path86" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 263.27841,180.53025 H 543.2936" id="path94" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 263.27841,180.53025 H 535.58265" id="path96" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="m 535.58265,180.53025 -2.53033,-2.53031 6.95195,2.53031 -6.95195,2.53031 z" id="path98" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 382.99136,219.87466 H 444.1251 V 181.96135 H 382.99136 Z" id="path106" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text116" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="389.74133" y="-202.56468"><tspan id="tspan114" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="389.74133 392.18335 398.29932 403.79932 409.91531 412.3573" y="-202.56468">local </tspan></text>
<text id="text120" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="389.74133" y="-189.06471"><tspan id="tspan118" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="389.74133 398.90433 401.34631 407.46231 409.9043 419.06729 425.18326" y="-189.06471">minimum</tspan></text>
<path d="M 358.85873,221.41495 388.19726,205.6827" id="path130" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 365.65429,217.77094 388.19726,205.6827" id="path132" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="m 365.65429,217.77094 3.42568,1.03417 -7.32245,1.05539 4.93094,-5.51524 z" id="path134" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 280.99156,219.87466 H 342.1253 V 181.96135 H 280.99156 Z" id="path142" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text152" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="287.74155" y="-202.56468"><tspan id="tspan150" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="287.74155 293.85754 296.29953 302.41553 308.53149 314.64749 317.08948" y="-202.56468">global </tspan></text>
<text id="text156" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="287.74155" y="-189.06471"><tspan id="tspan154" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="287.74155 296.90454 299.34653 305.46252 307.90451 317.0675 323.18347" y="-189.06471">minimum</tspan></text>
<path d="M 354.92356,221.42398 324.80551,206.63661" id="path166" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 348.00192,218.02558 324.80551,206.63661" id="path168" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="m 348.00192,218.02558 -1.15616,-3.38646 5.12521,5.33518 -7.35556,-0.79256 z" id="path170" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1"></path></g></svg>
</p>
<p>Nonconvex function A nonconvex function that looks like a curved "W" character, with two local minimum.</p>
<p>非凸函数的形状类似于字母<strong>W</strong>, 有两个局部最低点.</p>
<p>A <strong>strictly convex function</strong> has exactly one local minimum point, which is also the global minimum point. The classic U-shaped functions are strictly convex functions. However, some convex functions (for example, straight lines) are not.</p>
<p>A lot of the common <strong>loss functions</strong>, including the following, are convex functions:</p>
<ul>
<li><a href="#l2-loss"><strong>L2 loss</strong></a></li>
<li><a href="#log-loss"><strong>Log Loss</strong></a></li>
<li><a href="#l1-regularization"><strong>L1 regularization</strong></a></li>
<li><a href="#l2-regularization"><strong>L2 regularization</strong></a></li>
</ul>
<p>Many variations of <a href="#gradient-descent"><strong>gradient descent</strong></a> are guaranteed to find a point close to the minimum of a strictly convex function. Similarly, many variations of <a href="#stochastic-gradient-descent-sgd"><strong>stochastic gradient descent</strong></a> have a high probability (though, not a guarantee) of finding a point close to the minimum of a strictly convex function.</p>
<p>The sum of two convex functions (for example, L2 loss + L1 regularization) is a convex function.</p>
<p><a href="#deep-model"><strong>Deep models</strong></a> are never convex functions. Remarkably, algorithms designed for <a href="#convex-optimization"><strong>convex optimization</strong></a> tend to find reasonably good solutions on deep networks anyway, even though those solutions are not guaranteed to be a global minimum.</p>
<p><strong>严格凸函数</strong>只有一个局部最低点，该点也是全局最低点。经典的 U 形函数都是严格凸函数。不过，有些凸函数（例如直线）则不是这样。</p>
<p>很多常见的<strong>损失函数</strong>（包括下列函数）都是凸函数：</p>
<ul>
<li><a href="#l2-loss"><strong>L2 损失函数</strong></a></li>
<li><a href="#log-loss"><strong>对数损失函数</strong></a></li>
<li><a href="#l1-regularization"><strong>L1 正则化</strong></a></li>
<li><a href="#l2-regularization"><strong>L2 正则化</strong></a></li>
</ul>
<p><a href="#gradient-descent"><strong>梯度下降法</strong></a>的很多变体都一定能找到一个接近严格凸函数最小值的点。同样，<a href="#stochastic-gradient-descent-sgd"><strong>随机梯度下降法</strong></a>的很多变体都有很高的可能性能够找到接近严格凸函数最小值的点（但并非一定能找到）。</p>
<p>两个凸函数的和（例如 L2 损失函数 + L1 正则化）也是凸函数。</p>
<p><a href="#deep-model"><strong>深度模型</strong></a>绝不会是凸函数。值得注意的是，专门针对<a href="#convex-optimization"><strong>凸优化</strong></a>设计的算法往往总能在深度网络上找到非常好的解决方案，虽然这些解决方案并不一定对应于全局最小值。</p>
<h3 id="convex-optimization">convex optimization</h3>
<p>The process of using mathematical techniques such as <a href="#gradient-descent"><strong>gradient descent</strong></a> to find the minimum of a <a href="#convex-function"><strong>convex function</strong></a>. A great deal of research in machine learning has focused on formulating various problems as convex optimization problems and in solving those problems more efficiently.</p>
<p>For complete details, see Boyd and Vandenberghe, <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>.</p>
<p>使用数学方法（例如<a href="#gradient-descent"><strong>梯度下降法</strong></a>）寻找<a href="#convex-function"><strong>凸函数</strong></a>最小值的过程。机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。</p>
<p>如需完整的详细信息，请参阅 Boyd 和 Vandenberghe 合著的 <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>（《凸优化》）。</p>
<h3 id="convex-set">convex set</h3>
<p>A subset of Euclidean space such that a line drawn between any two points in the subset remains completely within the subset. For instance, the following two shapes are convex sets:</p>
<p>欧几里德空间的一个子集，其中任意两点之间的连线仍完全落在该子集内。例如，下面的两个图形都是凸集：</p>
<p align="center">
<img src="/images/convex_set.png" width="80%"/>
<figcaption>
A rectangle and a semi-ellipse are both convex sets.
矩形和半椭圆形都是凸集
</figcaption>
</p>
<p>By contrast, the following two shapes are not convex sets:</p>
<p>相反，下面的两个图形都不是凸集：</p>
<p align="center">
<img src="/images/nonconvex_set.png" width="80%"/>
<figcaption>
A pie-chart with a missing slice and a firework are both nonconvex sets.
缺少一块的饼图以及烟花图都是非凸集
</figcaption>
</p>
<h3 id="cost">cost</h3>
<p>Synonym for <a href="#loss"><strong>loss</strong></a>.</p>
<h3 id="cross-entropy">cross-entropy</h3>
<p>A generalization of <a href="#log-loss"><strong>Log Loss</strong></a> to <a href="#multi-class-classification"><strong>multi-class classification problems</strong></a>. Cross-entropy quantifies the difference between two probability distributions. See also <a href="#perplexity"><strong>perplexity</strong></a>.</p>
<p><a href="#log-loss"><strong>对数损失函数</strong></a>向<a href="#multi-class-classification"><strong>多类别分类问题</strong></a>进行的一种泛化。交叉熵可以量化两种概率分布之间的差异。另请参阅<a href="#perplexity"><strong>困惑度</strong></a>。</p>
<h3 id="custom-estimator">custom Estimator</h3>
<p>An <a href="#estimator"><strong>Estimator</strong></a> that you write yourself by following <a href="https://www.tensorflow.org/extend/estimators">these directions</a>.</p>
<p>Contrast with <a href="#pre-made-estimator"><strong>pre-made Estimators</strong></a>.</p>
<h2 id="d_1">D</h2>
<h3 id="data-set">data set</h3>
<p>A collection of <a href="#example"><strong>examples</strong></a>.</p>
<p>一组<a href="#example"><strong>样本</strong></a>的集合。</p>
<h3 id="dataset-api-tfdata">Dataset API (tf.data)</h3>
<p>A high-level TensorFlow API for reading data and transforming it into a form that a machine learning algorithm requires. A <code>tf.data.Dataset</code> object represents a sequence of elements, in which each element contains one or more <a href="#tensor"><strong>Tensors</strong></a>. A <code>tf.data.Iterator</code> object provides access to the elements of a <code>Dataset</code>.</p>
<p>For details about the Dataset API, see <a href="https://www.tensorflow.org/programmers_guide/datasets">Importing Data</a> in the TensorFlow Programmer's Guide.</p>
<p>一种高级别的 TensorFlow API，用于读取数据并将其转换为机器学习算法所需的格式。<code>tf.data.Dataset</code> 对象表示一系列元素，其中每个元素都包含一个或多个<a href="#tensor"><strong>张量</strong></a>。<code>tf.data.Iterator</code> 对象可获取 <code>Dataset</code> 中的元素。</p>
<p>如需详细了解 Dataset API，请参阅《TensorFlow 编程人员指南》中的<a href="https://www.tensorflow.org/programmers_guide/datasets">导入数据</a>。</p>
<h3 id="decision-boundary">decision boundary</h3>
<p>The separator between classes learned by a model in a <a href="#binary-classification"><strong>binary class</strong></a> or <a href="#multi-class-classification"><strong>multi-class classification problems</strong></a>. For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange class and the blue class:</p>
<p>在<a href="#binary-classification"><strong>二元分类</strong></a>或<a href="#multi-class-classification"><strong>多类别分类问题</strong></a>中，模型学到的类别之间的分界线。例如，在以下表示某个二元分类问题的图片中，决策边界是橙色类别和蓝色类别之间的分界线：</p>
<p><img alt="A
well-defined boundary between one class and another." src="/images/decision_boundary.png"/></p>
<h3 id="dense-layer">dense layer</h3>
<p>Synonym for <a href="#fully-connected-layer"><strong>fully connected layer</strong></a>.</p>
<p>是<a href="#fully-connected-layer"><strong>全连接层</strong></a>的同义词。</p>
<h3 id="deep-model">deep model</h3>
<p>A type of <a href="#neural-network"><strong>neural network</strong></a> containing multiple <a href="#hidden-layer"><strong>hidden layers</strong></a>. Deep models rely on trainable nonlinearities.</p>
<p>Contrast with <a href="#wide-model"><strong>wide model</strong></a>.</p>
<p>一种<a href="#neural-network"><strong>神经网络</strong></a>，其中包含多个<a href="#hidden-layer"><strong>隐藏层</strong></a>。深度模型依赖于可训练的非线性关系。</p>
<p>与<a href="#wide-model"><strong>宽度模型</strong></a>相对。</p>
<h3 id="dense-feature">dense feature</h3>
<p>A <a href="#feature"><strong>feature</strong></a> in which most values are non-zero, typically a <a href="#tensor"><strong>Tensor</strong></a> of floating-point values. Contrast with <a href="#sparse-feature"><strong>sparse feature</strong></a>.</p>
<p>一种大部分数值是非零值的<a href="#feature"><strong>特征</strong></a>，通常是一个浮点值<a href="#tensor"><strong>张量</strong></a>。参照<a href="#sparse-feature"><strong>稀疏特征</strong></a>。</p>
<h3 id="derived-feature">derived feature</h3>
<p>Synonym for <a href="#synthetic-feature"><strong>synthetic feature</strong></a>.</p>
<p>是<a href="#synthetic-feature"><strong>合成特征</strong></a>的同义词。</p>
<h3 id="discrete-feature">discrete feature</h3>
<p>A <a href="#feature"><strong>feature</strong></a> with a finite set of possible values. For example, a feature whose values may only be <em>animal</em>, <em>vegetable</em>, or <em>mineral</em> is a discrete (or categorical) feature. Contrast with <a href="#continuous-feature"><strong>continuous feature</strong></a>.</p>
<p>一种<a href="#feature"><strong>特征</strong></a>，包含有限个可能值。例如，某个值只能是&ldquo;动物&rdquo;、&ldquo;蔬菜&rdquo;或&ldquo;矿物&rdquo;的特征便是一个离散特征（或分类特征）。与<a href="#continuous-feature"><strong>连续特征</strong></a>相对。</p>
<h3 id="dropout-regularization">dropout regularization</h3>
<p>A form of <a href="#regularization"><strong>regularization</strong></a> useful in training <a href="#neural-network"><strong>neural networks</strong></a>. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. For full details, see <a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>.</p>
<p>一种形式的<a href="#regularization"><strong>正则化</strong></a>，在训练<a href="#neural-network"><strong>神经网络</strong></a>方面非常有用。丢弃正则化的运作机制是，在神经网络层的一个梯度步长中移除随机选择的固定数量的单元。丢弃的单元越多，正则化效果就越强。这类似于训练神经网络以模拟较小网络的指数级规模集成学习。如需完整的详细信息，请参阅 <a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>（《丢弃：一种防止神经网络过拟合的简单方法》）。</p>
<h3 id="dynamic-model">dynamic model</h3>
<p>A <a href="#model"><strong>model</strong></a> that is trained online in a continuously updating fashion. That is, data is continuously entering the model.</p>
<p>一种<a href="#model"><strong>模型</strong></a>，以持续更新的方式在线接受训练。也就是说，数据会源源不断地进入这种模型。</p>
<h2 id="e_1">E</h2>
<h3 id="early-stopping">early stopping</h3>
<p>A method for <a href="#regularization"><strong>regularization</strong></a> that involves ending model training <em>before</em> training loss finishes decreasing. In early stopping, you end model training when the loss on a <a href="#validation-set"><strong>validation data set</strong></a> starts to increase, that is, when <a href="#generalization"><strong>generalization</strong></a> performance worsens.</p>
<p>一种<a href="#regularization"><strong>正则化</strong></a>方法，涉及在训练损失仍可以继续减少之前结束模型训练。使用早停法时，您会在基于<a href="#validation-set"><strong>验证数据集</strong></a>的损失开始增加（也就是<a href="#generalization"><strong>泛化</strong></a>效果变差）时结束模型训练。</p>
<h3 id="embeddings">embeddings</h3>
<p>A categorical feature represented as a continuous-valued feature. Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. For example, you can represent the words in an English sentence in either of the following two ways:</p>
<ul>
<li>As a million-element (high-dimensional) <a href="#sparse-feature"><strong>sparse vector</strong></a> in which all elements are integers. Each cell in the vector represents a separate English word; the value in a cell represents the number of times that word appears in a sentence. Since a single English sentence is unlikely to contain more than 50 words, nearly every cell in the vector will contain a 0. The few cells that aren't 0 will contain a low integer (usually 1) representing the number of times that word appeared in the sentence.</li>
<li>As a several-hundred-element (low-dimensional) <a href="#dense-feature"><strong>dense vector</strong></a> in which each element holds a floating-point value between 0 and 1. This is an embedding.</li>
</ul>
<p>In TensorFlow, embeddings are trained by <a href="#backpropagation"><strong>backpropagating</strong></a> <a href="#loss"><strong>loss</strong></a> just like any other parameter in a <a href="#neural-network"><strong>neural network</strong></a>.</p>
<p>一种分类特征，以连续值特征表示。通常，嵌套是指将高维度向量映射到低维度的空间。例如，您可以采用以下两种方式之一来表示英文句子中的单词：</p>
<ul>
<li>表示成包含百万个元素（高维度）的<a href="#sparse-feature"><strong>稀疏向量</strong></a>，其中所有元素都是整数。向量中的每个单元格都表示一个单独的英文单词，单元格中的值表示相应单词在句子中出现的次数。由于单个英文句子包含的单词不太可能超过 50 个，因此向量中几乎每个单元格都包含 0。少数非 0 的单元格中将包含一个非常小的整数（通常为 1），该整数表示相应单词在句子中出现的次数。</li>
<li>表示成包含数百个元素（低维度）的<a href="#dense-feature"><strong>密集向量</strong></a>，其中每个元素都包含一个介于 0 到 1 之间的浮点值。这就是一种嵌套。</li>
</ul>
<p>在 TensorFlow 中，会按<a href="#backpropagation"><strong>反向传播</strong></a><a href="#loss"><strong>损失</strong></a>训练嵌套，和训练<a href="#neural-network"><strong>神经网络</strong></a>中的任何其他参数时一样。</p>
<h3 id="empirical-risk-minimization-erm">empirical risk minimization (ERM)</h3>
<p>Choosing the model function that minimizes loss on the training set. Contrast with <a href="#structural-risk-minimization-srm"><strong>structural risk minimization</strong></a>.</p>
<p>用于选择可以将基于训练集的损失降至最低的模型函数。与<a href="#structural-risk-minimization-srm"><strong>结构风险最小化</strong></a>相对。</p>
<h3 id="ensemble">ensemble</h3>
<p>A merger of the predictions of multiple <a href="#model"><strong>models</strong></a>. You can create an ensemble via one or more of the following:</p>
<ul>
<li>different initializations</li>
<li>different <a href="#hyperparameter"><strong>hyperparameters</strong></a></li>
<li>different overall structure</li>
</ul>
<p><a href="https://www.tensorflow.org/tutorials/wide_and_deep">Deep and wide models</a> are a kind of ensemble.</p>
<p>多个<a href="#model"><strong>模型</strong></a>的预测结果的并集。您可以通过以下一项或多项来创建集成学习：</p>
<ul>
<li>不同的初始化</li>
<li>不同的<a href="#hyperparameter"><strong>超参数</strong></a></li>
<li>不同的整体结构</li>
</ul>
<p><a href="https://www.tensorflow.org/tutorials/wide_and_deep">深度模型和宽度模型</a>属于一种集成学习。</p>
<h3 id="epoch">epoch</h3>
<p>A full training pass over the entire data set such that each example has been seen once. Thus, an epoch represents <code>N</code>/<a href="#batch-size"><strong>batch size</strong></a> training <a href="#iteration"><strong>iterations</strong></a>, where <code>N</code> is the total number of examples.</p>
<p>在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。因此，一个周期表示（<code>N</code>/<a href="#batch-size"><strong>批次规模</strong></a>）次训练<a href="#iteration"><strong>迭代</strong></a>，其中 <code>N</code> 是样本总数。</p>
<h3 id="estimator">Estimator</h3>
<p>An instance of the <code>tf.Estimator</code> class, which encapsulates logic that builds a TensorFlow graph and runs a TensorFlow session. You may create your own <a href="#custom-estimator"><strong>custom Estimators</strong></a> (as described <a href="https://www.tensorflow.org/extend/estimators">here</a>) or instantiate <a href="#pre-made-estimator"><strong>pre-made Estimators</strong></a> created by others.</p>
<p><code>tf.Estimator</code> 类的一个实例，用于封装负责构建 TensorFlow 图并运行 TensorFlow 会话的逻辑。您可以创建自己的<a href="#custom-estimator"><strong>自定义 Estimator</strong></a>（如需相关介绍，请<a href="https://www.tensorflow.org/extend/estimators">点击此处</a>），也可以将其他人<a href="#pre-made-estimator"><strong>预创建的 Estimator</strong></a> 实例化。</p>
<h3 id="example">example</h3>
<p>One row of a data set. An example contains one or more <a href="#feature"><strong>features</strong></a> and possibly a <a href="#label"><strong>label</strong></a>. See also <a href="#labeled-example"><strong>labeled example</strong></a> and <a href="#unlabeled-example"><strong>unlabeled example</strong></a>.</p>
<p>数据集的一行。一个样本包含一个或多个<a href="#feature"><strong>特征</strong></a>，此外还可能包含一个<a href="#label"><strong>标签</strong></a>。另请参阅<a href="#labeled-example"><strong>有标签样本</strong></a>和<a href="#unlabeled-example"><strong>无标签样本</strong></a>。</p>
<h2 id="f_1">F</h2>
<h3 id="f1-score">F1 Score</h3>
<p>F1分数（F1 Score），是统计学中用来衡量二分类模型精确度的一种指标。
它同时兼顾了分类模型的<a href="#precision">准确率</a>和<a href="#recall">召回率</a>。
F1分数可以看作是模型准确率和召回率的一种加权平均，它的最大值是1，最小值是0。</p>
<div class="math">$$F_1 = 2 \cdot {precison \cdot recall \over precison + recall}$$</div>
<p>除了<span class="math">\(F_1\)</span> 分数之外，<span class="math">\(F_2\)</span> 分数和 <span class="math">\(F_{0.5}\)</span> 分数在统计学中也得到大量的应用。其中，<span class="math">\(F_2\)</span> 分数中，召回率的权重高于准确率，
而 <span class="math">\(F_{0.5}\)</span> 分数中，准确率的权重高于召回率。因此，更一般的公式为：</p>
<div class="math">$$F_\beta = (1+\beta^2) \cdot {precison \cdot recall \over \beta^2 \cdot precison + recall}$$</div>
<h3 id="false-negative-fn">false negative (FN)</h3>
<p>An example in which the model mistakenly predicted the <a href="#negative-class"><strong>negative class</strong></a>. For example, the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam.</p>
<p>被模型错误地预测为<a href="#negative-class"><strong>负类别</strong></a>的样本。例如，模型推断出某封电子邮件不是垃圾邮件（负类别），但该电子邮件其实是垃圾邮件。</p>
<h3 id="false-positive-fp">false positive (FP)</h3>
<p>An example in which the model mistakenly predicted the <a href="#positive-class"><strong>positive class</strong></a>. For example, the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam.</p>
<p>被模型错误地预测为<a href="#positive-class"><strong>正类别</strong></a>的样本。例如，模型推断出某封电子邮件是垃圾邮件（正类别），但该电子邮件其实不是垃圾邮件。</p>
<h3 id="false-positive-rate-fp-rate">false positive rate (FP rate)</h3>
<p>The x-axis in an <a href="#roc-receiver-operating-characteristic-curve"><strong>ROC curve</strong></a>. The FP rate is defined as follows:</p>
<div class="math">$$\text{False Positive Rate} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}$$</div>
<p><a href="#roc-receiver-operating-characteristic-curve"><strong>ROC 曲线</strong></a>中的 x 轴。FP 率的定义如下：</p>
<p><mj></mj></p>
<div class="math">$$\text{假正例率} = \frac{\text{假正例数}}{\text{假正例数} + \text{真负例数}}$$</div>
<p></p>
<h3 id="feature">feature</h3>
<p>An input variable used in making <a href="#prediction"><strong>predictions</strong></a>.</p>
<p>在进行<a href="#prediction"><strong>预测</strong></a>时使用的输入变量。</p>
<h3 id="feature-columns-featurecolumns">feature columns (FeatureColumns)</h3>
<p>A set of related features, such as the set of all possible countries in which users might live. An example may have one or more features present in a feature column.</p>
<p>Feature columns in TensorFlow also encapsulate metadata such as:</p>
<ul>
<li>the feature's data type</li>
<li>whether a feature is fixed length or should be converted to an embedding</li>
</ul>
<p>A feature column can contain a single feature.</p>
<p>"Feature column" is Google-specific terminology. A feature column is referred to as a "namespace" in the <a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit">VW</a> system (at Yahoo/Microsoft), or a <a href="https://www.csie.ntu.edu.tw/~cjlin/libffm/">field</a>.</p>
<p>一组相关特征，例如用户可能居住的所有国家/地区的集合。样本的特征列中可能包含一个或多个特征。</p>
<p>TensorFlow 中的特征列内还封装了元数据，例如：</p>
<ul>
<li>特征的数据类型</li>
<li>特征是固定长度还是应转换为嵌套</li>
</ul>
<p>特征列可以包含单个特征。</p>
<p>&ldquo;特征列&rdquo;是 Google 专用的术语。特征列在 Yahoo/Microsoft 使用的 <a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit">VW</a> 系统中称为&ldquo;命名空间&rdquo;，也称为<a href="https://www.csie.ntu.edu.tw/~cjlin/libffm/">场</a>。</p>
<h3 id="feature-cross">feature cross</h3>
<p>A <a href="#synthetic-feature"><strong>synthetic feature</strong></a> formed by crossing (multiplying or taking a Cartesian product of) individual features. Feature crosses help represent nonlinear relationships.</p>
<p>通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的<a href="#synthetic-feature"><strong>合成特征</strong></a>。特征组合有助于表示非线性关系。</p>
<h3 id="feature-engineering">feature engineering</h3>
<p>The process of determining which <a href="#feature"><strong>features</strong></a> might be useful in training a model, and then converting raw data from log files and other sources into said features. In TensorFlow, feature engineering often means converting raw log file entries to <a href="#tf.example"><strong>tf.Example</strong></a> protocol buffers. See also <a href="https://github.com/tensorflow/transform">tf.Transform</a>.</p>
<p>Feature engineering is sometimes called <strong>feature extraction</strong>.</p>
<p>指以下过程：确定哪些<a href="#feature"><strong>特征</strong></a>可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。在 TensorFlow 中，特征工程通常是指将原始日志文件条目转换为 <a href="#tf.Example"><strong>tf.Example</strong></a> proto buffer。另请参阅 <a href="https://github.com/tensorflow/transform">tf.Transform</a>。</p>
<p>特征工程有时称为<strong>特征提取</strong>。</p>
<h3 id="feature-set">feature set</h3>
<p>The group of <a href="#feature"><strong>features</strong></a> your machine learning model trains on. For example, postal code, property size, and property condition might comprise a simple feature set for a model that predicts housing prices.</p>
<p>训练机器学习模型时采用的一组<a href="#feature"><strong>特征</strong></a>。例如，对于某个用于预测房价的模型，邮政编码、房屋面积以及房屋状况可以组成一个简单的特征集。</p>
<h3 id="feature-spec">feature spec</h3>
<p>Describes the information required to extract <a href="#feature"><strong>features</strong></a> data from the <a href="#tf.example"><strong>tf.Example</strong></a> protocol buffer. Because the tf.Example protocol buffer is just a container for data, you must specify the following:</p>
<ul>
<li>the data to extract (that is, the keys for the features)</li>
<li>the data type (for example, float or int)</li>
<li>The length (fixed or variable)</li>
</ul>
<p>The <a href="#estimator"><strong>Estimator API</strong></a> provides facilities for producing a feature spec from a list of <a href="#feature-columns-featurecolumns"><strong>FeatureColumns</strong></a>.</p>
<p>用于描述如何从 <a href="#tf.example"><strong>tf.Example</strong></a> proto buffer 提取<a href="#feature"><strong>特征</strong></a>数据。由于 tf.Example proto buffer 只是一个数据容器，因此您必须指定以下内容：</p>
<ul>
<li>要提取的数据（即特征的键）</li>
<li>数据类型（例如 float 或 int）</li>
<li>长度（固定或可变）</li>
</ul>
<p><a href="#estimator"><strong>Estimator API</strong></a> 提供了一些可用来根据给定 <a href="#feature-columns-featurecolumns"><strong>FeatureColumns</strong></a> 列表生成特征规范的工具。</p>
<h3 id="full-softmax">full softmax</h3>
<p>See <a href="#softmax"><strong>softmax</strong></a>. Contrast with <a href="#candidate-sampling"><strong>candidate sampling</strong></a>.</p>
<p>请参阅 <a href="#softmax"><strong>softmax</strong></a>。与<a href="#candidate-sampling"><strong>候选采样</strong></a>相对。</p>
<h3 id="fully-connected-layer">fully connected layer</h3>
<p>A <a href="#hidden-layer"><strong>hidden layer</strong></a> in which each <a href="#node"><strong>node</strong></a> is connected to <em>every</em> node in the subsequent hidden layer.</p>
<p>A fully connected layer is also known as a <a href="#dense-layer"><strong>dense layer</strong></a>.</p>
<p>一种<a href="#hidden-layer"><strong>隐藏层</strong></a>，其中的每个<a href="#node"><strong>节点</strong></a>均与下一个隐藏层中的每个节点相连。</p>
<p>全连接层又称为<a href="#dense-layer"><strong>密集层</strong></a>。</p>
<h2 id="g_1">G</h2>
<h3 id="generalization">generalization</h3>
<p>Refers to your model's ability to make correct predictions on new, previously unseen data as opposed to the data used to train the model.</p>
<p>指的是模型依据训练时采用的数据，针对以前未见过的新数据做出正确预测的能力。</p>
<h3 id="generalized-linear-model">generalized linear model</h3>
<p>A generalization of <a href="https://developers.google.com/machine-learning/glossary/least_squares_regression"><strong>least squares regression</strong></a> models, which are based on <a href="https://en.wikipedia.org/wiki/Gaussian_noise">Gaussian noise</a>, to other types of models based on other types of noise, such as <a href="https://en.wikipedia.org/wiki/Shot_noise">Poisson noise</a> or categorical noise. Examples of generalized linear models include:</p>
<ul>
<li><a href="#logistic-regression"><strong>logistic regression</strong></a></li>
<li>multi-class regression</li>
<li>least squares regression</li>
</ul>
<p>The parameters of a generalized linear model can be found through <a href="https://en.wikipedia.org/wiki/Convex_optimization">convex optimization</a>.</p>
<p>Generalized linear models exhibit the following properties:</p>
<ul>
<li>The average prediction of the optimal least squares regression model is equal to the average label on the training data.</li>
<li>The average probability predicted by the optimal logistic regression model is equal to the average label on the training data.</li>
</ul>
<p>The power of a generalized linear model is limited by its features. Unlike a deep model, a generalized linear model cannot "learn new features."</p>
<p><a href="https://developers.google.com/machine-learning/glossary/least_squares_regression"><strong>最小二乘回归</strong></a>模型（基于<a href="https://en.wikipedia.org/wiki/Gaussian_noise">高斯噪声</a>）向其他类型的模型（基于其他类型的噪声，例如<a href="https://en.wikipedia.org/wiki/Shot_noise">泊松噪声</a>或分类噪声）进行的一种泛化。广义线性模型的示例包括：</p>
<ul>
<li><a href="#logistic-regression"><strong>逻辑回归</strong></a></li>
<li>多类别回归</li>
<li>最小二乘回归</li>
</ul>
<p>可以通过<a href="https://en.wikipedia.org/wiki/Convex_optimization">凸优化</a>找到广义线性模型的参数。</p>
<p>广义线性模型具有以下特性：</p>
<ul>
<li>最优的最小二乘回归模型的平均预测结果等于训练数据的平均标签。</li>
<li>最优的逻辑回归模型预测的平均概率等于训练数据的平均标签。</li>
</ul>
<p>广义线性模型的功能受其特征的限制。与深度模型不同，广义线性模型无法&ldquo;学习新特征&rdquo;。</p>
<h3 id="gradient">gradient</h3>
<p>The vector of <a href="#partial-derivative"><strong>partial derivatives</strong></a> with respect to all of the independent variables. In machine learning, the gradient is the the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent.</p>
<p><a href="#partial-derivative"><strong>偏导数</strong></a>相对于所有自变量的向量。在机器学习中，梯度是模型函数偏导数的向量。梯度指向最速上升的方向。</p>
<h3 id="gradient-clipping">gradient clipping</h3>
<p>Capping <a href="#gradient"><strong>gradient</strong></a> values before applying them. Gradient clipping helps ensure numerical stability and prevents <a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf">exploding gradients</a>.</p>
<p>在应用<a href="#gradient"><strong>梯度</strong></a>值之前先设置其上限。梯度裁剪有助于确保数值稳定性以及防止<a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf">梯度爆炸</a>。</p>
<h3 id="gradient-descent">gradient descent</h3>
<p>A technique to minimize <a href="#loss"><strong>loss</strong></a> by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of <a href="#weight"><strong>weights</strong></a> and bias to minimize loss.</p>
<p>一种通过计算并且减小梯度将<a href="#loss"><strong>损失</strong></a>降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到<a href="#weight"><strong>权重</strong></a>和偏差的最佳组合，从而将损失降至最低。</p>
<h3 id="graph">graph</h3>
<p>In TensorFlow, a computation specification. Nodes in the graph represent operations. Edges are directed and represent passing the result of an operation (a <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor">Tensor</a>) as an operand to another operation. Use <a href="#tensorboard"><strong>TensorBoard</strong></a> to visualize a graph.</p>
<p>TensorFlow 中的一种计算规范。图中的节点表示操作。边缘具有方向，表示将某项操作的结果（一个<a href="https://www.tensorflow.org/api_docs/python/tf/Tensor">张量</a>）作为一个操作数传递给另一项操作。可以使用 <a href="#tensorboard"><strong>TensorBoard</strong></a> 直观呈现图。</p>
<h2 id="h_1">H</h2>
<h3 id="heuristic">heuristic</h3>
<p>A practical and nonoptimal solution to a problem, which is sufficient for making progress or for learning from.</p>
<p>一种非最优但实用的问题解决方案，足以用于进行改进或从中学习。</p>
<h3 id="hidden-layer">hidden layer</h3>
<p>A synthetic layer in a <a href="#neural-network"><strong>neural network</strong></a> between the <a href="#input-layer"><strong>input layer</strong></a> (that is, the features) and the <a href="#output-layer"><strong>output layer</strong></a> (the prediction). A neural network contains one or more hidden layers.</p>
<p><a href="#neural-network"><strong>神经网络</strong></a>中的合成层，介于<a href="#input-layer"><strong>输入层</strong></a>（即特征）和<a href="#output-layer"><strong>输出层</strong></a>（即预测）之间。神经网络包含一个或多个隐藏层。</p>
<h3 id="hinge-loss">hinge loss</h3>
<p>A family of <a href="#loss"><strong>loss</strong></a> functions for <a href="#classification-model"><strong>classification</strong></a> designed to find the <a href="#decision-boundary"><strong>decision boundary</strong></a> as distant as possible from each training example, thus maximizing the margin between examples and the boundary. <a href="#kernel-support-vector-machines-ksvms"><strong>KSVMs</strong></a> use hinge loss (or a related function, such as squared hinge loss). For binary classification, the hinge loss function is defined as follows:</p>
<p>一系列用于<a href="#classification-model"><strong>分类</strong></a>的<a href="#loss"><strong>损失</strong></a>函数，旨在找到距离每个训练样本都尽可能远的<a href="#decision-boundary"><strong>决策边界</strong></a>，从而使样本和边界之间的裕度最大化。 <a href="#kernel-support-vector-machines-ksvms"><strong>KSVM</strong></a> 使用合页损失函数（或相关函数，例如平方合页损失函数）。对于二元分类，合页损失函数的定义如下：</p>
<div class="math">$$\text{loss} = \text{max}(0, 1 - (y' * y))$$</div>
<p>where <em>y'</em> is the raw output of the classifier model:</p>
<p>其中&ldquo;y'&rdquo;表示分类器模型的原始输出：</p>
<div class="math">$$y' = b + w_1x_1 + w_2x_2 + &hellip; w_nx_n$$</div>
<p>and <em>y</em> is the true label, either -1 or +1.</p>
<p>Consequently, a plot of hinge loss vs. (y * y') looks as follows:</p>
<p>&ldquo;y&rdquo;表示真标签，值为 -1 或 +1。</p>
<p>因此，合页损失与 (y * y') 的关系图如下所示：</p>
<p>
<svg height="566.00629" id="svg2" version="1.1" viewbox="0 0 659.45264 566.00629" width="659.45264" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg">
<title>Hinge loss vs. (y * y')</title>
<desc>A plot of hinge loss vs. raw classifier score shows a distinct hinge at the
coordinate (1,0).</desc>
<defs id="defs6"><clippath clippathunits="userSpaceOnUse" id="clipPath20"><path d="M 0,0 H 365760 V 274320 H 0 Z" id="path18"></path></clippath></defs><g id="g10" transform="matrix(1.3333333,0,0,-1.3333333,-22.422343,676.03376)"><path d="M 0,540 H 719.99856 V 0.00108 H 0 Z" id="path28" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 95.419101,498.0867 V 157.52832" id="path36" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 95.419101,498.0867 V 157.52832" id="path38" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 95.419101,157.76328 H 503.46553" id="path46" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 95.419101,157.76328 H 503.46553" id="path48" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 294.60847,173.49483 V 142.03032" id="path56" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 294.60847,173.49483 V 142.03032" id="path58" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 362.51227,173.49483 V 142.03032" id="path66" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 362.51227,173.49483 V 142.03032" id="path68" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 430.41607,173.49483 V 142.03032" id="path76" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 430.41607,173.49483 V 142.03032" id="path78" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 226.70466,173.49483 V 142.03032" id="path86" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 226.70466,173.49483 V 142.03032" id="path88" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 158.80086,173.49483 V 142.03032" id="path96" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 158.80086,173.49483 V 142.03032" id="path98" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 498.31987,173.49483 V 142.03032" id="path106" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 498.31987,173.49483 V 142.03032" id="path108" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="m 282.09479,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path116" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text126" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="292.508" y="-122.12723"><tspan id="tspan124" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="292.508" y="-122.12723">0</tspan></text>
<path d="m 124.85634,138.47718 h 66.992 v -31.4645 h -66.992 z" id="path136" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text146" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="153.90829" y="-122.12723"><tspan id="tspan144" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="153.90829 157.23828" y="-122.12723">-2</tspan></text>
<path d="m 198.28295,138.47718 h 56.50382 v -31.4645 h -56.50382 z" id="path156" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text166" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="222.09081" y="-122.12723"><tspan id="tspan164" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="222.09081 225.42079" y="-122.12723">-1</tspan></text>
<path d="m 350.12473,138.47718 h 26.38578 v -31.4645 h -26.38578 z" id="path176" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text186" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="360.53796" y="-122.12723"><tspan id="tspan184" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="360.53796" y="-122.12723">1</tspan></text>
<path d="m 417.22358,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path196" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text206" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="427.63678" y="-122.12723"><tspan id="tspan204" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="427.63678" y="-122.12723">2</tspan></text>
<path d="m 485.02047,138.47718 h 26.38577 v -31.4645 h -26.38577 z" id="path216" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text226" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="495.43369" y="-122.12723"><tspan id="tspan224" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="495.43369" y="-122.12723">3</tspan></text>
<path d="M 111.32608,415.76206 H 79.861573" id="path236" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 111.32608,415.76206 H 79.861573" id="path238" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 111.32608,347.85826 H 79.861573" id="path246" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 111.32608,347.85826 H 79.861573" id="path248" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 111.32608,279.95446 H 79.861573" id="path256" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 111.32608,279.95446 H 79.861573" id="path258" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 111.32608,483.66586 H 79.861573" id="path266" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 111.32608,483.66586 H 79.861573" id="path268" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 50.757773,302.93551 H 77.143547 V 255.21907 H 50.757773 Z" id="path276" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text286" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="61.170982" y="-275.47729"><tspan id="tspan284" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="61.170982" y="-275.47729">1</tspan></text>
<path d="M 53.622428,371.98048 H 80.008206 V 324.26403 H 53.622428 Z" id="path296" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text306" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-344.52228"><tspan id="tspan304" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-344.52228">2</tspan></text>
<path d="M 53.622428,438.99539 H 80.008206 V 391.27895 H 53.622428 Z" id="path316" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text326" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-411.53717"><tspan id="tspan324" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-411.53717">3</tspan></text>
<path d="M 53.622428,507.02533 H 80.008206 V 459.30889 H 53.622428 Z" id="path336" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text346" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="64.035637" y="-479.56711"><tspan id="tspan344" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="64.035637" y="-479.56711">4</tspan></text>
<path d="M 95.053282,483.16711 360.44646,217.77393" id="path356" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 95.053282,483.16711 360.44646,217.77393" id="path358" style="fill:none;stroke:#ff0000;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 359.32928,218.08535 H 501.43923" id="path366" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 359.32928,218.08535 H 501.43923" id="path368" style="fill:none;stroke:#ff0000;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 50.757773,241.9206 H 77.143547 V 194.20416 H 50.757773 Z" id="path376" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text386" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="61.170982" y="-214.46239"><tspan id="tspan384" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="61.170982" y="-214.46239">0</tspan></text>
<path d="M 111.32608,217.92452 H 79.861573" id="path396" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="M 111.32608,217.92452 H 79.861573" id="path398" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1"></path><path d="M 9.299194,394.88415 H 62.614048 V 344.4748 H 9.299194 Z" id="path406" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><text id="text416" style="font-variant:normal;font-weight:normal;font-size:11.99997616px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="16.049181" y="-376.6142"><tspan id="tspan414" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="16.049181 22.721169 25.385162 32.057148 38.729134 45.401123" y="-376.6142">hinge </tspan></text>
<text id="text420" style="font-variant:normal;font-weight:normal;font-size:11.99997616px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="16.049181" y="-362.36423"><tspan id="tspan418" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="16.049181 18.713175 25.385162 31.385151" y="-362.36423">loss</tspan></text>
<path d="M 95.419101,110.41818 H 498.31593 V 82.5206 H 95.419101 Z" id="path430" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685"></path><path d="m 266.48095,103.66819 h 60.77311 V 88.068225 h -60.77311 z" id="path438" style="fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851"></path><text id="text442" style="font-variant:normal;font-weight:normal;font-size:12.99997425px;font-family:Consolas;-inkscape-font-specification:Consolas;writing-mode:lr-tb;fill:#222222;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)" x="270.09167" y="-91.188217"><tspan id="tspan440" style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" x="270.09167 277.22867 284.36563 291.50262 298.63962 305.77661 312.91357 320.05057" y="-91.188217">(y * y')</tspan></text>
</g></svg>
</p>
<p>Hinge loss vs. (y * y') A plot of hinge loss vs. raw classifier score shows a distinct hinge at the coordinate (1,0). </p>
<h3 id="holdout-data">holdout data</h3>
<p><a href="#example"><strong>Examples</strong></a> intentionally not used ("held out") during training. The <a href="#validation-set"><strong>validation data set</strong></a> and <a href="#test-set"><strong>test data set</strong></a> are examples of holdout data. Holdout data helps evaluate your model's ability to generalize to data other than the data it was trained on. The loss on the holdout set provides a better estimate of the loss on an unseen data set than does the loss on the training set.</p>
<p>训练期间故意不使用（&ldquo;维持&rdquo;）的<a href="#example"><strong>样本</strong></a>。<a href="#validation-set"><strong>验证数据集</strong></a>和<a href="#test-set"><strong>测试数据集</strong></a>都属于维持数据。维持数据有助于评估模型向训练时所用数据之外的数据进行泛化的能力。与基于训练数据集的损失相比，基于维持数据集的损失有助于更好地估算基于未见过的数据集的损失。</p>
<h3 id="hyperparameter">hyperparameter</h3>
<p>The "knobs" that you tweak during successive runs of training a model. For example, <a href="#learning-rate"><strong>learning rate</strong></a> is a hyperparameter.</p>
<p>Contrast with <a href="#parameter"><strong>parameter</strong></a>.</p>
<p>在模型训练的连续过程中，您调节的&ldquo;旋钮&rdquo;。例如，<a href="#learning-rate"><strong>学习速率</strong></a>就是一种超参数。</p>
<p>与<a href="#parameter"><strong>参数</strong></a>相对。</p>
<h3 id="hyperplane">hyperplane</h3>
<p>A boundary that separates a space into two subspaces. For example, a line is a hyperplane in two dimensions and a plane is a hyperplane in three dimensions. More typically in machine learning, a hyperplane is the boundary separating a high-dimensional space. <a href="#kernel-support-vector-machines-ksvms"><strong>Kernel Support Vector Machines</strong></a> use hyperplanes to separate positive classes from negative classes, often in a very high-dimensional space.</p>
<p>将一个空间划分为两个子空间的边界。例如，在二维空间中，直线就是一个超平面，在三维空间中，平面则是一个超平面。在机器学习中更典型的是：超平面是分隔高维度空间的边界。<a href="#kernel-support-vector-machines-ksvms"><strong>核支持向量机</strong></a>利用超平面将正类别和负类别区分开来（通常是在极高维度空间中）。</p>
<h2 id="i_1">I</h2>
<h3 id="independently-and-identically-distributed-iid">independently and identically distributed (i.i.d)</h3>
<p>Data drawn from a distribution that doesn't change, and where each value drawn doesn't depend on values that have been drawn previously. An i.i.d. is the <a href="https://en.wikipedia.org/wiki/Ideal_gas">ideal gas</a> of machine learning&mdash;a useful mathematical construct but almost never exactly found in the real world. For example, the distribution of visitors to a web page may be i.i.d. over a brief window of time; that is, the distribution doesn't change during that brief window and one person's visit is generally independent of another's visit. However, if you expand that window of time, seasonal differences in the web page's visitors may appear.</p>
<p>从不会改变的分布中提取的数据，其中提取的每个值都不依赖于之前提取的值。i.i.d. 是机器学习的<a href="https://en.wikipedia.org/wiki/Ideal_gas">理想气体</a> - 一种实用的数学结构，但在现实世界中几乎从未发现过。例如，某个网页的访问者在短时间内的分布可能为 i.i.d.，即分布在该短时间内没有变化，且一位用户的访问行为通常与另一位用户的访问行为无关。不过，如果将时间窗口扩大，网页访问者的分布可能呈现出季节性变化。</p>
<h3 id="inference">inference</h3>
<p>In machine learning, often refers to the process of making predictions by applying the trained model to <a href="#unlabeled-example"><strong>unlabeled examples</strong></a>. In statistics, inference refers to the process of fitting the parameters of a distribution conditioned on some observed data. (See the <a href="https://en.wikipedia.org/wiki/Statistical_inference">Wikipedia article on statistical inference</a>.)</p>
<p>在机器学习中，推断通常指以下过程：通过将训练过的模型应用于<a href="#unlabeled-example"><strong>无标签样本</strong></a>来做出预测。在统计学中，推断是指在某些观测数据条件下拟合分布参数的过程。（请参阅<a href="https://en.wikipedia.org/wiki/Statistical_inference">维基百科中有关统计学推断的文章</a>。）</p>
<h3 id="input-function">input function</h3>
<p>In TensorFlow, a function that returns input data to the training, evaluation, or prediction method of an <a href="#estimator"><strong>Estimator</strong></a>. For example, the training input function returns a <a href="#batch"><strong>batch</strong></a> of features and labels from the <a href="#training-set"><strong>training set</strong></a>.</p>
<p>在 TensorFlow 中，用于将输入数据返回到 <a href="#estimator"><strong>Estimator</strong></a> 的训练、评估或预测方法的函数。例如，训练输入函数用于返回<a href="#training-set"><strong>训练集</strong></a>中的<a href="#batch"><strong>批次</strong></a>特征和标签。</p>
<h3 id="input-layer">input layer</h3>
<p>The first layer (the one that receives the input data) in a <a href="#neural-network"><strong>neural network</strong></a>.</p>
<p><a href="#neural-network"><strong>神经网络</strong></a>中的第一层（接收输入数据的层）。</p>
<h3 id="instance">instance</h3>
<p>Synonym for <a href="#example"><strong>example</strong></a>.</p>
<p>是<a href="#example"><strong>样本</strong></a>的同义词。</p>
<h3 id="interpretability">interpretability</h3>
<p>The degree to which a model's predictions can be readily explained. Deep models are often non-interpretable; that is, a deep model's different layers can be hard to decipher. By contrast, linear regression models and <a href="#wide-model"><strong>wide models</strong></a> are typically far more interpretable.</p>
<p>模型的预测可解释的难易程度。深度模型通常不可解释，也就是说，很难对深度模型的不同层进行解释。相比之下，线性回归模型和<a href="#wide-model"><strong>宽度模型</strong></a>的可解释性通常要好得多。</p>
<h3 id="inter-rater-agreement">inter-rater agreement</h3>
<p>A measurement of how often human raters agree when doing a task. If raters disagree, the task instructions may need to be improved. Also sometimes called <strong>inter-annotator agreement</strong> or <strong>inter-rater reliability</strong>. See also <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen's kappa</a>, which is one of the most popular inter-rater agreement measurements.</p>
<p>一种衡量指标，用于衡量在执行某项任务时评分者达成一致的频率。如果评分者未达成一致，则可能需要改进任务说明。有时也称为<strong>注释者间一致性信度</strong>或<strong>评分者间可靠性信度</strong>。另请参阅 <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen's kappa</a>（最热门的评分者间一致性信度衡量指标之一）。</p>
<h3 id="iou">IoU</h3>
<p>IoU( Intersection over Union ), 交集并集比。</p>
<p align="center">
<img src="/images/IoU_sample.png" width="70%">
<figcaption>
A typical convex function is shaped like the letter 'U'.
</figcaption>
</img></p>
<p>在目标检测任务中，要用矩形框标出目标物体的位置。上图绿色框是贴合目标物体的区域，红色框是预测区域，这种情况下交集确实是最大的，但是红色框并不能准确预测物体位置。因为预测区域总是试图覆盖目标物体而不是正好预测物体位置。如果我们能除以一个并集的大小，就可以规避这种问题, 公式如下：</p>
<div class="math">$$IoU = {Area(B_p \bigcap B_{gt}) \over Area(B_p \bigcup B_{gt})}$$</div>
<p>其中， <span class="math">\(B_p\)</span>代表预测框， <span class="math">\(B_{gt}\)</span>代表真值框。</p>
<h3 id="iteration">iteration</h3>
<p>A single update of a model's weights during training. An iteration consists of computing the gradients of the parameters with respect to the loss on a single <a href="#batch"><strong>batch</strong></a> of data.</p>
<p>模型的权重在训练期间的一次更新。迭代包含计算参数在单个<a href="#batch"><strong>批量</strong></a>数据上的梯度损失。</p>
<h2 id="k_1">K</h2>
<h3 id="keras">Keras</h3>
<p>A popular Python machine learning API. <a href="https://keras.io">Keras</a> runs on several deep learning frameworks, including TensorFlow, where it is made available as <a href="https://www.tensorflow.org/api_docs/python/tf/keras"><strong>tf.keras</strong></a>.</p>
<p>一种热门的 Python 机器学习 API。<a href="https://keras.io">Keras</a> 能够在多种深度学习框架上运行，其中包括 TensorFlow（在该框架上，Keras 作为 <a href="https://www.tensorflow.org/api_docs/python/tf/keras"><strong>tf.keras</strong></a> 提供）。</p>
<h3 id="kernel-support-vector-machines-ksvms">Kernel Support Vector Machines (KSVMs)</h3>
<p>A classification algorithm that seeks to maximize the margin between <a href="#positive-class"><strong>positive</strong></a> and <a href="#negative-class"><strong>negative classes</strong></a> by mapping input data vectors to a higher dimensional space. For example, consider a classification problem in which the input data set consists of a hundred features. In order to maximize the margin between positive and negative classes, KSVMs could internally map those features into a million-dimension space. KSVMs uses a loss function called <a href="#hinge-loss">hinge loss</a>.</p>
<p>一种分类算法，旨在通过将输入数据向量映射到更高维度的空间，来最大化<a href="#positive-class"><strong>正类别</strong></a>和<a href="#negative-class"><strong>负类别</strong></a>之间的裕度。以某个输入数据集包含一百个特征的分类问题为例。为了最大化正类别和负类别之间的裕度，KSVM 可以在内部将这些特征映射到百万维度的空间。KSVM 使用<a href="#hinge-loss">合页损失函数</a>。</p>
<h2 id="l_1">L</h2>
<h3 id="l1-loss">L1 loss</h3>
<p><a href="#loss"><strong>Loss</strong></a> function based on the absolute value of the difference between the values that a model is predicting and the actual values of the <a href="#label"><strong>labels</strong></a>. L1 loss is less sensitive to outliers than <a href="#squared-loss"><strong>L2 loss</strong></a>.</p>
<p>一种<a href="#loss"><strong>损失</strong></a>函数，基于模型预测的值与<a href="#label"><strong>标签</strong></a>的实际值之差的绝对值。与 <a href="#squared-loss"><strong>L2 损失函数</strong></a>相比，L1 损失函数对离群值的敏感性弱一些。</p>
<h3 id="l1-regularization">L1 regularization</h3>
<p>A type of <a href="#regularization"><strong>regularization</strong></a> that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on <a href="#sparse-feature"><strong>sparse features</strong></a>, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with <a href="#l2-regularization"><strong>L2 regularization</strong></a>.</p>
<p>一种<a href="#regularization"><strong>正则化</strong></a>，根据权重的绝对值的总和来惩罚权重。在依赖<a href="#sparse-feature"><strong>稀疏特征</strong></a>的模型中，L1 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 <a href="#l2-regularization"><strong>L2 正则化</strong></a>相对。</p>
<h3 id="l2-loss">L2 loss</h3>
<p>See <a href="#squared-loss"><strong>squared loss</strong></a>.</p>
<p>请参阅<a href="#squared-loss"><strong>平方损失函数</strong></a>。</p>
<h3 id="l2-regularization">L2 regularization</h3>
<p>A type of <a href="#regularization"><strong>regularization</strong></a> that penalizes weights in proportion to the sum of the <em>squares</em> of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with <a href="#l1-regularization"><strong>L1 regularization</strong></a>.) L2 regularization always improves generalization in linear models.</p>
<p>一种<a href="#regularization"><strong>正则化</strong></a>，根据权重的平方和来惩罚权重。L2 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 <a href="#l1-regularization"><strong>L1 正则化</strong></a>相对。）在线性模型中，L2 正则化始终可以改进泛化。</p>
<h3 id="label">label</h3>
<p>In supervised learning, the "answer" or "result" portion of an <a href="#example"><strong>example</strong></a>. Each example in a labeled data set consists of one or more features and a label. For instance, in a housing data set, the features might include the number of bedrooms, the number of bathrooms, and the age of the house, while the label might be the house's price. in a spam detection dataset, the features might include the subject line, the sender, and the email message itself, while the label would probably be either "spam" or "not spam."</p>
<p>在监督式学习中，标签指<a href="#example"><strong>样本</strong></a>的&ldquo;答案&rdquo;或&ldquo;结果&rdquo;部分。有标签数据集中的每个样本都包含一个或多个特征以及一个标签。例如，在房屋数据集中，特征可以包括卧室数、卫生间数以及房龄，而标签则可以是房价。在垃圾邮件检测数据集中，特征可以包括主题行、发件人以及电子邮件本身，而标签则可以是&ldquo;垃圾邮件&rdquo;或&ldquo;非垃圾邮件&rdquo;。</p>
<h3 id="labeled-example">labeled example</h3>
<p>An example that contains <a href="#feature"><strong>features</strong></a> and a <a href="#label"><strong>label</strong></a>. In supervised training, models learn from labeled examples.</p>
<p>包含<a href="#feature"><strong>特征</strong></a>和<a href="#label"><strong>标签</strong></a>的样本。在监督式训练中，模型从有标签样本中进行学习。</p>
<h3 id="lambda">lambda</h3>
<p>Synonym for <a href="#regularization-rate"><strong>regularization rate</strong></a>.</p>
<p>(This is an overloaded term. Here we're focusing on the term's definition within <a href="#regularization"><strong>regularization</strong></a>.)</p>
<p>是<a href="#regularization-rate"><strong>正则化率</strong></a>的同义词。</p>
<p>（多含义术语，我们在此关注的是该术语在<a href="#regularization"><strong>正则化</strong></a>中的定义。）</p>
<h3 id="layer">layer</h3>
<p>A set of <a href="#neuron"><strong>neurons</strong></a> in a <a href="#neural-network"><strong>neural network</strong></a> that process a set of input features, or the output of those neurons.</p>
<p>Also, an abstraction in TensorFlow. Layers are Python functions that take <a href="#tensor"><strong>Tensors</strong></a> and configuration options as input and produce other tensors as output. Once the necessary Tensors have been composed, the user can convert the result into an <a href="#estimator"><strong>Estimator</strong></a> via a model function.</p>
<p><a href="#neural-network"><strong>神经网络</strong></a>中的一组<a href="#neuron"><strong>神经元</strong></a>，处理一组输入特征，或一组神经元的输出。</p>
<p>此外还指 TensorFlow 中的抽象层。层是 Python 函数，以<a href="#tensor"><strong>张量</strong></a>和配置选项作为输入，然后生成其他张量作为输出。当必要的张量组合起来，用户便可以通过模型函数将结果转换为 <a href="#estimator"><strong>Estimator</strong></a>。</p>
<h3 id="layers-api-tflayers">Layers API (tf.layers)</h3>
<p>A TensorFlow API for constructing a <a href="#deep-model"><strong>deep</strong></a> neural network as a composition of layers. The Layers API enables you to build different types of <a href="#layer"><strong>layers</strong></a>, such as:</p>
<ul>
<li><code>tf.layers.Dense</code> for a <a href="#fully-connected-layer"><strong>fully-connected layer</strong></a>.</li>
<li><code>tf.layers.Conv2D</code> for a convolutional layer.</li>
</ul>
<p>When writing a <a href="#custom-estimator"><strong>custom Estimator</strong></a>, you compose Layers objects to define the characteristics of all the <a href="#hidden-layer"><strong>hidden layers</strong></a>.</p>
<p>The Layers API follows the <a href="#keras"><strong>Keras</strong></a> layers API conventions. That is, aside from a different prefix, all functions in the Layers API have the same names and signatures as their counterparts in the Keras layers API.</p>
<p>一种 TensorFlow API，用于以层组合的方式构建<a href="#deep-model"><strong>深度</strong></a>神经网络。通过 Layers API，您可以构建不同类型的<a href="#layer"><strong>层</strong></a>，例如：</p>
<ul>
<li>通过 <code>tf.layers.Dense</code> 构建<a href="#fully-connected-layer"><strong>全连接层</strong></a>。</li>
<li>通过 <code>tf.layers.Conv2D</code> 构建卷积层。</li>
</ul>
<p>在编写<a href="#custom-estimator"><strong>自定义 Estimator</strong></a> 时，您可以编写&ldquo;层&rdquo;对象来定义所有<a href="#hidden-layers"><strong>隐藏层</strong></a>的特征。</p>
<p>Layers API 遵循 <a href="#keras"><strong>Keras</strong></a> layers API 规范。也就是说，除了前缀不同以外，Layers API 中的所有函数均与 Keras layers API 中的对应函数具有相同的名称和签名。</p>
<h3 id="learning-rate">learning rate</h3>
<p>A scalar used to train a model via gradient descent. During each iteration, the <a href="#gradient-descent"><strong>gradient descent</strong></a> algorithm multiplies the learning rate by the gradient. The resulting product is called the <strong>gradient step</strong>.</p>
<p>Learning rate is a key <a href="#hyperparameter"><strong>hyperparameter</strong></a>.</p>
<p>在训练模型时用于梯度下降的一个变量。在每次迭代期间，<a href="#gradient-descent"><strong>梯度下降法</strong></a>都会将学习速率与梯度相乘。得出的乘积称为<strong>梯度步长</strong>。</p>
<p>学习速率是一个重要的<a href="#hyperparameter"><strong>超参数</strong></a>。</p>
<h3 id="least-squares-regression">least squares regression</h3>
<p>A linear regression model trained by minimizing <a href="#l2-loss"><strong>L2 Loss</strong></a>.</p>
<p>一种通过最小化 <a href="#l2-loss"><strong>L2 损失</strong></a>训练出的线性回归模型。</p>
<h3 id="linear-regression">linear regression</h3>
<p>A type of <a href="#regression-model"><strong>regression model</strong></a> that outputs a continuous value from a linear combination of input features.</p>
<p>一种<a href="#regression-model"><strong>回归模型</strong></a>，通过将输入特征进行线性组合，以连续值作为输出。</p>
<h3 id="logistic-regression">logistic regression</h3>
<p>A model that generates a probability for each possible discrete label value in classification problems by applying a <a href="#sigmoid-function"><strong>sigmoid function</strong></a> to a linear prediction. Although logistic regression is often used in <a href="#binary-classification"><strong>binary classification</strong></a> problems, it can also be used in <a href="#multi-class-classification"><strong>multi-class</strong></a> classification problems (where it becomes called <strong>multi-class logistic regression</strong> or <strong>multinomial regression</strong>).</p>
<p>一种模型，通过将 <a href="#sigmoid-function"><strong>S 型函数</strong></a>应用于线性预测，生成分类问题中每个可能的离散标签值的概率。虽然逻辑回归经常用于<a href="#binary-classification"><strong>二元分类</strong></a>问题，但也可用于<a href="#multi-class-classification"><strong>多类别</strong></a>分类问题（其叫法变为<strong>多类别逻辑回归</strong>或<strong>多项回归</strong>）。</p>
<h3 id="log-loss">Log Loss</h3>
<p>The <a href="#loss"><strong>loss</strong></a> function used in binary <a href="#logistic-regression"><strong>logistic regression</strong></a>.</p>
<p>二元<a href="#logistic-regression"><strong>逻辑回归</strong></a>中使用的<a href="#loss"><strong>损失</strong></a>函数。</p>
<h3 id="loss">loss</h3>
<p>A measure of how far a model's <a href="#prediction"><strong>predictions</strong></a> are from its <a href="#label"><strong>label</strong></a>. Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use <a href="#mean-squared-error-mse"><strong>mean squared error</strong></a> for a loss function, while logistic regression models use <a href="#log-loss"><strong>Log Loss</strong></a>.</p>
<p>一种衡量指标，用于衡量模型的<a href="#prediction"><strong>预测</strong></a>偏离其<a href="#label"><strong>标签</strong></a>的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将<a href="#mean-squared-error-mse"><strong>均方误差</strong></a>用于损失函数，而逻辑回归模型则使用<a href="#log-loss"><strong>对数损失函数</strong></a>。</p>
<h2 id="m_1">M</h2>
<h3 id="machine-learning">machine learning</h3>
<p>A program or system that builds (trains) a predictive model from input data. The system uses the learned model to make useful predictions from new (never-before-seen) data drawn from the same distribution as the one used to train the model. Machine learning also refers to the field of study concerned with these programs or systems.</p>
<p>一种程序或系统，用于根据输入数据构建（训练）预测模型。这种系统会利用学到的模型根据从分布（训练该模型时使用的同一分布）中提取的新数据（以前从未见过的数据）进行实用的预测。机器学习还指与这些程序或系统相关的研究领域。</p>
<h3 id="map">mAP</h3>
<p>平均精度均值，衡量学出的模型在所有类别上的好坏。</p>
<div class="math">$$mAP = {\sum_{i=1}^{C}AP_i \over C} $$</div>
<p>其中，<span class="math">\(C\)</span> 表示类别的数量。</p>
<h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3>
<p>The average squared loss per example. MSE is calculated by dividing the <a href="#squared-loss"><strong>squared loss</strong></a> by the number of <a href="#example"><strong>examples</strong></a>. The values that <a href="#tensorflow-playground"><strong>TensorFlow Playground</strong></a> displays for "Training loss" and "Test loss" are MSE.</p>
<p>每个样本的平均平方损失。MSE 的计算方法是<a href="#squared-loss"><strong>平方损失</strong></a>除以<a href="#example"><strong>样本</strong></a>数。<a href="#tensorflow-playground"><strong>TensorFlow Playground</strong></a> 显示的&ldquo;训练损失&rdquo;值和&ldquo;测试损失&rdquo;值都是 MSE。</p>
<h3 id="metric">metric</h3>
<p>A number that you care about. May or may not be directly optimized in a machine-learning system. A metric that your system tries to optimize is called an <a href="#objective"><strong>objective</strong></a>.</p>
<p>您关心的一个数值。可能可以也可能不可以直接在机器学习系统中得到优化。您的系统尝试优化的指标称为<a href="#objective"><strong>目标</strong></a>。</p>
<h3 id="metrics-api-tfmetrics">Metrics API (tf.metrics)</h3>
<p>A TensorFlow API for evaluating models. For example, <code>tf.metrics.accuracy</code> determines how often a model's predictions match labels. When writing a <a href="#custom-estimator"><strong>custom Estimator</strong></a>, you invoke Metrics API functions to specify how your model should be evaluated.</p>
<p>一种用于评估模型的 TensorFlow API。例如，<code>tf.metrics.accuracy</code> 用于确定模型的预测与标签匹配的频率。在编写<a href="#custom-estimator"><strong>自定义 Estimator</strong></a> 时，您可以调用 Metrics API 函数来指定应如何评估您的模型。</p>
<h3 id="mini-batch">mini-batch</h3>
<p>A small, randomly selected subset of the entire batch of <a href="#example"><strong>examples</strong></a> run together in a single iteration of training or inference. The <a href="#batch-size"><strong>batch size</strong></a> of a mini-batch is usually between 10 and 1,000. It is much more efficient to calculate the loss on a mini-batch than on the full training data.</p>
<p>从训练或推断过程的一次迭代中一起运行的整批<a href="#example"><strong>样本</strong></a>内随机选择的一小部分。小批次的<a href="#batch-size"><strong>规模</strong></a>通常介于 10 到 1000 之间。与基于完整的训练数据计算损失相比，基于小批次数据计算损失要高效得多。</p>
<h3 id="mini-batch-stochastic-gradient-descent-sgd">mini-batch stochastic gradient descent (SGD)</h3>
<p>A <a href="#gradient-descent"><strong>gradient descent</strong></a> algorithm that uses <a href="#mini-batch"><strong>mini-batches</strong></a>. In other words, mini-batch SGD estimates the gradient based on a small subset of the training data. <a href="#stochastic-gradient-descent-sgd"><strong>Vanilla SGD</strong></a> uses a mini-batch of size 1.</p>
<p>一种采用<a href="#mini-batch"><strong>小批次</strong></a>样本的<a href="#gradient-descent"><strong>梯度下降法</strong></a>。也就是说，小批次 SGD 会根据一小部分训练数据来估算梯度。<a href="#stochastic-gradient-descent-sgd"><strong>Vanilla SGD</strong></a> 使用的小批次的规模为 1。</p>
<h3 id="ml">ML</h3>
<p>Abbreviation for <a href="#machine-learning"><strong>machine learning</strong></a>.</p>
<p><a href="#machine-learning"><strong>机器学习</strong></a>的缩写。</p>
<h3 id="model">model</h3>
<p>The representation of what an ML system has learned from the training data. This is an overloaded term, which can have either of the following two related meanings:</p>
<ul>
<li>The <a href="#tensorflow"><strong>TensorFlow</strong></a> graph that expresses the structure of how a prediction will be computed.</li>
<li>The particular weights and biases of that TensorFlow graph, which are determined by <a href="#model-training"><strong>training</strong></a>.</li>
</ul>
<p>机器学习系统从训练数据学到的内容的表示形式。多含义术语，可以理解为下列两种相关含义之一：</p>
<ul>
<li>一种 <a href="#tensorflow"><strong>TensorFlow</strong></a> 图，用于表示预测计算结构。</li>
<li>该 TensorFlow 图的特定权重和偏差，通过<a href="#model-training"><strong>训练</strong></a>决定。</li>
</ul>
<h3 id="model-training">model training</h3>
<p>The process of determining the best <a href="#model"><strong>model</strong></a>.</p>
<p>确定最佳<a href="#model"><strong>模型</strong></a>的过程。</p>
<h3 id="momentum">Momentum</h3>
<p>A sophisticated gradient descent algorithm in which a learning step depends not only on the derivative in the current step, but also on the derivatives of the step(s) that immediately preceded it. Momentum involves computing an exponentially weighted moving average of the gradients over time, analogous to momentum in physics. Momentum sometimes prevents learning from getting stuck in local minima.</p>
<p>一种先进的梯度下降法，其中学习步长不仅取决于当前步长的导数，还取决于之前一步或多步的步长的导数。动量涉及计算梯度随时间而变化的指数级加权移动平均值，与物理学中的动量类似。动量有时可以防止学习过程被卡在局部最小的情况。</p>
<h3 id="multi-class-classification">multi-class classification</h3>
<p>Classification problems that distinguish among more than two classes. For example, there are approximately 128 species of maple trees, so a model that categorized maple tree species would be multi-class. Conversely, a model that divided emails into only two categories (<em>spam</em> and <em>not spam</em>) would be a <a href="#binary-classification"><strong>binary classification model</strong></a>.</p>
<p>区分两种以上类别的分类问题。例如，枫树大约有 128 种，因此，确定枫树种类的模型就属于多类别模型。反之，仅将电子邮件分为两类（&ldquo;垃圾邮件&rdquo;和&ldquo;非垃圾邮件&rdquo;）的模型属于<a href="#binary-classification"><strong>二元分类模型</strong></a>。</p>
<h3 id="multinomial-classification">multinomial classification</h3>
<p>Synonym for <a href="#multi-class-classification"><strong>multi-class classification</strong></a>.</p>
<p>是<a href="#multi-class-classification"><strong>多类别分类</strong></a>的同义词。</p>
<h2 id="n_1">N</h2>
<h3 id="n-gram">N-gram</h3>
<p>N-Gram（有时也称为N元模型）是自然语言处理中一个非常重要的概念。
假设有一个字符串 s，那么该字符串的 N-Gram 就表示按长度 N 切分原词得到的词段，
也就是 s 中所有长度为 N 的子字符串序列。</p>
<p>设想如果有两个字符串，然后分别求它们的N-Gram，
那么就可以从它们的共有子串的数量这个角度去定义两个字符串间的 N-Gram 距离。
N-Gram 距离可以初步评价两个字符串的相似程度。</p>
<h3 id="nan-trap">NaN trap</h3>
<p>When one number in your model becomes a <a href="https://en.wikipedia.org/wiki/NaN">NaN</a> during training, which causes many or all other numbers in your model to eventually become a NaN.</p>
<p>NaN is an abbreviation for "Not a Number."</p>
<p>模型中的一个数字在训练期间变成 <a href="https://en.wikipedia.org/wiki/NaN">NaN</a>，这会导致模型中的很多或所有其他数字最终也会变成 NaN。</p>
<p>NaN 是&ldquo;非数字&rdquo;的缩写。</p>
<h3 id="negative-class">negative class</h3>
<p>In <a href="#binary-classification"><strong>binary classification</strong></a>, one class is termed positive and the other is termed negative. The positive class is the thing we're looking for and the negative class is the other possibility. For example, the negative class in a medical test might be "not tumor." The negative class in an email classifier might be "not spam." See also <a href="#positive-class"><strong>positive class</strong></a>.</p>
<p>在<a href="#binary-classification"><strong>二元分类</strong></a>中，一种类别称为正类别，另一种类别称为负类别。正类别是我们要寻找的类别，负类别则是另一种可能性。例如，在医学检查中，负类别可以是&ldquo;非肿瘤&rdquo;。在电子邮件分类器中，负类别可以是&ldquo;非垃圾邮件&rdquo;。另请参阅<a href="#positive-class"><strong>正类别</strong></a>。</p>
<h3 id="neural-network">neural network</h3>
<p>A model that, taking inspiration from the brain, is composed of layers (at least one of which is <a href="#hidden-layer"><strong>hidden</strong></a>) consisting of simple connected units or <a href="#neuron"><strong>neurons</strong></a> followed by nonlinearities.</p>
<p>一种模型，灵感来源于脑部结构，由多个层构成（至少有一个是<a href="#hidden-layer"><strong>隐藏层</strong></a>），每个层都包含简单相连的单元或<a href="#neuron"><strong>神经元</strong></a>（具有非线性关系）。</p>
<h3 id="neuron">neuron</h3>
<p>A node in a <a href="#neural-network"><strong>neural network</strong></a>, typically taking in multiple input values and generating one output value. The neuron calculates the output value by applying an <a href="#activation-function"><strong>activation function</strong></a> (nonlinear transformation) to a weighted sum of input values.</p>
<p><a href="#neural-network"><strong>神经网络</strong></a>中的节点，通常是接收多个输入值并生成一个输出值。神经元通过将<a href="#activation-function"><strong>激活函数</strong></a>（非线性转换）应用于输入值的加权和来计算输出值。</p>
<h3 id="node">node</h3>
<p>An overloaded term that means either of the following:</p>
<ul>
<li>A neuron in a <a href="#hidden-layer"><strong>hidden layer</strong></a>.</li>
<li>An operation in a TensorFlow <a href="#graph"><strong>graph</strong></a>.</li>
</ul>
<p>多含义术语，可以理解为下列两种含义之一：</p>
<ul>
<li><a href="#hidden-layer"><strong>隐藏层</strong></a>中的神经元。</li>
<li>TensorFlow <a href="#graph"><strong>图</strong></a>中的操作。</li>
</ul>
<h3 id="normalization">normalization</h3>
<p>The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. For example, suppose the natural range of a certain feature is 800 to 6,000. Through subtraction and division, you can normalize those values into the range -1 to +1.</p>
<p>See also <a href="#scaling"><strong>scaling</strong></a>.</p>
<p>将实际的值区间转换为标准的值区间（通常为 -1 到 +1 或 0 到 1）的过程。例如，假设某个特征的自然区间是 800 到 6000。通过减法和除法运算，您可以将这些值标准化为位于 -1 到 +1 区间内。</p>
<p>另请参阅<a href="#scaling"><strong>缩放</strong></a>。</p>
<h3 id="numerical-data">numerical data</h3>
<p><a href="#feature"><strong>Features</strong></a> represented as integers or real-valued numbers. For example, in a real estate model, you would probably represent the size of a house (in square feet or square meters) as numerical data. Representing a feature as numerical data indicates that the feature's values have a <em>mathematical</em> relationship to each other and possibly to the label. For example, representing the size of a house as numerical data indicates that a 200 square-meter house is twice as large as a 100 square-meter house. Furthermore, the number of square meters in a house probably has some mathematical relationship to the price of the house.</p>
<p>Not all integer data should be represented as numerical data. For example, postal codes in some parts of the world are integers; however, integer postal codes should not be represented as numerical data in models. That's because a postal code of <code>20000</code> is not twice (or half) as potent as a postal code of 10000. Furthermore, although different postal codes <em>do</em> correlate to different real estate values, we can't assume that real estate values at postal code 20000 are twice as valuable as real estate values at postal code 10000. Postal codes should be represented as <a href="#categorical-data"><strong>categorical data</strong></a> instead.</p>
<p>Numerical features are sometimes called <a href="#continuous-feature"><strong>continuous features</strong></a>.</p>
<p>用整数或实数表示的<a href="#feature"><strong>特征</strong></a>。例如，在房地产模型中，您可能会用数值数据表示房子大小（以平方英尺或平方米为单位）。如果用数值数据表示特征，则可以表明特征的值相互之间具有数学关系，并且与标签可能也有数学关系。例如，如果用数值数据表示房子大小，则可以表明面积为 200 平方米的房子是面积为 100 平方米的房子的两倍。此外，房子面积的平方米数可能与房价存在一定的数学关系。</p>
<p>并非所有整数数据都应表示成数值数据。例如，世界上某些地区的邮政编码是整数，但在模型中，不应将整数邮政编码表示成数值数据。这是因为邮政编码 <code>20000</code> 在效力上并不是邮政编码 10000 的两倍（或一半）。此外，虽然不同的邮政编码确实与不同的房地产价值有关，但我们也不能假设邮政编码为 20000 的房地产在价值上是邮政编码为 10000 的房地产的两倍。邮政编码应表示成<a href="#categorical-data"><strong>分类数据</strong></a>。</p>
<p>数值特征有时称为<a href="#continuous-feature"><strong>连续特征</strong></a>。</p>
<h3 id="numpy">numpy</h3>
<p>An <a href="http://www.numpy.org/">open-source math library</a> that provides efficient array operations in Python. <a href="#pandas"><strong>pandas</strong></a> is built on numpy.</p>
<p>一个<a href="http://www.numpy.org/">开放源代码数学库</a>，在 Python 中提供高效的数组操作。<a href="#pandas"><strong>Pandas</strong></a> 就建立在 Numpy 之上。</p>
<h2 id="o_1">O</h2>
<h3 id="objective">objective</h3>
<p>A metric that your algorithm is trying to optimize.</p>
<p>算法尝试优化的指标。</p>
<h3 id="offline-inference">offline inference</h3>
<p>Generating a group of <a href="#prediction"><strong>predictions</strong></a>, storing those predictions, and then retrieving those predictions on demand. Contrast with <a href="#online-inference"><strong>online inference</strong></a>.</p>
<p>生成一组<a href="#prediction"><strong>预测</strong></a>，存储这些预测，然后根据需求检索这些预测。与<a href="#online-inference"><strong>在线推断</strong></a>相对。</p>
<h3 id="one-hot-encoding">one-hot encoding</h3>
<p>A sparse vector in which:</p>
<ul>
<li>One element is set to 1.</li>
<li>All other elements are set to 0.</li>
</ul>
<p>One-hot encoding is commonly used to represent strings or identifiers that have a finite set of possible values. For example, suppose a given botany data set chronicles 15,000 different species, each denoted with a unique string identifier. As part of feature engineering, you'll probably encode those string identifiers as one-hot vectors in which the vector has a size of 15,000.</p>
<p>一种稀疏向量，其中：</p>
<ul>
<li>一个元素设为 1。</li>
<li>所有其他元素均设为 0。</li>
</ul>
<p>one-hot 编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为 one-hot 向量，向量的大小为 15000。</p>
<h3 id="one-vs-all">one-vs.-all</h3>
<p>Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate <a href="#binary-classification"><strong>binary classifiers</strong></a>&mdash;one binary classifier for each possible outcome. For example, given a model that classifies examples as animal, vegetable, or mineral, a one-vs.-all solution would provide the following three separate binary classifiers:</p>
<ul>
<li>animal vs. not animal</li>
<li>vegetable vs. not vegetable</li>
<li>mineral vs. not mineral</li>
</ul>
<p>假设某个分类问题有 N 种可能的解决方案，一对多解决方案将包含 N 个单独的<a href="#binary-classification"><strong>二元分类器</strong></a> - 一个二元分类器对应一种可能的结果。例如，假设某个模型用于区分样本属于动物、蔬菜还是矿物，一对多解决方案将提供下列三个单独的二元分类器：</p>
<ul>
<li>动物和非动物</li>
<li>蔬菜和非蔬菜</li>
<li>矿物和非矿物</li>
</ul>
<h3 id="online-inference">online inference</h3>
<p>Generating <a href="#prediction"><strong>predictions</strong></a> on demand. Contrast with <a href="#offline-inference"><strong>offline inference</strong></a>.</p>
<p>根据需求生成<a href="#prediction"><strong>预测</strong></a>。与<a href="#offline-inference"><strong>离线推断</strong></a>相对。</p>
<h3 id="operation-op">Operation (op)</h3>
<p>A node in the TensorFlow graph. In TensorFlow, any procedure that creates, manipulates, or destroys a <a href="#tensor"><strong>Tensor</strong></a> is an operation. For example, a matrix multiply is an operation that takes two Tensors as input and generates one Tensor as output.</p>
<p>TensorFlow 图中的节点。在 TensorFlow 中，任何创建、操纵或销毁<a href="#tensor"><strong>张量</strong></a>的过程都属于操作。例如，矩阵相乘就是一种操作，该操作以两个张量作为输入，并生成一个张量作为输出。</p>
<h3 id="optimizer">optimizer</h3>
<p>A specific implementation of the <a href="#gradient-descent"><strong>gradient descent</strong></a> algorithm. TensorFlow's base class for optimizers is <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer">tf.train.Optimizer</a>. Different optimizers may leverage one or more of the following concepts to enhance the effectiveness of gradient descent on a given <a href="#training-set"><strong>training set</strong></a>:</p>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer">momentum</a> (Momentum)</li>
<li>update frequency (<a href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer">AdaGrad</a> = ADAptive GRADient descent; <a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer">Adam</a> = ADAptive with Momentum; RMSProp)</li>
<li>sparsity/regularization (<a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer">Ftrl</a>)</li>
<li>more complex math (<a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer">Proximal</a>, and others)</li>
</ul>
<p>You might even imagine an <a href="https://arxiv.org/abs/1606.04474">NN-driven optimizer</a>.</p>
<p><a href="#gradient-descent"><strong>梯度下降法</strong></a>的一种具体实现。TensorFlow 的优化器基类是 <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer">tf.train.Optimizer</a>。不同的优化器（<code>tf.train.Optimizer</code> 的子类）会考虑如下概念：</p>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer">动量</a> (Momentum)</li>
<li>更新频率 （<a href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer">AdaGrad</a> = ADAptive GRADient descent； <a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer">Adam</a> = ADAptive with Momentum；RMSProp）</li>
<li>稀疏性/正则化 (<a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer">Ftrl</a>)</li>
<li>更复杂的计算方法 （<a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer">Proximal</a>， 等等）</li>
</ul>
<p>甚至还包括 <a href="https://arxiv.org/abs/1606.04474">NN 驱动的优化器</a>。</p>
<h3 id="outliers">outliers</h3>
<p>Values distant from most other values. In machine learning, any of the following are outliers:</p>
<ul>
<li><a href="#weight"><strong>Weights</strong></a> with high absolute values.</li>
<li>Predicted values relatively far away from the actual values.</li>
<li>Input data whose values are more than roughly 3 standard deviations from the mean.</li>
</ul>
<p>Outliers often cause problems in model training.</p>
<p>与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。</p>
<ul>
<li>绝对值很高的<a href="#weight"><strong>权重</strong></a>。</li>
<li>与实际值相差很大的预测值。</li>
<li>值比平均值高大约 3 个标准偏差的输入数据。</li>
</ul>
<p>离群值常常会导致模型训练出现问题。</p>
<h3 id="output-layer">output layer</h3>
<p>The "final" layer of a neural network. The layer containing the answer(s).</p>
<p>神经网络的&ldquo;最后&rdquo;一层，也是包含答案的层。</p>
<h3 id="overfitting">overfitting</h3>
<p>Creating a model that matches the <a href="#training-set"><strong>training data</strong></a> so closely that the model fails to make correct predictions on new data.</p>
<p>创建的模型与<a href="#training-set"><strong>训练数据</strong></a>过于匹配，以致于模型无法根据新数据做出正确的预测。</p>
<h2 id="p_1">P</h2>
<h3 id="pandas">pandas</h3>
<p>A column-oriented data analysis API. Many ML frameworks, including TensorFlow, support pandas data structures as input. See <a href="http://pandas.pydata.org/">pandas documentation</a>.</p>
<p>面向列的数据分析 API。很多机器学习框架（包括 TensorFlow）都支持将 Pandas 数据结构作为输入。请参阅 <a href="http://pandas.pydata.org/">Pandas 文档</a>。</p>
<h3 id="parameter">parameter</h3>
<p>A variable of a model that the ML system trains on its own. For example, <a href="#weight"><strong>weights</strong></a> are parameters whose values the ML system gradually learns through successive training iterations. Contrast with <a href="#hyperparameter"><strong>hyperparameter</strong></a>.</p>
<p>机器学习系统自行训练的模型的变量。例如，<a href="#weight"><strong>权重</strong></a>就是一种参数，它们的值是机器学习系统通过连续的训练迭代逐渐学习到的。与<a href="#hyperparameter"><strong>超参数</strong></a>相对。</p>
<h3 id="parameter-server-ps">Parameter Server (PS)</h3>
<p>A job that keeps track of a model's <a href="#parameter"><strong>parameters</strong></a> in a distributed setting.</p>
<p>一种作业，负责在分布式设置中跟踪模型<a href="#parameter"><strong>参数</strong></a>。</p>
<h3 id="parameter-update">parameter update</h3>
<p>The operation of adjusting a model's <a href="#parameter"><strong>parameters</strong></a> during training, typically within a single iteration of <a href="#gradient-descent"><strong>gradient descent</strong></a>.</p>
<p>在训练期间（通常是在<a href="#gradient-descent"><strong>梯度下降法</strong></a>的单次迭代中）调整模型<a href="#parameter"><strong>参数</strong></a>的操作。</p>
<h3 id="partial-derivative">partial derivative</h3>
<p>A derivative in which all but one of the variables is considered a constant. For example, the partial derivative of <em>f(x, y)</em> with respect to <em>x</em> is the derivative of <em>f</em> considered as a function of <em>x</em> alone (that is, keeping <em>y</em> constant). The partial derivative of <em>f</em> with respect to <em>x</em> focuses only on how <em>x</em> is changing and ignores all other variables in the equation.</p>
<p>一种导数，除一个变量之外的所有变量都被视为常量。例如，f(x, y) 对 x 的偏导数就是 f(x) 的导数（即，使 y 保持恒定）。f 对 x 的偏导数仅关注 x 如何变化，而忽略公式中的所有其他变量。</p>
<h3 id="partitioning-strategy">partitioning strategy</h3>
<p>The algorithm by which variables are divided across <a href="#parameter-server-ps"><strong>parameter servers</strong></a>.</p>
<p><a href="#parameter-server-ps"><strong>参数服务器</strong></a>中分割变量的算法。</p>
<h3 id="performance">performance</h3>
<p>Overloaded term with the following meanings:</p>
<ul>
<li>The traditional meaning within software engineering. Namely: How fast (or efficiently) does this piece of software run?</li>
<li>The meaning within ML. Here, performance answers the following question: How correct is this <a href="#model"><strong>model</strong></a>? That is, how good are the model's predictions?</li>
</ul>
<p>多含义术语，具有以下含义：</p>
<ul>
<li>在软件工程中的传统含义。即：相应软件的运行速度有多快（或有多高效）？</li>
<li>在机器学习中的含义。在机器学习领域，性能旨在回答以下问题：相应<a href="#model"><strong>模型</strong></a>的准确度有多高？即模型在预测方面的表现有多好？</li>
</ul>
<h3 id="perplexity">perplexity</h3>
<p>One measure of how well a <a href="#model"><strong>model</strong></a> is accomplishing its task. For example, suppose your task is to read the first few letters of a word a user is typing on a smartphone keyboard, and to offer a list of possible completion words. Perplexity, P, for this task is approximately the number of guesses you need to offer in order for your list to contain the actual word the user is trying to type.</p>
<p>Perplexity is related to <a href="#cross-entropy"><strong>cross-entropy</strong></a> as follows:</p>
<p>一种衡量指标，用于衡量<a href="#model"><strong>模型</strong></a>能够多好地完成任务。例如，假设任务是读取用户使用智能手机键盘输入字词时输入的前几个字母，然后列出一组可能的完整字词。此任务的困惑度 (P) 是：为了使列出的字词中包含用户尝试输入的实际字词，您需要提供的猜测项的个数。</p>
<p>困惑度与<a href="#cross-entropy"><strong>交叉熵</strong></a>的关系如下：</p>
<div class="math">$$P= 2^{-\text{cross entropy}}$$</div>
<h3 id="pipeline">pipeline</h3>
<p>The infrastructure surrounding a machine learning algorithm. A pipeline includes gathering the data, putting the data into training data files, training one or more models, and exporting the models to production.</p>
<p>机器学习算法的基础架构。流水线包括收集数据、将数据放入训练数据文件、训练一个或多个模型，以及将模型导出到生产环境。</p>
<h3 id="positive-class">positive class</h3>
<p>In <a href="#binary-classification"><strong>binary classification</strong></a>, the two possible classes are labeled as positive and negative. The positive outcome is the thing we're testing for. (Admittedly, we're simultaneously testing for both outcomes, but play along.) For example, the positive class in a medical test might be "tumor." The positive class in an email classifier might be "spam."</p>
<p>Contrast with <a href="#negative-class"><strong>negative class</strong></a>.</p>
<p>在<a href="#binary-classification"><strong>二元分类</strong></a>中，两种可能的类别分别被标记为正类别和负类别。正类别结果是我们要测试的对象。（不可否认的是，我们会同时测试这两种结果，但只关注正类别结果。）例如，在医学检查中，正类别可以是&ldquo;肿瘤&rdquo;。在电子邮件分类器中，正类别可以是&ldquo;垃圾邮件&rdquo;。</p>
<p>与<a href="#negative-class"><strong>负类别</strong></a>相对。</p>
<h3 id="precision">precision</h3>
<p>A metric for <a href="#classification-model"><strong>classification models</strong></a>. Precision identifies the frequency with which a model was correct when predicting the <a href="#positive-class"><strong>positive class</strong></a>. That is:</p>
<p>中文常称为：查准率、准确率。
对应的指标为：<a href="#recall">召回率recall</a></p>
<p>一种<a href="#classification_model"><strong>分类模型</strong></a>指标。查准率指模型正确预测<a href="#positive-class"><strong>正类别</strong></a>的频率, 即：</p>
<div class="math">$$\text{Precision} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Positives}}$$</div>
<h3 id="prediction">prediction</h3>
<p>A model's output when provided with an input <a href="#example"><strong>example</strong></a>.</p>
<p>模型在收到输入的<a href="#example"><strong>样本</strong></a>后的输出。</p>
<h3 id="prediction-bias">prediction bias</h3>
<p>A value indicating how far apart the average of <a href="#prediction"><strong>predictions</strong></a> is from the average of <a href="#label"><strong>labels</strong></a> in the data set.</p>
<p>一个值，用于表明<a href="#prediction"><strong>预测</strong></a>平均值与数据集中<a href="#label"><strong>标签</strong></a>的平均值相差有多大。</p>
<h3 id="pre-made-estimator">pre-made Estimator</h3>
<p>An <a href="#estimator"><strong>Estimator</strong></a> that someone has already built. TensorFlow provides several pre-made Estimators, including <code>DNNClassifier</code>, <code>DNNRegressor</code>, and <code>LinearClassifier</code>. You may build your own pre-made Estimators by following <a href="https://www.tensorflow.org/extend/estimators">these instructions</a>.</p>
<p>其他人已建好的 <a href="#estimator"><strong>Estimator</strong></a>。TensorFlow 提供了一些预创建的 Estimator，包括 <code>DNNClassifier</code>、<code>DNNRegressor</code> 和 <code>LinearClassifier</code>。您可以按照<a href="https://www.tensorflow.org/extend/estimators">这些说明</a>构建自己预创建的 Estimator。</p>
<h3 id="pre-trained-model">pre-trained model</h3>
<p>Models or model components (such as <a href="#embeddings"><strong>embeddings</strong></a>) that have been already been trained. Sometimes, you'll feed pre-trained embeddings into a <a href="#neural-network"><strong>neural network</strong></a>. Other times, your model will train the embeddings itself rather than rely on the pre-trained embeddings.</p>
<p>已经过训练的模型或模型组件（例如<a href="#embeddings"><strong>嵌套</strong></a>）。有时，您需要将预训练的嵌套馈送到<a href="#neural-network"><strong>神经网络</strong></a>。在其他时候，您的模型将自行训练嵌套，而不依赖于预训练的嵌套。</p>
<h3 id="prior-belief">prior belief</h3>
<p>What you believe about the data before you begin training on it. For example, <a href="#l2-regularization"><strong>L2 regularization</strong></a> relies on a prior belief that <a href="#weight"><strong>weights</strong></a> should be small and normally distributed around zero.</p>
<p>在开始采用相应数据进行训练之前，您对这些数据抱有的信念。例如，<a href="#l2-regularization"><strong>L2 正则化</strong></a>依赖的先验信念是<a href="#weight"><strong>权重</strong></a>应该很小且应以 0 为中心呈正态分布。</p>
<h2 id="q_1">Q</h2>
<h3 id="queue">queue</h3>
<p>A TensorFlow <a href="#operation-op"><strong>Operation</strong></a> that implements a queue data structure. Typically used in I/O.</p>
<p>一种 TensorFlow <a href="#operation-op"><strong>操作</strong></a>，用于实现队列数据结构。通常用于 I/O 中。</p>
<h2 id="r_1">R</h2>
<h3 id="rank">rank</h3>
<p>Overloaded term in ML that can mean either of the following:</p>
<ul>
<li>The number of dimensions in a <a href="#tensor"><strong>Tensor</strong></a>. For instance, a scalar has rank 0, a vector has rank 1, and a matrix has rank 2.</li>
<li>The ordinal position of a class in an ML problem that categorizes classes from highest to lowest. For example, a behavior ranking system could rank a dog's rewards from highest (a steak) to lowest (wilted kale).</li>
</ul>
<p>机器学习中的一个多含义术语，可以理解为下列含义之一：</p>
<ul>
<li><a href="#tensor"><strong>张量</strong></a>中的维度数量。例如，标量等级为 0，向量等级为 1，矩阵等级为 2。</li>
<li>在将类别从最高到最低进行排序的机器学习问题中，类别的顺序位置。例如，行为排序系统可以将狗狗的奖励从最高（牛排）到最低（枯萎的羽衣甘蓝）进行排序。</li>
</ul>
<h3 id="rater">rater</h3>
<p>A human who provides <a href="#label"><strong>labels</strong></a> in <a href="#example"><strong>examples</strong></a>. Sometimes called an "annotator."</p>
<p>为<a href="#example"><strong>样本</strong></a>提供<a href="#label"><strong>标签</strong></a>的人。有时称为&ldquo;注释者&rdquo;。</p>
<h3 id="recall">recall</h3>
<p>A metric for <a href="#classification-model"><strong>classification models</strong></a> that answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is:</p>
<div class="math">$$\text{Recall} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Negatives}} $$</div>
<p>中文常称为：召回率、查全率。
相关指标为：查准率<a href="#precision">precision</a></p>
<p>一种<a href="#classification-model"><strong>分类模型</strong></a>指标，用于回答以下问题：在所有可能的正类别标签中，模型正确地识别出了多少个？即：</p>
<p><mj></mj></p>
<div class="math">$$\text{召回率} = \frac{\text{真正例数}} {\text{真正例数} + \text{假负例数}} $$</div>
<p></p>
<h3 id="rectified-linear-unit-relu">Rectified Linear Unit (ReLU)</h3>
<p>An <a href="#activation-function"><strong>activation function</strong></a> with the following rules:</p>
<ul>
<li>If input is negative or zero, output is 0.</li>
<li>If input is positive, output is equal to input.</li>
</ul>
<p>一种<a href="#activation-function"><strong>激活函数</strong></a>，其规则如下：</p>
<ul>
<li>如果输入为负数或 0，则输出 0。</li>
<li>如果输入为正数，则输出等于输入。</li>
</ul>
<h3 id="regression-model">regression model</h3>
<p>A type of model that outputs continuous (typically, floating-point) values. Compare with <a href="#classification-model"><strong>classification models</strong></a>, which output discrete values, such as "day lily" or "tiger lily."</p>
<p>一种模型，能够输出连续的值（通常为浮点值）。请与<a href="#classification-model"><strong>分类模型</strong></a>进行比较，分类模型输出离散值，例如&ldquo;黄花菜&rdquo;或&ldquo;虎皮百合&rdquo;。</p>
<h3 id="regularization">regularization</h3>
<p>The penalty on a model's complexity. Regularization helps prevent <a href="#overfitting"><strong>overfitting</strong></a>. Different kinds of regularization include:</p>
<ul>
<li><a href="#l1-regularization"><strong>L1 regularization</strong></a></li>
<li><a href="#l2-regularization"><strong>L2 regularization</strong></a></li>
<li><a href="#dropout-regularization"><strong>dropout regularization</strong></a></li>
<li><a href="#early-stopping"><strong>early stopping</strong></a> (this is not a formal regularization method, but can effectively limit overfitting)</li>
</ul>
<p>对模型复杂度的惩罚。正则化有助于防止出现<a href="#overfitting"><strong>过拟合</strong></a>，包含以下类型：</p>
<ul>
<li><a href="#l1-regularization"><strong>L1 正则化</strong></a></li>
<li><a href="#l2-regularization"><strong>L2 正则化</strong></a></li>
<li><a href="#dropout-regularization"><strong>丢弃正则化</strong></a></li>
<li><a href="#early-stopping"><strong>早停法</strong></a>（这不是正式的正则化方法，但可以有效限制过拟合）</li>
</ul>
<h3 id="regularization-rate">regularization rate</h3>
<p>A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified <a href="#loss"><strong>loss</strong></a> equation shows the regularization rate's influence:</p>
<p>一种标量值，以 lambda 表示，用于指定正则化函数的相对重要性。从下面简化的<a href="#loss"><strong>损失</strong></a>公式中可以看出正则化率的影响：</p>
<div class="math">$$\text{minimize(loss function + }\lambda\text{(regularization function))}$$</div>
<p>Raising the regularization rate reduces <a href="#overfitting"><strong>overfitting</strong></a> but may make the model less <a href="#accuracy"><strong>accurate</strong></a>.</p>
<p>提高正则化率可以减少<a href="#overfitting"><strong>过拟合</strong></a>，但可能会使模型的<a href="#accuracy"><strong>准确率</strong></a>降低。</p>
<h3 id="representation">representation</h3>
<p>The process of mapping data to useful <a href="#feature"><strong>features</strong></a>.</p>
<p>将数据映射到实用<a href="#feature"><strong>特征</strong></a>的过程。</p>
<h3 id="roc-receiver-operating-characteristic-curve">ROC (receiver operating characteristic) Curve</h3>
<p>A curve of <a href="#true-positive-rate-tp-rate"><strong>true positive rate</strong></a> vs. <a href="#false-positive-rate-fp-rate"><strong>false positive rate</strong></a> at different <a href="#classification-threshold"><strong>classification thresholds</strong></a>. See also <a href="#auc-area-under-the-roc-curve"><strong>AUC</strong></a>.</p>
<p>不同<a href="#classification-threshold"><strong>分类阈值</strong></a>下的<a href="#true-positive-rate-tp-rate"><strong>真正例率 或 真阳率</strong></a>和<a href="#false-positive-rate-fp-rate"><strong>假正例率 或 假阳率</strong></a>构成的曲线。另请参阅<a href="#auc-area-under-the-roc-curve"><strong>曲线下面积</strong></a>。</p>
<p>ROC 翻成中文叫做<strong>受试者工作特征曲线</strong>，该术语由医疗领域引入。
它是把<a href="#true-positive-tp">真正例率 或 真阳率 TP</a>作为纵坐标，<a href="#false-positive-fp">假正例率 或 假阳率 FP</a>作为横坐标，对一个类别绘制的曲线。</p>
<p>曲线下的区域就是<a href="#auc-area-under-the-roc-curve"><strong>曲线下区域AUC(Area Under the Curve)</strong></a>, 如果AUC的面积为1，表示在这个类别上你的准确率是最高的。AUC一般越大越好，说明某个类别的分类准确度越高。 </p>
<h3 id="roi-region-of-interest">ROI (region of interest)</h3>
<p>目标检测领域术语，机器视觉、图像处理中，从被处理的图像以方框、圆、椭圆、不规则多边形等方式勾勒出需要处理的区域，称为<strong>感兴趣区域(ROI)</strong>。 这个区域是你的图像分析所关注的重点, 圈定该区域以便进行进一步处理。使用ROI圈定你想读的目标，可以减少处理时间，增加精度。</p>
<h3 id="root-directory">root directory</h3>
<p>The directory you specify for hosting subdirectories of the TensorFlow checkpoint and events files of multiple models.</p>
<p>您指定的目录，用于托管多个模型的 TensorFlow 检查点和事件文件的子目录。</p>
<h3 id="root-mean-squared-error-rmse">Root Mean Squared Error (RMSE)</h3>
<p>The square root of the <a href="#mean-squared-error-mse"><strong>Mean Squared Error</strong></a>.</p>
<p><a href="#mean-squared-error-mse"><strong>均方误差</strong></a>的平方根。</p>
<h2 id="s_1">S</h2>
<h3 id="savedmodel">SavedModel</h3>
<p>The recommended format for saving and recovering TensorFlow models. SavedModel is a language-neutral, recoverable serialization format, which enables higher-level systems and tools to produce, consume, and transform TensorFlow models.</p>
<p>See <a href="https://www.tensorflow.org/programmers_guide/saved_model">Saving and Restoring</a> in the TensorFlow Programmer's Guide for complete details.</p>
<p>保存和恢复 TensorFlow 模型时建议使用的格式。SavedModel 是一种独立于语言且可恢复的序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型。</p>
<p>如需完整的详细信息，请参阅《TensorFlow 编程人员指南》中的<a href="https://www.tensorflow.org/programmers_guide/saved_model">保存和恢复</a>。</p>
<h3 id="saver">Saver</h3>
<p>A <a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver">TensorFlow object</a> responsible for saving model checkpoints.</p>
<p>一种 <a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver">TensorFlow 对象</a>，负责保存模型检查点。</p>
<h3 id="scaling">scaling</h3>
<p>A commonly used practice in <a href="#feature-engineering"><strong>feature engineering</strong></a> to tame a feature's range of values to match the range of other features in the data set. For example, suppose that you want all floating-point features in the data set to have a range of 0 to 1. Given a particular feature's range of 0 to 500, you could scale that feature by dividing each value by 500.</p>
<p>See also <a href="#normalization"><strong>normalization</strong></a>.</p>
<p><a href="#feature-engineering"><strong>特征工程</strong></a>中的一种常用做法，是对某个特征的值区间进行调整，使之与数据集中其他特征的值区间一致。例如，假设您希望数据集中所有浮点特征的值都位于 0 到 1 区间内，如果某个特征的值位于 0 到 500 区间内，您就可以通过将每个值除以 500 来缩放该特征。</p>
<p>另请参阅<a href="#normalization"><strong>标准化</strong></a>。</p>
<h3 id="scikit-learn">scikit-learn</h3>
<p>A popular open-source ML platform. See <a href="http://www.scikit-learn.org/">www.scikit-learn.org</a>.</p>
<p>一个热门的开放源代码机器学习平台。请访问 <a href="http://www.scikit-learn.org/">www.scikit-learn.org</a>。</p>
<h3 id="semi-supervised-learning">semi-supervised learning</h3>
<p>Training a model on data where some of the training examples have labels but others don&rsquo;t. One technique for semi-supervised learning is to infer labels for the unlabeled examples, and then to train on the inferred labels to create a new model. Semi-supervised learning can be useful if labels are expensive to obtain but unlabeled examples are plentiful.</p>
<p>训练模型时采用的数据中，某些训练样本有标签，而其他样本则没有标签。半监督式学习采用的一种技术是推断无标签样本的标签，然后使用推断出的标签进行训练，以创建新模型。如果获得有标签样本需要高昂的成本，而无标签样本则有很多，那么半监督式学习将非常有用。</p>
<h3 id="sequence-model">sequence model</h3>
<p>A model whose inputs have a sequential dependence. For example, predicting the next video watched from a sequence of previously watched videos.</p>
<p>一种模型，其输入具有序列依赖性。例如，根据之前观看过的一系列视频对观看的下一个视频进行预测。</p>
<h3 id="session">session</h3>
<p>Maintains state (for example, variables) within a TensorFlow program.</p>
<p>维持 TensorFlow 程序中的状态（例如变量）。</p>
<h3 id="sigmoid-function">sigmoid function</h3>
<p>A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1. The sigmoid function has the following formula:</p>
<p>一种函数，可将逻辑回归输出或多项回归输出（对数几率）映射到概率，以返回介于 0 到 1 之间的值。S 型函数的公式如下：</p>
<div class="math">$$y = \frac{1}{1 + e^{-\sigma}}$$</div>
<p>where <span class="math">\(\sigma\)</span> in <a href="#logistic-regression"><strong>logistic regression</strong></a> problems is simply:</p>
<p>在<a href="#logistic-regression"><strong>逻辑回归</strong></a>问题中，<span class="math">\(\sigma\)</span> 非常简单：</p>
<div class="math">$$\\sigma = b + w_1x_1 + w_2x_2 + &hellip; w_nx_n$$</div>
<p>In other words, the sigmoid function converts <span class="math">\(\sigma\)</span> into a probability between 0 and 1.</p>
<p>换句话说，S 型函数可将 <span class="math">\(\sigma\)</span> 转换为介于 0 到 1 之间的概率。</p>
<p>In some <a href="#neural-network"><strong>neural networks</strong></a>, the sigmoid function acts as the <a href="#activation-function"><strong>activation function</strong></a>.</p>
<p>在某些<a href="#neural-network"><strong>神经网络</strong></a>中，S 型函数可作为<a href="#activation-function"><strong>激活函数</strong></a>使用。</p>
<h3 id="softmax">softmax</h3>
<p>A function that provides probabilities for each possible class in a <a href="#multi-class-classification"><strong>multi-class classification model</strong></a>. The probabilities add up to exactly 1.0. For example, softmax might determine that the probability of a particular image being a dog at 0.9, a cat at 0.08, and a horse at 0.02. (Also called <strong>full softmax</strong>.)</p>
<p>Contrast with <a href="#candidate-sampling"><strong>candidate sampling</strong></a>.</p>
<p>一种函数，可提供<a href="#multi-class-classification"><strong>多类别分类模型</strong></a>中每个可能类别的概率。这些概率的总和正好为 1.0。例如，softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为<strong>完整 softmax</strong>。）</p>
<p>与<a href="#candidate-sampling"><strong>候选采样</strong></a>相对。</p>
<h3 id="sparse-feature">sparse feature</h3>
<p><a href="#feature"><strong>Feature</strong></a> vector whose values are predominately zero or empty. For example, a vector containing a single 1 value and a million 0 values is sparse. As another example, words in a search query could also be a sparse feature&mdash;there are many possible words in a given language, but only a few of them occur in a given query.</p>
<p>Contrast with <a href="#dense-feature"><strong>dense feature</strong></a>.</p>
<p>一种<a href="#feature"><strong>特征</strong></a>向量，其中的大多数值都为 0 或为空。例如，某个向量包含一个为 1 的值和一百万个为 0 的值，则该向量就属于稀疏向量。再举一个例子，搜索查询中的单词也可能属于稀疏特征 - 在某种指定语言中有很多可能的单词，但在某个指定的查询中仅包含其中几个。</p>
<p>与<a href="#dense-feature"><strong>密集特征</strong></a>相对。</p>
<h3 id="squared-hinge-loss">squared hinge loss</h3>
<p>The square of the <a href="#hinge-loss"><strong>hinge loss</strong></a>. Squared hinge loss penalizes outliers more harshly than regular hinge loss.</p>
<p><a href="#hinge-loss"><strong>合页损失函数</strong></a>的平方。与常规合页损失函数相比，平方合页损失函数对离群值的惩罚更严厉。</p>
<h3 id="squared-loss">squared loss</h3>
<p>The <a href="#loss"><strong>loss</strong></a> function used in <a href="#linear-regression"><strong>linear regression</strong></a>. (Also known as <strong>L2 Loss</strong>.) This function calculates the squares of the difference between a model's predicted value for a labeled <a href="#example"><strong>example</strong></a> and the actual value of the <a href="#label"><strong>label</strong></a>. Due to squaring, this loss function amplifies the influence of bad predictions. That is, squared loss reacts more strongly to outliers than <a href="#l1-loss"><strong>L1 loss</strong></a>.</p>
<p>在<a href="#linear-regression"><strong>线性回归</strong></a>中使用的<a href="#loss"><strong>损失</strong></a>函数（也称为 <strong>L2 损失函数</strong>）。该函数可计算模型为有标签<a href="#example"><strong>样本</strong></a>预测的值和<a href="#label"><strong>标签</strong></a>的实际值之差的平方。由于取平方值，因此该损失函数会放大不佳预测的影响。也就是说，与 <a href="#l1-loss"><strong>L1 损失函数</strong></a>相比，平方损失函数对离群值的反应更强烈。</p>
<h3 id="static-model">static model</h3>
<p>A model that is trained offline.</p>
<p>离线训练的一种模型。</p>
<h3 id="stationarity">stationarity</h3>
<p>A property of data in a data set, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December.</p>
<p>数据集中数据的一种属性，表示数据分布在一个或多个维度保持不变。这种维度最常见的是时间，即表明平稳性的数据不随时间而变化。例如，从 9 月到 12 月，表明平稳性的数据没有发生变化。</p>
<h3 id="step">step</h3>
<p>A forward and backward evaluation of one <a href="#batch"><strong>batch</strong></a>.</p>
<p>对一个<a href="#batch"><strong>批次</strong></a>的向前和向后评估。</p>
<h3 id="step-size">step size</h3>
<p>Synonym for <a href="#learning-rate"><strong>learning rate</strong></a>.</p>
<p>是<a href="#learning-rate"><strong>学习速率</strong></a>的同义词。</p>
<h3 id="stochastic-gradient-descent-sgd">stochastic gradient descent (SGD)</h3>
<p>A <a href="#gradient-descent"><strong>gradient descent</strong></a> algorithm in which the batch size is one. In other words, SGD relies on a single example chosen uniformly at random from a data set to calculate an estimate of the gradient at each step.</p>
<p>批次规模为 1 的一种<a href="#gradient-descent"><strong>梯度下降法</strong></a>。换句话说，SGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。</p>
<h3 id="structural-risk-minimization-srm">structural risk minimization (SRM)</h3>
<p>An algorithm that balances two goals:</p>
<ul>
<li>The desire to build the most predictive model (for example, lowest loss).</li>
<li>The desire to keep the model as simple as possible (for example, strong regularization).</li>
</ul>
<p>For example, a model function that minimizes loss+regularization on the training set is a structural risk minimization algorithm.</p>
<p>For more information, see <a href="http://www.svms.org/srm/">http://www.svms.org/srm/</a>.</p>
<p>Contrast with <a href="#empirical-risk-minimization-erm"><strong>empirical risk minimization</strong></a>.</p>
<p>一种算法，用于平衡以下两个目标：</p>
<ul>
<li>期望构建最具预测性的模型（例如损失最低）。</li>
<li>期望使模型尽可能简单（例如强大的正则化）。</li>
</ul>
<p>例如，旨在将基于训练集的损失和正则化降至最低的模型函数就是一种结构风险最小化算法。</p>
<p>如需更多信息，请参阅 <a href="http://www.svms.org/srm/">http://www.svms.org/srm/</a>。</p>
<p>与<a href="#empirical-risk-minimization-erm"><strong>经验风险最小化</strong></a>相对。</p>
<h3 id="summary">summary</h3>
<p>In TensorFlow, a value or set of values calculated at a particular <a href="#step"><strong>step</strong></a>, usually used for tracking model metrics during training.</p>
<p>在 TensorFlow 中的某一<a href="#step"><strong>步</strong></a>计算出的一个值或一组值，通常用于在训练期间跟踪模型指标。</p>
<h3 id="supervised-machine-learning">supervised machine learning</h3>
<p>Training a <a href="#model"><strong>model</strong></a> from input data and its corresponding <a href="#label"><strong>labels</strong></a>. Supervised machine learning is analogous to a student learning a subject by studying a set of questions and their corresponding answers. After mastering the mapping between questions and answers, the student can then provide answers to new (never-before-seen) questions on the same topic. Compare with <a href="#unsupervised-machine-learning"><strong>unsupervised machine learning</strong></a>.</p>
<p>根据输入数据及其对应的<a href="#label"><strong>标签</strong></a>来训练<a href="#model"><strong>模型</strong></a>。监督式机器学习类似于学生通过研究一系列问题及其对应的答案来学习某个主题。在掌握了问题和答案之间的对应关系后，学生便可以回答关于同一主题的新问题（以前从未见过的问题）。请与<a href="#unsupervised-machine-learning"><strong>非监督式机器学习</strong></a>进行比较。</p>
<h3 id="synthetic-feature">synthetic feature</h3>
<p>A <a href="#feature"><strong>feature</strong></a> that is not present among the input features, but is derived from one or more of them. Kinds of synthetic features include the following:</p>
<ul>
<li>Multiplying one feature by itself or by other feature(s). (These are termed <a href="#feature-cross"><strong>feature crosses</strong></a>.)</li>
<li>Dividing one feature by a second feature.</li>
<li><a href="#bucketing"><strong>Bucketing</strong></a> a continuous feature into range bins.</li>
</ul>
<p>Features created by <a href="#normalization"><strong>normalizing</strong></a> or <a href="#scaling"><strong>scaling</strong></a> alone are not considered synthetic features.</p>
<p>一种<a href="#feature"><strong>特征</strong></a>，不在输入特征之列，而是从一个或多个输入特征衍生而来。合成特征包括以下类型：</p>
<ul>
<li>将一个特征与其本身或其他特征相乘（称为<a href="#feature-cross"><strong>特征组合</strong></a>）。</li>
<li>两个特征相除。</li>
<li>对连续特征进行<a href="#bucketing"><strong>分桶</strong></a>，以分为多个区间分箱。</li>
</ul>
<p>通过<a href="#normalization"><strong>标准化</strong></a>或<a href="#scaling"><strong>缩放</strong></a>单独创建的特征不属于合成特征。</p>
<h2 id="t_1">T</h2>
<h3 id="target">target</h3>
<p>Synonym for <a href="#label"><strong>label</strong></a>.</p>
<p>是<a href="#label"><strong>标签</strong></a>的同义词。</p>
<h3 id="temporal-data">temporal data</h3>
<p>Data recorded at different points in time. For example, winter coat sales recorded for each day of the year would be temporal data.</p>
<p>在不同时间点记录的数据。例如，记录的一年中每一天的冬外套销量就属于时态数据。</p>
<h3 id="tensor">Tensor</h3>
<p>The primary data structure in TensorFlow programs. Tensors are N-dimensional (where N could be very large) data structures, most commonly scalars, vectors, or matrices. The elements of a Tensor can hold integer, floating-point, or string values.</p>
<p>TensorFlow 程序中的主要数据结构。张量是 N 维（其中 N 可能非常大）数据结构，最常见的是标量、向量或矩阵。张量的元素可以包含整数值、浮点值或字符串值。</p>
<h3 id="tensor-processing-unit-tpu">Tensor Processing Unit (TPU)</h3>
<p>An ASIC (application-specific integrated circuit) that optimizes the performance of TensorFlow programs.</p>
<p>一种 ASIC（应用专用集成电路），用于优化 TensorFlow 程序的性能。</p>
<h3 id="tensor-rank">Tensor rank</h3>
<p>See <a href="#rank"><strong>rank</strong></a>.</p>
<h3 id="tensor-shape">Tensor shape</h3>
<p>The number of elements a <a href="#tensor"><strong>Tensor</strong></a> contains in various dimensions. For example, a [5, 10] Tensor has a shape of 5 in one dimension and 10 in another.</p>
<p><a href="#tensor"><strong>张量</strong></a>在各种维度中包含的元素数。例如，张量 [5, 10] 在一个维度中的形状为 5，在另一个维度中的形状为 10。</p>
<h3 id="tensor-size">Tensor size</h3>
<p>The total number of scalars a <a href="#tensor"><strong>Tensor</strong></a> contains. For example, a [5, 10] Tensor has a size of 50.</p>
<p><a href="#tensor"><strong>张量</strong></a>包含的标量总数。例如，张量 [5, 10] 的大小为 50。</p>
<h3 id="tensorboard">TensorBoard</h3>
<p>The dashboard that displays the summaries saved during the execution of one or more TensorFlow programs.</p>
<p>一个信息中心，用于显示在执行一个或多个 TensorFlow 程序期间保存的摘要信息。</p>
<h3 id="tensorflow">TensorFlow</h3>
<p>A large-scale, distributed, machine learning platform. The term also refers to the base API layer in the TensorFlow stack, which supports general computation on dataflow graphs.</p>
<p>Although TensorFlow is primarily used for machine learning, you may also use TensorFlow for non-ML tasks that require numerical computation using dataflow graphs.</p>
<p>一个大型的分布式机器学习平台。该术语还指 TensorFlow 堆栈中的基本 API 层，该层支持对数据流图进行一般计算。</p>
<p>虽然 TensorFlow 主要应用于机器学习领域，但也可用于需要使用数据流图进行数值计算的非机器学习任务。</p>
<h3 id="tensorflow-playground">TensorFlow Playground</h3>
<p>A program that visualizes how different <a href="#hyperparameters"><strong>hyperparameters</strong></a> influence model (primarily neural network) training. Go to <a href="http://playground.tensorflow.org">http://playground.tensorflow.org</a> to experiment with TensorFlow Playground.</p>
<p>一款用于直观呈现不同的<a href="#hyperparameters"><strong>超参数</strong></a>对模型（主要是神经网络）训练的影响的程序。要试用 TensorFlow Playground，请前往 <a href="http://playground.tensorflow.org">http://playground.tensorflow.org</a>。</p>
<h3 id="tensorflow-serving">TensorFlow Serving</h3>
<p>A platform to deploy trained models in production.</p>
<p>一个平台，用于将训练过的模型部署到生产环境。</p>
<h3 id="test-set">test set</h3>
<p>The subset of the data set that you use to test your <a href="#model"><strong>model</strong></a> after the model has gone through initial vetting by the validation set.</p>
<p>Contrast with <a href="#training-set"><strong>training set</strong></a> and <a href="#validation-set"><strong>validation set</strong></a>.</p>
<p>数据集的子集，用于在<a href="#model"><strong>模型</strong></a>经由验证集的初步验证之后测试模型。</p>
<p>与<a href="#training-set"><strong>训练集</strong></a>和<a href="#validation-set"><strong>验证集</strong></a>相对。</p>
<h3 id="tfexample">tf.Example</h3>
<p>A standard <a href="https://developers.google.com/protocol-buffers/">protocol buffer</a> for describing input data for machine learning model training or inference.</p>
<p>一种标准的 <a href="https://developers.google.com/protocol-buffers/">proto buffer</a>，旨在描述用于机器学习模型训练或推断的输入数据。</p>
<h3 id="time-series-analysis">time series analysis</h3>
<p>A subfield of machine learning and statistics that analyzes <a href="#temporal-data"><strong>temporal data</strong></a>. Many types of machine learning problems require time series analysis, including classification, clustering, forecasting, and anomaly detection. For example, you could use time series analysis to forecast the future sales of winter coats by month based on historical sales data.</p>
<p>机器学习和统计学的一个子领域，旨在分析<a href="#temporal-data"><strong>时态数据</strong></a>。很多类型的机器学习问题都需要时间序列分析，其中包括分类、聚类、预测和异常检测。例如，您可以利用时间序列分析根据历史销量数据预测未来每月的冬外套销量。</p>
<h3 id="training">training</h3>
<p>The process of determining the ideal <a href="#parameter"><strong>parameters</strong></a> comprising a model.</p>
<p>确定构成模型的理想<a href="#parameter"><strong>参数</strong></a>的过程。</p>
<h3 id="training-set">training set</h3>
<p>The subset of the data set used to train a model.</p>
<p>Contrast with <a href="#validation-set"><strong>validation set</strong></a> and <a href="#test-set"><strong>test set</strong></a>.</p>
<p>数据集的子集，用于训练模型。</p>
<p>与<a href="#validation-set"><strong>验证集</strong></a>和<a href="#test-set"><strong>测试集</strong></a>相对。</p>
<h3 id="transfer-learning">transfer learning</h3>
<p>Transferring information from one machine learning task to another. For example, in multi-task learning, a single model solves multiple tasks, such as a <a href="#deep-model"><strong>deep model</strong></a> that has different output nodes for different tasks. Transfer learning might involve transferring knowledge from the solution of a simpler task to a more complex one, or involve transferring knowledge from a task where there is more data to one where there is less data.</p>
<p>Most machine learning systems solve a <em>single</em> task. Transfer learning is a baby step towards artificial intelligence in which a single program can solve <em>multiple</em> tasks.</p>
<p>将信息从一个机器学习任务转移到另一个机器学习任务。例如，在多任务学习中，一个模型可以完成多项任务，例如针对不同任务具有不同输出节点的<a href="#deep-model"><strong>深度模型</strong></a>。转移学习可能涉及将知识从较简单任务的解决方案转移到较复杂的任务，或者将知识从数据较多的任务转移到数据较少的任务。</p>
<p>大多数机器学习系统都只能完成一项任务。转移学习是迈向人工智能的一小步；在人工智能中，单个程序可以完成多项任务。</p>
<h3 id="true-negative-tn">true negative (TN)</h3>
<p>An example in which the model <em>correctly</em> predicted the <a href="#negative-class"><strong>negative class</strong></a>. For example, the model inferred that a particular email message was not spam, and that email message really was not spam.</p>
<p>被模型正确地预测为<a href="#negative-class"><strong>负类别</strong></a>的样本。例如，模型推断出某封电子邮件不是垃圾邮件，而该电子邮件确实不是垃圾邮件。</p>
<h3 id="true-positive-tp">true positive (TP)</h3>
<p>An example in which the model <em>correctly</em> predicted the <a href="#positive-class"><strong>positive class</strong></a>. For example, the model inferred that a particular email message was spam, and that email message really was spam.</p>
<p>被模型正确地预测为<a href="#positive-class"><strong>正类别</strong></a>的样本。例如，模型推断出某封电子邮件是垃圾邮件，而该电子邮件确实是垃圾邮件。</p>
<h3 id="true-positive-rate-tp-rate">true positive rate (TP rate)</h3>
<p>Synonym for <a href="#recall"><strong>recall</strong></a>. That is:</p>
<p>是<a href="#recall"><strong>召回率</strong></a>的同义词，即：</p>
<div class="math">$$\text{True Positive Rate} = \frac{\text{True Positives}} {\text{True Positives} + \text{False Negatives}}$$</div>
<p>True positive rate is the y-axis in an <a href="#roc-receiver-operating-characteristic-curve"><strong>ROC curve</strong></a>.</p>
<p>真正例率是 <a href="#roc-receiver-operating-characteristic-curve"><strong>ROC 曲线</strong></a>的 y 轴。</p>
<h2 id="u_1">U</h2>
<h3 id="unlabeled-example">unlabeled example</h3>
<p>An example that contains <a href="#feature"><strong>features</strong></a> but no <a href="#label"><strong>label</strong></a>. Unlabeled examples are the input to <a href="#inference"><strong>inference</strong></a>. In <a href="#semi-supervised-learning"><strong>semi-supervised</strong></a> and <a href="#unsupervised-machine-learning"><strong>unsupervised</strong></a> learning, unlabeled examples are used during training.</p>
<p>包含<a href="#feature"><strong>特征</strong></a>但没有<a href="#label"><strong>标签</strong></a>的样本。无标签样本是用于进行<a href="#inference"><strong>推断</strong></a>的输入内容。在<a href="#semi-supervised-learning"><strong>半监督式</strong></a>和<a href="#unsupervised-machine-learning"><strong>非监督式</strong></a>学习中，无标签样本在训练期间被使用。</p>
<h3 id="unpooling">unpooling</h3>
<p align="center">
<img src="/images/unpooling.png" width="90%"/>
<figcaption>
反池化示意图. 注：图b中的 unsampling 应为 unpooling
</figcaption>
</p>
<p>图（a）的特点是在最大池化(Maxpooling)的时候保留最大值的位置信息，
之后在反池化(unpooling)阶段使用该信息扩充 Feature Map，除最大值位置以外，其余补0。</p>
<p>图（b）的特点是反池化(unpooling)时没有使用MaxPooling时的位置信息，而是直接将内容复制来扩充Feature Map。</p>
<h3 id="unsupervised-machine-learning">unsupervised machine learning</h3>
<p>Training a <a href="#model"><strong>model</strong></a> to find patterns in a data set, typically an unlabeled data set.</p>
<p>The most common use of unsupervised machine learning is to cluster data into groups of similar examples. For example, an unsupervised machine learning algorithm can cluster songs together based on various properties of the music. The resulting clusters can become an input to other machine learning algorithms (for example, to a music recommendation service). Clustering can be helpful in domains where true labels are hard to obtain. For example, in domains such as anti-abuse and fraud, clusters can help humans better understand the data.</p>
<p>Another example of unsupervised machine learning is <a href="https://en.wikipedia.org/wiki/Principal_component_analysis"><strong>principal component analysis (PCA)</strong></a>. For example, applying PCA on a data set containing the contents of millions of shopping carts might reveal that shopping carts containing lemons frequently also contain antacids.</p>
<p>Compare with <a href="#supervised-machine-learning"><strong>supervised machine learning</strong></a>.</p>
<p>训练<a href="#model"><strong>模型</strong></a>，以找出数据集（通常是无标签数据集）中的模式。</p>
<p>非监督式机器学习最常见的用途是将数据分为不同的聚类，使相似的样本位于同一组中。例如，非监督式机器学习算法可以根据音乐的各种属性将歌曲分为不同的聚类。所得聚类可以作为其他机器学习算法（例如音乐推荐服务）的输入。在很难获取真标签的领域，聚类可能会非常有用。例如，在反滥用和反欺诈等领域，聚类有助于人们更好地了解相关数据。</p>
<p>非监督式机器学习的另一个例子是<a href="https://en.wikipedia.org/wiki/Principal_component_analysis"><strong>主成分分析 (PCA)</strong></a>。例如，通过对包含数百万购物车中物品的数据集进行主成分分析，可能会发现有柠檬的购物车中往往也有抗酸药。</p>
<p>请与<a href="#supervised-machine-learning"><strong>监督式机器学习</strong></a>进行比较。</p>
<h2 id="v_1">V</h2>
<h3 id="validation-set">validation set</h3>
<p>A subset of the data set&mdash;disjunct from the training set&mdash;that you use to adjust <a href="#hyperparameter"><strong>hyperparameters</strong></a>.</p>
<p>Contrast with <a href="#training-set"><strong>training set</strong></a> and <a href="#test-set"><strong>test set</strong></a>.</p>
<p>数据集的一个子集，从训练集分离而来，用于调整<a href="#hyperparameter"><strong>超参数</strong></a>。</p>
<p>与<a href="#training-set"><strong>训练集</strong></a>和<a href="#test-set"><strong>测试集</strong></a>相对。</p>
<h2 id="w_1">W</h2>
<h3 id="weight">weight</h3>
<p>A coefficient for a <a href="#feature"><strong>feature</strong></a> in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0, then its corresponding feature does not contribute to the model.</p>
<p>线性模型中<a href="#feature"><strong>特征</strong></a>的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。</p>
<h3 id="wide-model">wide model</h3>
<p>A linear model that typically has many <a href="#sparse-feature"><strong>sparse input features</strong></a>. We refer to it as "wide" since such a model is a special type of <a href="#neural-network"><strong>neural network</strong></a> with a large number of inputs that connect directly to the output node. Wide models are often easier to debug and inspect than deep models. Although wide models cannot express nonlinearities through <a href="#hidden-layer"><strong>hidden layers</strong></a>, they can use transformations such as <a href="#feature-cross"><strong>feature crossing</strong></a> and <a href="#bucketing"><strong>bucketization</strong></a> to model nonlinearities in different ways.</p>
<p>Contrast with <a href="#deep-model"><strong>deep model</strong></a>.</p>
<p>一种线性模型，通常有很多<a href="#sparse-feature"><strong>稀疏输入特征</strong></a>。我们之所以称之为&ldquo;宽度模型&rdquo;，是因为这是一种特殊类型的<a href="#neural-network"><strong>神经网络</strong></a>，其大量输入均直接与输出节点相连。与深度模型相比，宽度模型通常更易于调试和检查。虽然宽度模型无法通过<a href="#hidden-layer"><strong>隐藏层</strong></a>来表示非线性关系，但可以利用<a href="#feature-cross"><strong>特征组合</strong></a>、<a href="#bucketing"><strong>分桶</strong></a>等转换以不同的方式为非线性关系建模。</p>
<p>与<a href="#deep-model"><strong>深度模型</strong></a>相对。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdn.bootcdn.net/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div><!-- /.entry-content -->

  <!-- DISQUS 评论系统 -->

  <!-- giteement评论系统 -->

  <!-- Gitalk 评论 start  -->
  <!-- Link Gitalk 的支持文件  -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

  <div id="gitalk-container"></div>
      <script type="text/javascript">
      var gitalk = new Gitalk({

      // gitalk的主要参数
          clientID: 'a9a5636ca9f9c44f749f',
          clientSecret: 'a1d32a98cb3190ab1978faf73830add15f6179fb',
          repo: 'blog-discuss',
          owner: 'freeopen',
          admin: ['freeopen'],
          id: 'posts/ml-glossary',
          proxy: 'https://royal-mouse-3637.freeopen.workers.dev/https://github.com/login/oauth/access_token'
      
      });
      gitalk.render('gitalk-container');
  </script>
  <!-- Gitalk end -->

</section>
          </div>

          <footer id="contentinfo" class="footer">
            <div class="footer-inner">
              无节操小广告<a href="https://freeopen.github.io/pages/da-shang.html"> 欢迎打赏 </a>
            </div>
          </footer><!-- /#contentinfo -->
        </div>
      </div>

      <div class="hidden-print hidden-xs hidden-sm hidden-md col-lg" id="post_toc" role="complementary">
        <nav class="sidebar-toc">
<!-- <section id="main_toc"> -->
    <div id="toc"><ul><li><a class="toc-href" href="#a" title="A">A</a><ul><li><a class="toc-href" href="#ab-testing" title="A/B testing">A/B testing</a></li><li><a class="toc-href" href="#accuracy" title="accuracy">accuracy</a></li><li><a class="toc-href" href="#activation-function" title="activation function">activation function</a></li><li><a class="toc-href" href="#adagrad" title="AdaGrad">AdaGrad</a></li><li><a class="toc-href" href="#ap" title="AP">AP</a></li><li><a class="toc-href" href="#auc-area-under-the-roc-curve" title="AUC (Area under the ROC Curve)">AUC (Area under the ROC Curve)</a></li></ul></li><li><a class="toc-href" href="#b_1" title="B">B</a><ul><li><a class="toc-href" href="#backpropagation" title="backpropagation">backpropagation</a></li><li><a class="toc-href" href="#baseline" title="baseline">baseline</a></li><li><a class="toc-href" href="#batch" title="batch">batch</a></li><li><a class="toc-href" href="#batch-size" title="batch size">batch size</a></li><li><a class="toc-href" href="#bias" title="bias">bias</a></li><li><a class="toc-href" href="#binary-classification" title="binary classification">binary classification</a></li><li><a class="toc-href" href="#binning" title="binning">binning</a></li><li><a class="toc-href" href="#bleu" title="BLEU">BLEU</a></li><li><a class="toc-href" href="#bucketing" title="bucketing">bucketing</a></li></ul></li><li><a class="toc-href" href="#c_1" title="C">C</a><ul><li><a class="toc-href" href="#calibration-layer" title="calibration layer">calibration layer</a></li><li><a class="toc-href" href="#candidate-sampling" title="candidate sampling">candidate sampling</a></li><li><a class="toc-href" href="#categorical-data" title="categorical data">categorical data</a></li><li><a class="toc-href" href="#checkpoint" title="checkpoint">checkpoint</a></li><li><a class="toc-href" href="#class" title="class">class</a></li><li><a class="toc-href" href="#class-imbalanced-data-set" title="class-imbalanced data set">class-imbalanced data set</a></li><li><a class="toc-href" href="#classification-model" title="classification model">classification model</a></li><li><a class="toc-href" href="#classification-threshold" title="classification threshold">classification threshold</a></li><li><a class="toc-href" href="#collaborative-filtering" title="collaborative filtering">collaborative filtering</a></li><li><a class="toc-href" href="#confusion-matrix" title="confusion matrix">confusion matrix</a></li><li><a class="toc-href" href="#continuous-feature" title="continuous feature">continuous feature</a></li><li><a class="toc-href" href="#convergence" title="convergence">convergence</a></li><li><a class="toc-href" href="#convex-function" title="convex function">convex function</a></li><li><a class="toc-href" href="#convex-optimization" title="convex optimization">convex optimization</a></li><li><a class="toc-href" href="#convex-set" title="convex set">convex set</a></li><li><a class="toc-href" href="#cost" title="cost">cost</a></li><li><a class="toc-href" href="#cross-entropy" title="cross-entropy">cross-entropy</a></li><li><a class="toc-href" href="#custom-estimator" title="custom Estimator">custom Estimator</a></li></ul></li><li><a class="toc-href" href="#d_1" title="D">D</a><ul><li><a class="toc-href" href="#data-set" title="data set">data set</a></li><li><a class="toc-href" href="#dataset-api-tfdata" title="Dataset API (tf.data)">Dataset API (tf.data)</a></li><li><a class="toc-href" href="#decision-boundary" title="decision boundary">decision boundary</a></li><li><a class="toc-href" href="#dense-layer" title="dense layer">dense layer</a></li><li><a class="toc-href" href="#deep-model" title="deep model">deep model</a></li><li><a class="toc-href" href="#dense-feature" title="dense feature">dense feature</a></li><li><a class="toc-href" href="#derived-feature" title="derived feature">derived feature</a></li><li><a class="toc-href" href="#discrete-feature" title="discrete feature">discrete feature</a></li><li><a class="toc-href" href="#dropout-regularization" title="dropout regularization">dropout regularization</a></li><li><a class="toc-href" href="#dynamic-model" title="dynamic model">dynamic model</a></li></ul></li><li><a class="toc-href" href="#e_1" title="E">E</a><ul><li><a class="toc-href" href="#early-stopping" title="early stopping">early stopping</a></li><li><a class="toc-href" href="#embeddings" title="embeddings">embeddings</a></li><li><a class="toc-href" href="#empirical-risk-minimization-erm" title="empirical risk minimization (ERM)">empirical risk minimization (ERM)</a></li><li><a class="toc-href" href="#ensemble" title="ensemble">ensemble</a></li><li><a class="toc-href" href="#epoch" title="epoch">epoch</a></li><li><a class="toc-href" href="#estimator" title="Estimator">Estimator</a></li><li><a class="toc-href" href="#example" title="example">example</a></li></ul></li><li><a class="toc-href" href="#f_1" title="F">F</a><ul><li><a class="toc-href" href="#f1-score" title="F1 Score">F1 Score</a></li><li><a class="toc-href" href="#false-negative-fn" title="false negative (FN)">false negative (FN)</a></li><li><a class="toc-href" href="#false-positive-fp" title="false positive (FP)">false positive (FP)</a></li><li><a class="toc-href" href="#false-positive-rate-fp-rate" title="false positive rate (FP rate)">false positive rate (FP rate)</a></li><li><a class="toc-href" href="#feature" title="feature">feature</a></li><li><a class="toc-href" href="#feature-columns-featurecolumns" title="feature columns (FeatureColumns)">feature columns (FeatureColumns)</a></li><li><a class="toc-href" href="#feature-cross" title="feature cross">feature cross</a></li><li><a class="toc-href" href="#feature-engineering" title="feature engineering">feature engineering</a></li><li><a class="toc-href" href="#feature-set" title="feature set">feature set</a></li><li><a class="toc-href" href="#feature-spec" title="feature spec">feature spec</a></li><li><a class="toc-href" href="#full-softmax" title="full softmax">full softmax</a></li><li><a class="toc-href" href="#fully-connected-layer" title="fully connected layer">fully connected layer</a></li></ul></li><li><a class="toc-href" href="#g_1" title="G">G</a><ul><li><a class="toc-href" href="#generalization" title="generalization">generalization</a></li><li><a class="toc-href" href="#generalized-linear-model" title="generalized linear model">generalized linear model</a></li><li><a class="toc-href" href="#gradient" title="gradient">gradient</a></li><li><a class="toc-href" href="#gradient-clipping" title="gradient clipping">gradient clipping</a></li><li><a class="toc-href" href="#gradient-descent" title="gradient descent">gradient descent</a></li><li><a class="toc-href" href="#graph" title="graph">graph</a></li></ul></li><li><a class="toc-href" href="#h_1" title="H">H</a><ul><li><a class="toc-href" href="#heuristic" title="heuristic">heuristic</a></li><li><a class="toc-href" href="#hidden-layer" title="hidden layer">hidden layer</a></li><li><a class="toc-href" href="#hinge-loss" title="hinge loss">hinge loss</a></li><li><a class="toc-href" href="#holdout-data" title="holdout data">holdout data</a></li><li><a class="toc-href" href="#hyperparameter" title="hyperparameter">hyperparameter</a></li><li><a class="toc-href" href="#hyperplane" title="hyperplane">hyperplane</a></li></ul></li><li><a class="toc-href" href="#i_1" title="I">I</a><ul><li><a class="toc-href" href="#independently-and-identically-distributed-iid" title="independently and identically distributed (i.i.d)">independently and identically distributed (i.i.d)</a></li><li><a class="toc-href" href="#inference" title="inference">inference</a></li><li><a class="toc-href" href="#input-function" title="input function">input function</a></li><li><a class="toc-href" href="#input-layer" title="input layer">input layer</a></li><li><a class="toc-href" href="#instance" title="instance">instance</a></li><li><a class="toc-href" href="#interpretability" title="interpretability">interpretability</a></li><li><a class="toc-href" href="#inter-rater-agreement" title="inter-rater agreement">inter-rater agreement</a></li><li><a class="toc-href" href="#iou" title="IoU">IoU</a></li><li><a class="toc-href" href="#iteration" title="iteration">iteration</a></li></ul></li><li><a class="toc-href" href="#k_1" title="K">K</a><ul><li><a class="toc-href" href="#keras" title="Keras">Keras</a></li><li><a class="toc-href" href="#kernel-support-vector-machines-ksvms" title="Kernel Support Vector Machines (KSVMs)">Kernel Support Vector Machines (KSVMs)</a></li></ul></li><li><a class="toc-href" href="#l_1" title="L">L</a><ul><li><a class="toc-href" href="#l1-loss" title="L1 loss">L1 loss</a></li><li><a class="toc-href" href="#l1-regularization" title="L1 regularization">L1 regularization</a></li><li><a class="toc-href" href="#l2-loss" title="L2 loss">L2 loss</a></li><li><a class="toc-href" href="#l2-regularization" title="L2 regularization">L2 regularization</a></li><li><a class="toc-href" href="#label" title="label">label</a></li><li><a class="toc-href" href="#labeled-example" title="labeled example">labeled example</a></li><li><a class="toc-href" href="#lambda" title="lambda">lambda</a></li><li><a class="toc-href" href="#layer" title="layer">layer</a></li><li><a class="toc-href" href="#layers-api-tflayers" title="Layers API (tf.layers)">Layers API (tf.layers)</a></li><li><a class="toc-href" href="#learning-rate" title="learning rate">learning rate</a></li><li><a class="toc-href" href="#least-squares-regression" title="least squares regression">least squares regression</a></li><li><a class="toc-href" href="#linear-regression" title="linear regression">linear regression</a></li><li><a class="toc-href" href="#logistic-regression" title="logistic regression">logistic regression</a></li><li><a class="toc-href" href="#log-loss" title="Log Loss">Log Loss</a></li><li><a class="toc-href" href="#loss" title="loss">loss</a></li></ul></li><li><a class="toc-href" href="#m_1" title="M">M</a><ul><li><a class="toc-href" href="#machine-learning" title="machine learning">machine learning</a></li><li><a class="toc-href" href="#map" title="mAP">mAP</a></li><li><a class="toc-href" href="#mean-squared-error-mse" title="Mean Squared Error (MSE)">Mean Squared Error (MSE)</a></li><li><a class="toc-href" href="#metric" title="metric">metric</a></li><li><a class="toc-href" href="#metrics-api-tfmetrics" title="Metrics API (tf.metrics)">Metrics API (tf.metrics)</a></li><li><a class="toc-href" href="#mini-batch" title="mini-batch">mini-batch</a></li><li><a class="toc-href" href="#mini-batch-stochastic-gradient-descent-sgd" title="mini-batch stochastic gradient descent (SGD)">mini-batch stochastic gradient descent (SGD)</a></li><li><a class="toc-href" href="#ml" title="ML">ML</a></li><li><a class="toc-href" href="#model" title="model">model</a></li><li><a class="toc-href" href="#model-training" title="model training">model training</a></li><li><a class="toc-href" href="#momentum" title="Momentum">Momentum</a></li><li><a class="toc-href" href="#multi-class-classification" title="multi-class classification">multi-class classification</a></li><li><a class="toc-href" href="#multinomial-classification" title="multinomial classification">multinomial classification</a></li></ul></li><li><a class="toc-href" href="#n_1" title="N">N</a><ul><li><a class="toc-href" href="#n-gram" title="N-gram">N-gram</a></li><li><a class="toc-href" href="#nan-trap" title="NaN trap">NaN trap</a></li><li><a class="toc-href" href="#negative-class" title="negative class">negative class</a></li><li><a class="toc-href" href="#neural-network" title="neural network">neural network</a></li><li><a class="toc-href" href="#neuron" title="neuron">neuron</a></li><li><a class="toc-href" href="#node" title="node">node</a></li><li><a class="toc-href" href="#normalization" title="normalization">normalization</a></li><li><a class="toc-href" href="#numerical-data" title="numerical data">numerical data</a></li><li><a class="toc-href" href="#numpy" title="numpy">numpy</a></li></ul></li><li><a class="toc-href" href="#o_1" title="O">O</a><ul><li><a class="toc-href" href="#objective" title="objective">objective</a></li><li><a class="toc-href" href="#offline-inference" title="offline inference">offline inference</a></li><li><a class="toc-href" href="#one-hot-encoding" title="one-hot encoding">one-hot encoding</a></li><li><a class="toc-href" href="#one-vs-all" title="one-vs.-all">one-vs.-all</a></li><li><a class="toc-href" href="#online-inference" title="online inference">online inference</a></li><li><a class="toc-href" href="#operation-op" title="Operation (op)">Operation (op)</a></li><li><a class="toc-href" href="#optimizer" title="optimizer">optimizer</a></li><li><a class="toc-href" href="#outliers" title="outliers">outliers</a></li><li><a class="toc-href" href="#output-layer" title="output layer">output layer</a></li><li><a class="toc-href" href="#overfitting" title="overfitting">overfitting</a></li></ul></li><li><a class="toc-href" href="#p_1" title="P">P</a><ul><li><a class="toc-href" href="#pandas" title="pandas">pandas</a></li><li><a class="toc-href" href="#parameter" title="parameter">parameter</a></li><li><a class="toc-href" href="#parameter-server-ps" title="Parameter Server (PS)">Parameter Server (PS)</a></li><li><a class="toc-href" href="#parameter-update" title="parameter update">parameter update</a></li><li><a class="toc-href" href="#partial-derivative" title="partial derivative">partial derivative</a></li><li><a class="toc-href" href="#partitioning-strategy" title="partitioning strategy">partitioning strategy</a></li><li><a class="toc-href" href="#performance" title="performance">performance</a></li><li><a class="toc-href" href="#perplexity" title="perplexity">perplexity</a></li><li><a class="toc-href" href="#pipeline" title="pipeline">pipeline</a></li><li><a class="toc-href" href="#positive-class" title="positive class">positive class</a></li><li><a class="toc-href" href="#precision" title="precision">precision</a></li><li><a class="toc-href" href="#prediction" title="prediction">prediction</a></li><li><a class="toc-href" href="#prediction-bias" title="prediction bias">prediction bias</a></li><li><a class="toc-href" href="#pre-made-estimator" title="pre-made Estimator">pre-made Estimator</a></li><li><a class="toc-href" href="#pre-trained-model" title="pre-trained model">pre-trained model</a></li><li><a class="toc-href" href="#prior-belief" title="prior belief">prior belief</a></li></ul></li><li><a class="toc-href" href="#q_1" title="Q">Q</a><ul><li><a class="toc-href" href="#queue" title="queue">queue</a></li></ul></li><li><a class="toc-href" href="#r_1" title="R">R</a><ul><li><a class="toc-href" href="#rank" title="rank">rank</a></li><li><a class="toc-href" href="#rater" title="rater">rater</a></li><li><a class="toc-href" href="#recall" title="recall">recall</a></li><li><a class="toc-href" href="#rectified-linear-unit-relu" title="Rectified Linear Unit (ReLU)">Rectified Linear Unit (ReLU)</a></li><li><a class="toc-href" href="#regression-model" title="regression model">regression model</a></li><li><a class="toc-href" href="#regularization" title="regularization">regularization</a></li><li><a class="toc-href" href="#regularization-rate" title="regularization rate">regularization rate</a></li><li><a class="toc-href" href="#representation" title="representation">representation</a></li><li><a class="toc-href" href="#roc-receiver-operating-characteristic-curve" title="ROC (receiver operating characteristic) Curve">ROC (receiver operating characteristic) Curve</a></li><li><a class="toc-href" href="#roi-region-of-interest" title="ROI (region of interest)">ROI (region of interest)</a></li><li><a class="toc-href" href="#root-directory" title="root directory">root directory</a></li><li><a class="toc-href" href="#root-mean-squared-error-rmse" title="Root Mean Squared Error (RMSE)">Root Mean Squared Error (RMSE)</a></li></ul></li><li><a class="toc-href" href="#s_1" title="S">S</a><ul><li><a class="toc-href" href="#savedmodel" title="SavedModel">SavedModel</a></li><li><a class="toc-href" href="#saver" title="Saver">Saver</a></li><li><a class="toc-href" href="#scaling" title="scaling">scaling</a></li><li><a class="toc-href" href="#scikit-learn" title="scikit-learn">scikit-learn</a></li><li><a class="toc-href" href="#semi-supervised-learning" title="semi-supervised learning">semi-supervised learning</a></li><li><a class="toc-href" href="#sequence-model" title="sequence model">sequence model</a></li><li><a class="toc-href" href="#session" title="session">session</a></li><li><a class="toc-href" href="#sigmoid-function" title="sigmoid function">sigmoid function</a></li><li><a class="toc-href" href="#softmax" title="softmax">softmax</a></li><li><a class="toc-href" href="#sparse-feature" title="sparse feature">sparse feature</a></li><li><a class="toc-href" href="#squared-hinge-loss" title="squared hinge loss">squared hinge loss</a></li><li><a class="toc-href" href="#squared-loss" title="squared loss">squared loss</a></li><li><a class="toc-href" href="#static-model" title="static model">static model</a></li><li><a class="toc-href" href="#stationarity" title="stationarity">stationarity</a></li><li><a class="toc-href" href="#step" title="step">step</a></li><li><a class="toc-href" href="#step-size" title="step size">step size</a></li><li><a class="toc-href" href="#stochastic-gradient-descent-sgd" title="stochastic gradient descent (SGD)">stochastic gradient descent (SGD)</a></li><li><a class="toc-href" href="#structural-risk-minimization-srm" title="structural risk minimization (SRM)">structural risk minimization (SRM)</a></li><li><a class="toc-href" href="#summary" title="summary">summary</a></li><li><a class="toc-href" href="#supervised-machine-learning" title="supervised machine learning">supervised machine learning</a></li><li><a class="toc-href" href="#synthetic-feature" title="synthetic feature">synthetic feature</a></li></ul></li><li><a class="toc-href" href="#t_1" title="T">T</a><ul><li><a class="toc-href" href="#target" title="target">target</a></li><li><a class="toc-href" href="#temporal-data" title="temporal data">temporal data</a></li><li><a class="toc-href" href="#tensor" title="Tensor">Tensor</a></li><li><a class="toc-href" href="#tensor-processing-unit-tpu" title="Tensor Processing Unit (TPU)">Tensor Processing Unit (TPU)</a></li><li><a class="toc-href" href="#tensor-rank" title="Tensor rank">Tensor rank</a></li><li><a class="toc-href" href="#tensor-shape" title="Tensor shape">Tensor shape</a></li><li><a class="toc-href" href="#tensor-size" title="Tensor size">Tensor size</a></li><li><a class="toc-href" href="#tensorboard" title="TensorBoard">TensorBoard</a></li><li><a class="toc-href" href="#tensorflow" title="TensorFlow">TensorFlow</a></li><li><a class="toc-href" href="#tensorflow-playground" title="TensorFlow Playground">TensorFlow Playground</a></li><li><a class="toc-href" href="#tensorflow-serving" title="TensorFlow Serving">TensorFlow Serving</a></li><li><a class="toc-href" href="#test-set" title="test set">test set</a></li><li><a class="toc-href" href="#tfexample" title="tf.Example">tf.Example</a></li><li><a class="toc-href" href="#time-series-analysis" title="time series analysis">time series analysis</a></li><li><a class="toc-href" href="#training" title="training">training</a></li><li><a class="toc-href" href="#training-set" title="training set">training set</a></li><li><a class="toc-href" href="#transfer-learning" title="transfer learning">transfer learning</a></li><li><a class="toc-href" href="#true-negative-tn" title="true negative (TN)">true negative (TN)</a></li><li><a class="toc-href" href="#true-positive-tp" title="true positive (TP)">true positive (TP)</a></li><li><a class="toc-href" href="#true-positive-rate-tp-rate" title="true positive rate (TP rate)">true positive rate (TP rate)</a></li></ul></li><li><a class="toc-href" href="#u_1" title="U">U</a><ul><li><a class="toc-href" href="#unlabeled-example" title="unlabeled example">unlabeled example</a></li><li><a class="toc-href" href="#unpooling" title="unpooling">unpooling</a></li><li><a class="toc-href" href="#unsupervised-machine-learning" title="unsupervised machine learning">unsupervised machine learning</a></li></ul></li><li><a class="toc-href" href="#v_1" title="V">V</a><ul><li><a class="toc-href" href="#validation-set" title="validation set">validation set</a></li></ul></li><li><a class="toc-href" href="#w_1" title="W">W</a><ul><li><a class="toc-href" href="#weight" title="weight">weight</a></li><li><a class="toc-href" href="#wide-model" title="wide model">wide model</a></li></ul></li></ul></div>
<!-- </section> -->
<!-- </section> -->
        </nav>
      </div>
    </div>
  </div>


<!--  -->
<!--  -->

<script src="https://freeopen.github.io/theme/js/jquery-3.3.1.slim.min.js"></script>
<script src="https://freeopen.github.io/theme/js/popper.min.js"></script>
<script src="https://freeopen.github.io/theme/js/bootstrap.min.js"></script>
</body>
</html>