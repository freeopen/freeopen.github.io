<!DOCTYPE html>
<html lang="en">
<head>
          <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta http-equiv="content-type" content="text/html; charset=utf-8">
        <!-- Enable responsiveness on mobile devices-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

        <title>Freeopen - TensorFlow 的 MNIST 教程</title>
        <link rel="stylesheet" href="https://freeopen.github.io/theme/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://freeopen.github.io/theme/css/main.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/academicons/css/academicons.min.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/font-awesome/css/font-awesome.min.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/fonts-googleapis/css/googleapi-fonts.css" />

        






</head>

<body id="index" class="theme-myblue" data-spy="scroll" data-target="#post_toc">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 col-lg">
        <nav class="navbar navbar-expand-lg navbar-light sidebar" role="navigation">
          <a class="sidebar-about" href="https://freeopen.github.io/">Freeopen</a>
          <button class="navbar-toggler " type="button" data-toggle="collapse" data-target="#navbarNav">
            <span class="navbar-toggler-icon"></span>
          </button>
          <nav class="collapse navbar-collapse sidebar-nav flex-column" id="navbarNav">

              <a class="nav-link" href="https://freeopen.github.io/pages/about.html">About</a>
              <a class="nav-link" href="https://freeopen.github.io/pages/da-shang.html">打赏</a>

              <a class="nav-link" href="https://freeopen.github.io/category/bian-cheng.html">编程</a>
              <a class="nav-link" href="https://freeopen.github.io/category/ji-qi-xue-xi.html">机器学习</a>
              <a class="nav-link" href="https://freeopen.github.io/category/shou-ce.html">手册</a>
              <a class="nav-link" href="https://freeopen.github.io/category/shu-xue.html">数学</a>
            <div class="sidebar-icons">
              <hr>
                <a href="" title="Atom feed" target="_blank" 
                    style="display: inline; padding: 0px 0px 0px 0; margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;">
                  <i class="fa fa-rss-square"></i>
                </a>
                <a href="https://github.com/freeopen" title="Software I released on Github" 
                      target="_blank" style="display: inline; padding: 0px 0px 0px 0; margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;"><i class="fa fa-github-square"></i> </a>
                <a href="Mailto:freeopen@163.com" title="My email" 
                    target="_blank" style="display: inline; padding: 0px 0px 0px 0;
                    margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;"><i
                  class="fa fa-envelope-square"></i> </a>

            </div>
          </nav>

        </nav>

      </div>

      <div class="col-12 col-lg-7" role="main">
        <div id="content" class="content">
          <div class="post">
<section id="content" class="body">
  <header>
    <h1 class="entry-title">TensorFlow 的 MNIST 教程</h2>
 
  </header>
  <div class="post-info">

    <span>2017-08-02</span>


    <span>| By             <a class="url fn" href="https://freeopen.github.io/author/freeopen.html">freeopen</a>
    </span>

    <span>
      | 分类于 <a href="https://freeopen.github.io/category/ji-qi-xue-xi.html">机器学习</a>
    </span>

  </div><!-- /.post-info -->
  <div class="post content">
    <p><a href="https://www.tensorflow.org/get_started/mnist/pros">原文</a></p>
<blockquote>
<p>译者注：这篇文章对初学tensorflow的朋友来说，有很好的参考作用。曾经在网上看过本教程的翻译稿，但版本偏老。本文翻译时，tensorflow的版本为1.3.0-rc1. 新版相较于老版，示例代码有变化，文字说明也有增补。</p>
</blockquote>
<p>TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。
在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。</p>
<p>这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看新手指南.</p>
<h2 id="guan-yu-ben-jiao-cheng">关于本教程</h2>
<p>本教程第一部分讲解 <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_softmax.py">mnist_softmax.py</a> 代码, 一个简单的tensorflow模型实现。第二部分介绍一些提高精度的方法。</p>
<p>你可以从本教程拷贝和粘贴代码块到你的python环境，或者下载完整代码 <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_deep.py">mnist_deep.py</a>.</p>
<p>我们将完成如下目标：</p>
<ul>
<li>基于图片像素，创建一个softmax回归函数的模型来识别MNIST数字。</li>
<li>用tensorflow训练模型识别数字。</li>
<li>用测试数据检查模型精度</li>
<li>构建、训练和测试一个多层卷积神经网络来提升模型精度。</li>
</ul>
<h2 id="an-zhuang">安装</h2>
<p>在创建模型之前，我们会先加载MNIST数据集，然后启动一个TensorFlow的session。</p>
<h3 id="jia-zai-mnistshu-ju">加载MNIST数据</h3>
<p>为了方便起见，我们已经准备了一个脚本来自动下载和导入MNIST数据集。它会自动创建一个'MNIST_data'的目录来存储数据。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s1">'MNIST_data'</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>这里，<code>mnist</code>是一个轻量级的类。它以Numpy数组的形式存储着训练、校验和测试数据集。同时提供了一个函数，用于在迭代中获得minibatch，后面我们将会用到。</p>
<h3 id="yun-xing-tensorflowde-interactivesession">运行TensorFlow的InteractiveSession</h3>
<p>Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。</p>
<p>这里，我们使用更加方便的InteractiveSession类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些计算图，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用InteractiveSession，那么你需要在启动session之前构建整个计算图，然后启动该计算图。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">InteractiveSession</span><span class="p">()</span>
</pre></div>
<h3 id="ji-suan-tu">计算图</h3>
<p>为了在Python中进行高效的数值计算，我们通常会使用像NumPy一类的库，将一些诸如矩阵乘法的耗时操作在Python环境的外部来计算，这些计算通常会通过其它语言并用更为高效的代码来实现。</p>
<p>但遗憾的是，每一个操作切换回Python环境时仍需要不小的开销。如果你想在GPU或者分布式环境中计算时，这一开销更加可怖，这一开销主要可能是用来进行数据迁移。</p>
<p>TensorFlow也是在Python外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在Python外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在Python外部。这与Theano或Torch的做法类似。</p>
<p>因此Python代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。详情请查看基本用法中的计算图表一节。</p>
<h2 id="gou-jian--softmax-hui-gui-mo-xing_1">构建 Softmax 回归模型</h2>
<p>在这一节中我们将建立一个拥有一个线性层的softmax回归模型。在下一节，我们会将其扩展为一个拥有多层卷积网络的softmax回归模型。</p>
<h3 id="zhan-wei-fu-placeholders">占位符(placeholders)</h3>
<p>我们通过为输入图像和目标输出类别创建节点，来构建计算图。</p>
<div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">"float"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">"float"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
<p>这里的<code>x</code>和<code>y_</code>并不是特定的值，相反，他们都只是一个<code>占位符</code>，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。</p>
<p>输入图片<code>x</code>是一个2维的浮点数张量。这里，分配给它的shape为[None, 784]，其中784是一张展平的MNIST图片的维度。None表示其值大小不定，在这里作为第一个维度值，用以指代batch的大小，意即x的数量不定。输出类别值y_也是一个2维张量，其中每一行为一个10维的one-hot向量,用于代表对应某一MNIST图片的类别。</p>
<p>虽然placeholder的shape参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。</p>
<h3 id="bian-liang">变量</h3>
<p>我们现在为模型定义权重W和偏置b。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：变量。一个变量代表着TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用Variable来表示。</p>
<div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">]))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">]))</span>
</pre></div>
<p>我们在调用<code>tf.Variable</code>的时候传入初始值。
在这个例子里，我们把<code>W</code>和<code>b</code>都初始化为零向量。<code>W</code>是一个784x10的矩阵（因为我们有784个特征和10个输出值）。<code>b</code>是一个10维的向量（因为我们有10个分类）。</p>
<p>变量需要通过session初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个变量,可以一次性为所有变量完成此操作。</p>
<div class="highlight"><pre><span></span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
</pre></div>
<h2 id="lei-bie-yu-ce-yu-sun-shi-han-shu_1">类别预测与损失函数</h2>
<p>现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片<code>x</code>和权重矩阵<code>W</code>相乘，加上偏置<code>b</code>。</p>
<div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
<p>可以很容易的为训练过程指定最小化误差用的损失函数，损失表示在一个样本上模型预测有多差; 我们试图在所有的样本上最小化这个损失。这里， 我们的损失函数是目标类别和softmax激活函数之间的交叉熵。在教程开始，我们使用如下公式：</p>
<div class="highlight"><pre><span></span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y_</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">y</span><span class="p">))</span>
</pre></div>
<p>注意，<code>tf.nn.softmax_cross_entropy_with_logits</code>在内部用softmax规范化模型，
并对所有分类求和，而<code>tf.reduce_mean</code>对这些和求平均 。</p>
<h2 id="xun-lian-mo-xing">训练模型</h2>
<p>我们已经定义好模型和训练用的损失函数，那么用TensorFlow进行训练就很简单了。因为TensorFlow知道整个计算图，它可以使用自动微分法找到对于各个变量的损失的梯度值。TensorFlow有大量内置的优化算法 这个例子中，我们用最速下降法让交叉熵下降，步长为0.5.</p>
<div class="highlight"><pre><span></span><span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
</pre></div>
<p>这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。</p>
<p>返回的<code>train_step</code>操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行<code>train_step</code>来完成。</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">train_step</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]})</span>
</pre></div>
<p>每一步迭代，我们都会加载100个训练样本，然后执行一次<code>train_step</code>，并通过<code>feed_dict</code>将<code>x</code> 和 <code>y_</code> 张量占位符用训练训练数据替代。
注意，在计算图中，你可以用<code>feed_dict</code>来替代任何张量，并不仅限于替换占位符。</p>
<h3 id="ping-gu-mo-xing">评估模型</h3>
<p>那么我们的模型性能如何呢？</p>
<p>首先让我们找出那些预测正确的标签。<code>tf.argmax</code> 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如<code>tf.argmax(y,1)</code>返回的是模型对于任一输入<code>x</code>预测到的标签值，而<code>tf.argmax(y_,1)</code> 代表正确的标签，我们可以用 <code>tf.equal</code> 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。</p>
<div class="highlight"><pre><span></span><span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
<p>这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：[True, False, True, True]变为[1,0,1,1]，计算出平均值为0.75。</p>
<div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
<p>最后，我们可以计算出在测试数据上的准确率，大概是92%。</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">}))</span>
</pre></div>
<h2 id="gou-jian-duo-ceng-juan-ji-wang-luo_1">构建多层卷积网络</h2>
<p>在MNIST上只有92% 的准确率，实在太糟糕。在这节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。</p>
<h3 id="quan-zhong-chu-shi-hua">权重初始化</h3>
<p>为了创建这个模型，我们需要创建大量的权重和偏置项。
这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。
由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">weight_variable</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
  <span class="n">initial</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">bias_variable</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
  <span class="n">initial</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span>
</pre></div>
<h3 id="juan-ji-he-chi-hua">卷积和池化</h3>
<p align="center">
<img src="/images/convgaus.gif"/>
<br/>
</p>
<blockquote>
<p>译者注：卷积，从数学角度说，指计算一个函数通过另一个函数时，有多少重叠的积分。也可以视为通过相乘的方式将两个函数进行混合。详见上图,
红色曲线（函数f）下的区域是积分，蓝色曲线(函数g)从左向右缓慢移动，绿色曲线下的区域是红色曲线和蓝色曲线的卷积，灰色阴影表示在绿色垂直线位置时，红色曲线和蓝色曲线的卷积, 其值为f(a)*g(x-a)。 <br> <br/>
下面再给个我非常喜欢的说明，帮助理解这个公式，写得非常有意思。  <br> <br/>
比如说你的老板命令你干活，你却到楼下打台球去了，后来被老板发现，他非常气愤，扇了你一巴掌（注意，这就是输入信号，脉冲），于是你的脸上会渐渐地（贱贱地）鼓起来一个包，你的脸就是一个系统，而鼓起来的包就是你的脸对巴掌的响应，好，这样就和信号系统建立起来意义对应的联系。下面还需要一些假设来保证论证的严谨：假定你的脸是线性时不变系统，也就是说，无论什么时候老板打你一巴掌，打在你脸的同一位置（这似乎要求你的脸足够光滑，如果你说你长了很多青春痘，甚至整个脸皮处处连续处处不可导，那难度太大了，我就无话可说了哈哈），你的脸上总是会在相同的时间间隔内鼓起来一个相同高度的包来，并且假定以鼓起来的包的大小作为系统输出。好了，那么，下面可以进入核心内容&mdash;&mdash;卷积了！  <br> <br/>
如果你每天都到楼下去打台球，那么老板每天都要扇你一巴掌，不过当老板打你一巴掌后，你5分钟就消肿了，所以时间长了，你甚至就适应这种生活了&hellip;&hellip;如果有一天，老板忍无可忍，以0.5秒的间隔开始不间断的扇你的过程，这样问题就来了，第一次扇你鼓起来的包还没消肿，第二个巴掌就来了，你脸上的包就可能鼓起来两倍高，老板不断扇你，脉冲不断作用在你脸上，效果不断叠加了，这样这些效果就可以求和了，结果就是你脸上的包的高度随时间变化的一个函数了（注意理解）；如果老板再狠一点，频率越来越高，以至于你都辨别不清时间间隔了，那么，求和就变成积分了。可以这样理解，在这个过程中的某一固定的时刻，你的脸上的包的鼓起程度和什么有关呢？和之前每次打你都有关！但是各次的贡献是不一样的，越早打的巴掌，贡献越小，所以这就是说，某一时刻的输出是之前很多次输入乘以各自的衰减系数之后的叠加而形成某一点的输出，然后再把不同时刻的输出点放在一起，形成一个函数，这就是卷积，卷积之后的函数就是你脸上的包的大小随时间变化的函数。本来你的包几分钟就可以消肿，可是如果连续打，几个小时也消不了肿了，这难道不是一种平滑过程么？反映到剑桥大学的公式上，f(a)就是第a个巴掌，g(x-a)就是第a个巴掌在x时刻的作用程度，乘起来再叠加就ok了，大家说是不是这个道理呢？</br></br></br></p>
</blockquote>
<p>TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？
在这个实例里，我们会一直使用普通版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。
我们的池化用简单传统的2x2大小的模板做最大池化。为了代码更简洁，我们把这部分抽象成一个函数。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'SAME'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">max_pool_2x2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'SAME'</span><span class="p">)</span>
</pre></div>
<h3 id="di-yi-ceng-juan-ji">第一层卷积</h3>
<p>现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。</p>
<div class="highlight"><pre><span></span><span class="n">W_conv1</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
<span class="n">b_conv1</span> <span class="o">=</span> <span class="n">bias_variable</span><span class="p">([</span><span class="mi">32</span><span class="p">])</span>
</pre></div>
<p>为了用这一层，我们把<code>x</code>变成一个4维向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，
如果是rgb彩色图，则为3)。</p>
<blockquote>
<p>译者注：这里<code>x</code>是2维矩阵（m, 784), 其中m表示样本数量，所以下面的reshape中的 <code>-1</code>的位置实质表示的是m值，此处的<code>-1</code>可以推断出m值，在具体执行时，将用m值替换.</p>
</blockquote>
<div class="highlight"><pre><span></span><span class="n">x_image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<p>我们把<code>x_image</code>和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行最大池化。</p>
<div class="highlight"><pre><span></span><span class="n">h_conv1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x_image</span><span class="p">,</span> <span class="n">W_conv1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_conv1</span><span class="p">)</span>
<span class="n">h_pool1</span> <span class="o">=</span> <span class="n">max_pool_2x2</span><span class="p">(</span><span class="n">h_conv1</span><span class="p">)</span>
</pre></div>
<h3 id="di-er-ceng-juan-ji">第二层卷积</h3>
<p>为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。</p>
<div class="highlight"><pre><span></span><span class="n">W_conv2</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>
<span class="n">b_conv2</span> <span class="o">=</span> <span class="n">bias_variable</span><span class="p">([</span><span class="mi">64</span><span class="p">])</span>

<span class="n">h_conv2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">conv2d</span><span class="p">(</span><span class="n">h_pool1</span><span class="p">,</span> <span class="n">W_conv2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_conv2</span><span class="p">)</span>
<span class="n">h_pool2</span> <span class="o">=</span> <span class="n">max_pool_2x2</span><span class="p">(</span><span class="n">h_conv2</span><span class="p">)</span>
</pre></div>
<h3 id="quan-lian-jie-ceng">全连接层</h3>
<p>现在，图片尺寸缩小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。</p>
<div class="highlight"><pre><span></span><span class="n">W_fc1</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">([</span><span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1024</span><span class="p">])</span>
<span class="n">b_fc1</span> <span class="o">=</span> <span class="n">bias_variable</span><span class="p">([</span><span class="mi">1024</span><span class="p">])</span>

<span class="n">h_pool2_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">h_pool2</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">64</span><span class="p">])</span>
<span class="n">h_fc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_pool2_flat</span><span class="p">,</span> <span class="n">W_fc1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_fc1</span><span class="p">)</span>
</pre></div>
<h3 id="dropout">Dropout</h3>
<p>为了减少过拟合，我们在输出层之前加入<a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">dropout</a>。我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。
这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 
TensorFlow的<code>tf.nn.dropout</code>操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup>。</p>
<div class="highlight"><pre><span></span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_fc1_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">h_fc1</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
</pre></div>
<h3 id="shu-chu-ceng">输出层</h3>
<p>最后，我们添加一个softmax层，就像前面的单层 softmax 回归一样。</p>
<div class="highlight"><pre><span></span><span class="n">W_fc2</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">b_fc2</span> <span class="o">=</span> <span class="n">bias_variable</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>

<span class="n">y_conv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_fc1_drop</span><span class="p">,</span> <span class="n">W_fc2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_fc2</span>
</pre></div>
<h3 id="xun-lian-he-ping-gu-mo-xing">训练和评估模型</h3>
<p>这个模型的效果如何呢？</p>
<p>为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，
这个模型和上个模型的不同之处：</p>
<ul>
<li>只是我们会用更加复杂的<a href="http://arxiv.org/abs/1412.6980">ADAM优化器</a>来做梯度最速下降，</li>
<li>在<code>feed_dict</code>中加入额外的参数<code>keep_prob</code>来控制dropout比例。</li>
<li>然后每100次迭代输出一次日志。</li>
</ul>
<p>我们使用了 <code>tf.Session</code> 而不是 <code>tf.InteractiveSession</code>. 将创建图(模型规格)和评估图(模型匹配)的处理分开, 这样便产生更清晰的代码。tf.Session被with代码块创建，以至于一旦块退出时它将自动销毁。</p>
<p>随时运行此代码。请注意, 它会进行2万次训练迭代, 并且需要一段时间 (可能长达半小时), 这取决于你的处理器。</p>
<div class="highlight"><pre><span></span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y_</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">y_conv</span><span class="p">))</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
<span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_conv</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20000</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
          <span class="n">x</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
      <span class="k">print</span><span class="p">(</span><span class="s1">'step </span><span class="si">%d</span><span class="s1">, training accuracy </span><span class="si">%g</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">))</span>
    <span class="n">train_step</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>

  <span class="k">print</span><span class="p">(</span><span class="s1">'test accuracy </span><span class="si">%g</span><span class="s1">'</span> <span class="o">%</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
      <span class="n">x</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}))</span>
</pre></div>
<p>以上代码，在最终测试集上的准确率大概是99.2%。</p>
<p>目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。</p>
<p>如果你想用力鼓励一下，欢迎<a href="../../pay.html">打赏</a>，官方建议零售价 &yen;2 。</p>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p>在小型卷积网络中，有没有dropout对性能影响不大。Dropout 通常可以降低过拟合，它常常被用于大型的神经网络中。&nbsp;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
  </div><!-- /.entry-content -->

  <div class="comments">
    <!-- <h2>Comments !</h2> -->
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'freeopen';
      var disqus_identifier = 'posts/tensorflow-de-mnist-jiao-cheng';
      var disqus_url = 'https://freeopen.github.io/posts/tensorflow-de-mnist-jiao-cheng';
      (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//freeopen.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the comments.</noscript>
  </div>
</section>
          </div>

          <footer id="contentinfo" class="footer">
            <div class="footer-inner">
              无节操小广告<a href="https://freeopen.github.io/pages/da-shang.html"> 欢迎打赏 </a>
            </div>
          </footer><!-- /#contentinfo -->
        </div>
      </div>

      <div class="hidden-print hidden-xs hidden-sm hidden-md col-lg" id="post_toc" role="complementary">
        <nav class="sidebar-toc">
<!-- <section id="main_toc"> -->
    <ul class="nav" id="toc"><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#guan-yu-ben-jiao-cheng" title="关于本教程">关于本教程</a></li><li class="nav-item"><a class="nav-link" href="#an-zhuang" title="安装">安装</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#jia-zai-mnistshu-ju" title="加载MNIST数据">加载MNIST数据</a></li><li class="nav-item"><a class="nav-link" href="#yun-xing-tensorflowde-interactivesession" title="运行TensorFlow的InteractiveSession">运行TensorFlow的InteractiveSession</a></li><li class="nav-item"><a class="nav-link" href="#ji-suan-tu" title="计算图">计算图</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#gou-jian--softmax-hui-gui-mo-xing_1" title="构建 Softmax 回归模型">构建 Softmax 回归模型</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#zhan-wei-fu-placeholders" title="占位符(placeholders)">占位符(placeholders)</a></li><li class="nav-item"><a class="nav-link" href="#bian-liang" title="变量">变量</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#lei-bie-yu-ce-yu-sun-shi-han-shu_1" title="类别预测与损失函数">类别预测与损失函数</a></li><li class="nav-item"><a class="nav-link" href="#xun-lian-mo-xing" title="训练模型">训练模型</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#ping-gu-mo-xing" title="评估模型">评估模型</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#gou-jian-duo-ceng-juan-ji-wang-luo_1" title="构建多层卷积网络">构建多层卷积网络</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#quan-zhong-chu-shi-hua" title="权重初始化">权重初始化</a></li><li class="nav-item"><a class="nav-link" href="#juan-ji-he-chi-hua" title="卷积和池化">卷积和池化</a></li><li class="nav-item"><a class="nav-link" href="#di-yi-ceng-juan-ji" title="第一层卷积">第一层卷积</a></li><li class="nav-item"><a class="nav-link" href="#di-er-ceng-juan-ji" title="第二层卷积">第二层卷积</a></li><li class="nav-item"><a class="nav-link" href="#quan-lian-jie-ceng" title="全连接层">全连接层</a></li><li class="nav-item"><a class="nav-link" href="#dropout" title="Dropout">Dropout</a></li><li class="nav-item"><a class="nav-link" href="#shu-chu-ceng" title="输出层">输出层</a></li><li class="nav-item"><a class="nav-link" href="#xun-lian-he-ping-gu-mo-xing" title="训练和评估模型">训练和评估模型</a></li></ul></li></ul></ul>
<!-- </section> -->
<!-- </section> -->
        </nav>
      </div>
    </div>
  </div>


    <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-98793057-1', 'auto');
    ga('send', 'pageview');
    </script>
<script type="text/javascript">
    var disqus_shortname = 'freeopen';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'https://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<script src="https://freeopen.github.io/theme/js/jquery-3.3.1.slim.min.js"></script>
<script src="https://freeopen.github.io/theme/js/popper.min.js"></script>
<script src="https://freeopen.github.io/theme/js/bootstrap.min.js"></script>

</body>
</html>