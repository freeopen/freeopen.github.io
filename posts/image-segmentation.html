<!DOCTYPE html>
<html lang="en">
<head>
          <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta http-equiv="content-type" content="text/html; charset=utf-8">
        <!-- Enable responsiveness on mobile devices-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

        <title>Freeopen - 基于深度学习的图像分割算法思路演进</title>
        <link rel="stylesheet" href="https://freeopen.github.io/theme/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://freeopen.github.io/theme/css/main.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/academicons/css/academicons.min.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/font-awesome/css/font-awesome.min.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/fonts-googleapis/css/googleapi-fonts.css" />

        






</head>

<body id="index" class="theme-myblue" data-spy="scroll" data-target="#post_toc">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 col-lg">
        <nav class="navbar navbar-expand-lg navbar-light sidebar" role="navigation">
          <a class="sidebar-about" href="https://freeopen.github.io/">Freeopen</a>
          <button class="navbar-toggler " type="button" data-toggle="collapse" data-target="#navbarNav">
            <span class="navbar-toggler-icon"></span>
          </button>
          <nav class="collapse navbar-collapse sidebar-nav flex-column" id="navbarNav">

              <a class="nav-link" href="https://freeopen.github.io/pages/about.html">About</a>
              <a class="nav-link" href="https://freeopen.github.io/pages/da-shang.html">打赏</a>

              <a class="nav-link" href="https://freeopen.github.io/category/bian-cheng.html">编程</a>
              <a class="nav-link" href="https://freeopen.github.io/category/ji-qi-xue-xi.html">机器学习</a>
              <a class="nav-link" href="https://freeopen.github.io/category/shou-ce.html">手册</a>
              <a class="nav-link" href="https://freeopen.github.io/category/shu-xue.html">数学</a>
            <div class="sidebar-icons">
              <hr>
                <a href="" title="Atom feed" target="_blank" 
                    style="display: inline; padding: 0px 0px 0px 0; margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;">
                  <i class="fa fa-rss-square"></i>
                </a>
                <a href="https://github.com/freeopen" title="Software I released on Github" 
                      target="_blank" style="display: inline; padding: 0px 0px 0px 0; margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;"><i class="fa fa-github-square"></i> </a>
                <a href="Mailto:freeopen@163.com" title="My email" 
                    target="_blank" style="display: inline; padding: 0px 0px 0px 0;
                    margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;"><i
                  class="fa fa-envelope-square"></i> </a>

            </div>
          </nav>

        </nav>

      </div>

      <div class="col-12 col-lg-7" role="main">
        <div id="content" class="content">
          <div class="post">
<section id="content" class="body">
  <header>
    <h1 class="entry-title">基于深度学习的图像分割算法思路演进</h2>
 
  </header>
  <div class="post-info">

    <span>2019-03-05</span>

    <span>| 更新于 2019-03-09</span>

    <span>| By             <a class="url fn" href="https://freeopen.github.io/author/freeopen.html">freeopen</a>
    </span>

    <span>
      | 分类于 <a href="https://freeopen.github.io/category/ji-qi-xue-xi.html">机器学习</a>
    </span>

  </div><!-- /.post-info -->
  <div class="post content">
    <blockquote>
<p>又是一篇不会写完的文章，和前面那篇《卷积备忘录》一样，
会跟进技术发展不断更新。读了一系列论文，有必要再次整理自己的思考了，
就怕笔不勤忘得快。如有理解错误之处，请读者及时来信告知，如果
你精通翻墙术，文章末尾可以留言。（已架设 Disqus 评论系统，
因国内不可名状的原因，翻墙后才能显示。）</p>
<p>边写边发，未完待续</p>
</blockquote>
<h2 id="tu-xiang-fen-ge-dao-di-gan-shi-yao">图像分割到底干什么</h2>
<p>图像分割问题主要流行四类任务，它们分别是目标检测( Object Detection )、语义分割( Semantic Segmentation )、实例分割( Instance Segmentation ) 和全景分割( Panoptic Segmentation )。这四类任务的意义分别为：</p>
<ol>
<li>目标检测：计算机看一张图，用矩形框框出我们感兴趣的目标（定位任务），
   并告诉我们该目标是什么（分类任务）。</li>
<li>语义分割：把整张图像的每个像素赋予一个类别标签。
   不过语义分割的任务是只判断类别，不区分个体。 </li>
<li>实例分割：比目标分割更进一步，对我们感兴趣目标的像素赋予类别标签，
   且对于挨在一起的同种类型的目标，需要区分出个体（即实例），
   比如几个人相互重叠的站在一起，实例分割时要用不同颜色的色块(论文中称为&ldquo;掩膜&rdquo;, mask)覆盖住每个人。</li>
<li>全景分割：是语义分割和实体分割的结合，每个像素都被分为一类，
   如果一种类别里有多个实例，会用不同的颜色进行区分，
   我们可以知道哪个像素属于哪个类中的哪个实例。</li>
</ol>
<p>上面四类任务我把它统称为图像分割任务，它实质上由两个子任务构成，一个是定位任务，一个是分类任务。
其中，目标检测和实例分割只分割出感兴趣区域，不关心其他区域；语义分割和全景分割对图片的所有区域进行分割，
但语义分割不区分连在一起的同类个体。</p>
<h2 id="tu-xiang-fen-ge-de-suan-fa-si-lu">图像分割的算法思路</h2>
<p>如果你同意图像分割问题均由定位任务和分类任务组成，那么上面的四类任务的算法应该能互相借鉴，
并最终会统一在一起（即一个模型能同时干完所有的活 -- 四类任务）。</p>
<p>我先按照人类直觉来思考图像分割问题, 步骤为：</p>
<ol>
<li>看一眼图，找到哪些位置有东西；</li>
<li>再看看有东西的位置都是些什么东西；</li>
<li>把我们关心的那类东西标记出来。</li>
</ol>
<p>再按计算机直觉来细化上面的步骤。计算机看图实质上看到的是密密麻麻的像素点，这些点在计算机眼里没有大小，没有颜色，只是一堆数字而已。计算机识别一张图片是什么内容，根据一块区域的一堆数字分布规律来做出判断，
所以，机器判断一张图哪些位置有东西时，只能用各种大小的矩形框从图中抽取出像素点，再来判断这些像素点们到底是不是东西，进而再判断它是什么东西。故根据计算机看图的特点，算法进一步描述为：</p>
<ol>
<li>扫描图片，找到一堆可能有东西的候选区域；</li>
<li>取得候选区域对应的视觉特征，对其进行识别，判断有东西还是没东西；</li>
<li>如果有东西，判断它是什么东西，并标记出它的位置。</li>
</ol>
<p><strong>算法面临的挑战：</strong></p>
<ul>
<li>
<p>在第 1 步中，由于目标的大小、轮廓千变万化，要找出候选区域，有多种方案。</p>
<ul>
<li>方法1: 把图片均匀分成细小的网格，再根据网格的特征相似度拼接网格，
    拼接后的网格即为候选区域。</li>
<li>方法2: 用不同大小，不同长宽比的矩形框作为滑动窗口，
    在图片上从左到右、由上至下滑动，被框住的区域即为候选区域。</li>
<li>方法3: 利用图片的几何特征(比如物体的轮廓位置的颜色梯度会较大)，用算法找出候选区域。</li>
</ul>
<p>对于方法1, 优点是候选区域不会重叠，难点在于网格的拼接策略。</p>
<p>对于方法2，会产生相互重叠的候选区域，
且候选区域不一定贴合目标的轮廓，需要有办法调整候选区域大小，
让它最接近目标物体的轮廓。</p>
<p>对于方法3, 只要效率高，个人认为是最优策略，但目前这类算法的性能还需要提高。</p>
<p>思考：方法 1 和方法 2 本质上都可以看成滑动窗口扫描图片，找出候选区。
但，如果我们用卷积网络抽取图片特征，则较低的卷积层反映图片较细节
的特征(如纹理、边缘
等)，较高的卷积层反映图片较宏观的特征（如物体的轮廓等），所以
候选区域在较高的卷积层输出的特征图上取得，
理论上效果会比直接从原图上取得的要好, 且所需参数也较少。</p>
</li>
<li>
<p>在第 2 步中，相当于一个二分类问题，判断候选区域框住的东西是
    前景还是背景。
    面临的难题是候选区域输出的特征图大小不一，
    这些特征图在进行分类前，须调成固定尺寸的向量。</p>
</li>
<li>
<p>在第 3 步中，有两个分支，一个判断目标类型的分类器和一个标记目标位置
    的定位器。难点在于定位任务，因为这时看到的特征图尺寸小于原图，
    需要找到一个办法把特征图的坐标还原成原图的坐标。</p>
</li>
</ul>
<p>上述的这些难题被解决的越好，那么图像分割的效果也就越好。
下面看看研究者们都有些什么巧思，来跨越这些鸿沟。</p>
<h2 id="jing-dian-suan-fa-de-yan-jin">经典算法的演进</h2>
<h3 id="r-cnn">R-CNN</h3>
<p>R-CNN( Regions with Convolutional Neural Network Features )
模型可称为深度学习用于目标检测的开山之作，来自 
Ross Girshick 于2013年11月发表的一篇论文，
论文名为《Rich feature hierarchies for Accurate Object Detection and Segmentation》。</p>
<p>模型结构如下：</p>
<p align="center">
<img src="/images/r-cnn.png" width="75%"/>
<figcaption>
R-CNN 概览：输入一张图片，定位出2000个物体候选框，
然后采用CNN提取每个候选框中图片的特征向量，
特征向量的维度为4096维，接着采用svm算法对各个候选框中的物体进行分类识别。
</figcaption>
</p>
<p>上图仅为模型概要，算法实质上分为三步：</p>
<ol>
<li>找出候选区域(regions proposals), 将其作为边界框(Bounding Box)；</li>
<li>用预训练好的卷积网络(如 AlexNet)提取特征向量, 并用 SVM
   算法判断边界框中对应的物体类型；</li>
<li>对已分类物体的边界框进行线性回归，输出其对应的贴近物体轮廓的紧身边界框(tighter bounding boxes)。</li>
</ol>
<h4>如何找出候选区域</h4>
<p>R-CNN 用选择性搜索( Selective Search )算法从原始图片上找出一堆候选框，
并选出得分最高的2000个。</p>
<p><strong>选择性搜索( Selective Search )算法</strong></p>
<ol>
<li>生成原始的候选框集合 R（利用<strong>felzenszwalb算法</strong><sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup>）;</li>
<li>计算候选框集 R 里每个相邻区域的相似度 S={s1,s2,&hellip;},
   这里考虑了四种类型的相似度，包括纹理、颜色、尺寸及交叠;</li>
<li>找出相似度最高的两个区域，将其合并为新集，添加进 R&nbsp;;</li>
<li>从 S 中移除所有与第 3 步中有关的子集;&nbsp;</li>
<li>计算新集与所有子集的相似度;&nbsp;</li>
<li>跳至第三步，不断循环，合并，直至 S 为空（到不能再合并时为止）。</li>
</ol>
<h4>如何从不同大小的候选框提取空间特征</h4>
<p>直接缩放成固定大小的正方形（论文采用227&times;227）, 送进
卷积网络( 修改版的 AlexNet )抽取空间特征。</p>
<h4>如何选择最好的边界框</h4>
<p>比如定位一个物体，算法可能会找出一堆边界框，我们需要选出最好的边界框。
这里用到了<strong>非极大值抑制(Non-maximum suppression, NMS)</strong>算法，该算法
其本质是搜索局部极大值，抑制非极大值元素, 具体如下：</p>
<p>假设有 6 个矩形框都是目标车辆的边界框，用 SVM 分类器对它们打分并排序，
从小到大属于车辆的概率分别为A、B、C、D、E、F。</p>
<ol>
<li>从最大概率矩形框F开始，分别判断A~E与F的重叠度<a href="/posts/ml-glossary#iou">IOU</a>是否
   大于某个设定的阈值;</li>
<li>假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</li>
<li>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，
   重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</li>
</ol>
<p>就这样一直重复，找到所有被保留下来的矩形框。</p>
<h4>如何调整边界框</h4>
<p>找到边界框中的目标物体后，进一步调整边界框，使其更贴近物体周围。
这里的创新点为，用线性回归代替过去的DPM算法
(依赖图像几何特征训练出物体边界框的方法), 训练出更贴身的边界框</p>
<p>其思路为:</p>
<p>令<span class="math">\(P = (P_x, P_y, P_w, P_h)\)</span> 表示由选择性搜索算法找出，
并被SVM分类器打过分的边界框，简称&ldquo;预测框&rdquo;，
其中<span class="math">\(P_x, P_y\)</span> 表示边界框的中心点坐标，
<span class="math">\(P_w, P_h\)</span> 表示边界框的宽、高；</p>
<p><span class="math">\(G = (G_x, G_y, G_w, G_h)\)</span> 表示训练集边界框标准答案，简称&ldquo;目标框&rdquo;，
括号内的值与预测框一一对应。</p>
<p>欲得到调整后的预测框，即是训练四个函数<span class="math">\(d_x(P),d_y(P),d_w(P),d_h(P)\)</span>，
这几个函数满足：</p>
<div class="math">$$\hat{G}_x = P_wd_x(P)+P_x  \tag{1}$$</div>
<div class="math">$$\hat{G}_y = P_hd_y(P)+P_y  \tag{2}$$</div>
<div class="math">$$\hat{G}_w = P_w exp(d_w(P))  \tag{3}$$</div>
<div class="math">$$\hat{G}_h = P_h exp(d_h(P))  \tag{4}$$</div>
<p>函数<span class="math">\(d_*(P)\)</span>(<span class="math">\(*\)</span>号代表<span class="math">\(x,y,h,w\)</span>中的一个)是候选框对应的
特征图(记为<span class="math">\(\phi_5(P)\)</span>, 下标5表示是第5层池化层输出的特征图)的线性函数，
代表从预测框到调整后的预测框的变换关系，所以有：</p>
<div class="math">$$d_*(P) = w_*^T\phi_5(P)$$</div>
<p>训练的目标函数为岭回归（ridge regression）：
</p>
<div class="math">$$w_* = \underset{\hat{w}_*}{\operatorname{argmin}} 
\sum_i^N(t_*^i-\hat{w}_*\phi_5(P^i))^2+\lambda\lVert\hat{w}_*\rVert^2 \tag{5}$$</div>
<p>(5)式中回归目标 <span class="math">\(t_*\)</span> 分别定义如下：</p>
<div class="math">$$t_x = (G_x-P_x)/P_w$$</div>
<div class="math">$$t_y = (G_y-P_y)/P_h$$</div>
<div class="math">$$t_w = log(G_w/P_w)$$</div>
<div class="math">$$t_h = log(G_h/P_h )$$</div>
<h4>训练配置</h4>
<p>空间特征提取时，学习率设为0.001, 候选框与目标框重叠率大于等于0.5
的设为正例，否则设为负例; batch_size 设为128， 正负例比例为1:3。</p>
<p>SVM分类时，分类数量为物体类别数加一，增加的一个分类表示&ldquo;背景&rdquo;类型。
这里有个特殊设置，即但候选框部分包含物体时，如果区分正例还是负例呢？
论文在这种情况下选择 IoU 小于0.3时，就标记为负例。</p>
<p>回归训练时，其目的是调整边界框大小，很显然当预测框与目标框差得很远时，
回归训练是无效的，所以回归训练只训练靠近目标框的预测框，
即选择 IoU 阈值设为大于等于 0.6 的预测框。</p>
<h4>R-CNN 面临的问题</h4>
<ul>
<li>选择性搜索算法不能 GPU 加速，降低模型速度；</li>
<li>对每张图做2000次前向CNN, 效率过低；</li>
<li>CNN提取图像特征、分类器预测类别、回归模型提取紧身边界框，
  但在算法上它们是分开训练的，训练起来较麻烦。</li>
</ul>
<h3 id="overfeat">OverFeat</h3>
<p>OverFeat 的论文首发于2013年12月份，OverFeat 模型分为前后两部分，
前面部分可叫做特征提取层，后面部分可叫做任务层，
只需要改变网络的最后几层，就可以实现分类、定位、检测等任务。
任务层共享特征提取层的参数，基本采用同一网络结构。</p>
<p>R-CNN 论文的修订版有一小节专门提到OverFeat模型，认为该模型的方法
虽然在预测效果上不如R-CNN, 但做一定改进后，训练速度能9倍于R-CNN。
所以，对于OverFeat，只分析它最值得借鉴的闪光点。</p>
<p align="center">
<img src="/images/overfeat.png"/>
<figcaption>
图2: OverFeat模型分为快速版和精确版, 此为精确版结构
</figcaption>
</p>
<h4>预测时的多尺度分类</h4>
<p>OverFeat在训练时，将每张256x256原图片随机裁剪为221x221的大小, 作为CNN输入。
但预测时，不再是用一张221x221大小的图片作为网络的输入，
而是用任意大小都不相同的图片，也就是所谓的多尺度输入预测，如下表格所示：</p>
<p align="center">
<img src="/images/multi_scale.png" width="90%"/>
<figcaption>
图3: 多尺度方法的空间维度, 列举 6 种尺寸图片作为输入 
</figcaption>
</p>
<p>在AlexNet的文献中，他们预测的方法是输入一张图片256x256，然后进行
multi-view裁剪，也就是从图片的四个角进行裁剪，还有就是一图片的中心进行裁剪，这样可以裁剪到5张
224x224的图片。然后把原图片水平翻转一下，再用同样的方式进行裁剪，又可以裁剪到5张图片。
把这10张图片作为输入，分别进行预测分类，在后在softmax的最后一层，求取个各类的总概率，求取平均值。</p>
<p>这种方法的弊端在于：</p>
<ol>
<li>这种裁剪方式，可能把图片的某些区域都给忽略掉；</li>
<li>裁剪窗口的重叠部分存在冗余计算。</li>
</ol>
<p>要解决的问题是，输入不同尺寸的图片，经过特征提取层后，
会输出不同尺寸的特征图(如图3第3列)，再进行下采样时，如果保证不同尺寸情况下，
信息不丢失呢？ 论文介绍了一种<strong>offset池化</strong>方法，说明如下：</p>
<p align="center">
<img src="/images/offset_poolling_1.png" width="90%"/>
</p>
<p>以一维情况来说明，设x轴上有20个神经元，以poolling size=3做非重叠池化,
而20除以3除不尽，为保证池化信息完备性，将20个神经元分为三组：</p>
<ol>
<li>△=0分组：[1,2,3]，[4,5,6]，&hellip;&hellip;，[16,17,18]；</li>
<li>△=1分组：[2,3,4]，[5,6,7]，&hellip;&hellip;，[17,18,19]；</li>
<li>△=2分组：[3,4,5]，[6,7,8]，&hellip;&hellip;，[18,19,20]；</li>
</ol>
<p>对应图片如下：</p>
<p align="center">
<img src="/images/offset_poolling_2.png"/>
</p>
<p>把上面的△=0、△=1、△=2的三种组合方式的池化结果，分别送入网络的下一层。
这样的话，我们网络在最后输出的时候，就会出现3种预测结果了。</p>
<p>如果是2维图片的话，那么就会有 9 种池化结果, 
最后我们的图片分类输出结果就可以得到9个预测结果(每个类别都可以得到9种概率值), 然后我们对每个类别的9种概率，取其最大值，做为此类别的预测概率值。</p>
<p>用大小为3x3的核、按步幅3做offset池化后，就形成图3第4列的空间维度;
再用大小为5x5的核、按步幅1做卷积后，就形成图3第5列的空间维度(参考图2第7层)。
论文里把这种方法称为全卷积网络，我没把这种结构归类的<a href="/posts/cnn-notes">《卷积备忘录》</a>中,
因为它仅仅有一个固定大小的卷积核，再把输出接上一个<a href="/posts/cnn-notes#ping-jing-juan-ji">瓶颈卷积层</a>,
使最后输出一个 C 维向量, 其中 C 表示分类数量(个人觉得没什么特别)。
而这个C维分类向量的形状就是图3最后一列的样子。
写到这里，多尺度图片的分类预测方法就说完了。</p>
<h4>如何找出候选区域</h4>
<p>在R-CNN论文的修订版本中专门提到，如果对图片的特征提取只做一次，再
用不同尺寸的滑动窗口在提取后的特征图上来选出候选区域，
R-CNN的性能至少提高9倍。所以，用多尺寸滑动窗口选出候选区，相对
于选择性搜索算法在原图上选出很多个候选框，是一大进步。</p>
<h4>如何找出边界框</h4>
<p>即OverFeat的定位任务，把用图片分类学习的特征提取层的参数固定下来，
然后继续训练后面的回归层的参数，网络包含了4个输出，
对应于边界框的上左上角点和右下角点的纵横坐标，
然后损失函数采用欧式距离L2损失函数。</p>
<p>OverFeat 用到的回归方法不如R-CNN, 略过不提。</p>
<h3 id="fcn">FCN</h3>
<h3 id="spp-net">SPP-Net</h3>
<h3 id="fast-r-cnn">Fast R-CNN</h3>
<h3 id="faster-r-cnn">Faster R-CNN</h3>
<h3 id="fpn">FPN</h3>
<h3 id="mask-r-cnn">Mask R-CNN</h3>
<h3 id="cornernet">CornerNet</h3>
<h3 id="pfpn">PFPN</h3>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p>基于图论进行图像分割的算法，把图像中的像素点看做是一个个节点，
像素点之间的不相似度作为边的权重，通过将相似的像素聚合到一起，
产生同一区域(表现为最小生成树)。像素聚合成区域后，判断两个相邻区域是否应该合并，
要检查它们的区域间间距(即所有分别属于两个区域且有边连接的点对中，寻找权重最小的那对,若两个区域内的点没有边相连，则定义间距为正无穷大)和区域内间距(即区域对应最小生成树中权重最大的边的权重值)的值。
如果两个区域的区域间间距明显大于其中任意一个区域的区域内间距，
那么就认为这两个区域之间存在明显的界限（即不能合并）。(freeopen:
只能写个大概，详述篇幅太长)&nbsp;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div><!-- /.entry-content -->

  <div class="comments">
    <!-- <h2>Comments !</h2> -->
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'freeopen';
      var disqus_identifier = 'posts/image-segmentation';
      var disqus_url = 'https://freeopen.github.io/posts/image-segmentation';
      (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//freeopen.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the comments.</noscript>
  </div>
</section>
          </div>

          <footer id="contentinfo" class="footer">
            <div class="footer-inner">
              无节操小广告<a href="https://freeopen.github.io/pages/da-shang.html"> 欢迎打赏 </a>
            </div>
          </footer><!-- /#contentinfo -->
        </div>
      </div>

      <div class="hidden-print hidden-xs hidden-sm hidden-md col-lg" id="post_toc" role="complementary">
        <nav class="sidebar-toc">
<!-- <section id="main_toc"> -->
    <ul class="nav" id="toc"><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#tu-xiang-fen-ge-dao-di-gan-shi-yao" title="图像分割到底干什么">图像分割到底干什么</a></li><li class="nav-item"><a class="nav-link" href="#tu-xiang-fen-ge-de-suan-fa-si-lu" title="图像分割的算法思路">图像分割的算法思路</a></li><li class="nav-item"><a class="nav-link" href="#jing-dian-suan-fa-de-yan-jin" title="经典算法的演进">经典算法的演进</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#r-cnn" title="R-CNN">R-CNN</a></li><li class="nav-item"><a class="nav-link" href="#overfeat" title="OverFeat">OverFeat</a></li><li class="nav-item"><a class="nav-link" href="#fcn" title="FCN">FCN</a></li><li class="nav-item"><a class="nav-link" href="#spp-net" title="SPP-Net">SPP-Net</a></li><li class="nav-item"><a class="nav-link" href="#fast-r-cnn" title="Fast R-CNN">Fast R-CNN</a></li><li class="nav-item"><a class="nav-link" href="#faster-r-cnn" title="Faster R-CNN">Faster R-CNN</a></li><li class="nav-item"><a class="nav-link" href="#fpn" title="FPN">FPN</a></li><li class="nav-item"><a class="nav-link" href="#mask-r-cnn" title="Mask R-CNN">Mask R-CNN</a></li><li class="nav-item"><a class="nav-link" href="#cornernet" title="CornerNet">CornerNet</a></li><li class="nav-item"><a class="nav-link" href="#pfpn" title="PFPN">PFPN</a></li></ul></li></ul></ul>
<!-- </section> -->
<!-- </section> -->
        </nav>
      </div>
    </div>
  </div>


    <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-98793057-1', 'auto');
    ga('send', 'pageview');
    </script>
<script type="text/javascript">
    var disqus_shortname = 'freeopen';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'https://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<script src="https://freeopen.github.io/theme/js/jquery-3.3.1.slim.min.js"></script>
<script src="https://freeopen.github.io/theme/js/popper.min.js"></script>
<script src="https://freeopen.github.io/theme/js/bootstrap.min.js"></script>

</body>
</html>