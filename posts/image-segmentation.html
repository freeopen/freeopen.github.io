<!DOCTYPE html>
<html lang="en">
<head>
          <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta http-equiv="content-type" content="text/html; charset=utf-8">
        <!-- Enable responsiveness on mobile devices-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

        <title>Freeopen - 基于深度学习的图像分割算法思路演进</title>
        <link rel="stylesheet" href="https://freeopen.github.io/theme/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://freeopen.github.io/theme/css/main.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/academicons/css/academicons.min.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/font-awesome/css/font-awesome.min.css" />
        <link rel="stylesheet" href="https://freeopen.github.io/theme/assets/fonts-googleapis/css/googleapi-fonts.css" />

        






</head>

<body id="index" class="theme-myblue" data-spy="scroll" data-target="#post_toc">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 col-lg">
        <nav class="navbar navbar-expand-lg navbar-light sidebar" role="navigation">
          <a class="sidebar-about" href="https://freeopen.github.io/">Freeopen</a>
          <button class="navbar-toggler " type="button" data-toggle="collapse" data-target="#navbarNav">
            <span class="navbar-toggler-icon"></span>
          </button>
          <nav class="collapse navbar-collapse sidebar-nav flex-column" id="navbarNav">

              <a class="nav-link" href="https://freeopen.github.io/pages/about.html">About</a>
              <a class="nav-link" href="https://freeopen.github.io/pages/da-shang.html">打赏</a>

              <a class="nav-link" href="https://freeopen.github.io/category/bian-cheng.html">编程</a>
              <a class="nav-link" href="https://freeopen.github.io/category/ji-qi-xue-xi.html">机器学习</a>
              <a class="nav-link" href="https://freeopen.github.io/category/shou-ce.html">手册</a>
              <a class="nav-link" href="https://freeopen.github.io/category/shu-xue.html">数学</a>
            <div class="sidebar-icons">
              <hr>
                <a href="" title="Atom feed" target="_blank" 
                    style="display: inline; padding: 0px 0px 0px 0; margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;">
                  <i class="fa fa-rss-square"></i>
                </a>
                <a href="https://github.com/freeopen" title="Software I released on Github" 
                      target="_blank" style="display: inline; padding: 0px 0px 0px 0; margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;"><i class="fa fa-github-square"></i> </a>
                <a href="Mailto:freeopen@163.com" title="My email" 
                    target="_blank" style="display: inline; padding: 0px 0px 0px 0;
                    margin: 3px 4px 0 0; white-space: nowrap; font-size:2.5em;"><i
                  class="fa fa-envelope-square"></i> </a>

            </div>
          </nav>

        </nav>

      </div>

      <div class="col-12 col-lg-7" role="main">
        <div id="content" class="content">
          <div class="post">
<section id="content" class="body">
  <header>
    <h1 class="entry-title">基于深度学习的图像分割算法思路演进</h2>
 
  </header>
  <div class="post-info">

    <span>2019-03-05</span>

    <span>| 更新于 2019-03-11</span>

    <span>| By             <a class="url fn" href="https://freeopen.github.io/author/freeopen.html">freeopen</a>
    </span>

    <span>
      | 分类于 <a href="https://freeopen.github.io/category/ji-qi-xue-xi.html">机器学习</a>
    </span>

  </div><!-- /.post-info -->
  <div class="post content">
    <blockquote>
<p>又是一篇不会写完的文章，和前面那篇《卷积备忘录》一样，
会跟进技术发展不断更新。读了一系列论文，有必要再次整理自己的思考了，
就怕笔不勤忘得快。如有理解错误之处，请读者及时来信告知，如果
你精通翻墙术，文章末尾可以留言。（已架设 Disqus 评论系统，
因国内不可名状的原因，翻墙后才能显示。）</p>
<p>边写边发，未完待续</p>
</blockquote>
<h2 id="tu-xiang-fen-ge-dao-di-gan-shi-yao">图像分割到底干什么</h2>
<p>图像分割问题主要流行四类任务，它们分别是目标检测( Object Detection )、语义分割( Semantic Segmentation )、实例分割( Instance Segmentation ) 和全景分割( Panoptic Segmentation )。这四类任务的意义分别为：</p>
<ol>
<li>目标检测：计算机看一张图，用矩形框框出我们感兴趣的目标（定位任务），
   并告诉我们该目标是什么（分类任务）。</li>
<li>语义分割：把整张图像的每个像素赋予一个类别标签。
   不过语义分割的任务是只判断类别，不区分个体。 </li>
<li>实例分割：比目标分割更进一步，对我们感兴趣目标的像素赋予类别标签，
   且对于挨在一起的同种类型的目标，需要区分出个体（即实例），
   比如几个人相互重叠的站在一起，实例分割时要用不同颜色的色块(论文中称为&ldquo;掩膜&rdquo;, mask)覆盖住每个人。</li>
<li>全景分割：是语义分割和实体分割的结合，每个像素都被分为一类，
   如果一种类别里有多个实例，会用不同的颜色进行区分，
   我们可以知道哪个像素属于哪个类中的哪个实例。</li>
</ol>
<p>上面四类任务我把它统称为图像分割任务，它实质上由两个子任务构成，一个是定位任务，一个是分类任务。
其中，目标检测和实例分割只分割出感兴趣区域，不关心其他区域；语义分割和全景分割对图片的所有区域进行分割，
但语义分割不区分连在一起的同类个体。</p>
<h2 id="tu-xiang-fen-ge-de-suan-fa-si-lu">图像分割的算法思路</h2>
<p>如果你同意图像分割问题均由定位任务和分类任务组成，那么上面的四类任务的算法应该能互相借鉴，
并最终会统一在一起（即一个模型能同时干完所有的活 -- 四类任务）。</p>
<p>我先按照人类直觉来思考图像分割问题, 步骤为：</p>
<ol>
<li>看一眼图，找到哪些位置有东西；</li>
<li>再看看有东西的位置都是些什么东西；</li>
<li>把我们关心的那类东西标记出来。</li>
</ol>
<p>再按计算机直觉来细化上面的步骤。计算机看图实质上看到的是密密麻麻的像素点，这些点在计算机眼里没有大小，没有颜色，只是一堆数字而已。计算机识别一张图片是什么内容，根据一块区域的一堆数字分布规律来做出判断，
所以，机器判断一张图哪些位置有东西时，只能用各种大小的矩形框从图中抽取出像素点，再来判断这些像素点们到底是不是东西，进而再判断它是什么东西。故根据计算机看图的特点，算法进一步描述为：</p>
<ol>
<li>扫描图片，找到一堆可能有东西的候选区域；</li>
<li>取得候选区域对应的视觉特征，对其进行识别，判断有东西还是没东西；</li>
<li>如果有东西，判断它是什么东西，并标记出它的位置。</li>
</ol>
<p><strong>算法面临的挑战：</strong></p>
<ul>
<li>
<p>在第 1 步中，由于目标的大小、轮廓千变万化，要找出候选区域，有多种方案。</p>
<ul>
<li>方法1: 把图片均匀分成细小的网格，再根据网格的特征相似度拼接网格，
    拼接后的网格即为候选区域。</li>
<li>方法2: 用不同大小，不同长宽比的矩形框作为滑动窗口，
    在图片上从左到右、由上至下滑动，被框住的区域即为候选区域。</li>
<li>方法3: 利用图片的几何特征(比如物体的轮廓位置的颜色梯度会较大)，用算法找出候选区域。</li>
</ul>
<p>对于方法1, 优点是候选区域不会重叠，难点在于网格的拼接策略。</p>
<p>对于方法2，会产生相互重叠的候选区域，
且候选区域不一定贴合目标的轮廓，需要有办法调整候选区域大小，
让它最接近目标物体的轮廓。</p>
<p>对于方法3, 只要效率高，个人认为是最优策略，但目前这类算法的性能还需要提高。</p>
<p>思考：方法 1 和方法 2 本质上都可以看成滑动窗口扫描图片，找出候选区。
但，如果我们用卷积网络抽取图片特征，则较低的卷积层反映图片较细节
的特征(如纹理、边缘
等)，较高的卷积层反映图片较宏观的特征（如物体的轮廓等），所以
候选区域在较高的卷积层输出的特征图上取得，
理论上效果会比直接从原图上取得的要好, 且所需参数也较少。</p>
</li>
<li>
<p>在第 2 步中，相当于一个二分类问题，判断候选区域框住的东西是
    前景还是背景。
    面临的难题是候选区域输出的特征图大小不一，
    这些特征图在进行分类前，须调成固定尺寸的向量。</p>
</li>
<li>
<p>在第 3 步中，有两个分支，一个判断目标类型的分类器和一个标记目标位置
    的定位器。难点在于定位任务，因为这时看到的特征图尺寸小于原图，
    需要找到一个办法把特征图的坐标还原成原图的坐标。</p>
</li>
</ul>
<p>上述的这些难题被解决的越好，那么图像分割的效果也就越好。
下面看看研究者们都有些什么巧思，来跨越这些鸿沟。</p>
<h2 id="jing-dian-suan-fa-de-yan-jin">经典算法的演进</h2>
<h2 id="r-cnn">R-CNN</h2>
<p>R-CNN( Regions with Convolutional Neural Network Features )
模型可称为深度学习用于目标检测的开山之作，来自 
Ross Girshick 于2013年11月发表的一篇论文，
论文名为《Rich feature hierarchies for Accurate Object Detection and Segmentation》。</p>
<p>模型结构如下：</p>
<p align="center">
<img src="/images/r-cnn.png" width="75%"/>
<figcaption>
R-CNN 概览：输入一张图片，定位出2000个物体候选框，
然后采用CNN提取每个候选框中图片的特征向量，
特征向量的维度为4096维，接着采用svm算法对各个候选框中的物体进行分类识别。
</figcaption>
</p>
<p>上图仅为模型概要，算法实质上分为三步：</p>
<ol>
<li>找出候选区域(regions proposals), 将其作为边界框(Bounding Box)；</li>
<li>用预训练好的卷积网络(如 AlexNet)提取特征向量, 并用 SVM
   算法判断边界框中对应的物体类型；</li>
<li>对已分类物体的边界框进行线性回归，输出其对应的贴近物体轮廓的紧身边界框(tighter bounding boxes)。</li>
</ol>
<h3 id="ru-he-zhao-chu-hou-xuan-qu-yu">如何找出候选区域</h3>
<p>R-CNN 用选择性搜索( Selective Search )算法从原始图片上找出一堆候选框，
并选出得分最高的2000个。</p>
<p><strong>选择性搜索( Selective Search )算法</strong></p>
<ol>
<li>生成原始的候选框集合 R（利用<strong>felzenszwalb算法</strong><sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup>）;</li>
<li>计算候选框集 R 里每个相邻区域的相似度 S={s1,s2,&hellip;},
   这里考虑了四种类型的相似度，包括纹理、颜色、尺寸及交叠;</li>
<li>找出相似度最高的两个区域，将其合并为新集，添加进 R&nbsp;;</li>
<li>从 S 中移除所有与第 3 步中有关的子集;&nbsp;</li>
<li>计算新集与所有子集的相似度;&nbsp;</li>
<li>跳至第三步，不断循环，合并，直至 S 为空（到不能再合并时为止）。</li>
</ol>
<h3 id="ru-he-cong-bu-tong-da-xiao-de-hou-xuan-kuang-ti-qu-kong-jian-te-zheng">如何从不同大小的候选框提取空间特征</h3>
<p>直接缩放成固定大小的正方形（论文采用227&times;227）, 送进
卷积网络( 修改版的 AlexNet )抽取空间特征。</p>
<h3 id="ru-he-xuan-ze-zui-hao-de-bian-jie-kuang">如何选择最好的边界框</h3>
<p>比如定位一个物体，算法可能会找出一堆边界框，我们需要选出最好的边界框。
这里用到了<strong>非极大值抑制(Non-maximum suppression, NMS)</strong>算法，该算法
其本质是搜索局部极大值，抑制非极大值元素, 具体如下：</p>
<p>假设有 6 个矩形框都是目标车辆的边界框，用 SVM 分类器对它们打分并排序，
从小到大属于车辆的概率分别为A、B、C、D、E、F。</p>
<ol>
<li>从最大概率矩形框F开始，分别判断A~E与F的重叠度<a href="/posts/ml-glossary#iou">IOU</a>是否
   大于某个设定的阈值;</li>
<li>假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</li>
<li>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，
   重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</li>
</ol>
<p>就这样一直重复，找到所有被保留下来的矩形框。</p>
<h3 id="ru-he-diao-zheng-bian-jie-kuang">如何调整边界框</h3>
<p>找到边界框中的目标物体后，进一步调整边界框，使其更贴近物体周围。
这里的创新点为，用线性回归代替过去的DPM算法
(依赖图像几何特征训练出物体边界框的方法), 训练出更贴身的边界框</p>
<p>其思路为:</p>
<p>令<span class="math">\(P = (P_x, P_y, P_w, P_h)\)</span> 表示由选择性搜索算法找出，
并被SVM分类器打过分的边界框，简称&ldquo;预测框&rdquo;，
其中<span class="math">\(P_x, P_y\)</span> 表示边界框的中心点坐标，
<span class="math">\(P_w, P_h\)</span> 表示边界框的宽、高；</p>
<p><span class="math">\(G = (G_x, G_y, G_w, G_h)\)</span> 表示训练集边界框标准答案，简称&ldquo;目标框&rdquo;，
括号内的值与预测框一一对应。</p>
<p>欲得到调整后的预测框，即是训练四个函数<span class="math">\(d_x(P),d_y(P),d_w(P),d_h(P)\)</span>，
这几个函数满足：</p>
<div class="math">$$\hat{G}_x = P_wd_x(P)+P_x  \tag{1}$$</div>
<div class="math">$$\hat{G}_y = P_hd_y(P)+P_y  \tag{2}$$</div>
<div class="math">$$\hat{G}_w = P_w exp(d_w(P))  \tag{3}$$</div>
<div class="math">$$\hat{G}_h = P_h exp(d_h(P))  \tag{4}$$</div>
<p>函数<span class="math">\(d_*(P)\)</span>(<span class="math">\(*\)</span>号代表<span class="math">\(x,y,h,w\)</span>中的一个)是候选框对应的
特征图(记为<span class="math">\(\phi_5(P)\)</span>, 下标5表示是第5层池化层输出的特征图)的线性函数，
代表从预测框到调整后的预测框的变换关系，所以有：</p>
<div class="math">$$d_*(P) = w_*^T\phi_5(P)$$</div>
<p>训练的目标函数为岭回归（ridge regression）：
</p>
<div class="math">$$w_* = \underset{\hat{w}_*}{\operatorname{argmin}} 
\sum_i^N(t_*^i-\hat{w}_*\phi_5(P^i))^2+\lambda\lVert\hat{w}_*\rVert^2 \tag{5}$$</div>
<p>(5)式中回归目标 <span class="math">\(t_*\)</span> 分别定义如下：</p>
<div class="math">$$t_x = (G_x-P_x)/P_w$$</div>
<div class="math">$$t_y = (G_y-P_y)/P_h$$</div>
<div class="math">$$t_w = log(G_w/P_w)$$</div>
<div class="math">$$t_h = log(G_h/P_h )$$</div>
<h3 id="xun-lian-pei-zhi">训练配置</h3>
<p>空间特征提取时，学习率设为0.001, 候选框与目标框重叠率大于等于0.5
的设为正例，否则设为负例; batch_size 设为128， 正负例比例为1:3。</p>
<p>SVM分类时，分类数量为物体类别数加一，增加的一个分类表示&ldquo;背景&rdquo;类型。
这里有个特殊设置，即但候选框部分包含物体时，如果区分正例还是负例呢？
论文在这种情况下选择 IoU 小于0.3时，就标记为负例。</p>
<p>回归训练时，其目的是调整边界框大小，很显然当预测框与目标框差得很远时，
回归训练是无效的，所以回归训练只训练靠近目标框的预测框，
即选择 IoU 阈值设为大于等于 0.6 的预测框。</p>
<h3 id="r-cnn-de-que-xian">R-CNN 的缺陷</h3>
<ul>
<li>选择性搜索算法不能 GPU 加速，降低模型速度；</li>
<li>对每张图做2000次前向CNN, 效率过低；</li>
<li>把提取的图片特征写进磁盘，既废空间又拖慢速度；</li>
<li>CNN提取图像特征、分类器预测类别、回归模型提取紧身边界框，
  但在算法上它们是分开训练的，训练起来较麻烦。</li>
</ul>
<h2 id="overfeat_1">OverFeat</h2>
<p>OverFeat 的论文首发于2013年12月份，OverFeat 模型分为前后两部分，
前面部分可叫做特征提取层，后面部分可叫做任务层，
只需要改变网络的最后几层，就可以实现分类、定位、检测等任务。
任务层共享特征提取层的参数，基本采用同一网络结构。</p>
<p>R-CNN 论文的修订版有一小节专门提到OverFeat模型，认为该模型的方法
虽然在预测效果上不如R-CNN, 但做一定改进后，训练速度能9倍于R-CNN。
所以，对于OverFeat，只分析它最值得借鉴的闪光点。</p>
<p align="center">
<img src="/images/overfeat.png"/>
<figcaption>
图2: OverFeat模型分为快速版和精确版, 此为精确版结构
</figcaption>
</p>
<h3 id="yu-ce-shi-de-duo-chi-du-fen-lei">预测时的多尺度分类</h3>
<p>OverFeat在训练时，将每张256x256原图片随机裁剪为221x221的大小, 作为CNN输入。
但预测时，不再是用一张221x221大小的图片作为网络的输入，
而是用任意大小都不相同的图片，也就是所谓的多尺度输入预测，如下表格所示：</p>
<p align="center">
<img src="/images/multi_scale.png" width="90%"/>
<figcaption>
图3: 多尺度方法的空间维度, 列举 6 种尺寸图片作为输入 
</figcaption>
</p>
<p>在AlexNet的文献中，他们预测的方法是输入一张图片256x256，然后进行
multi-view裁剪，也就是从图片的四个角进行裁剪，还有就是一图片的中心进行裁剪，这样可以裁剪到5张
224x224的图片。然后把原图片水平翻转一下，再用同样的方式进行裁剪，又可以裁剪到5张图片。
把这10张图片作为输入，分别进行预测分类，在后在softmax的最后一层，求取个各类的总概率，求取平均值。</p>
<p>这种方法的弊端在于：</p>
<ol>
<li>这种裁剪方式，可能把图片的某些区域都给忽略掉；</li>
<li>裁剪窗口的重叠部分存在冗余计算。</li>
</ol>
<p>要解决的问题是，输入不同尺寸的图片，经过特征提取层后，
会输出不同尺寸的特征图(如图3第3列)，再进行下采样时，如果保证不同尺寸情况下，
信息不丢失呢？ 论文介绍了一种<strong>offset池化</strong>方法，说明如下：</p>
<p align="center">
<img src="/images/offset_poolling_1.png" width="70%"/>
</p>
<p>以一维情况来说明，设x轴上有20个神经元，以poolling size=3做非重叠池化,
而20除以3除不尽，为保证池化信息完备性，将20个神经元分为三组：</p>
<ol>
<li>△=0分组：[1,2,3]，[4,5,6]，&hellip;&hellip;，[16,17,18]；</li>
<li>△=1分组：[2,3,4]，[5,6,7]，&hellip;&hellip;，[17,18,19]；</li>
<li>△=2分组：[3,4,5]，[6,7,8]，&hellip;&hellip;，[18,19,20]；</li>
</ol>
<p>对应图片如下：</p>
<p align="center">
<img src="/images/offset_poolling_2.png"/>
</p>
<p>把上面的△=0、△=1、△=2的三种组合方式的池化结果，分别送入网络的下一层。
这样的话，我们网络在最后输出的时候，就会出现3种预测结果了。</p>
<p>如果是2维图片的话，那么就会有 9 种池化结果, 
最后我们的图片分类输出结果就可以得到9个预测结果(每个类别都可以得到9种概率值), 然后我们对每个类别的9种概率，取其最大值，做为此类别的预测概率值。</p>
<p>用大小为3x3的滑动窗口、按步幅3做offset池化后，就形成图3第4列的空间维度;
再用大小为5x5的核、按步幅1做卷积后，就形成图3第5列的空间维度(参考图2第7层)。
论文里把这种方法称为全卷积网络，我没把这种结构归类到<a href="/posts/cnn-notes">《卷积备忘录》</a>中,
因为它仅仅有一个固定大小的卷积核，再把输出接上一个<a href="/posts/cnn-notes#ping-jing-juan-ji">瓶颈卷积层</a>,
使最后输出一个 C 维向量, 其中 C 表示分类数量(个人觉得没什么特别)。
而这个C维分类向量的形状就是图3最后一列的样子。
写到这里，多尺度图片的分类预测方法就说完了。</p>
<h3 id="ru-he-zhao-chu-hou-xuan-qu-yu_1">如何找出候选区域</h3>
<p>在R-CNN论文的修订版本中, 有一节专门提到受OverFeat模型的启发，
如果对图片的特征提取只做一次，再
用不同尺寸的滑动窗口在提取后的特征图上来选出候选区域，
R-CNN的性能至少提高9倍。所以，用多尺寸滑动窗口选出候选区，相对
于选择性搜索算法在原图上选出很多个候选框，是一大进步。</p>
<p>多说一句，滑动窗口方式为什么不在原图上圈出候选区呢？没有必要，
因为卷积网络结构一旦确定，对于图片来说就有固定的缩放比，而对候选区而言，
用来分析的是抽取出来的特征图，所以在特征图上圈出候选区域，简单直接靠谱。</p>
<h3 id="ru-he-zhao-chu-bian-jie-kuang">如何找出边界框</h3>
<p>即OverFeat的定位任务，把用图片分类学习的特征提取层的参数固定下来，
然后继续训练后面的回归层的参数，网络包含了4个输出，
对应于边界框的上左上角点和右下角点的纵横坐标，
然后损失函数采用欧式距离L2损失函数。</p>
<p>OverFeat 用到的回归方法不如R-CNN, 略过不提。</p>
<h2 id="fcn_1">FCN</h2>
<h2 id="spp-net">SPP-Net</h2>
<p>SPP-Net( Spatial Pyramid Pooling in Deep Convolutional Networks )模型算是 
R-CNN 的进化版, 论文第一作者何凯明，首发于2014年6月，
当时他还在微软中国研究院。</p>
<p>该模型对 R-CNN 进行了针对性的改进。首先，R-CNN中对候选区图片统一缩放为
固定尺寸进行训练，这样对于图片内的物体来说，会遭到切割和缩放，将导致图片
信息的丢失和变形, 限制了识别精度。</p>
<p>那么，为什么要固定输入图片的大小呢？</p>
<p>卷积层的参数和输入大小无关，它仅仅是一个卷积核在图像上滑动，
不管输入图像多大都没关系，只是对不同大小的图片卷积出不同大小的特征图，</p>
<p>池化对图片大小会有要求吗？比如池化大小为2x2, 输入一张30x40的图片，
那么经过池化后可以得到15x20的图片; 输入一张53x22大小的图片，经过池化后，
可以得到26x11大小的图片。因此池化这一步也没对图片大小有要求。</p>
<p>但是全连接层的参数就和输入图像大小有关，因为它要把输入的所有像素点连接起来,需要指定输入层神经元个数和输出层神经元个数，所以需要规定输入的特征图的大小。
因此，固定长度的约束仅限于全连接层。</p>
<p>SPP-Net 的思路是在特征提取层和全连接层之间，增加一个层，
使得从特征提取层输出的特征图, 经过这层后，得到一个固定维度的表达。
该模型的结构如下：</p>
<p align="center">
<img src="/images/spp-net.png" width="80%"/>
<figcaption>
图4:  SPP-Net 模型结构, 中间层称为空间金字塔池化层
</figcaption>
</p>
<h3 id="kong-jian-jin-zi-ta-chi-hua-ceng">空间金字塔池化层</h3>
<p>该池化层的作用为把不同尺寸的卷积网络最后一层输出变成固定维度的特征表达。
图4中黑色图片代表卷积之后的特征图, 它被4x4，2x2，1x1
大小的网格划分，每个块提取出一个特征, 得到 16 + 4 + 1 = 21 个的特征块,
把它们拼接在一起就得到长度为21，深度为256维的空间特征表达(如图4所示)。</p>
<p>空间金字塔池化的思想是不固定滑动窗口的大小，而根据输入特征图的尺寸计算出
滑动窗口尺寸，令输入特征图的宽高分别为 w 和 h, 对于 nxn 的网格而言，滑动窗口的宽高分别为
<span class="math">\(\lceil w/n \rceil\)</span> 和 <span class="math">\(\lceil h/n \rceil\)</span>(<span class="math">\(\lceil \cdot \rceil\)</span>
表示向上取整，不足部分用 pad 补齐), 
滑动步幅分别为<span class="math">\(\lfloor w/n \rfloor\)</span>和<span class="math">\(\lfloor h/n \rfloor\)</span>(<span class="math">\(\lfloor \cdot
\rfloor\)</span> 表示向下取整)。</p>
<h3 id="te-zheng-ti-qu-jia-su">特征提取加速</h3>
<p>SPP-Net 相对于 R-CNN 来说，特征提取的速度有了质的提高。具体操作如下：</p>
<p>把整张待检测的图片，输入卷积网络中，进行一次性特征提取，得到
特征图， 然后在特征图中找到各个候选框的区域，
再对各个候选框采用空间金字塔池化，提取出固定长度的特征向量。</p>
<p>而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，
因为R-CNN就相当于遍历一个CNN两千次，而SPP-Net只需要遍历1次,
光这一个改进，就有百倍加速的效果。</p>
<h3 id="ru-he-zai-cnnzui-hou-yi-ceng-te-zheng-tu-zhong-zhao-dao-yuan-shi-tu-pian-zhong-hou-xuan-kuang-de-dui-ying-qu-yu-?">如何在CNN最后一层特征图中找到原始图片中候选框的对应区域？</h3>
<p>办法只有一条，算出来，前面提过，一旦卷积网络结构选定，从输入端到输出端的
缩小比例就能算出来，影响这个比例的包括每层的卷积方式和池化方式。</p>
<p>论文提出，为简化计算，在卷积时，填充<span class="math">\(\lfloor p/2 \rfloor\)</span>,
其中<span class="math">\(p\)</span>表示卷积核的尺寸， 
令<span class="math">\(S\)</span>表示卷积层的缩小比例，<span class="math">\((x,y)\)</span>表示原图片上候选框的中心坐标, 
则与特征图上对应候选框的中心坐标<span class="math">\(( x^{'}, y^{'})\)</span>之间的关系为:  </p>
<div class="math">$$(x,y)=(S x^{'},S y^{'})$$</div>
<p>给定原图片上候选框左上角坐标<span class="math">\((x_0, y_0)\)</span>和右下角坐标<span class="math">\((x_1, y_1)\)</span>，
对应的特征图坐标分别为：</p>
<div class="math">$$x_0^&prime; = \lfloor x_0/S \rfloor + 1$$</div>
<div class="math">$$y_0^&prime; = \lfloor y_0/S \rfloor + 1$$</div>
<div class="math">$$x_1^&prime; = \lceil x_1/S \rceil - 1$$</div>
<div class="math">$$y_1^&prime; = \lceil y_1/S \rceil - 1$$</div>
<h3 id="xiao-jie">小结</h3>
<p>SPP-Net 相对于 R-CNN 最大改进有两点：</p>
<ol>
<li>由于找到图片坐标到特征图坐标的换算关系，
   故对图片特征提取只做一次，也能取出候选框对应的特征图；</li>
<li>采用空间金字塔池化方法，把不同尺寸的候选框特征图输出为固定大小的特征表达</li>
</ol>
<h2 id="fast-r-cnn_1">Fast R-CNN</h2>
<p>2015年4月，R-CNN 的第一作者 Ross Girshick 又发了一篇论文《Fast R-CNN》，论文开篇
就列举 R-CNN 各种明显缺点，同时也提出SPP-Net存在的两个缺陷：</p>
<ol>
<li>和R-CNN一样，也把提取的特征写进磁盘；</li>
<li>边界框微调算法不能更新空间金字塔池化层前的卷积网络，这限制了目标定位的准确性。</li>
</ol>
<p>为什么SPP-Net不能更新空间金字塔池化层前的卷积网络呢？</p>
<p>准确说，不是不能更新，而是效率低下。</p>
<ol>
<li>因为SPP-Net在训练时，假设batch_size=128,
   就是从128张不同图片中随机抽样一个候选区域，所以对于边界框微调算法来说，
   不同图像对应不同特征图，它们之间的参数不具备共享特性，所以梯度反向传播时
   的参数更新不具有相关性，这是效率低下的原因之一。</li>
<li>空间金字塔池化方法一般至少有三层，常常最高层只一个网格，第二层四个网格，
   这将导致这些网格对应的感受野过大，从而使反向传播时涉及的参数量也过多，
   这也将降低反向传播的效率。</li>
</ol>
<p>作者针对这些问题改进后，训练速度提高约 64 倍。(详见<a href="#xun-lian-wei-diao">训练微调</a>)</p>
<p>然后在 R-CNN 和 SPP-Net 基础上提出了新的模型 Fast R-CNN。</p>
<p align="center">
<img src="/images/fast.r-cnn.png" width="80%"/>
<figcaption>
图5:  Fast R-CNN 结构. 
一个图像和多个感兴趣区域(RoI, regions of interest)
输入到全卷积网络中。
每个 RoI 被池化为固定大小的特征图，然后通过全连接层(FC)映射到特征向量。
网络对于每个RoI有两个输出向量：一个为Softmax的分类概率, 
一个为每类边界框的回归偏移量。该架构采用多任务损失函数端到端训练的。
</figcaption>
</p>
<p>Fast R-CNN 网络将整个图像和一组候选框作为输入。
候选框仍采用选择性搜索算法找出。
网络首先使用几个卷积层和最大池化层来处理整个图像，以产生卷积特征图。
然后，对于每个候选框，RoI 池化层从特征图中提取固定长度的特征向量。
每个特征向量被送入一系列全连接层中，最终分支成两个同级输出层 ：
一个输出K个类别加上1个背景类别的Softmax概率估计，
另一个为K个类别的每一个类别输出四个实数值。
每组4个值对K类之一的精细边界框位置进行编码。</p>
<h3 id="gan-xing-qu-qu-yu-roichi-hua">感兴趣区域(RoI)池化</h3>
<p>这里的感兴趣区域(RoI)和前面提到的候选区域(regions proposals)是一回事， 
它是一个的矩形窗口, 每个RoI由其左上角坐标及其高度和宽度的四元组(r,c,h,w)定义。
RoI 池化的思路来自空间金字塔池化，只取空间金字塔中的一层， 
把一个感兴趣区域分为 H&times;W 个网格， 每个网格大小约为h/H &times; w/W，
然后对网格最大池化，从而输出固定大小的向量。</p>
<h3 id="xun-lian-wei-diao">训练微调</h3>
<p>对边界框进行回归训练时，选择少量图片多个候选区域。
假设每批仍然为128，可同时选择2个不同图片，每个图片选出64个候选框，
这时，能较好共享参数，反向传播时具备较好相关性。
另外，RoI池化也使感受野范围变小，使得需要更新的参数也变少，从而全部提升训练速度。</p>
<h3 id="duo-ren-wu-sun-shi--multi-task-loss">多任务损失 (Multi-task loss)</h3>
<p>Fast R-CNN 把分类任务和边界框的回归任务分别对应的两个损失函数合二为一，
让它们共享参数，一起训练。这个调整很小，但训练起来更方便，且共同训练还
起到互相促进的作用。</p>
<h2 id="faster-r-cnn_1">Faster R-CNN</h2>
<p>Faster R-CNN 的论文发表于2015年6月，相对于Fast
R-CNN，这个版本是真正的端到端模型。过去的版本，图片上候选区域都是用
选择性搜索算法(selective search)、或者边缘框算法(edge bbox)来得到的，
这个步骤和模型训练是分阶段的，所以不算真正的端到端版本。
从Faster R-CNN 开始，首次使用神经网络--候选区域网络(RPN, 
Regions Proposals Networks)来提取这些候选框。</p>
<p align="center">
<img src="/images/faster.r-cnn.png" width="60%"/>
<figcaption>
图6:  Faster R-CNN 是用于目标检测的单个统一网络，
而 RPN 模块相当于这个网络的"注意力机制"。
</figcaption>
</p>
<p>Faster R-CNN 主要包括4个关键模块，
特征提取网络、生成候选框(ROI)、候选框(ROI)分类、候选框(ROI)回归。</p>
<ul>
<li>
<p>特征提取网络：它用来从大量的图片中提取出一些不同目标的重要特征，
  通常由conv+relu+pool层构成，常用一些预训练好的网络（VGG、Inception、Resnet等），
  获得的结果叫做特征图；</p>
</li>
<li>
<p>生成候选框：在获得的特征图的每一个点上做多个候选框；</p>
</li>
<li>
<p>候选框分类：在 RPN 阶段(<a href="#faster-r-cnn-xun-lian-ce-lue">图8</a>第1步)，用来区分前景（与标定边界框的重叠区域大于0.5）
  和背景（不与任何标定边界框重叠或者其重叠区域小于0.1）；
  在 Fast R-CNN 阶段(<a href="#faster-r-cnn-xun-lian-ce-lue">图8</a>第4步)，用于区分不同种类的目标（猫、狗、人等）；</p>
</li>
</ul>
<p>-&nbsp;候选框回归：在 RPN 阶段(<a href="#faster-r-cnn-xun-lian-ce-lue">图8</a>第1步)，进行初步调整；
  在Fast R-CNN 阶段(<a href="#faster-r-cnn-xun-lian-ce-lue">图8</a>第4步)进行精确调整；</p>
<p>其整体流程如下：</p>
<ul>
<li>首先对输入的图片进行裁剪操作，并将裁剪后的图片送入预训练好的分类网络中获取该图像对应的特征图；</li>
<li>然后在特征图上的每一个锚点上取9个候选框（3个不同尺度，3个不同长宽比），
  并根据相应的比例将其映射到原始图像中；</li>
<li>接着将这些候选框输入到 RPN 网络中，RPN 网络对这些候选框进行分类
 （即确定这些候选框是前景还是背景）, 同时对其进行初步回归
 （即计算这些前景候选框与标定边界框之间的偏差值，包括&Delta;x、&Delta;y、&Delta;w、&Delta;h）,
  然后做NMS（非极大值抑制，即根据分类的得分对这些候选框进行排序，
  然后选择其中的前N个候选框）；</li>
<li>接着对这些不同大小的候选框进行ROI Pooling操作
 （即将其映射为特定大小的网格, 文中是7x7），输出固定大小的特征图；</li>
<li>最后将其输入简单的检测网络中，然后利用1x1的卷积进行分类
 （区分不同的类别，N+1类，
  多余的一类是背景，用于删除不准确的候选框），
  同时进行边界框回归，输出更精准的一个边界框集合。&nbsp;</li>
</ul>
<h3 id="mao-dian-de-gai-nian">锚点的概念</h3>
<p>即特征图上的最小单位点，比如原始图像的大小是256x256,
特征提取网络中含有 4 个池化层，
然后最终获得的特征图的大小为 256/16 x 256/16，
即获得一个 16x16 的特征图，该图中的最小单位即是锚点，
由于特征图和原始图像之间存在比例关系，
在特征图上面密集的点对应到原始图像上面就有16个像素的间隔。</p>
<p>一个锚点对应一套候选框，它们是在原图上以锚点为中心，
根据不同尺度(128、256、512), 不同长宽比(1:1, 1:2: 2:1)产生
的9个候选框。比如16x16的特征图，将产生16x16x9=2304个候选框。</p>
<p>一个候选框表达为(x,y,w,h), 对应原始图片上的锚点横纵坐标和宽高。</p>
<h3 id="zheng-fu-yang-ben-de-hua-fen">正负样本的划分</h3>
<p>上面那些候选框，在训练和测试时将被进一步筛选。规则如下：</p>
<ol>
<li>与每个标定边界框(ground true box)的重叠率(IoU)最高锚点候选框，记为正样本；</li>
<li>与某个标定边界框重叠率(IoU)大于0.7的锚点候选框，记为正样本。</li>
</ol>
<p>注意，一个标定边界框可以为多个锚点分配正样本，且通常第2个条件就足以确定正样本，
但在少数情况下须用第1个条件来确定正样本。</p>
<ol>
<li>与任意一个标定边界框的重叠率(IoU)都小于0.3的，记为负样本。</li>
<li>既不是正样本，也不是负样本的锚点候选框，对训练目标没有贡献，抛弃不用。</li>
</ol>
<p>在训练阶段，如果锚点候选框超过了原始图片的边界，则忽略这个候选框;
在测试阶段，如果锚点候选框超过了原始图片的边界, 则把候选框裁剪到图片边界。</p>
<h3 id="rpn-wang-luo-jie-gou">RPN 网络结构</h3>
<p align="center">
<img src="/images/rpn.png" width="70%"/>
<figcaption>
图7:  RPN 网络结构 
</figcaption>
</p>
<ul>
<li>第一层，3x3的卷积核，输出特征图的数量为256；</li>
<li>第二层，有两个分支，每个分支都用1x1的卷积核，
  第一个分支过滤器的shape=(1,1,256,9x2), 
  得到的特征图shape=(h,w,9x2)；第二个分支过滤器的
  shape=(1,1,256,9x4), 得到特征图的shape=(h,w,9x4)。
  这样特征图的每个像素点就有一个9x6维向量，对应到原始图片的9个
  候选框，每个候选框有一个6维向量，前两个维度用于判断候选框中
  是否有物体，后四个维度用于判断候选框中物体的坐标。</li>
</ul>
<h3 id="faster-r-cnn-xun-lian-ce-lue">Faster R-CNN 训练策略</h3>
<p align="center">
<img src="/images/faster-rcnn.training.jpeg" width="90%"/>
<figcaption>
图8:  Faster R-CNN 训练策略
</figcaption>
</p>
<ul>
<li>用ImageNet模型初始化，独立训练一个RPN网络；</li>
<li>仍然用ImageNet模型初始化，但是使用上一步 RPN 网络产生的候选框作为输入，
  训练一个Fast-RCNN网络，至此，两个网络每一层的参数完全不共享；</li>
<li>使用第二步的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、
  Fast-RCNN共享的那些卷积层的learning rate设置为0，也就是不更新，
  仅仅更新RPN特有的那些网络层，重新训练，此时，两个网络已经共享了所有公共的卷积层；</li>
<li>仍然固定共享的那些网络层，把Fast-RCNN的非共享网络层也加入进来，形成一个统一网络，继续训练，
  微调 Fast-RCNN 非共享网络层，此时，该网络已经实现我们设想的目标，
  即网络内部预测边界框并实现检测的功能。</li>
</ul>
<p>即进行交替训练，迭代2次的原因是作者发现多次的迭代并没有显著的改善性能。</p>
<h3 id="que-xian-:roichi-hua-de-er-ci-liang-hua-pian-chai-wen-ti">缺陷：ROI池化的二次量化偏差问题</h3>
<p>由于候选框的位置由模型回归得到，一般来说是浮点数，而池化后的特征图要求尺度固定，
因此感兴趣区域(ROI)池化这个操作存在两次数据量化的过程。</p>
<ol>
<li>将候选框边界量化为整数点坐标值；</li>
<li>将量化后的边界区域平均分割成 HxW 个单元，对每个单元的边界进行量化。</li>
</ol>
<p>事实上，经过上面的两次量化操作，此时的ROI已经和最开始的ROI之间存在一定的偏差，
这个偏差会影响检测的精确度。</p>
<p>举例说明：输入一张800x800的图片，图片中有一个665x665的边界框。
图片经过特征提取网络之后，整个图片的特征图变为800/32 x 800/32, 即25x25，但是665/32=20.87，带有小数，
ROI Pooling直接将它量化为20。在这里引入了一次偏差。
由于最终的特征映射的大小为7x7，即需要将20x20的区域映射成7x7,
矩形区域的边长为2.86，又一次将其量化为2。这里再次引入了一次量化误差。
经过这两次的量化，候选ROI已经出现了严重的偏差。
更重要的是，在特征图上差0.1个像素，对应到原图上就是3.2个像素。</p>
<h2 id="fpn_1">FPN</h2>
<p>R-CNN系列是在最后一层特征图上进行特征提取，从而进行目标识别的。
对于卷积神经网络而言，不同深度对应着不同层次的语义特征，浅层网络分辨率高，
学的更多是细节特征，深层网络分辨率低，学的更多是语义特征。
因此这样做的弊端在于，顶层(即深层)特征中忽略了小物体的一些信息，
因此只根据顶层特征进行目标识别，不能完整地反映小目标物体的信息。
如果可以结合多层级的特征，就可以大大提高多尺度检测的准确性。    </p>
<h3 id="fpnte-zheng-jin-zi-ta-wang-luo-jie-gou">FPN(特征金字塔网络)结构</h3>
<p align="center">
<img src="/images/fpn.png" width="60%"/>
<figcaption>
图9:  特征金字塔网络
</figcaption>
</p>
<p>如图9结构分为三部分：</p>
<ol>
<li>自下而上的卷积神经网络（上图左）;</li>
<li>自上而下过程（上图右）;</li>
<li>和特征与特征之间的侧边连接。</li>
</ol>
<p>自上而下的过程采用上采样进行。上采样几乎都是采用内插值方法，
即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素，从而扩大原图像的大小。
通过对特征图进行上采样，使得上采样后的特征图具有和下一层的特征图相同的大小。</p>
<p align="center">
<img src="/images/fpn_add.png" width="60%"/>
<figcaption>
图10: 横向连接和自上而下的特征融合
</figcaption>
</p>
<p>侧边之间的横向连接的过程在如图10所示。根本上来说，
是将上采样的结果和自下而上生成的特征图进行融合。
我们将卷积神经网络中生成的对应层的特征图进行1&times;1的卷积操作，
将之与经过上采样的特征图融合，得到一个新的特征图，
这个特征图融合了不同层的特征，具有更丰富的信息。 
这里1&times;1的卷积操作目的是改变通道数，要求和后一层的通道数相同。
在融合之后还会再采用3x3的卷积核对每个融合结果进行卷积，目的是消除上采样的混叠效应，
如此就得到了一个新的特征图。这样一层一层地迭代下去，就可以得到多个新的特征图。
假设生成的特征图结果是P2，P3，P4，P5，它们和原来自底向上的卷积结果C2，C3，C4，C5一一对应。
金字塔结构中所有层级共享分类层（回归层）。</p>
<h3 id="fpndui-rpnde-gai-zao">FPN对RPN的改造</h3>
<p>RPN是 Faster R-CNN 用于候选框选择的子网络。RPN在特征图上应用9种不同尺度的
锚点候选框，然后进行二分类和边界框回归。考虑到越是低层的特征图包含的信息越精细，
于是没必要在所有特征图上都应用9种不同尺寸的候选框，可以在低层特征图中应用尺度较小的候选框，
在高层特征图中应用尺度较大的候选框。</p>
<p>比如P2, P3, P4, P5 层特征图分别对应的候选框尺度为32, 64, 128, 256, 而候选框的
长宽比依然采用1:1, 1:2, 2:1。</p>
<h3 id="fpndui-roichi-hua-ceng-de-gai-zao">FPN对ROI池化层的改造</h3>
<p>考虑到不同层次的特征图包含的物体大小不一样，故大尺度的ROI用在较深层的特征图上，
小尺度的ROI用在较浅层的特征图上。论文给了一个参考公式，用来计算多大尺寸的候选框
适用于哪一层特征图。</p>
<div class="math">$$k = \lfloor k_0+log_2(\sqrt{wh}/224) \rfloor$$</div>
<p>其中，224是原始图片的输入尺寸，<span class="math">\(k_0\)</span>是基准值，设置为5，代表P5层的输出，
w 和 h 是ROI区域(候选框)的长和宽。
假设ROI是 112x112 的大小，那么<span class="math">\(k = k_0-1 = 5-1 = 4\)</span>，
意味着该ROI应该使用P4的特征层。</p>
<h2 id="mask-r-cnn_1">Mask R-CNN</h2>
<h2 id="cornernet">CornerNet</h2>
<h2 id="pfpn">PFPN</h2>
<h2 id="can-kao-wen-xian">参考文献</h2>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p>基于图论进行图像分割的算法，把图像中的像素点看做是一个个节点，
像素点之间的不相似度作为边的权重，通过将相似的像素聚合到一起，
产生同一区域(表现为最小生成树)。像素聚合成区域后，判断两个相邻区域是否应该合并，
要检查它们的区域间间距(即所有分别属于两个区域且有边连接的点对中，寻找权重最小的那对,若两个区域内的点没有边相连，则定义间距为正无穷大)和区域内间距(即区域对应最小生成树中权重最大的边的权重值)的值。
如果两个区域的区域间间距明显大于其中任意一个区域的区域内间距，
那么就认为这两个区域之间存在明显的界限（即不能合并）。(freeopen:
只能写个大概，详述篇幅太长)&nbsp;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div><!-- /.entry-content -->

  <div class="comments">
    <!-- <h2>Comments !</h2> -->
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'freeopen';
      var disqus_identifier = 'posts/image-segmentation';
      var disqus_url = 'https://freeopen.github.io/posts/image-segmentation';
      (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//freeopen.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the comments.</noscript>
  </div>
</section>
          </div>

          <footer id="contentinfo" class="footer">
            <div class="footer-inner">
              无节操小广告<a href="https://freeopen.github.io/pages/da-shang.html"> 欢迎打赏 </a>
            </div>
          </footer><!-- /#contentinfo -->
        </div>
      </div>

      <div class="hidden-print hidden-xs hidden-sm hidden-md col-lg" id="post_toc" role="complementary">
        <nav class="sidebar-toc">
<!-- <section id="main_toc"> -->
    <ul class="nav" id="toc"><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#tu-xiang-fen-ge-dao-di-gan-shi-yao" title="图像分割到底干什么">图像分割到底干什么</a></li><li class="nav-item"><a class="nav-link" href="#tu-xiang-fen-ge-de-suan-fa-si-lu" title="图像分割的算法思路">图像分割的算法思路</a></li><li class="nav-item"><a class="nav-link" href="#jing-dian-suan-fa-de-yan-jin" title="经典算法的演进">经典算法的演进</a></li><li class="nav-item"><a class="nav-link" href="#r-cnn" title="R-CNN">R-CNN</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#ru-he-zhao-chu-hou-xuan-qu-yu" title="如何找出候选区域">如何找出候选区域</a></li><li class="nav-item"><a class="nav-link" href="#ru-he-cong-bu-tong-da-xiao-de-hou-xuan-kuang-ti-qu-kong-jian-te-zheng" title="如何从不同大小的候选框提取空间特征">如何从不同大小的候选框提取空间特征</a></li><li class="nav-item"><a class="nav-link" href="#ru-he-xuan-ze-zui-hao-de-bian-jie-kuang" title="如何选择最好的边界框">如何选择最好的边界框</a></li><li class="nav-item"><a class="nav-link" href="#ru-he-diao-zheng-bian-jie-kuang" title="如何调整边界框">如何调整边界框</a></li><li class="nav-item"><a class="nav-link" href="#xun-lian-pei-zhi" title="训练配置">训练配置</a></li><li class="nav-item"><a class="nav-link" href="#r-cnn-de-que-xian" title="R-CNN 的缺陷">R-CNN 的缺陷</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#overfeat_1" title="OverFeat">OverFeat</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#yu-ce-shi-de-duo-chi-du-fen-lei" title="预测时的多尺度分类">预测时的多尺度分类</a></li><li class="nav-item"><a class="nav-link" href="#ru-he-zhao-chu-hou-xuan-qu-yu_1" title="如何找出候选区域">如何找出候选区域</a></li><li class="nav-item"><a class="nav-link" href="#ru-he-zhao-chu-bian-jie-kuang" title="如何找出边界框">如何找出边界框</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#fcn_1" title="FCN">FCN</a></li><li class="nav-item"><a class="nav-link" href="#spp-net" title="SPP-Net">SPP-Net</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#kong-jian-jin-zi-ta-chi-hua-ceng" title="空间金字塔池化层">空间金字塔池化层</a></li><li class="nav-item"><a class="nav-link" href="#te-zheng-ti-qu-jia-su" title="特征提取加速">特征提取加速</a></li><li class="nav-item"><a class="nav-link" href="#ru-he-zai-cnnzui-hou-yi-ceng-te-zheng-tu-zhong-zhao-dao-yuan-shi-tu-pian-zhong-hou-xuan-kuang-de-dui-ying-qu-yu-?" title="如何在CNN最后一层特征图中找到原始图片中候选框的对应区域？">如何在CNN最后一层特征图中找到原始图片中候选框的对应区域？</a></li><li class="nav-item"><a class="nav-link" href="#xiao-jie" title="小结">小结</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#fast-r-cnn_1" title="Fast R-CNN">Fast R-CNN</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#gan-xing-qu-qu-yu-roichi-hua" title="感兴趣区域(RoI)池化">感兴趣区域(RoI)池化</a></li><li class="nav-item"><a class="nav-link" href="#xun-lian-wei-diao" title="训练微调">训练微调</a></li><li class="nav-item"><a class="nav-link" href="#duo-ren-wu-sun-shi--multi-task-loss" title="多任务损失 (Multi-task loss)">多任务损失 (Multi-task loss)</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#faster-r-cnn_1" title="Faster R-CNN">Faster R-CNN</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#mao-dian-de-gai-nian" title="锚点的概念">锚点的概念</a></li><li class="nav-item"><a class="nav-link" href="#zheng-fu-yang-ben-de-hua-fen" title="正负样本的划分">正负样本的划分</a></li><li class="nav-item"><a class="nav-link" href="#rpn-wang-luo-jie-gou" title="RPN 网络结构">RPN 网络结构</a></li><li class="nav-item"><a class="nav-link" href="#faster-r-cnn-xun-lian-ce-lue" title="Faster R-CNN 训练策略">Faster R-CNN 训练策略</a></li><li class="nav-item"><a class="nav-link" href="#que-xian-:roichi-hua-de-er-ci-liang-hua-pian-chai-wen-ti" title="缺陷：ROI池化的二次量化偏差问题">缺陷：ROI池化的二次量化偏差问题</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#fpn_1" title="FPN">FPN</a><ul class="nav-child"><li class="nav-item"><a class="nav-link" href="#fpnte-zheng-jin-zi-ta-wang-luo-jie-gou" title="FPN(特征金字塔网络)结构">FPN(特征金字塔网络)结构</a></li><li class="nav-item"><a class="nav-link" href="#fpndui-rpnde-gai-zao" title="FPN对RPN的改造">FPN对RPN的改造</a></li><li class="nav-item"><a class="nav-link" href="#fpndui-roichi-hua-ceng-de-gai-zao" title="FPN对ROI池化层的改造">FPN对ROI池化层的改造</a></li></ul></li><li class="nav-item"><a class="nav-link" href="#mask-r-cnn_1" title="Mask R-CNN">Mask R-CNN</a></li><li class="nav-item"><a class="nav-link" href="#cornernet" title="CornerNet">CornerNet</a></li><li class="nav-item"><a class="nav-link" href="#pfpn" title="PFPN">PFPN</a></li><li class="nav-item"><a class="nav-link" href="#can-kao-wen-xian" title="参考文献">参考文献</a></li></ul></ul>
<!-- </section> -->
<!-- </section> -->
        </nav>
      </div>
    </div>
  </div>


    <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-98793057-1', 'auto');
    ga('send', 'pageview');
    </script>
<script type="text/javascript">
    var disqus_shortname = 'freeopen';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'https://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<script src="https://freeopen.github.io/theme/js/jquery-3.3.1.slim.min.js"></script>
<script src="https://freeopen.github.io/theme/js/popper.min.js"></script>
<script src="https://freeopen.github.io/theme/js/bootstrap.min.js"></script>

</body>
</html>