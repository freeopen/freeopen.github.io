<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Freeopen - top1</title>
    <subtitle>Freeopen 的个人兴趣研究所.</subtitle>
    <link href="https://www.freeopen.tech/tags/top1/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://www.freeopen.tech"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2021-02-14T00:00:00+00:00</updated>
    <id>https://www.freeopen.tech/tags/top1/atom.xml</id>
    <entry xml:lang="en">
        <title>冠军方案之 宫颈癌风险智能诊断</title>
        <published>2021-02-14T00:00:00+00:00</published>
        <updated>2021-02-14T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/cervical/" type="text/html"/>
        <id>https://www.freeopen.tech/cervical/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;赛道1冠军：deep-thinker 团队&lt;&#x2F;p&gt;
&lt;p&gt;赛道2冠军：LLLLC 队&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;通过提供大规模经过专业医师标注的宫颈癌液基薄层细胞检测数据，选手能够提出并综合运用目标检测、深度学习等方法对宫颈癌细胞学异常鳞状上皮细胞进行定位以及对宫颈癌细胞学图片分类，提高模型检测的速度和精度，辅助医生进行诊断。&lt;&#x2F;p&gt;
&lt;p&gt;宫颈癌细胞学图片采用kfb格式，每张数据在20倍数字扫描仪下获取，大小300～400M。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;chu-sai-huan-jie&quot;&gt;初赛环节&lt;&#x2F;h4&gt;
&lt;p&gt;宫颈癌细胞学图片800张，其中阳性图片500张，阴性图片300张。阳性图片会提供多个ROI区域，在ROI区域里面标注异常鳞状上皮细胞位置，阴性图片不包含异常鳞状上皮细胞，无标注。初赛讨论的异常鳞状上皮细胞主要包括四类：ASC-US(非典型鳞状细胞不能明确意义)，LSIL(上皮内低度病变)，ASC-H(非典型鳞状细胞倾向上皮细胞内高度)，HSIL(上皮内高度病变)。（特别注明：阳性图片ROI区域之外不保证没有异常鳞状上皮细胞）&lt;&#x2F;p&gt;
&lt;h4 id=&quot;fu-sai-huan-jie&quot;&gt;复赛环节&lt;&#x2F;h4&gt;
&lt;p&gt;通过线上赛的方式，不允许选手下载数据，在线完成模型训练。&lt;&#x2F;p&gt;
&lt;p&gt;复赛训练集共提供1690张数据，其中1440张包含标注，250张没有标注。1440张有标注数据在ROI区域内标注了6类异常细胞，分别是阳性类别“ASC-H”、“ASC-US”、“HSIL”、“LSIL”，和阴性类别“Candida”、“Trichomonas”。250张没有标注数据表示未见上皮内细胞病变（NILM，可以理解为整图中不含任何前述六类细胞）。复赛测试集提供350张数据，给出ROI区域内6类异常细胞的&lt;strong&gt;位置、类别和概率&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;biao-zhu-shu-ju&quot;&gt;标注数据&lt;&#x2F;h4&gt;
&lt;p&gt;一张宫颈癌细胞学图片kfb文件和对应一个标注json文件。标注json文件内容是一个list文件，里面记录了每个ROI区域的位置和异常鳞状上皮细胞的位置坐标（细胞所在矩形框的左上角坐标和矩形宽高）。类别roi表示感兴趣区域，pos表示异常鳞状上皮细胞。json标注文件示例如下：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;[{&amp;quot;x&amp;quot;: 33842, &amp;quot;y&amp;quot;: 31905, &amp;quot;w&amp;quot;: 101, &amp;quot;h&amp;quot;: 106, &amp;quot;class&amp;quot;: &amp;quot;pos&amp;quot;},
{&amp;quot;x&amp;quot;: 31755, &amp;quot;y&amp;quot;: 31016, &amp;quot;w&amp;quot;: 4728, &amp;quot;h&amp;quot;: 3696, &amp;quot;class&amp;quot;: &amp;quot;roi&amp;quot;},
{&amp;quot;x&amp;quot;: 32770, &amp;quot;y&amp;quot;: 34121, &amp;quot;w&amp;quot;: 84, &amp;quot;h&amp;quot;: 71, &amp;quot;class&amp;quot;: &amp;quot;pos&amp;quot;},
{&amp;quot;x&amp;quot;: 13991, &amp;quot;y&amp;quot;: 38929, &amp;quot;w&amp;quot;: 131, &amp;quot;h&amp;quot;: 115, &amp;quot;class&amp;quot;: &amp;quot;pos&amp;quot;},
{&amp;quot;x&amp;quot;: 9598, &amp;quot;y&amp;quot;: 35063, &amp;quot;w&amp;quot;: 5247, &amp;quot;h&amp;quot;: 5407, &amp;quot;class&amp;quot;: &amp;quot;roi&amp;quot;},
{&amp;quot;x&amp;quot;: 25030, &amp;quot;y&amp;quot;: 40115, &amp;quot;w&amp;quot;: 250, &amp;quot;h&amp;quot;: 173, &amp;quot;class&amp;quot;: &amp;quot;pos&amp;quot;}]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;sai-dao-yi-suan-fa-sai-dao&quot;&gt;赛道一: 算法赛道&lt;&#x2F;h4&gt;
&lt;p&gt;用常规机器学习算法得出结果。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;sai-dao-er-vnnimo-xing-liang-hua&quot;&gt;赛道二: VNNI模型量化&lt;&#x2F;h4&gt;
&lt;p&gt;由于病理图像输入尺寸非常大，通常可以达到几G几十亿个像素，传统的NvidiaGPU无法容纳更多的全局图像信息，并且低效的推理过程。本次大赛将由intel支持，参赛者可以摆脱GPU显存限制，验证intel VNNI在超高分辨率病理图像上的工程效率。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ping-gu-zhi-biao&quot;&gt;评估指标&lt;&#x2F;h4&gt;
&lt;p&gt;采用目标检测任务常用的mAP（mean Average Precision）指标作为本次宫颈癌肿瘤细胞检测的评测指标。我们采用两个IoU阈值（0.3，0.5）分别来计算AP，再综合平均作为最终的评测结果。&lt;&#x2F;p&gt;
&lt;p&gt;赛道二的评价指标，mAP@0.5 和 QPS，即精度和速度&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-fen-xi&quot;&gt;数据分析&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;数据集目标尺度差异较大，最大最小可相差将近十万倍。比如，大目标面积可达5500x4000像素，小的只有10x10像素。因此，要求模型具有较好的多尺度检测能力。&lt;&#x2F;li&gt;
&lt;li&gt;目标宽高比主要集中在0.5～2的区间，但仍存在一定数量的极端目标。因此，将增加anchor设计难度，也较为依赖特定的先验知识。&lt;&#x2F;li&gt;
&lt;li&gt;图像尺寸非常大，背景复杂。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;sai-dao-yi-fang-an&quot;&gt;赛道一方案&lt;&#x2F;h2&gt;
&lt;p&gt;没有采用常用的anchor-based模型，而是选择了非常契合本次赛题特点的anchor-free模型RepPoints。&lt;&#x2F;p&gt;
&lt;p&gt;RepPoints(ResNeXt101 + FPN + SE + DCN)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;cervical&#x2F;.&#x2F;20210214113701.jpg&quot; alt=&quot;20210214113701&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zai-xian-sui-ji-cai-jian&quot;&gt;在线随机裁剪&lt;&#x2F;h3&gt;
&lt;p&gt;随机选择输入图片中的一个目标，围绕目标随机切出边长在768~2048范围内的子图，然后缩放至边长为1024后，再送进网络。若目标边长超过了范围，则将目标与少量背景直接切出，再进行缩放。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;reppoints-mo-xing&quot;&gt;RepPoints 模型&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;核心： 物体表示上采用&lt;strong&gt;点集&lt;&#x2F;strong&gt;来替代传统边界框&lt;&#x2F;li&gt;
&lt;li&gt;边界框只提供粗糙定位，而RepPoints可自适应地分布于物体重要的局部语义区域，可提供更加细致的几何描述，有利于目标特征提取&lt;&#x2F;li&gt;
&lt;li&gt;采用&lt;strong&gt;中心点代替anchor&lt;&#x2F;strong&gt;作为初始时目标表示方式，相比传统Anchor机制，中心点更易覆盖定位二维的假设空间，无须依赖尺度和宽高比设置&lt;&#x2F;li&gt;
&lt;li&gt;其他各点可由中心点加上预测的偏移量计算得到，并自适应地分布于目标重要语义区域&lt;&#x2F;li&gt;
&lt;li&gt;为了利用仍为边界框的标注，训练时将点集转为边界框，从而计算目标定位的损失&lt;&#x2F;li&gt;
&lt;li&gt;对比其他anchor-free模型，不需要额外监督，自顶向下&lt;&#x2F;li&gt;
&lt;li&gt;通过两次预测偏移量，对目标表示中各点位置进行优化，最终获得精确定位&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;shi-yan-jie-guo-ji-mo-xing-rong-he&quot;&gt;实验结果及模型融合&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;cervical&#x2F;.&#x2F;20210214120840.jpg&quot; alt=&quot;20210214120840&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;cervical&#x2F;.&#x2F;20210214120902.jpg&quot; alt=&quot;20210214120902&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sai-dao-er-fang-an&quot;&gt;赛道二方案&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;shu-ju-zeng-qiang&quot;&gt;数据增强&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;从ROI中“在线”随机裁剪 （Online crop）&lt;&#x2F;li&gt;
&lt;li&gt;随机翻转（Random flip）&lt;&#x2F;li&gt;
&lt;li&gt;移动标注框（Shift GT），采用cv2.inpaint进行填补
&lt;img src=&quot;.&#x2F;shift-gt.png&quot; alt=&quot;shift-gt&quot; style=&quot;zoom:50%;&quot; &#x2F;&gt;&lt;&#x2F;li&gt;
&lt;li&gt;背景替换（Replace BG），利用阴性图片作物背景，并染色剂归一化
&lt;img src=&quot;.&#x2F;replace-bg.png&quot; alt=&quot;replace-bg&quot; style=&quot;zoom:50%;&quot; &#x2F;&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;zheng-ti-jia-gou&quot;&gt;整体架构&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;ji-yu-openvinode-liang-hua-tui-li-jia-gou&quot;&gt;基于OpenVINO的量化推理架构&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;cervical&#x2F;.&#x2F;openvino.png&quot; alt=&quot;openvino&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;模型结构：&lt;&#x2F;p&gt;
&lt;p&gt;主模型为RetinaNet ， 用开源的 imageNet 预训练模型初始化。&lt;&#x2F;p&gt;
&lt;p&gt;比较两种类型的backbone:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Res50-FPN256-Head256&lt;&#x2F;li&gt;
&lt;li&gt;MbV2-FPN128-Head64, 相较于上面的Res50 约12倍加速&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;mo-xing-xun-lian&quot;&gt;模型训练&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;xun-lian-ce-lue&quot;&gt;训练策略&lt;&#x2F;h4&gt;
&lt;p&gt;超参数：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;学习率：余弦下降，初始学习率0.01， 终止学习率为0.00001&lt;&#x2F;li&gt;
&lt;li&gt;Batch size: 8&lt;&#x2F;li&gt;
&lt;li&gt;预训练模型： ImageNet，训练时不固定BN参数&lt;&#x2F;li&gt;
&lt;li&gt;Epoch: 100&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;裁剪策略：&lt;&#x2F;p&gt;
&lt;p&gt;从ROI中裁剪1600 x 1600，在缩小到 800 x 800，这样可以增加标注框的数量，提高训练效率，在推理时也可以减少滑窗数量。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;liang-hua-ce-lue&quot;&gt;量化策略&lt;&#x2F;h4&gt;
&lt;p&gt;OpenVINO的量化工具，该工具的后两个步骤是将一些INT8层切换回FP32，用于提升acc，实验中发现这两个步骤对我们的模型不起作用，精度损失仍然很大。通过经验化的方法，我们发现FPN部分对量化比较敏感，因此在量化时不对FPN部分进行量化。&lt;&#x2F;p&gt;
&lt;p&gt;校验选择300张训练图片，除FPN部分的卷积层外，其余卷积层全部量化。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mo-xing-tui-li&quot;&gt;模型推理&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;tu-pian-du-qu-ji-yu-chu-li&quot;&gt;图片读取及预处理&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;图片读取部分我们采用了GDAL库，发现跟OpenCV相比有接近2倍的性能提升。&lt;&#x2F;li&gt;
&lt;li&gt;多个子进程同时读取图片，存放到共享队列中。&lt;&#x2F;li&gt;
&lt;li&gt;丢弃边界像素&lt;&#x2F;li&gt;
&lt;li&gt;裁剪1600x1600缩小到800x800&lt;&#x2F;li&gt;
&lt;li&gt;无重叠滑窗&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;qian-xiang-zhi-xing&quot;&gt;前向执行&lt;&#x2F;h4&gt;
&lt;p&gt;采用OpenVINO的异步模式&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;发起执行请求好，控制权交还回主程序，分摊数据读取和后处理的时间。其中子进程负责图片的读取、裁剪、缩放、拼batch 等数据操作，处理完的数据存放到共享队列中；&lt;&#x2F;li&gt;
&lt;li&gt;执行完成后，通过回调函数通知主程序。主进程从共享队列读取数据，负责模型推理、后处理操作。（使用 “生产者-消费者”模式，采用共享队列实现数据的通信）&lt;&#x2F;li&gt;
&lt;li&gt;可并发多个 infer request 
&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;cervical&#x2F;20210214165514.jpg&quot; alt=&quot;20210214165514&quot; &#x2F;&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;shi-yan-jie-guo&quot;&gt;实验结果&lt;&#x2F;h4&gt;
&lt;p&gt;map0.5: 33.54%, 推理总时长：24s。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 X光限制品监测</title>
        <published>2021-02-14T00:00:00+00:00</published>
        <updated>2021-02-14T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/x-cargo/" type="text/html"/>
        <id>https://www.freeopen.tech/x-cargo/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：YuanXu&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;包裹X光限制品监测作为日常包裹物流行业及安防行业的重要环节，承担着防止易燃易爆等危险品进入货运渠道，管理刀具等特殊货运物品，监测毒品等国家重点违禁品偷运等工作。随着线上购物的普及和快速发展，线上物流包裹数量已经远超人工可以处理的范围，给物流包裹监管带来了巨大挑战。&lt;&#x2F;p&gt;
&lt;p&gt;针对给出的限制品种类，利用X光图像及标注数据，研究开发高效的计算机视觉算法，监测图像是否包含危险品及其大致位置。通过自动化监测包裹携带品算法，降低漏检风险及误报率，提升危险品管理效率。&lt;&#x2F;p&gt;
&lt;p&gt;限制品包括：铁壳打火机、黑钉打火机、刀具、电池电容以及剪刀五类（类别id依次从1到5）。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ping-gu-zhi-biao&quot;&gt;评估指标&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;评测方式采用计算 &lt;code&gt;box mAP&lt;&#x2F;code&gt;的方式，对 &lt;code&gt;IoU = 0.5:0.05:0.95&lt;&#x2F;code&gt;，分别计算&lt;code&gt;mAP&lt;&#x2F;code&gt;，再做平均得到最后的&lt;code&gt;mAP&lt;&#x2F;code&gt;。&lt;&#x2F;li&gt;
&lt;li&gt;单个模型整体大小需不超过&lt;code&gt;600MB&lt;&#x2F;code&gt;（即不超过&lt;code&gt;VGG19&lt;&#x2F;code&gt;大小），模型不得超过&lt;code&gt;2&lt;&#x2F;code&gt;个。&lt;&#x2F;li&gt;
&lt;li&gt;响应时间越快越好&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;wen-ti-fen-xi&quot;&gt;问题分析&lt;&#x2F;h2&gt;
&lt;p&gt;比赛任务是经典的图像语义分割（semantic-segmentation）的问题，简单说就是要在像素级别将前景类别标识出来。研究kaggle上的几个图像语义分割的比赛，发现Unet和Mask-RCNN的成绩最好。&lt;&#x2F;p&gt;
&lt;p&gt;因为时间原因，最后选择 Unet。因为Unet能直接输出与图像1: 1的mask，且超参数少，属于端到端网络。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h2&gt;
&lt;p&gt;采用经典Unet模型。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;resnet 做 encoder&lt;&#x2F;li&gt;
&lt;li&gt;将各个decoder的输出cat在一起，作为最终的输出特征&lt;&#x2F;li&gt;
&lt;li&gt;除了输出前景物体的mask， 还单独输出物体的边沿。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;zhang-fen-dian&quot;&gt;涨分点&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;shu-ju-zeng-qiang&quot;&gt;数据增强&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;常用的图像数据增强，包括旋转、翻转、颜色、噪声、形变等。&lt;&#x2F;li&gt;
&lt;li&gt;考虑到X光的穿透性，将没有危险品的图片和有危险品的图片进行合成（blend操作）。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;kuai-su-xun-lian&quot;&gt;快速训练&lt;&#x2F;h3&gt;
&lt;p&gt;训练速度直接决定了开发的迭代速度和实验的总次数。&lt;&#x2F;p&gt;
&lt;p&gt;在训练的初期使用比较小的图像作为输入，然后再使用较大的图像作为输入。这个过程就像人学习一样，先从简单的、粗略的开始学起，然后在学习复杂的、精细的，这样最后网络收敛会更快、更好。具体为先训练128p的图像，再在原模型上训练256p的图像。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;rong-he-ce-lue&quot;&gt;融合策略&lt;&#x2F;h3&gt;
&lt;p&gt;因为数据集比较小，即使使用了各种数据增强技术，训练使用以rsenet154作为encoder这样大网络，选取一个snapshot作为最终模型的参数还是会有过拟合的风险，选取多个snapshot使用参数均值的方法对模型参数进行融合可以提高模型的泛化能力。&lt;&#x2F;p&gt;
&lt;p&gt;但是传统的方法是对一个模型进行多次训练来取得多个snapshot，这会需要很多的计算时间。根据不同评价标准选择融合的候选参数的方法，也就是选择最小验证loss，最大mIoU，和最小训练loss的三个模型参数进行融合。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ban-jian-du-xue-xi&quot;&gt;半监督学习&lt;&#x2F;h3&gt;
&lt;p&gt;使用已训练的模型在测试数据上的结果作为训练数据，来达到增加数据集、进而提高模型精度的方法。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ce-shi-zeng-qiang&quot;&gt;测试增强&lt;&#x2F;h3&gt;
&lt;p&gt;在推理时，通过对图像的旋转和翻转，并对结果取平均也能提高精度。但是这样推理测试的速度慢了8倍，考虑到数据中有很多图片没有危险品，而测试增强对于这些图片没有改进，所以避免对这些图片多次测试可大幅提高测试速度。代码改动也很少。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;zong-jie&quot;&gt;总结&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;端到端的一体化网络设计,方便使用不同大小的预训练网络&lt;&#x2F;li&gt;
&lt;li&gt;使用逐步增加训练图像大小等方法,提高训练速度和精度&lt;&#x2F;li&gt;
&lt;li&gt;使用数据增强、模型参数均值、半监督学习的方法提高模型泛化能力&lt;&#x2F;li&gt;
&lt;li&gt;使用测试增强提高结果精度(并优化)&lt;&#x2F;li&gt;
&lt;li&gt;一些实验结果
&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;x-cargo&#x2F;.&#x2F;20210214174954.jpg&quot; alt=&quot;20210214174954&quot; &#x2F;&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 面料剪裁利用率优化</title>
        <published>2021-02-13T00:00:00+00:00</published>
        <updated>2021-02-13T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/cloth-cut/" type="text/html"/>
        <id>https://www.freeopen.tech/cloth-cut/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：行星防御理事会 团队（乔德平等）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;面料切割利用率的提升是纺织行业长期追求的目标。如何提升面料切割利用率，既是企业生产精益化的难点，也是痛点。在切割之前，需要确定多个零件在面料上的位置和角度，再充分利用零件在形状上的互补特征，对零件排布的方式进行优化。面料切割问题的特性，是零件存在多种尺寸、形状，比如用作衬衫制作的袖子、后背等零件，用来切割的布匹本身存在多类瑕疵，如破洞、折皱、漏纱等，在排版中需要避开。此外，某些订单，对零件存在个性化排版需求，因此在下料环节中，需要依照订单要求进行排版下料。当前纺织行业布匹原材料的成本占到40%左右，价值较高。&lt;&#x2F;p&gt;
&lt;p&gt;本赛场聚焦面料剪裁利用率优化，要求选手研究开发高效可靠的算法，在较短时间范围内计算获得高质量可执行的排版结果，减少切割中形成的边角废料，提升面料切割利用率，减少计划时间、提高工作效率和避免人工计算的失误，提升价值降低成本。&lt;&#x2F;p&gt;
&lt;p&gt;在规则面料的情况下，满足零件旋转角度、零件最小间距、最小边距的约束，解决以下两类问题：&lt;&#x2F;p&gt;
&lt;p&gt;初赛赛题：基于所给零件，进行面料排版加工，耗料长度最短，面料利用率最高；&lt;&#x2F;p&gt;
&lt;p&gt;复赛赛题：在问题一的基础上，避开瑕疵区域面料加工，耗料长度最短，面料利用率最高。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ling-jian-shu-ju&quot;&gt;零件数据&lt;&#x2F;h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;编号&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;列名&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;说明&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;示例&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;1&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;下料批次号&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;Primary key&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;L0001&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;2&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;零件号&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;Primary key&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;s000001&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;3&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;数量&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;1&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;4&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;外轮廓&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;[[1420.0, 5998.6], [1420.0, 6062.8], [2183.1, 6062.8],[2183.1, 5998.6], [1420.0, 5998.6]]&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;5&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;允许旋转角度&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;逆时针旋转角度&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;0,90,180,270&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;6&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;面料号&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;M0001&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;注：外轮廓曲线数据均离散化为点坐标序列；所有尺寸的单位为毫米(mm).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mian-liao-shu-ju-shuo-ming&quot;&gt;面料数据说明&lt;&#x2F;h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;编号&lt;&#x2F;th&gt;&lt;th&gt;列名&lt;&#x2F;th&gt;&lt;th&gt;说明&lt;&#x2F;th&gt;&lt;th&gt;示例&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;面料号&lt;&#x2F;td&gt;&lt;td&gt;Primary key&lt;&#x2F;td&gt;&lt;td&gt;M0001&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;面料规格&lt;&#x2F;td&gt;&lt;td&gt;规则（矩形）面料，长度x宽度（单位：mm）&lt;&#x2F;td&gt;&lt;td&gt;10000x100&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;瑕疵区域&lt;&#x2F;td&gt;&lt;td&gt;瑕疵均为圆形区域，标注方式为圆形中心、圆形半径。比如[[2000,400],80]，即圆形中心坐标点为[2000,400]，半径为80。坐标系的原点为面料的左下角（参考“约束说明“第（7）条说明）&lt;&#x2F;td&gt;&lt;td&gt;[[[2000,400],80], [[1000,1200],50], ⋯]&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;零件间最小间距&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;最小边距&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;注：瑕疵区域均为圆形；所有尺寸的单位为毫米(mm)。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;yue-shu-shuo-ming&quot;&gt;约束说明&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;排样规则&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;1）排版的零件不能超出面料的可行区域；&lt;&#x2F;p&gt;
&lt;p&gt;2）排版零件互不重叠；&lt;&#x2F;p&gt;
&lt;p&gt;3）零件按批次，在同一面料上排版；&lt;&#x2F;p&gt;
&lt;p&gt;4）面料可能存在多个长宽度规格，如宽度为900mm、1000mm等、长度为10000mm、12000mm等；&lt;&#x2F;p&gt;
&lt;p&gt;5）允许用户设置切边预留量，如面料四边各预留5mm（最小边距）；切割零件间预留量5mm（最小间距）；&lt;&#x2F;p&gt;
&lt;p&gt;6）某些零件存在旋转角度上的要求，比如零件纹理方向必须保持一致；旋转角度为0表示，零件不允许发生旋转，必须原样放在面料上，面料的放置方向为面料窄边（宽度）在垂直方向，面料宽边（长度）在水平方向；旋转角度为90表示允许零件逆时针旋转90度。&lt;&#x2F;p&gt;
&lt;p&gt;7）切割零件需要避开面料上的瑕疵，瑕疵均为圆形区域，标注方式为圆形中心、圆形半径，坐标系的原点为面料的左下角（参考“数据说明”第（2）条“面料数据说明”），面料的放置方向为面料窄边（宽度）在垂直方向，面料宽边（长度）在水平方向；瑕疵与零件间间距视同零件间间距，即，如果零件间间距（最小距离）为5mm，零件与瑕疵的间距（最小距离）也为5mm。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ping-gu-zhi-biao&quot;&gt;评估指标&lt;&#x2F;h3&gt;
&lt;p&gt;决赛总分 = 0.3∗&lt;em&gt;B&lt;&#x2F;em&gt;榜成绩 + 0.4∗&lt;em&gt;C&lt;&#x2F;em&gt;榜成绩 + 0.3∗现场答辩&lt;&#x2F;p&gt;
&lt;p&gt;其中：&lt;&#x2F;p&gt;
&lt;p&gt;B榜成绩 =（0.5∗批次1面料利用率+0.5∗批次2面料利用率）∗100&lt;&#x2F;p&gt;
&lt;p&gt;C榜成绩 = 权重参数1∗面料利用率 − 权重参数2∗计算时间分值&lt;&#x2F;p&gt;
&lt;p&gt;面料利用率 = 一个批次包含的零件总面积&#x2F;消耗的面料总面积&lt;&#x2F;p&gt;
&lt;p&gt;计算时间分值 = f(一个批次排版的平均计算时间)&lt;&#x2F;p&gt;
&lt;p&gt;权重参数1 = 100.0
权重系数2 = 1.0&lt;&#x2F;p&gt;
&lt;h2 id=&quot;fang-an-si-lu&quot;&gt;方案思路&lt;&#x2F;h2&gt;
&lt;p&gt;问题： 二维不规则多边形放置&lt;&#x2F;p&gt;
&lt;p&gt;最先考虑山寨开源的svgnest，发现初赛结束前可能搞不出来。&lt;&#x2F;p&gt;
&lt;p&gt;又考虑NFP，但计算太复杂，初赛结束前可能做不出来。&lt;&#x2F;p&gt;
&lt;p&gt;按像素暴力枚举实现左底法，原理和实现都很简单，但性能和内存可能惨不忍睹。&lt;&#x2F;p&gt;
&lt;p&gt;预期纯左底优化到1小时内出个解就行，后来优化到1秒内，就觉得贪心暴力枚举也可以试试了。&lt;&#x2F;p&gt;
&lt;p&gt;复赛开始前把暴力贪心实现了，然后主要精力花在像素法的性能提高上，这个地方足够快以后，后面的贪心算法就可以暴力按像素枚举最佳位置了。&lt;&#x2F;p&gt;
&lt;p&gt;贪心算法原理和实现都很简单，缺陷也很明显，很容易挂在小规模或者不太随机的数据集上，比赛期间也没解决这个问题，以后有时间慢慢优化。&lt;&#x2F;p&gt;
&lt;p&gt;算法中还包含了遗传算法调优的部分，这部分直接参考了svgnest，只有很小的调整，提升效果不大。&lt;&#x2F;p&gt;
&lt;p&gt;遗传调优这部分直接删掉也行，效果略微降一点，时间可以降到3分钟内，内存降到几十兆。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen：冠军选手的思考轨迹，个人认为有很高学习价值，感谢冠军选手这么细致的分享。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;suan-fa-she-ji&quot;&gt;算法设计&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;第一部分：基于高精度像素法实现多边形相交检测（性能接近NFP且初始化时间很小，纯左底法单核总时间在1秒内出结果）&lt;&#x2F;li&gt;
&lt;li&gt;第二部分：贴合度+贪心算法得到初始解（L0004约86秒到85.7，L0005约170秒到85.1）&lt;&#x2F;li&gt;
&lt;li&gt;第三部分：左底+遗传算法持续迭代优化（约60秒左右接近最终解，利用率提升约0.4%）&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;ji-yu-gao-jing-du-xiang-su-fa-shi-xian-duo-bian-xing-xiang-jiao-jian-ce&quot;&gt;基于高精度像素法实现多边形相交检测&lt;&#x2F;h3&gt;
&lt;p&gt;像素法的基本原理：将不规则多边形零件及面料像素化（初始化）；检测零件的像素与已放置的像素是否有重叠（相交检测）；将零件的像素放到面料对应的像素上（放置）。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;zuo-di-fa&quot;&gt;左底法&lt;&#x2F;h4&gt;
&lt;p&gt;对每个零件，从左下角像素开始，逐像素上移直到与已放置像素无重叠为止。如果失败，向右移动一个像素并重复该过程。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;xiang-su-fa-de-jing-du-xiang-guan-wen-ti&quot;&gt;像素法的精度相关问题&lt;&#x2F;h4&gt;
&lt;p&gt;像素法将浮点数转换成了整数，必然会有精度丢失。为了保证间距边距要求，在多边形扩张时通常要将像素向上取整，这就造成了一定的浪费，通常为0到1像素大小。通常精度越低，浪费越大。&lt;&#x2F;p&gt;
&lt;p&gt;无论多高的精度，即使到原子级，都是有误差的；&lt;&#x2F;p&gt;
&lt;p&gt;为了减小浪费，通常需要提高精度，高精度往往意味着低性能。&lt;&#x2F;p&gt;
&lt;p&gt;本文实现的像素精度为0.1mm，对于面料即16000*200000=32亿像素。如果采用位图，仅面料就需要32亿像素&#x2F;8=4亿字节，约380M。如果全局搜索采用遗传算法之类的算法，内存消耗会再增加数十倍，性能也比较低。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ji-yu-sao-miao-xian-de-xiang-su-fa&quot;&gt;基于扫描线的像素法&lt;&#x2F;h4&gt;
&lt;p&gt;基于扫描线实现像素法，即将零件和面料横向和纵向的连续像素使用线段来表示。零件横向或纵向扫描线一般在1～2个线段，零件扫描线根数一般在几百到12000。&lt;&#x2F;p&gt;
&lt;p&gt;面料初始状态横向和纵向都是1个线段，在零件全部放置后，则纵向10个线段左右，最多20万根扫描线，一般内存消耗在20M左右；横向扫描线数量要高一些，但横向的扫描线只有16000根，内存消耗和纵向差不多。&lt;&#x2F;p&gt;
&lt;p&gt;纯左底法的内存消耗估计在20M左右，性能估计也会有数量级的提升。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;sao-miao-xian-xiang-jiao-jian-ce-de-hui-su-wen-ti&quot;&gt;扫描线相交检测的回溯问题&lt;&#x2F;h4&gt;
&lt;p&gt;左底法扫描线的基础版本，是对于零件的每条扫描线可以直接上移若干像素保证该条扫描线可放置或者直接失败。该方案当下一条扫描线需要上移时，需要回过头来从零件的第一根扫描线重新检测是否相交，这样会有大量的回溯检测，性能较低。&lt;&#x2F;p&gt;
&lt;p&gt;改进方法一（记录最小移动距离）：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;在将每条扫描线放置后，计算这条扫描线还可以继续移动多少像素，并记录所有扫描线中最小的移动长度。&lt;&#x2F;li&gt;
&lt;li&gt;每次放置扫描线时，如果需要移动扫描线，那么移动后就从剩余最小移动长度中扣除本次移动的长度，如果不能扣除了，才需要从第一根扫描线重新检测。&lt;&#x2F;li&gt;
&lt;li&gt;采用该方法后，纯左底法单核需要约120秒。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;改进方法二（调整扫描顺序，尽早失败，提前回溯）：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;考虑零件的几何连续性，对于扫描线相交检测，如果某条扫描线与已放置扫描线不相交，那么与该扫描线相邻的扫描线有较大几率不相交，反之同理。即检测一条扫描线后再检测相邻的一条扫描线，结果会大概率相同。&lt;&#x2F;li&gt;
&lt;li&gt;连续放置成功多条扫描线后，如果放置失败，有可能需要上移或者右移并重新扫描。如果我们能尽早检测到放置失败，就能尽可能避免这种回溯扫描。&lt;&#x2F;li&gt;
&lt;li&gt;不连续扫描的方案，以零件坐标的二进制表示的尾数0的数量排序，如某零件有100根线，检测顺序为 64-32-96-16-48-80 …&lt;&#x2F;li&gt;
&lt;li&gt;一般检测几根到几十根线就能确定当前X坐标上垂直方法是否可放置，以决定放置成功还是继续右移。这时，左底单核从120秒减少到0.8秒，加上初始化0.15秒，总时间在1秒内。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;ji-yu-sao-miao-xian-de-duo-bian-xing-bian-yuan-kuo-zhang&quot;&gt;基于扫描线的多边形边缘扩张&lt;&#x2F;h4&gt;
&lt;p&gt;零件间距可以通过边缘扩张来处理，对于像素法的边缘扩张，对每个顶点画圆，将预先计算的扫描线叠加到多边形上；对每个线段向平移构成平行四边形，画该平行四边形，将扫描线叠加到多边形上。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;xiang-su-fa-gai-jin-xiao-jie&quot;&gt;像素法改进小结&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;基于扫描线实现像素法，内存从1G降到20M&lt;&#x2F;li&gt;
&lt;li&gt;记录最小剩余移动距离，120秒出结果&lt;&#x2F;li&gt;
&lt;li&gt;调整扫描顺序，0.8秒出结果&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;tie-he-du-tan-xin-suan-fa&quot;&gt;贴合度+贪心算法&lt;&#x2F;h3&gt;
&lt;p&gt;贪心算法的主要原理：每次放置都从所有零件中跳出最贴合的来放置。&lt;&#x2F;p&gt;
&lt;p&gt;但由于零件数量较大，每次都从所有零件中来选择的话，性能较低，所以我们每次只从其中一部分来选择最贴合的零件。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;tan-xin-suan-fa-zheng-ti-bu-zou&quot;&gt;贪心算法整体步骤&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;将所有零件按包围盒面积倒排（按包围盒面积倒排主要是将那些面积不大但形状奇特的零件也排到前面）&lt;&#x2F;li&gt;
&lt;li&gt;对前N(N=64)个零件计算贴合度score=ShapeBestFitScore(shape)，选择最贴合的零件放置到面料上&lt;&#x2F;li&gt;
&lt;li&gt;反复执行上一步骤直到所有零件全部放置完&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;ji-yu-sao-miao-xian-tie-he-ju-chi-de-tie-he-du-gong-shi&quot;&gt;基于扫描线贴合距离的贴合度公式&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;总体上离得越近分数越高，离的越远分数越低，但分数最低的位置应当在距离刚好窄到较难放置其他的零件的地方，采用统计剩余零件的大小来计算这个最低分的距离值&lt;&#x2F;li&gt;
&lt;li&gt;其他相关积分加成
&lt;ul&gt;
&lt;li&gt;零件面积大的有加成，我们通过远距离分数不为0来实现&lt;&#x2F;li&gt;
&lt;li&gt;横向的长度较长的分数有加成，我们通过垂直方向的分数加成来实现&lt;&#x2F;li&gt;
&lt;li&gt;超出当前最右侧的有惩罚。超出越大，惩罚越大，最大不超过自身的长度（动态计算该惩罚时，可以让四份数据都上85.2，但最高的只能到85.5）&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;zuo-di-yi-chuan-suan-fa&quot;&gt;左底+遗传算法&lt;&#x2F;h3&gt;
&lt;p&gt;使用遗传算法对左底的零件放置顺序进行调整。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;遗传算法在初始解本身已经比较好的情况下，再全局提升会比较缓慢&lt;&#x2F;li&gt;
&lt;li&gt;使用贪心算法得到的初始解最后放置的一部分零件效果往往不够好，将这部分零件取出来继续优化&lt;&#x2F;li&gt;
&lt;li&gt;面积小的零件就像润滑剂，也拿出来一起参与迭代优化&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;方法&lt;&#x2F;th&gt;&lt;th&gt;效果&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;纯左底改为左底或左上&lt;&#x2F;td&gt;&lt;td&gt;+0.0%～0.2%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;旋转的选择通过直接比较2x+y 取小的&lt;&#x2F;td&gt;&lt;td&gt;搜索空间大幅下降&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;允许零件向右滑动一小段距离&lt;&#x2F;td&gt;&lt;td&gt;提升0.x%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;逐步放大参与优化的零件数&lt;&#x2F;td&gt;&lt;td&gt;略微提升&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;zong-jie&quot;&gt;总结&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;xiang-su-fa-de-you-dian&quot;&gt;像素法的优点&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;原理和实现很简单&lt;&#x2F;li&gt;
&lt;li&gt;计算也很简单，主要是加减法，少量的乘除法和三角函数&lt;&#x2F;li&gt;
&lt;li&gt;可以处理任意形状，包括曲线边缘&lt;&#x2F;li&gt;
&lt;li&gt;不需要简化顶点数&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;xiang-su-fa-de-que-dian&quot;&gt;像素法的缺点&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;处理超高精度的放置需要解决固有精度问题&lt;&#x2F;li&gt;
&lt;li&gt;一些几何手段不容易利用，比如法向，切线等概念&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;tan-xin-suan-fa-de-you-que-dian&quot;&gt;贪心算法的优缺点&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;原理和实现很简单，能较快的速度出个结果&lt;&#x2F;li&gt;
&lt;li&gt;容易陷入局部最优&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;yi-chuan-suan-fa-de-you-que-dian&quot;&gt;遗传算法的优缺点&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;原理和实现简单，能比较简单地处理复杂的组合问题&lt;&#x2F;li&gt;
&lt;li&gt;开始阶段提升很快，提升到一定程度就比较慢了&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;jin-yi-bu-de-gong-zuo&quot;&gt;进一步的工作&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;xiang-su-fa-shi-xian-fang-mian-dai-gai-jin-de-di-fang&quot;&gt;像素法实现方面待改进的地方&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;代码中包含大量动态内存分配，可进一步优化&lt;&#x2F;li&gt;
&lt;li&gt;需进一步支持任意旋转角度，目前只支持4个方向&lt;&#x2F;li&gt;
&lt;li&gt;精度的选择应当动态适应，目前是直接10倍精度&lt;&#x2F;li&gt;
&lt;li&gt;初始化部分的性能可以较大幅度的优化&lt;&#x2F;li&gt;
&lt;li&gt;目前没有实现从面料上取出零件，而是全部删了重新放&lt;&#x2F;li&gt;
&lt;li&gt;尝试完全消除固有精度误差&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;zhen-dui-bu-tong-gui-mo-bu-tong-xing-zhuang-fen-bu-de-shu-ju-ji-geng-hao-de-gua-ying&quot;&gt;针对不同规模不同形状分布的数据集更好的适应&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;搜集、构建更广泛的数据集用于测试分析改进&lt;&#x2F;li&gt;
&lt;li&gt;进一步结合基于几何的方法，比如NFP、三角化等，为搜索放置策略提供更多的手段&lt;&#x2F;li&gt;
&lt;li&gt;进一步考虑组合放置策略，目前的贴合度贪心和左底遗传都是一个一个单独放置的&lt;&#x2F;li&gt;
&lt;li&gt;进一步尝试结合其他优化迭代方法，比如重叠移除等&lt;&#x2F;li&gt;
&lt;li&gt;左底遗传算法可能仍有较大的上升空间，可以进一步探索&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之农作物范围识别</title>
        <published>2021-02-13T00:00:00+00:00</published>
        <updated>2021-02-13T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/crop/" type="text/html"/>
        <id>https://www.freeopen.tech/crop/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：华南理工黄钦建等（冲啊大黄 团队 ）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;通过无人机航拍的地面影像，探索像素级农作物分类的算法，具体的分类目标为薏仁米、玉米、烤烟、人造建筑（复赛新增），其余所有位置归为背景类。&lt;&#x2F;p&gt;
&lt;p&gt;初赛、复赛提供的数据是同一片区域的航拍影像。其中初赛提供数据为农作物生长的早期（大多没长出来），分割难度较大。复赛数据农作物长势良好，并在初赛赛题基础上增加了一类“建筑”。&lt;&#x2F;p&gt;
&lt;p&gt;提供的label为与原始图像1:1大小的单通道图像（mask），像素的大小对应不同的标注类别。其中“烤烟”像素值为1，“玉米”像素值为2，“薏仁米”像素值为3，“人造建筑”像素值为4，背景类像素值为0.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;评估指标&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;评估指标为mIoU，榜上排名分数为所有计算所有类别IoU后取平均的结果。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-fen-xi&quot;&gt;数据分析&lt;&#x2F;h2&gt;
&lt;p&gt;图像分辨率超大，背景类占比远高于其它类别，mask无效的区域面积也较大，类别不平衡问题突出。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-qie-ge&quot;&gt;数据切割&lt;&#x2F;h2&gt;
&lt;p&gt;使用gdal库分割遥感影像，采用两种切割策略：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;策略一：以1024x1024的窗口大小，步长900滑窗，当窗口中mask无效区域比例大于7&#x2F;8则跳过，当滑动窗口中背景类比例小于1&#x2F;3时，增加采样率，减小步长为512；&lt;&#x2F;li&gt;
&lt;li&gt;策略二：以1024x1024的窗口大小，步长512滑窗，当滑动窗口中无效mask比例大于1&#x2F;3则跳过。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;mo-xing-xuan-ze&quot;&gt;模型选择&lt;&#x2F;h2&gt;
&lt;p&gt;DeeplabV3+ （注：决赛5个队伍中3个用了它），backbone为Xception-65和ResNet-101以及DenseNet-121。从 A榜分数看，不加任何trick时，DenseNet分数略高于另外两个，但是显存占用太大以及训练时间太长，在后来的方案里就舍弃了。决赛复现时，使用了两个Xception-65和一个ResNet-101投票，投票的每个模型用不同的数据训练，增加模型差异。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;crop&#x2F;.&#x2F;157137269012195351571372690503.png&quot; alt=&quot;157137269012195351571372690503&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;crop&#x2F;.&#x2F;157137272713270821571372727612.png&quot; alt=&quot;157137272713270821571372727612&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gai-jin-cuo-shi&quot;&gt;改进措施&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;peng-zhang-yu-ce&quot;&gt;膨胀预测&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;方格效应&lt;&#x2F;strong&gt;：如果直接做不重叠滑窗预测拼接，得到的预测结果拼接痕迹明显。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;原因分析&lt;&#x2F;strong&gt;：网络卷积计算时，为了维持分辨率进行了大量zero-padding，导致网络对边缘预测不准。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;膨胀预测&lt;&#x2F;strong&gt;：预测时，只保留预测结果的中心区域，舍弃预测不准的边缘。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;具体实现&lt;&#x2F;strong&gt;：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;填充右下边界至滑窗预测窗口大小的整数倍（方便切割）；&lt;&#x2F;li&gt;
&lt;li&gt;填充1&#x2F;2滑窗步长大小的外边框（考虑边缘数据的膨胀预测）；&lt;&#x2F;li&gt;
&lt;li&gt;以1024x1024为滑窗，512为步长，每次预测只保留滑窗中心512x512的预测结果（可以调整更大的步长，或保留更大的中心区域，提高效率）。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;ce-shi-zeng-qiang&quot;&gt;测试增强&lt;&#x2F;h3&gt;
&lt;p&gt;测试时，通过对图像水平翻转，垂直翻转，水平垂直翻转等多次预测，再对预测结果取平均可以提高精度，但相对的，推理时间也会大幅度升高。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;with torch.no_grad():
    for (image,pos_list) in tqdm(dataloader):
        # forward --&amp;gt; predict
        image = image.cuda(device) # 复制image到model所在device上
        predict_1 = model(image)
        
        # 水平翻转
        predict_2 = model(torch.flip(image,[-1]))
        predict_2 = torch.flip(predict_2,[-1])
        # 垂直翻转
        predict_3 = model(torch.flip(image,[-2]))
        predict_3 = torch.flip(predict_3,[-2])
        # 水平垂直翻转
        predict_4 = model(torch.flip(image,[-1,-2]))
        predict_4 = torch.flip(predict_4,[-1,-2])
        
        predict_list = predict_1 + predict_2 + predict_3 + predict_4   
        predict_list = torch.argmax(predict_list.cpu(),1).byte().numpy() # n x h x w
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;snapshot-ensemble&quot;&gt;snapshot ensemble&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;snapshot ensemble&lt;&#x2F;strong&gt; 是一个简单通用的提分trick，通过余弦周期退火的学习率调整策略，保存多个收敛到局部最小值的模型，通过模型自融合提升模型效果。详细的实验和实现可以看黄高老师ICLR 2017的这篇&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1704.00109.pdf&quot;&gt;论文&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;snapshot ensemble&lt;&#x2F;strong&gt; 另一个作用是作新方案的验证。深度学习训练的结果具有一定的随机性，在做新改进方案验证时，有时难以确定线上分数的小幅度提升是来自于随机性，还是改进方案really work。在比赛提交次数有限的情况下，&lt;strong&gt;snapshot ensemble&lt;&#x2F;strong&gt; 不失为一个稳定新方案验证的方法。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;hou-chu-li&quot;&gt;后处理&lt;&#x2F;h3&gt;
&lt;p&gt;对输出结果做最简单的填充孔洞和去除小连通域。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;biao-qian-ping-hua&quot;&gt;标签平滑&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;标签平滑&lt;&#x2F;strong&gt;想法参考了Hinton大神关于的&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1503.02531&quot;&gt;知识蒸馏&lt;&#x2F;a&gt;和&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1906.02629&quot;&gt;When does label smoothing help?&lt;&#x2F;a&gt;的工作，标签平滑训练的模型更加稳定和泛化能力更强。&lt;&#x2F;p&gt;
&lt;p&gt;在知识蒸馏中，用teacher模型输出的soft target训练的student模型，比直接用硬标签（onehot-label）训练的模型具有更强的泛化能力。我对这部分提升理解是：软标签更加合理反映样本的真实分布情况，硬标签只有全概率和0概率，太过绝对。知识蒸馏时teacher模型实现了easy sample 和 hard sample 的“分拣”（标签平滑），对hard sample输出较低的置信度，对easy sample 输出较高的置信度，使得student模型学到了更加丰富的信息。&lt;&#x2F;p&gt;
&lt;p&gt;参考相关论文的实验数据可以看出，软标签训练的模型类内更加凝聚，更加可分。&lt;&#x2F;p&gt;
&lt;p&gt;在图像分割任务中，每个像素的分类结果很大程度依赖于周围像素，基于此，即使不通过teacher模型，我们也可以发掘部分样本中的hard sample。&lt;&#x2F;p&gt;
&lt;p&gt;图像边缘：卷积时零填充太多，信息缺少，难以正确分类&lt;&#x2F;p&gt;
&lt;p&gt;不同类间交界处：类间交界难以界定，存在许多标注错误，训练时梯度不稳定；类间交界的点，往往只相差几个像素偏移，对网络来说输入信息高度相似，但训练时label 却不同，也是训练过程的不稳定因素。&lt;&#x2F;p&gt;
&lt;p&gt;针对性的解决方案是在图像边缘和类间交界设置过渡带，过渡带内的像素视为 hard sample作标签平滑处理，平滑的程度取决于训练时每个batch中 hard sample 像素占总输入像素的比例。而过渡带width的大小为一个超参数，在本次比赛中我们取 &lt;strong&gt;width = 11 个像素点&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;不加hard sample 的损失函数：&lt;&#x2F;p&gt;
&lt;p&gt;$$H(y,p)=\sum^{K}_{k=1}-y_klog(p_k)$$&lt;&#x2F;p&gt;
&lt;p&gt;对hard sample 加入平滑后：&lt;&#x2F;p&gt;
&lt;p&gt;$$H(y,p)=\sum_{k=1}^{K}-y_k^{easy}log(p_k^{easy}) + \sum_{k=1}^{K}-y_k^{hard}log(p_k^{hard})$$&lt;&#x2F;p&gt;
&lt;p&gt;$$y_k^{hard} = y_k(1-\alpha)+ \alpha&#x2F;K$$ &lt;&#x2F;p&gt;
&lt;p&gt;其中，$\alpha$用于控制标签的平滑程度，取值为每次输入数据中hard sample像素占输入数据的比例。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wei-biao-qian-ruan-biao-qian&quot;&gt;伪标签 + 软标签&lt;&#x2F;h3&gt;
&lt;p&gt;伪标签是分类比赛中常用的trick之一，在模型分数已经较高的情况下可以尝试。提分显著，但对A榜过拟合的风险极大。具体实施是：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;利用在测试集表现最好的融合模型结果作伪标签，用多组不同置信度阈值过滤数据，结合训练集训练模型；&lt;&#x2F;li&gt;
&lt;li&gt;对所有伪标签数据进行标签平滑，缓解伪标签中错误数据对网络训练影响；&lt;&#x2F;li&gt;
&lt;li&gt;选取多个snapshot的方法对模型进行自融合提高模型的泛化能力；&lt;&#x2F;li&gt;
&lt;li&gt;利用3的结果，更新伪标签，重复步骤1~3。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;zong-jie&quot;&gt;总结&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;膨胀预测消除边缘预测不准问题；&lt;&#x2F;li&gt;
&lt;li&gt;使用测试增强、消除空洞和小连通域等后处理提高精度；&lt;&#x2F;li&gt;
&lt;li&gt;使用snapshot模型自融合、标签平滑、伪标签等方法提高模型稳定性。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之铝型材表面瑕疵识别</title>
        <published>2021-02-13T00:00:00+00:00</published>
        <updated>2021-02-13T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/gd-defect/" type="text/html"/>
        <id>https://www.freeopen.tech/gd-defect/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：Are you OK 战队（中山大学曾兆阳等）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;在铝型材的实际生产过程中，由于各方面因素的影响，铝型材表面会产生裂纹、起皮、划伤等瑕疵，这些瑕疵会严重影响铝型材的质量。铝型材的表面自身会含有纹路，与瑕疵的区分度不高。传统人工肉眼检查十分费力，不能及时准确的判断出表面瑕疵，质检的效率难以把控。铝型材制造商迫切希望采用最新的AI技术来革新现有质检流程，自动完成质检任务，减少漏检发生率，提高产品的质量，使铝型材产品的生产管理者彻底摆脱了无法全面掌握产品表面质量的状态。&lt;&#x2F;p&gt;
&lt;p&gt;初赛数据量3000张图片，复赛数据量5000张图片，包含单瑕疵图片，多瑕疵图片，无瑕疵图片，用于参赛者设计图像识别算法。图片所含瑕疵类型总计10种，分别为：不导电、擦花、角位漏底、桔皮、漏底、喷流、漆泡、起坑、杂色、脏点。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;评价指标&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;参照2010年之后PASCALVOC的评估标准，检测框和真实框的交并比(IOU)阈值设定为0.5，同时，采用Interpolating all points方法插值获得PR曲线，并在此基础上计算mAP的值，计算10类瑕疵的mAP值作为赛手的分数。&lt;&#x2F;p&gt;
&lt;p&gt;本次大赛计算mAP时，对同一个ground-truth框，重复预测n次，取置信度(confidence)最高的预测框作为TP（true positive）样本，其余的n-1个框都作为FP(False positive)样本进行处理。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-fen-xi&quot;&gt;数据分析&lt;&#x2F;h2&gt;
&lt;p&gt;从数据中可以看到，&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;脏点&lt;&#x2F;code&gt;的占比面积特别小，&lt;code&gt;喷流&lt;&#x2F;code&gt;与背景很相似，&lt;code&gt;擦花&lt;&#x2F;code&gt;很不规则。&lt;&#x2F;li&gt;
&lt;li&gt;大部分的类别是十分均衡的，&lt;code&gt;脏点&lt;&#x2F;code&gt;这个类的数量较多。&lt;code&gt;缺陷框&lt;&#x2F;code&gt;的大小两级分化 比较严重。在这其中，小样本的缺陷框基本上都是&lt;code&gt;脏点&lt;&#x2F;code&gt;的类别&lt;&#x2F;li&gt;
&lt;li&gt;原始图片的分辨率非常的大，是1920*2560&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h2&gt;
&lt;p&gt;基本架构采用Faster R-CNN, backbone选取Resnet-101。&lt;&#x2F;p&gt;
&lt;p&gt;原图输入=&amp;gt;下采样2倍=&amp;gt;Resnet-101(下采样16倍)，也就是说，从原图到最后一层的卷积特征，空间大小一共下降了32倍（$60\cdot80$）。 由于之后每一个候选框特征会被缩放到 $7\cdot7$ 的大小，如果说本身缩放前的特征就非常的小，那么缩放之后的特征是 不具有判别力的。统计了一下数据集中边长 &amp;lt;=64 的样本，发现这类小样本占了整个数据集的10%，这会严重地影响性能。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;gai-jin-fang-an&quot;&gt;改进方案&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;te-zheng-jin-zi-ta&quot;&gt;特征金字塔&lt;&#x2F;h4&gt;
&lt;p&gt;为了解决这个问题，我们采用了学术界非常常用的特征金字塔结构来对网络进行改进。我们总结了一下，特征金字 塔在这个任务中具有两个优点：第一，低层的特征经过卷积，上采样操作之后和高层的信息进行融合在卷积神经网络中，高层，也就是后面的特征具有强的语义信息，低层的特征具有结构信息，因此将高低层的信息进行结合，是可以增强特征的表达能力的。第二，我们将候选框产生和提取特征的位置分散到了特征金字塔的每一层，这样可以增加小目标的特征映射分辨率，对最后的预测也是有好处的。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ke-bian-xing-juan-ji&quot;&gt;可变形卷积&lt;&#x2F;h4&gt;
&lt;p&gt;我们采用的第二个改进方案是Deformable Convolutoin可变形卷积。我们发现在数据集中，铝材的瑕疵有很多是这种条状的，传统正规的正方形结构的卷积对这种形状的缺陷处理能力还不够强。因此我们采用了可变形的卷积， 在卷积计算的过程中能够自动地计算每个点的偏移，从而从最合适的地方取特征进行卷积。右边的示意图大致描述 了可变形卷积的过程，它能够让卷积的区域尽可能地集中在缺陷上。&lt;&#x2F;p&gt;
&lt;p&gt;具体实现上，将原本resent结构的最后一个block改成了可变卷积，原因是在可变卷积的实现中，需要基于前面 的特征来学习一个偏移，前面的特征得足够强才能保证这个偏移不会乱学，因此我们只改动了最后一个block。总体 的框架还是跟前面FPN的一样。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;contextual-roi-pooling&quot;&gt;Contextual ROI Pooling&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;gd-defect&#x2F;.&#x2F;20210213120121.jpg&quot; alt=&quot;20210213120121&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;我们的第三个改进方案，是在提取ROI特征的时候，引入了context上下文信息，我们把这个操作叫做contextual roi pooling。我们用上面两个例子来说明上下文信息的好处。Faster R-CNN是一个先生成候选框，然后精调候选框的过程，那么第一步生成的候选框势必会有偏大或者偏小的情况。之前的方法可以理解成用框内部的信息来推断框的 位置，左边这个例子是框偏大的情况，根据内部信息是可以知道框应该往里调的，但是右边这个例子框偏小了， 我们能知道该往外调整，但是该调多少呢这个是无从知晓的。因此一个显而易见的想法，就是把整张图片的信息也 送给这个候选框当特征，这样相当于让每个候选框以整张图片作为参考，这样呢每个框就知道该往哪调了。&lt;&#x2F;p&gt;
&lt;p&gt;具体的实现是这样，我们把整张图片也作为一个roi，用同样的ROI Pooling提取全局的特征，然后跟每一个候选框 的特征相加，再进行后面的分类和回归操作。这样的实现只多进行了一个roi的特征提取和一个特征相加的操作， 却能大大地提升准确率。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;gd-defect&#x2F;.&#x2F;20210213120731.jpg&quot; alt=&quot;20210213120731&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;xun-lian-ji-qiao&quot;&gt;训练技巧&lt;&#x2F;h2&gt;
&lt;p&gt;数据集里面是有提供无缺陷样本的，我们也对这些图片进行了使用。 在检测器的训练过程中，有一步是正负样本的选择。我们在训练的时候使用了一个策略，每次会随机选择一张缺陷样本和一张无缺陷样本，然后训练的正样本会在缺陷图片中选择，负样本会在两张图片中都选择，两张图片的所有 正负样本合起来做一个OHEM（Online hard example mining），再进行后面的训练操作。这样的好处是，充分利用了无缺陷样本，&lt;strong&gt;增大了模型判别背景信息的能力&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;铝材的缺陷是具有翻转不变性的，将一张图片水平和竖直翻转之后，他的瑕疵信 息是不会变的，也就是说，我们将图片进行翻转之后，再将框做一个变换到对应的位置，这样可以构建出一批新的数据来。通过这样的数据扩增方式，我们把训练数据扩增了四倍，也因此&lt;strong&gt;提升了模型的鲁棒性&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;shi-yan-jie-guo-yan-jiu&quot;&gt;实验结果研究&lt;&#x2F;h4&gt;
&lt;p&gt;通过分析实验和结果，我们发现擦花和喷流差的原因是基本都是召回率较低。在生成检测结果的时候，用了softnms来提高模型分数。softnms的作用是在框之间互 相抑制的时候使用了较温和的策略，让被抑制过的框还有机会重新被选上，从而提高召回率。从实验结果可以看到，softnms在每个类上都有提升。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之布匹疵点智能检测</title>
        <published>2021-02-13T00:00:00+00:00</published>
        <updated>2021-02-13T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/guangdong-cloth/" type="text/html"/>
        <id>https://www.freeopen.tech/guangdong-cloth/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：哪儿都是坑啊 团队（徐光福等）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;在布匹的实际生产过程中，由于各方面因素的影响，会产生污渍、破洞、毛粒等瑕疵，为保证产品质量，需要对布匹进行瑕疵检测。布匹疵点检验是纺织行业生产和质量管理的重要环节，目前人工检测易受主观因素影响，缺乏一致性；并且检测人员在强光下长时间工作对视力影响极大。由于布匹疵点种类繁多、形态变化多样、观察识别难道大，导致布匹疵点智能检测是困扰行业多年的技术瓶颈。&lt;&#x2F;p&gt;
&lt;p&gt;大赛数据涵盖了纺织业中布匹的各类重要瑕疵，每张图片含一个或多种瑕疵。数据包括包括素色布和花色布两类，其中，素色布数据约8000张，用于初赛；花色布数据约12000张，用于复赛。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ping-gu-zhi-biao&quot;&gt;评估指标&lt;&#x2F;h4&gt;
&lt;p&gt;赛题分数计算方式: &lt;strong&gt;0.2ACC+0.8mAP&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;ACC&lt;&#x2F;strong&gt;：是有瑕疵或无瑕疵的分类指标，考察瑕疵检出能力。
其中提交结果name字段中出现过的测试图片均认为有瑕疵，未出现的测试图片认为是无瑕疵。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;mAP&lt;&#x2F;strong&gt;：参照PASCALVOC的评估标准计算瑕疵的mAP值。&lt;&#x2F;p&gt;
&lt;p&gt;本次大赛评分计算过程中，分别在检测框和真实框的交并比(IOU)在阈值0.1，0.3，0.5下计算mAP，最终mAP取三个值的平均值。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-fen-xi-ji-yu-chu-li&quot;&gt;数据分析及预处理&lt;&#x2F;h2&gt;
&lt;p&gt;本次赛题主要难点是小目标占比较大，尺度差异性大（长宽比），类别不均衡很严重，布匹花色信息容易和瑕疵点混淆。&lt;&#x2F;p&gt;
&lt;p&gt;官方提供了布匹的模板图片，该图片包含布匹的花色信息，有效利用模版，能提高泛化性。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zu-he-san-tong-dao&quot;&gt;组合三通道&lt;&#x2F;h3&gt;
&lt;p&gt;第一层用原图，第三层用模版，中间层用原图和模版的差异值。另外，为减轻模板的不对齐情况，对模板图片进行上下左右随机10个像素点左右的抖动。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h2&gt;
&lt;p&gt;目标检测问题的框架： Cascade rcnn + big backbone，由于比赛的评估指标map为0.1，0.3，0.5，须对cascade rcnn的RPN和三个串联的RCNN结构的阈值进行调整。&lt;&#x2F;p&gt;
&lt;p&gt;相关参数：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;backbone: ResNeXt + FPN + DCN + SE&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;pos_iou_thr: 0.5&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;net_iou_thr: 0.3&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;三个串联rcnn阈值分别为 0.3 、0.4、0.5&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;anchorshe-ji&quot;&gt;anchor设计&lt;&#x2F;h4&gt;
&lt;p&gt;对瑕疵进行聚类分析，针对不同的长宽比，精细设计anchor参数。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;qi-ta&quot;&gt;其他&lt;&#x2F;h4&gt;
&lt;p&gt;探索CenterNet、FCOS、REPPOINTS等基于anchor free的新方法。尝试改进版的BiFPN, 该结构在coco表现上非常好。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 FashionAI 服饰关键点定位</title>
        <published>2021-02-11T00:00:00+00:00</published>
        <updated>2021-02-11T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/ai-keypoints/" type="text/html"/>
        <id>https://www.freeopen.tech/ai-keypoints/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：李weite 及 bilibili 团队&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;服装的机器分析很容易受到衣服的尺寸和形状，相机拍摄的距离和角度甚至服装的显示方式或模型摆放方式的影响。对图像中服装关键点的检测可以帮助提高应用程序的性能，例如衣服的对齐，衣服局部属性的识别以及服装图像的自动编辑。&lt;&#x2F;p&gt;
&lt;p&gt;基于服装设计知识，定义了一套服饰的关键点，并梳理了在女装6大专业类别（上衣、外套、裤子、半身裙、连身裙、连身裤）下的具体定义，要求参赛者设计算法进行定位预测。官方提供的数据集含前五个类别（连身裤类别被省略，因为在现实世界中并不常见），包括41个子类别和24种关键点。此数据集中总共有100,000个带批注的图像。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
 &lt;img width=&quot;90%&quot; src=&quot;.&#x2F;TB16Z8fXQCWBuNjy0FaXXXUlXXa.png&quot; &#x2F;&gt;
 &lt;figcaption&gt;
 女装关键点图例
 &lt;&#x2F;figcaption&gt;
 &lt;&#x2F;p&gt;
&lt;h3 id=&quot;guan-jian-dian-ding-yi&quot;&gt;关键点定义&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Keypoints\Category&lt;&#x2F;th&gt;&lt;th&gt;Blouse&lt;&#x2F;th&gt;&lt;th&gt;Outwear&lt;&#x2F;th&gt;&lt;th&gt;Trousers&lt;&#x2F;th&gt;&lt;th&gt;Skirt&lt;&#x2F;th&gt;&lt;th&gt;Dress&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;neckline_left&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;neckline_right&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;center_front&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;shoulder_left&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;shoulder_right&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;armpit_left&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;armpit_right&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;waistline_left&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;waistline_right&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;cuff_left_in&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;cuff_left_out&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;cuff_right_in&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;cuff_right_out&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;top_hem_left&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;top_hem_right&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;waistband_left&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;waistband_right&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;hemline_left&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;hemline_right&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;crotch&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bottom_left_in&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bottom_left_out&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bottom_right_in&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bottom_right_out&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;Y&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;td&gt;N&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;TOTAL&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;zhu-jie-ge-shi&quot;&gt;注解格式&lt;&#x2F;h3&gt;
&lt;p&gt;注释文件保存在csv格式表中，共有26列：第一列（image_id）包含图像文件名，第二列（image_category）表示图像所属的类别，其余24列记录了 上述24个关键点的位置。 仅显示两个图像项的示例表如下所示： &lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;image_id,image_category,neckline_left,neckline_right,center_front,shoulder_left,shoulder_right,armpit_left,armpit_right,waistline_left,waistline_right,cuff_left_in,cuff_left_out,cuff_right_in,cuff_right_out,top_hem_left,top_hem_right,waistband_left,waistband_right,hemline_left,hemline_right,crotch,bottom_left_in,bottom_left_out,bottom_right_in,bottom_right_out
Images&#x2F;blouse&#x2F;d21eab37ddc74ea5a5f1b4a5d3d9055a.jpg,blouse,241_135_1,301_135_1,259_136_1,216_142_1,319_144_1,212_186_1,307_202_1,-1_-1_-1,-1_-1_-1,203_236_1,195_256_1,278_241_1,283_261_1,206_243_0,292_252_0,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1
Images&#x2F;blouse&#x2F;02b54c183d2dbd2c056db14303064886.jpg,blouse,244_76_1,282_76_1,257_99_1,228_81_0,303_85_1,222_134_1,295_131_1,-1_-1_-1,-1_-1_-1,199_153_1,178_100_0,293_173_1,332_150_1,229_161_1,297_162_0,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1,-1_-1_-1
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;每个关键点由一个三元组表示，每个元素由下划线连接，表示为“ x_y_v”，其中x和y为坐标，v为可见性。 如果关键点可见，则可见性等于1；如果关键点被遮挡，则可见性等于0；如果类别中不存在或未定义，则可见性等于-1。 &lt;&#x2F;p&gt;
&lt;h3 id=&quot;ping-jie-biao-zhun&quot;&gt;评价标准&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;ai-keypoints&#x2F;.&#x2F;TB1nI4_FY1YBuNjSszeXXablFXa.tfsprivate.png&quot; alt=&quot;TB1nI4_FY1YBuNjSszeXXablFXa.tfsprivate&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Normalized Error（NE）用于评估提交的结果。 NE是预测关键点位置和注释位置之间的平均归一化距离。 注意，NE计算仅涉及可见的关键点。&lt;&#x2F;p&gt;
&lt;p&gt;其中 $k$ 为关键点 ID，$d_k$为预测关键点位置与带注释的关键点位置之间的距离，$s_k$为距离归一化参数（对于上衣，外套和衣服，它等于两个腋窝点之间的欧几里得距离;对于裤子和 裙边等于两个腰带点之间的距离），$v_k$即关键点的可见性。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;jie-ti-si-lu&quot;&gt;解题思路&lt;&#x2F;h2&gt;
&lt;p&gt;第一阶段：使用检测器把女装在图片上的位置检测出来；&lt;&#x2F;p&gt;
&lt;p&gt;第二阶段：再针对女装位置，做准确关键点的定位。&lt;&#x2F;p&gt;
&lt;p&gt;通过第一阶段的处理可以给第二阶段输入更干净的数据。&lt;&#x2F;p&gt;
&lt;p&gt;第一阶段的模型采用 Faster-RCNN，backbone resnet101&lt;&#x2F;p&gt;
&lt;p&gt;第二阶段的模型为自定义，命名为 asymmetric and dilated stacked hourglass networks 。参考google CVPR 2017年论文拟合两种信息，第一种信息判断每一个像素点是否在关键点的领域范围内，如果是在领悟范围内，则标注为临近点；拟合的第二种信息是临近点和关键点之间的向量偏差，有了这个向量偏差，就可以把预测出的向量点投票到对应偏差的坐标点上，这时就能取得一个准确的关键点定位。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h3&gt;
&lt;p&gt;asymmetric and dilated stacked hourglass networks (SHN)，非对称并带孔的SHN模型。主要特征为：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;有一个非对称的 encoder-decoder 结构；&lt;&#x2F;li&gt;
&lt;li&gt;设计了一个重量级的encoder（保有更多的空间信息）和一个轻量级的decoder；&lt;&#x2F;li&gt;
&lt;li&gt;对 encoder-decoder 结构进行叠加，通过多个stage对keypoints进行定位。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;采用非对称结构的理由，设计者认为重量级的encoder可以用来迁移学习，须保有更多空间信息，而decoder如果参数太多，就享受不到encoder初始化参数带来的好处。&lt;&#x2F;p&gt;
&lt;p&gt;当空间信息的重要性高于语义信息时的模型设计：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;图片尺寸的减少比例，从传统的 64 =&amp;gt; 32 =&amp;gt; 16 =&amp;gt; 8 改为 64 =&amp;gt; 32 =&amp;gt; 32 =&amp;gt; 32&lt;&#x2F;li&gt;
&lt;li&gt;采用空洞卷积&lt;&#x2F;li&gt;
&lt;li&gt;因为增大了空间尺度，通过减少通道数量(channel numbers)使模型的计算量不增加，即512(128) =&amp;gt; 1024(256) =&amp;gt; 2048(512) 改为 256(128) =&amp;gt; 512(128) =&amp;gt; 512(128)&lt;&#x2F;li&gt;
&lt;li&gt;不增加计算量的前提下，堆叠网络，即 256(128) =&amp;gt; 512(128) =&amp;gt; 512(128)改为两个  256(64) =&amp;gt; 512(64) =&amp;gt; 512(64) &lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;mo-xing-yan-bian-tu&quot;&gt;模型演变图&lt;&#x2F;h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: left&quot;&gt;Model&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;Param Size&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;FLOPs&lt;&#x2F;th&gt;&lt;th style=&quot;text-align: center&quot;&gt;Scores&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;ResNet50 backbone + 1 stage&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;49M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;7.86G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;4.11&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ 2 stage + more encoder layers&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;91M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;18.52G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.4x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ pre-trained&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;91M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;18.52G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.1x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ more data&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;91M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;18.52G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.1x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ ResNet101 + large input(352) + more channles&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;402M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;100G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.1x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;+ increase the number of boxes(more-crops)&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;402M&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;100G&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;-0.0x&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Final Submission&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;402M&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;strong&gt;100G&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td style=&quot;text-align: center&quot;&gt;3.30&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;qi-ta-ji-qiao&quot;&gt;其他技巧&lt;&#x2F;h2&gt;
&lt;p&gt;观察到难样本分布不均，采取在线难样本挖掘，使得训练聚集到难样本。难样本搜寻不是按照传统的在一张图中查找，而是在一个batch中搜寻。&lt;&#x2F;p&gt;
&lt;p&gt;最终提交模型时给了两个模型版本，一个高速版本一个低速版本，两个版本的分数差异不大，以证明模型的实用性很强。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 FashionAI 服饰属性标签识别</title>
        <published>2021-02-11T00:00:00+00:00</published>
        <updated>2021-02-11T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/ai-reconition/" type="text/html"/>
        <id>https://www.freeopen.tech/ai-reconition/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：西安交大在读博士及硕士组成的“禾思众成”团队&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;ren-wu-shuo-ming&quot;&gt;任务说明&lt;&#x2F;h2&gt;
&lt;p&gt;服装属性是时装领域的基础知识，它既庞大又复杂。 我们构建了一个层次结构的属性树作为结构化的分类目标，以描述服装的认知过程。 邀请您设计算法来识别服装图像的属性。 此任务可能会广泛应用于服装图片搜索，导航标签，混搭推荐等。 &lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;ai-reconition&#x2F;.&#x2F;cloth-attributes.png&quot; alt=&quot;女装属性树&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;要求检测上图中所有可区分的服装属性标签。 统计下来，训练集的图片约18万张，8个大类（4个设计属性，包括各种领口的款式，4个长度属性，包括衣长、裙长、裤长、袖长），54个小类。 &lt;&#x2F;p&gt;
&lt;h3 id=&quot;biao-qian&quot;&gt;标签&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;ai-reconition&#x2F;.&#x2F;cloth-model.png&quot; alt=&quot;模特及标签&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;a）上图的数据由训练有素的注释器标记。 然后，时尚专家会仔细检查这些标签，以确保较高的标签准确性。 带注释的数据中存在一定数量的缺失标签。 例如，图像中可能只有一个颈部设计标签，且颈部设计和袖子长度可见。 不再标记袖子长度以保持每个属性维度的数据均匀性。&lt;&#x2F;p&gt;
&lt;p&gt;b）为这条赛道选择了八个主要属性尺寸，即领口设计(Neckline)、领子设计(Collar)、高领设计(High Neck)、翻领设计(Lapel)、袖子长度(Sleeves length)、上衣长度(Length of top)、裙长(Length of skirt)和裤子长度(Length of trousers)。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shu-ju-te-zheng&quot;&gt;数据特征&lt;&#x2F;h3&gt;
&lt;p&gt;a）互斥：特定属性维度下的属性值是互斥的。例如，在高领设计尺寸中，高领和荷叶边半高领不能在同一图像中共存。必须注意一件事：考虑到挑战的严峻性，为了保证属性的相互排斥，我们放弃了一些特定的图像，在这些图像中，模型穿着多个重叠的服装，从而在一维中生成多个不同的属性。&lt;&#x2F;p&gt;
&lt;p&gt;b）独立性：不同维度下的属性值可以共存于单个图像中，并且彼此独立。例如，“脖子高领设计－龟颈”和“脖子领设计－衬衣领”可以共存于单个图像中。&lt;&#x2F;p&gt;
&lt;p&gt;c）在每个属性维度下，都有一个“不可见”值。这意味着在透视图中定义了特定的属性（顶视图，底视图或身体外观），但在特定图像中未出现或被遮挡。例如，给定一个穿着连衣裙的模特的图像，该图像包含两个透视图，即顶部外观和底部外观。裙子的下摆被遮挡，因此裙子的长度尺寸将被标记为“不可见”。该算法应考虑这种“否定”。但是，我们将不检查在相应透视图中未定义的属性的求反能力。例如，就像只有底部外观的裤子图像一样，我们将不检查其顶部外观的属性（例如“袖长”）。 &lt;&#x2F;p&gt;
&lt;h3 id=&quot;zhu-shi-wen-jian-de-ge-shi&quot;&gt;注释文件的格式&lt;&#x2F;h3&gt;
&lt;p&gt;ImageName：与“ Images”文件夹中特定图像文件相对应的图像名称。 &lt;&#x2F;p&gt;
&lt;p&gt;AttrKey：属性尺寸，例如袖长（sleeve_length_labels），裤子长度（pant_length_labels）等。 &lt;&#x2F;p&gt;
&lt;p&gt;AttrValues：与AttrKey中的属性维相对应的属性值。 例如，袖长尺寸有9个值：不可见，无袖，杯形袖，短袖，中长，3&#x2F;4袖，腕长袖，长袖和超长袖子，分别对应于“ nnnnnnmyn” 上图中的注释。 批注总共包含九个数字，每个数字代表以下三个字母之一： y（表示“是”，“必须”），m（表示“可能”，“可能”）和n（表示“否”，“必须”） 对于给定图像中的每个属性维度，可以有一个且只有一个“ y”带注释的数字，其他数字可以为“ m”或“ n”。 &lt;&#x2F;p&gt;
&lt;h4 id=&quot;mo-leng-liang-ke-de-bian-jie-de-ding-yi&quot;&gt;模棱两可的边界的定义&lt;&#x2F;h4&gt;
&lt;img src=&quot;.&#x2F;TB1hH7vXmBYBeNjy0FeXXbnmFXa.tfsprivate.png&quot; alt=&quot;TB1hH7vXmBYBeNjy0FeXXbnmFXa.tfsprivate&quot; style=&quot;zoom:48%;&quot; &#x2F;&gt;
&lt;p&gt;上面的示例图中出现歧义。 特别地，套筒长度在“长袖”和“超长袖”之间，但是前者的重量略大于后者。 在这种情况下，“长袖”数字标注为“ y”，“超长袖”数字标注为“ m”，其余数字为“ n”。 这种歧义经常发生，在现实世界的服装属性注释中是不可避免的。 &lt;&#x2F;p&gt;
&lt;p&gt;服装属性注释中的遮挡也是不可避免的。 对于裁剪了衣服的下摆的图像，很难准确预测衣服的长度。 在这种情况下，“不可见”数字应标记为“ y”，其他数字应标记为“ n”。 因此，“ skirt_length_labels” 被注释为“ ynnnnn”。 &lt;&#x2F;p&gt;
&lt;h3 id=&quot;ping-jie-biao-zhun&quot;&gt;评价标准&lt;&#x2F;h3&gt;
&lt;p&gt;计算所有属性维度的AP的均值以获得mAP，mAP用作服装属性识别数据集的最终排名得分。&lt;&#x2F;p&gt;
&lt;p&gt;另外，官方提出了BasicPrecision标准。当评估测试集的所有预测结果（ProbThreshold = 0）时，它是所有属性维度上的平均准确性。BasicPrecision是对准确性的更直接的估计，因此是合理的参考。通常，当BasicPrecision = 0.7时，排名得分（mAP）约为0.93。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-she-ji&quot;&gt;模型设计&lt;&#x2F;h2&gt;
&lt;p&gt;图片显示的服装分为两类，一类为模特图片，拥有两个或两个以上的属性，一类为服装平铺图，只有一个属性。属性也分为两类，一类长度属性，一类设计属性。对于长度属性，更关注整体风格和上下关联，对于设计属性，更关注细节区别。因此在模型设计上，须兼顾这些特点，思考如何挖掘无标签属性以及每个任务之间的关系。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;shu-ju-zeng-qiang&quot;&gt;&lt;strong&gt;数据增强&lt;&#x2F;strong&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;采用random crop， random flip，random erasing,  random border，都是标准方法，不再赘述。参考其他团队的方法，还有种更好的方法是用目标检测模型找出模特和衣服，然后做数据增强效果会更好。因为这时的目标图片像素分布会比较均匀，减少了背景的干扰。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;zheng-ti-kuang-jia&quot;&gt;整体框架&lt;&#x2F;h4&gt;
&lt;img src=&quot;.&#x2F;20210212171535.jpg&quot; alt=&quot;20210212171535&quot; style=&quot;zoom:50%;&quot; &#x2F;&gt;
&lt;p&gt;整体框架采用半监督联合训练，先训练好的 teacher model 对一张图片的未标记属性进行预测，将预测结果作为soft label， 联合已知label对图片进行联合训练。这样， 一个模型被多个任务共同监督，这时任务之间的关联性能够被更好的发掘。&lt;&#x2F;p&gt;
&lt;p&gt;针对属性的差异性，和大多数参数团队一样，采用分任务训练，设计属性训练一个模型，长度属性训练一个模型，可参考第二名的方案，对长度属性的损失函数做专门设计，对预测与label相差距离更大的给更大权重，输出更大损失。第三名方案改进了长度属性的label, 一个1代表第一类，三个1代表第三类，六个1代表第六类，效果也有百分点的提升。&lt;&#x2F;p&gt;
&lt;p&gt;soft label 的技术，使用多个模型预测同一张图片，这些模型有Resnet、Inception、NAS 、DPN，即已经训练好的教师模型，每个教师模型专注于各自的任务类型，然后把预测出来的 label 加进学生网络，联合监督学生网络的训练。&lt;&#x2F;p&gt;
&lt;p&gt;Net2Net模型（将一个神经网络中的知识快速转移到另一个神经网络中），把原模型直接转成更高分辨率，兼顾感受野保同性和函数保同性，计算量远小于把图片分辨率提高所增加的计算量。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;gan-shou-ye-bao-tong-xing&quot;&gt;感受野保同性&lt;&#x2F;h4&gt;
&lt;p&gt;降采样层增加后续卷积层的感受野，极大减少模型计算量，但是免不了会丢失一些信息。而dilated卷积不会丢失信息，因此用空洞卷积替换掉降采样层。 下面是换算公式，替换stage3和stage4的降采样层，在保持感受野增加的前提下，最后输出的分辨率提高了4倍。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;ai-reconition&#x2F;.&#x2F;20210211175358.jpg&quot; alt=&quot;20210211175358&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;优点：Net2Net 在不同计算能力的设备上转换模型，不需要重新训练。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;han-shu-bao-tong-xing&quot;&gt;函数保同性&lt;&#x2F;h4&gt;
&lt;p&gt;基本原理为多出来的参数维度，随机拷贝原来参数的数据，再将多出来的数值求平均，来保证原始模型的知识保有。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;stochasitc-weight-average&quot;&gt;Stochasitc Weight Average&lt;&#x2F;h4&gt;
&lt;p&gt;将训练后的参数在参数空间之间平均，从而得到更鲁棒的模型，即不需要付出太多代价，就能得到一个融合的模型。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;qi-ta-si-lu&quot;&gt;其他思路&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;把&lt;code&gt;模特图&lt;&#x2F;code&gt; 和 &lt;code&gt;平铺图&lt;&#x2F;code&gt; 分别训练&lt;&#x2F;li&gt;
&lt;li&gt;把&lt;code&gt;可见属性&lt;&#x2F;code&gt;和&lt;code&gt;不可见属性&lt;&#x2F;code&gt;分别训练&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 高德算法大赛</title>
        <published>2021-02-10T00:00:00+00:00</published>
        <updated>2021-02-10T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/amap/" type="text/html"/>
        <id>https://www.freeopen.tech/amap/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：北邮在读博士 朱奕达及团队&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;bi-sai-ren-wu&quot;&gt;比赛任务&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;chu-sai-shu-ju&quot;&gt;&lt;strong&gt;初赛数据：&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;&#x2F;strong&gt;：给定一组含有GPS时间的图像序列（包含3-5帧图像），其中一幅图像作为参考帧。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;&#x2F;strong&gt;：以参考帧为准，输出该图像序列对应的路况状态（畅通、缓行和拥堵）。&lt;&#x2F;p&gt;
&lt;p&gt;图像序列由行车记录仪拍摄，路况真值（ground truth）是对应道路当前时刻真实的路况状态。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;数据信息：&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;图像序列的参考帧图像名。&lt;&#x2F;li&gt;
&lt;li&gt;图像序列的路况状态。&lt;&#x2F;li&gt;
&lt;li&gt;0：畅通，1：缓行，2：拥堵，-1：测试集真值未给出。&lt;&#x2F;li&gt;
&lt;li&gt;每帧图像采集时刻的GPS时间。&lt;&#x2F;li&gt;
&lt;li&gt;单位为秒。如GPS时间 1552806926 比 1552806921 滞后5秒钟。&lt;&#x2F;li&gt;
&lt;li&gt;A榜测试数据集换成高速行车记录仪采集的数据，GPS时间间隔缩短为1-2秒。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;fu-sai-shu-ju&quot;&gt;&lt;strong&gt;复赛数据：&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;新增&lt;code&gt;封闭&lt;&#x2F;code&gt;的道路路况&lt;&#x2F;li&gt;
&lt;li&gt;移除了GPS时间信息&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;shu-ju-fen-xi&quot;&gt;数据分析&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;前车遮挡视野内道路情况，从而影响对道路的判断；&lt;&#x2F;li&gt;
&lt;li&gt;对向车道和路边停靠车辆对行驶车道路况判断的影响；&lt;&#x2F;li&gt;
&lt;li&gt;数据众包导致相机安装存在角度偏差，从而导致图像角度不一致；&lt;&#x2F;li&gt;
&lt;li&gt;大雾天气和夜晚导致的图像不清晰。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;复赛阶段数据集：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;存在重复采样的图像序列（多为封闭类型的路况）&lt;&#x2F;li&gt;
&lt;li&gt;数据不均衡，缓行只有100多个序列，而封闭路段有2000多个序列&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;te-zheng-gong-cheng&quot;&gt;特征工程&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;fen-che-dao-mu-biao-jian-ce&quot;&gt;分车道目标检测&lt;&#x2F;h3&gt;
&lt;p&gt;目标检测采用 Faster RCNN 模型，将道路车辆分成5个类别进行标注，分别是：&lt;strong&gt;当前车辆前方行驶车道车辆，同向行驶车道左侧车辆，同向行驶车道右侧车辆，对向车道行驶车辆，街边侧向停车车辆&lt;&#x2F;strong&gt;。通过这样的标注对当前道路环境的车辆进行细致区分，后续将通过目标检测结果提取不同车道的车辆信息特征。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;te-zheng-ti-qu&quot;&gt;特征提取&lt;&#x2F;h3&gt;
&lt;p&gt;基于目标检测的结果，提取60维特征，包括：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;GPS时间特征&lt;&#x2F;li&gt;
&lt;li&gt;关键帧中不同车道的车辆数量、面积、距离等&lt;&#x2F;li&gt;
&lt;li&gt;不同帧间检测框的动态特征，如车辆相似度、数目和面积变化等&lt;&#x2F;li&gt;
&lt;li&gt;Focal loss 降低样本不平衡&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;其中：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;距离通过检测框的中心点，计算两点间的直线距离&lt;&#x2F;li&gt;
&lt;li&gt;不同分车道设计了两个权重，一为box数量与时间间隔的关系，数量增加分数降低，数量减少分数增加；二为box大小与时间间隔的关系，大小增加分数降低，大小减少分数增加。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;基于车辆相关特征训练 LGB 模型，B榜分数为0.6108。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shu-ju-kuo-chong-yu-zeng-qiang&quot;&gt;数据扩充与增强&lt;&#x2F;h3&gt;
&lt;p&gt;复赛阶段，针对类别数量的差异性，对缓行和拥堵类别做了数据扩充。对同一个序列中的图片，对每一帧图像都做了相同的数据变化，保证序列图片所处的相对环境是一致的。具体方法有：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;模型输入按照采样顺序选择3张图片；&lt;&#x2F;li&gt;
&lt;li&gt;针对相机安装存在角度偏差的问题，使用了平移，尺度加旋转变换的数据增强方法；&lt;&#x2F;li&gt;
&lt;li&gt;考虑到视频在采集时包含了一天中的不同时间节点，光照强度和天气条件也对图片造成了较大的影响， 加入了对比度，亮度和颜色增强的方法尽可能贴合实际数据。&lt;&#x2F;li&gt;
&lt;li&gt;从赛方给的数据中我们发现图片的清晰度和质量在不同序列之间存在差异，为此引入了运动模糊，中值滤波，高斯滤波，高斯模糊等方法进行数据增强。&lt;&#x2F;li&gt;
&lt;li&gt;此外图像中还存在一些脱敏信息，比如马赛克或者黑色条，我们也引入了一些cutof操作，减少模型对图像特定部分的依赖。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;mo-xing-zeng-qiang&quot;&gt;模型增强&lt;&#x2F;h2&gt;
&lt;p&gt;考虑到&lt;strong&gt;道路情况不止和车辆信息有关，还和道路及场景相关&lt;&#x2F;strong&gt;，基于分车道目标检测模型只对图像中的车道信息进行提取，而忽略了图片中的其他场景细节。并且目标检测模型本身存在一些误差，将目标检测结果特征再训练一个 LGB 模型可能会对误差进行传递。&lt;&#x2F;p&gt;
&lt;p&gt;端到端模型方案：即训练一个考虑图片全局信息的序列检测模型。&lt;&#x2F;p&gt;
 
&lt;img src=&quot;densnet-amap.png&quot; alt=&quot;端到端模型&quot; width=&quot;1080&quot; height=&quot;344&quot; &#x2F;&gt;
&lt;p&gt;图片输入窗口大小是3，采用基于DenseNet121先对每张图片提取全局特征后，将图片特征按照时间顺序输入到GRU模型中得到最后的分类结果，在初赛中B榜上我们并没有使用数据增强的方法，最后取得了第一名的成绩0.6614。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shi-kong-te-zheng-ti-qu&quot;&gt;时空特征提取&lt;&#x2F;h3&gt;
&lt;p&gt;在复赛中，赛题发生了变化，新增了&lt;code&gt;封闭&lt;&#x2F;code&gt;这个类别后，基于特征的方案中需要根据障碍物标注进一步提高目标检测模型的鲁棒性，而实际障碍物标注数据质量并不高，尝试建模后发现效果并不如意。&lt;&#x2F;p&gt;
&lt;p&gt;为了减少误差的传递并考虑图片中更多的信息，基于端到端序列检测模型，提取了不同frame的特征后，再使用特征融合模块挖掘图片序列之间的时序特征关系。&lt;&#x2F;p&gt;
&lt;p&gt;特征融合模块中仍然使用双向GRU模型挖掘时间序列特征。将backbone替换成resnest101和SE_ResNext，在空间特征挖掘中，使用了&lt;strong&gt;位置注意力模块&lt;&#x2F;strong&gt;和&lt;strong&gt;通道注意力模块&lt;&#x2F;strong&gt;的双注意力网络。通道注意力模块校准序列图片中的特征图通道之间的关系，位置注意力模块对特征图中不同position进行关系强度的计算，从而挖掘序列图片中的局部关系变化和全局关系变化。&lt;&#x2F;p&gt;
&lt;p&gt;数据增强和样本扩充后，进行5折交叉验证的训练，选择了 5 个模型进行基于概率的融合，最后在B榜上得到了第二名的成绩0.7237，比第一名低了万分之9个点。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-luo-di-ji-zhan-wang&quot;&gt;模型落地及展望&lt;&#x2F;h2&gt;
&lt;p&gt;在实际场景下，不仅仅有图片序列信息，还有道路等级数据，GPS时间，POI点等信息，可以将这些路网和时间信息通过 embedding 等方式处理成相应的特征融合到我们的模型中，更丰富的感知源丰富了我们的时空信息特征空间。&lt;&#x2F;p&gt;
&lt;p&gt;除此之外，还可以不断优化障碍物和车辆目标检测模型识别精度，从而制定更多的约束规则进行前处理和后处理的规则约束。&lt;&#x2F;p&gt;
&lt;p&gt;在模型落地时，可以通过模型蒸馏、优化加速等方式将模型下放到边缘节点进行分布式计算。交通路况状态在局部区域是互相影响的，所以我们还可以根据路网信息对城市进行栅格化处理，将众包得到的数据分栅格进行管理，这样我们可以得到该路段信息的临近路况辅助修正模型结果。&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>冠军方案之 Apache Flink 极客挑战赛</title>
        <published>2021-02-10T00:00:00+00:00</published>
        <updated>2021-02-10T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/flink/" type="text/html"/>
        <id>https://www.freeopen.tech/flink/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;冠军：合肥工大 SkyPeaceLL&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;bi-sai-ren-wu&quot;&gt;比赛任务&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;shu-ju-ji&quot;&gt;数据集&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;新冠病例行动数据集&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;病例历史行动数据集（训练集1） 1M+&lt;&#x2F;li&gt;
&lt;li&gt;确诊病例数据 （测试集1） 500+&lt;&#x2F;li&gt;
&lt;li&gt;实时病例行动数据集（测试集2） 1000+&lt;&#x2F;li&gt;
&lt;li&gt;人脸特征512维&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;天猫精灵行为数据集&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;天猫精灵历史行为数据集（训练集2） 1M+&lt;&#x2F;li&gt;
&lt;li&gt;用户行为数据集（测试集3） 500+&lt;&#x2F;li&gt;
&lt;li&gt;实时用户行为数据集（测试集4） 1000+&lt;&#x2F;li&gt;
&lt;li&gt;行为特征700维&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;si-ge-ren-wu&quot;&gt;&lt;strong&gt;四个任务&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;根据测试集1每条数据的特征向量，在训练集1中找出该病例（人）对应的所有记录。&lt;&#x2F;li&gt;
&lt;li&gt;对测试集2的每条数据，根据其特征向量进行实时分类（人）。&lt;&#x2F;li&gt;
&lt;li&gt;根据测试集3每条数据的特征向量，在训练集2中找出该用户行为（领域+意图）对应的所有记录。&lt;&#x2F;li&gt;
&lt;li&gt;对测试集4的每条数据，根据其特征向量进行实时分类（领域+意图）。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;xing-neng-yao-qiu&quot;&gt;&lt;strong&gt;性能要求&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;四个任务总运行时间不能超过3小时。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;对每条实时数据完成实时分类的响应时间不能超过500ms。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;平台和组件&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Flink，PyFlink，Flink ai_flow，Proxima，Intel Zoo cluster serving&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;gong-zuo-liu&quot;&gt;工作流&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;flink&#x2F;flink-workflow.png&quot; alt=&quot;flink-workflow&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;官方提供一套docker环境及baseline代码，由于新冠病例和天猫精灵在算法任务上的相似性，编写两个 workflow 配置文件(yaml)，使得一套python代码运行两个算法。&lt;&#x2F;p&gt;
&lt;p&gt;基本思路：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;并发读入100万条训练集（并发100），训练AutoEncoder模型，为高维特征降维；&lt;&#x2F;li&gt;
&lt;li&gt;用训练好的模型对特征向量降维（并发16）；&lt;&#x2F;li&gt;
&lt;li&gt;用Proxima HnswBuilder 对降维后的特征向量创建索引；&lt;&#x2F;li&gt;
&lt;li&gt;对测试集1中的样本选出Top1024+1（样本）个候选者；&lt;&#x2F;li&gt;
&lt;li&gt;对候选者进行聚类（算法：Chinese Whisper），得出任务1的结果&lt;&#x2F;li&gt;
&lt;li&gt;从kafka读取测试集2，选出Top1，并以对应UUID作为分类label， 得出任务2的结果。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;shu-ju-yu-chu-li&quot;&gt;数据预处理&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;病例行动数据集&lt;&#x2F;strong&gt;
不含异常数据，且特征向量已经L2 Normalization，不需要特别的预处理。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;天猫精灵行为数据集&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;存在一些异常数据，需要做以下预处理：&lt;&#x2F;p&gt;
&lt;p&gt;移除某些特征向量数据末尾多出的空格（注：如果不做相应处理，score3通常得0分）&lt;&#x2F;p&gt;
&lt;p&gt;Re-generate UUID for duplicated UUID&lt;&#x2F;p&gt;
&lt;p&gt;Processing zero vector&lt;&#x2F;p&gt;
&lt;p&gt;Processing duplicated vector&lt;&#x2F;p&gt;
&lt;p&gt;L2 Normalization&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mo-xing-xun-lian&quot;&gt;模型训练&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;目标：对特征向量降维&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;算法比较：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Simple AutoEncoder （实测效果好，稳定，性能好，采用）&lt;&#x2F;li&gt;
&lt;li&gt;Deep AutoEncoder（实测效果好，性能一般，最终未采用）&lt;&#x2F;li&gt;
&lt;li&gt;VAE (Variational AutoEncoder) （实测效果相对较差，未采用）&lt;&#x2F;li&gt;
&lt;li&gt;PCA (Principal Component Analysis) （实测效果相对较差，未采用）&lt;&#x2F;li&gt;
&lt;li&gt;NMF (Non-negative matrix factorization) （实测效果相对较差，未采用）&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;模型参数&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Loss Function: MSE&lt;&#x2F;li&gt;
&lt;li&gt;Active Function: Linear&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;维度选择&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;新冠病例： 512 =&amp;gt; 128&lt;&#x2F;li&gt;
&lt;li&gt;天猫精灵： 700 =&amp;gt; 128&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;ji-zhu-zhan-xing-neng-zhi-biao&quot;&gt;技术栈性能指标&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Intel Zoo Cluster Serving&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;支持Tensorflow Saved Model 以及PyTorch Model for Inference&lt;&#x2F;li&gt;
&lt;li&gt;支持并发Inference（本赛题设置为16个并发），在多并发下运行稳定&lt;&#x2F;li&gt;
&lt;li&gt;模型针对CPU做了优化，无需GPU环境&lt;&#x2F;li&gt;
&lt;li&gt;自动生成配置，方便部署&lt;&#x2F;li&gt;
&lt;li&gt;响应时间短。平均每个请求响应时间实测小于35ms，充分满足本方案中的性能需求。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;达摩院proxima&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;使用Proxima HnswBuilder 创建索引，使用HnswSearch search vector&lt;&#x2F;li&gt;
&lt;li&gt;支持海量数据向量检索&lt;&#x2F;li&gt;
&lt;li&gt;召回率高，Top100 召回率超过98.5%&lt;&#x2F;li&gt;
&lt;li&gt;检索性能高，在本赛题中，平均每个请求(TopK=1024)的响应时间小于3ms，完全满足TopK筛选+再聚类这样类型的应用需求，对于实时的向量检索也毫无压力。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;online-data-kafka-chao-shi-wen-ti&quot;&gt;Online Data (kafka) 超时问题&lt;&#x2F;h2&gt;
&lt;p&gt;按参考代码标准流程，可能是初始化较慢的原因，在开始时有8秒延迟，将导致16条数据被超时。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;方法一、使用ai_flow 内建的算子&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用&lt;code&gt;ai_flow.read_example&lt;&#x2F;code&gt;、&lt;code&gt;ai_flow.predict&lt;&#x2F;code&gt;、&lt;code&gt;ai_flow.transform&lt;&#x2F;code&gt;和&lt;code&gt;ai_flow.write_example&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;在其中的&lt;code&gt;SourceExecutor&#x2F;SinkExecutor&lt;&#x2F;code&gt;实现类中使用PyFlink TABLE API(For Kafka) 读&#x2F;写Kafka Topic&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;为相应Flink job的&lt;code&gt;StreamExecutionEnvironment&lt;&#x2F;code&gt;设置参数：&lt;code&gt;stream_env.enable_checkpointing(250)&lt;&#x2F;code&gt;
该参数默认为&lt;code&gt;3000ms&lt;&#x2F;code&gt;，&lt;code&gt;3000ms&lt;&#x2F;code&gt;会导致每3秒才集中从Kafka Topic中读出6条数据。所以，如果不设置这个参数，必定会导致每6条数据中平均有5条会超时500ms，使得实时数据(score2和score4)得分很难超过100分（满分500分），因此必须改变这个参数设置。针对本赛题，可以设置为&lt;code&gt;250ms&lt;&#x2F;code&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;方法一在产线上应用没什么问题，但是在本比赛中它有一个小问题，那就是初始会有8秒延迟，这个延迟会使得赛题程序开始发送的约16条数据被TABLE API(For Kafka)读到时都会超时500ms，从而对最终评分有所影响（实测大概影响6分左右）。
使用方法一可确保只会有少量的初始数据（实测16条左右）产生超时。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;方法二、使用ai_flow的用户自定义算子&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;ai_flow支持更为灵活的用户自定义算子&lt;code&gt;af.user_define_operation&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;在用户自定义算子的Executor实现类中，直接使用Kafka Consumer&#x2F;Producer 读写Kafka Topic&lt;&#x2F;li&gt;
&lt;li&gt;直接通过 Kafka consumer从Kafka Topic读取数据，然后call Inference API (by Zoo cluster serving) 降维，然后使用Proxima search API search Top1 UUID，然后得出分类label，最后直接通过Kafka Producer 将结果数据写入Kafka Topic。
使用方法二可避免初始16条数据的超时问题，设置好关键参数 (如&lt;code&gt;fetch_max_wait_ms=200&lt;&#x2F;code&gt;)，可确保所有数据都不会超时。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</content>
        
    </entry>
</feed>
