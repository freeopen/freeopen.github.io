<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Freeopen - notes</title>
    <subtitle>Freeopen 的个人兴趣研究所.</subtitle>
    <link href="https://www.freeopen.tech/tags/notes/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://www.freeopen.tech"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2021-02-17T00:00:00+00:00</updated>
    <id>https://www.freeopen.tech/tags/notes/atom.xml</id>
    <entry xml:lang="en">
        <title>卷积备忘录</title>
        <published>2019-01-05T00:00:00+00:00</published>
        <updated>2021-02-17T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/convolution/" type="text/html"/>
        <id>https://www.freeopen.tech/convolution/</id>
        
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;学习了一段时间卷积，到了该整理的时候了。这是一篇不会写完的文章，它将跟进卷积技术的发展和演变，进行增补。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;juan-ji-de-shu-xue-yuan-li&quot;&gt;卷积的数学原理&lt;&#x2F;h2&gt;
&lt;p&gt;在图像处理中，卷积计算属于二维卷积的离散形式，公式如下：
$$(f*g)[n_1, n_2] = \sum_{m_1 = -\infty}^{+\infty}\sum_{m_2 = -\infty}^{+\infty}f[m_1, m_2]\cdot g[n_1 - m_1, n_2 - m_2]$$&lt;&#x2F;p&gt;
&lt;p&gt;其中，&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$(f*g)[n_1, n_2]$ 表示 $g$ 图像位于$(n_1,n_2)$ 坐标对应的值在过滤器 $f$ 上的卷积&lt;&#x2F;li&gt;
&lt;li&gt;$f[m_1,m_2]$ 为卷积核，方括号表示序列，
例如$f[m] = { \ldots, f_{-1}, f_0, f_1, f_2, \ldots }$&lt;&#x2F;li&gt;
&lt;li&gt;设$m_1$的取值范围为$[{-w\over 2}, \ldots, {w\over 2}]$，$m_2$的取值范围为$[{-h\over 2},\ldots, {h\over 2}]$, $g[n_1 - m_1, n_2 - m_2]$ 可理解为以图像$(n_1,n_2)$
位置为中心，宽w高h的一块矩形区域的所有值之列表&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;离散的二维卷积计算表现为卷积核（也称 &lt;code&gt;过滤器&lt;&#x2F;code&gt; ）在二维平面上平移， 且卷积核的每个元素与被卷积的图像像素点数值对位相乘，再求和。 为什么简单的相乘再求和运算，就能抽取出图像的特征呢？因为这种计算方式就是两个向量的内积计算，线性代数中两个向量的余弦相似度即是两个向量求内积再除以它们的模，或者先把两个向量标准化后再求它们的内积，所以卷积计算即是对相似度的衡量，其输出的值表示某区域图像与过滤器的相似程度。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;juan-ji-de-biao-shi&quot;&gt;卷积的表示&lt;&#x2F;h2&gt;
&lt;p&gt;在机器学习中，我们常以四个超参数来定义卷积，它们分别是:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;卷积核的尺寸 $K$ (kernal
size);&lt;&#x2F;li&gt;
&lt;li&gt;卷积核的数量 $C$ (number of channels);&lt;&#x2F;li&gt;
&lt;li&gt;卷积核平移的步幅 $S$ (stride);&lt;&#x2F;li&gt;
&lt;li&gt;以及被卷积对象边缘填充零的数量 $P$ (padding)。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;如果要表示&lt;a href=&quot;https:&#x2F;&#x2F;www.freeopen.tech&#x2F;convolution&#x2F;#kong-dong-juan-ji-dilated-conv&quot;&gt;空洞卷积&lt;&#x2F;a&gt;，还需要一个表示空洞数量的超参数，我喜欢把它记为 $D$。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;juan-ji-de-shu-chu-chi-cun&quot;&gt;卷积的输出尺寸&lt;&#x2F;h2&gt;
&lt;p&gt;卷积输出的空间尺寸的公式为:&lt;&#x2F;p&gt;
&lt;p&gt;$$ O = \lfloor {I - K + 2P \over S} + 1 \rfloor$$ &lt;&#x2F;p&gt;
&lt;p&gt;其中，&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$I$ 表示输入尺寸( size of input )&lt;&#x2F;li&gt;
&lt;li&gt;$O$ 表示输出尺寸( size of output )&lt;&#x2F;li&gt;
&lt;li&gt;$P$ 表示填充数量( padding )&lt;&#x2F;li&gt;
&lt;li&gt;$S$ 表示步幅数( strides )&lt;&#x2F;li&gt;
&lt;li&gt;$K$ 是过滤器或卷积核的尺寸( size of kernel )&lt;&#x2F;li&gt;
&lt;li&gt;$\lfloor \cdot \rfloor$ 符号表示向下取整，即不超过该值的最大整数。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;假如我们有一张 &lt;code&gt;32x32x3&lt;&#x2F;code&gt; 的输入图像，我们使用 10 个尺寸为 &lt;code&gt;3x3x3&lt;&#x2F;code&gt; 的过滤器，单步幅和零填充。根据上式的符号约定，
$W=32，K=3，P=0，S=1$， 输出深度等于过滤器的数量 10， 输出尺寸大小为 $((32 - 3 + 0)&#x2F;1) + 1 = 30$, 因此输出尺寸是 &lt;code&gt;30x30x10&lt;&#x2F;code&gt;。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;juan-ji-wang-luo-de-gan-shou-ye&quot;&gt;卷积网络的感受野&lt;&#x2F;h2&gt;
&lt;p&gt;感受野( Receptive Field )表示经过卷积网络输出的一个点能看到原始输入的范围大小。对仅有一层的卷积网络来说，感受野的大小就是卷积核的大小，但对于层叠网络来说，感受野的计算就会稍微复杂些。习惯上，我们把层叠模型中靠近原始输入端的叫做低层，把靠近最终结果的层称为高层，计算感受野的公式为：&lt;&#x2F;p&gt;
&lt;p&gt;$$r^{(l)} = r^{(l-1)} + (k^{(l)}-1)*j^{(l-1)} = r^{(l-1)} + (k^{(l)}-1)*\prod_{i=1}^{l-1}s^{(i)}$$&lt;&#x2F;p&gt;
&lt;p&gt;其中,&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$k$: kernel size 卷积核尺寸&lt;&#x2F;li&gt;
&lt;li&gt;$p$: padding size&lt;&#x2F;li&gt;
&lt;li&gt;$s$: stride size 步长&lt;&#x2F;li&gt;
&lt;li&gt;$n$: feature map size 特征图尺寸&lt;&#x2F;li&gt;
&lt;li&gt;$r$: receptive field size 感受野尺寸&lt;&#x2F;li&gt;
&lt;li&gt;$j$: feature map 上相邻元素间在原始图上的像素距离&lt;&#x2F;li&gt;
&lt;li&gt;$Layer$: 用$Layer$表示feature map，特别地 $Layer^{(0)}$为输入图像&lt;&#x2F;li&gt;
&lt;li&gt;$l$: 表示层，卷积层 $Conv^{(l)}$ ，其输入feature map 为 $Layer^{(l-1)}$, 输出为$Layer^{(l)}$ &lt;&#x2F;li&gt;
&lt;li&gt;$d$: dilation rate, 等于1时表示正常卷积，当dilation rate 大于1时，成为空洞卷积。比如，设1表示卷积核有效元素的值，0表示空洞，则一维卷积核“1001001”其间隔的空洞数为2，这时dilation rate = 3。所以空洞卷积相当于改变了卷积核的尺寸，只需将 $k$ 值替换为 $d*(k-1)+1$ 即可&lt;&#x2F;li&gt;
&lt;li&gt;$start$: feature map 左上角元素在输入图像上的感受野中心坐标$(start, start)$, 即视野的中心坐标，相邻的 $Layer$ 间，感受野中心的关系为：$start^{(l)} = start^{(l-1)} + ({k^{(l)}-1 \over 2}-p^{(l)})*j^{(l-1)}$,  将&lt;code&gt;padding size&lt;&#x2F;code&gt;设置为&lt;code&gt;kernel&lt;&#x2F;code&gt;的半径，即$p={k−1\over2}$，可得$start^{(l)}=start^{(l-1)}$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;juan-ji-de-jin-hua&quot;&gt;卷积的进化&lt;&#x2F;h2&gt;
&lt;p&gt;卷积形式经过各种排列组合，诞生了不少经典神经网络结构，我想要探索的不仅仅是这些结构本身，更想要搞明白这些结构设计的初衷，以及带来的好处是什么。&lt;&#x2F;p&gt;
&lt;p&gt;这里讨论的卷积结构，我分为两类，一类叫&lt;strong&gt;卷积单元&lt;&#x2F;strong&gt;，表示单个卷积，一类叫&lt;strong&gt;卷积结构&lt;&#x2F;strong&gt;，表示多个卷积的组合。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;font color=&quot;red&quot;&gt;分类一：卷积单元&lt;&#x2F;font&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;chang-gui-juan-ji-convolution&quot;&gt;常规卷积(Convolution)&lt;&#x2F;h3&gt;
&lt;p&gt;如前所述的基本原理，即为常规卷积。简而言之，即矩阵和卷积核进行对应元素的相乘并求和，一次卷积的结果输出一个数，然后卷积核在矩阵上边滑动边计算，当遍历完矩阵时，会输出一个结果矩阵。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;kong-dong-juan-ji-dilated-conv&quot;&gt;空洞卷积(Dilated Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;空洞卷积 (Dilated Convolution) 又叫膨胀卷积 ( Atrous Convolution)， 它的想法是在卷积核中增加“空洞”， 从而在不增加参数量的情况下捕捉到更远的距离。换种说法就是空洞卷积能增加感受野。&lt;&#x2F;p&gt;
&lt;p&gt;空洞卷积核的设计很简单，其实就是对卷积核的插 0 操作， 比如由“111”变为“10101”这样。空洞卷积常用于一维卷积的场景，比如 NLP  的语义分割、文本抽取等任务，语音合成等任务。&lt;&#x2F;p&gt;
&lt;p&gt;空洞卷积在不增加参数的前提下增加感受野，我们举个例子算一下。设有4层网络，第0层为一维序列，第1层为普通卷积(核尺寸,步幅数,空洞数) = $(k,s,d)$ = $(3,1,1)$，第2层为普通卷积$(3,2,1)$，第3层为空洞卷积$(3,1,3)$，那么第3层的感受野为 $13$，计算步骤如下：&lt;&#x2F;p&gt;
&lt;p&gt;$$r^{(0)} = 1$$
$$r^{(1)} = r^{(0)} + (k^{(1)}-1)*j^{(0)}=1 + (3-1)*1 = 3$$
$$r^{(2)} =r^{(1)} + (k^{(2)}-1)*j^{(1)}= 3 + (3-1)*1 = 5$$
$$r^{(3)} =r^{(2)} + d^{(3)}*(k^{(3)}-1)*j^{(2)}=5 + 2*(3-1)*2= 13$$&lt;&#x2F;p&gt;
&lt;h3 id=&quot;fan-juan-ji-deconvolution-conv&quot;&gt;反卷积(Deconvolution Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;反卷积的英文名为 Deconvolution 或 Transposed Convolution 或 Fractional-strided Convolutions， 它将特征图从低分辨率映射到大分辨率，用于扩大图像尺寸。&lt;&#x2F;p&gt;
&lt;p&gt;反卷积是一种特殊的正向卷积，而不是卷积的反过程。那么反卷积如何增加尺寸呢？方法和空洞卷积类似，只是
反卷积是在输入端填入空洞，来增加输入的尺寸，而空洞卷积是在卷积核填入空洞，来减少参数数量。&lt;&#x2F;p&gt;
&lt;p&gt;沿用前面的符号约定，我来描述一下反卷积令输出尺寸增大的过程。&lt;&#x2F;p&gt;
&lt;p&gt;设输入$I=3$, 卷积核$K=3$, 填充$P=1$, 步幅$S=1$, 空洞$D=1$，考虑二维的情况：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;首先填充空洞令输入的尺寸变大，则输入尺寸由 3x3 变为 5x5；&lt;&#x2F;li&gt;
&lt;li&gt;代入卷积的输出尺寸公式， 
$$O = {I - K + 2P \over S} + 1 = (5-3+2) + 1 = 5$$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;求得该反卷积的输出尺寸为 5x5。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ping-jing-juan-ji-pointwise-conv&quot;&gt;瓶颈卷积(Pointwise Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;常被称为瓶颈层( Bottleneck Layer)，用来缩减参数量，瓶颈层的叫法很形象，就像漏斗一样。瓶颈层的计算方式也常被称为 Pointwise 操作，所以瓶颈卷积也叫做 Pointwise Convolution。&lt;&#x2F;p&gt;
&lt;p&gt;举例解释一下参数如何被降低的。&lt;&#x2F;p&gt;
&lt;p&gt;$$input;256d \to 1\times 1\times 64 \to 3\times 3\times 64 \to 1\times1\times 256 \to output; 256d$$&lt;&#x2F;p&gt;
&lt;p&gt;上面的结构，256维的输入先经过一个1×1×64的卷积层，再经过一个3×3×64的卷积层，最后经过一个1×1×256的卷积层，输出256维。参数量为：
$$256\times 1\times 1\times 64 + 64\times 3\times 3\times 64 + 64\times 1\times 1\times 256 = 69,632$$&lt;&#x2F;p&gt;
&lt;p&gt;如果采用传统方案：&lt;&#x2F;p&gt;
&lt;p&gt;$$input; 256d \to 3\times 3\times 256 \to output; 256d$$&lt;&#x2F;p&gt;
&lt;p&gt;其参数量为：$256\times 3\times 3\times 256 = 589,824$, 瓶颈卷积的参数较传统方案降低了约 8.5 倍。&lt;&#x2F;p&gt;
&lt;p&gt;仔细想一想，发现瓶颈层在计算上的效果和全连接层一样，通过一个权重变换达到降维目的。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shen-du-juan-ji-depthwise-conv&quot;&gt;深度卷积(Depthwise Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;深度卷积( Depthwise Convolution
)的想法是一个卷积核仅对一个通道进行计算，且输入通道数等于输出通道数，这种计算方式又被叫做 depthwise
操作。之所以叫做深度卷积，是因为通道这个维度常被称为深度，比如一个RGB图像，我们可以用高x宽x深度(或通道数)来标定它的尺寸。
注意，深度卷积并不是什么特别的想法，它只是组卷积的极端形式，组卷积是把通道数分成了几组，深度卷积的分组数量等于输入的通道数量。&lt;&#x2F;p&gt;
&lt;p&gt;对于 Depthwise Convolution 的公式，我认为表达得比较好的版本是这个：&lt;&#x2F;p&gt;
&lt;p&gt;$$O_{i,c} = DepthwiseConv(X,W_{c,:},i,c) = \sum_{j=1}^k W_{c,j}\cdot
X_{(i+j-\lceil{k+1\over 2}\rceil),c}$$&lt;&#x2F;p&gt;
&lt;p&gt;其中，&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$X \in R^{n\times d}$, $O \in R^{n\times d}$, $W \in R^{d\times k}$, X 是输入，O
是输出，W 是权重，d 是维度（或通道数），k 是卷积核尺寸，n 是 X
一个维度中元素的数量，如果 X 为序列，则 n 为序列长度（time steps），如果 X
为图片，则 n 为一个图片通道中像素的数量。&lt;&#x2F;li&gt;
&lt;li&gt;$O_{i,c}$ 表示 X 的第 c 个通道第 i 个元素的卷积输出。&lt;&#x2F;li&gt;
&lt;li&gt;$X_{(i+j-\lceil{k+1\over 2}\rceil),c}$ 表示在 X 的第 c 个通道中，以第 i
个元素为中心，以 $\lceil{k+1\over 2}\rceil$ 为半径范围内的所有元素，$\lceil
\cdot \rceil$ 表示向上取整; $W_{c,j}$ 表示第 c 个卷积核中第 j 个元素。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;qing-quan-zhong-juan-ji-lightweight-conv&quot;&gt;轻权重卷积(Lightweight Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;轻权重卷积(Lightweight Convolution) 由 Depthwise
卷积进化而来，如果你理解了深度卷积的公式，那么你就能根据轻权重卷积的公式立即明白它改进了什么。&lt;&#x2F;p&gt;
&lt;p&gt;$$LightConv(X, W_{\lceil{cH\over d}\rceil, :}, i, c) = DepthwiseConv(X,
softmax(W_{\lceil{cH\over d}\rceil, :}), i, c)$$&lt;&#x2F;p&gt;
&lt;p&gt;解释一下，这里的权重 $W$ 即卷积核 ( $W\in R^{H\times k}$ ) ，Depthwise时，卷积核的数量有 d 个，Lightweight
时，卷积核的数量被降低到 H 
个。没错，就是对卷积核分组，把原来的 $d\over H$ 个卷积核融合成一个卷积核，外部再套一个 softmax
函数是归一化的考虑。经过这样调整后，参数量继续被降低到一个相当的程度。&lt;&#x2F;p&gt;
&lt;p&gt;H 表示共分多少组，$\lceil{cH\over d}\rceil$ 向上取整，算出分组编号 h , 这时
softmax 函数的公式为：&lt;&#x2F;p&gt;
&lt;!-- $$ softmax(W)_{h,j} = {exp W_{h,j}\over \sum_{j&#x27;=1}^k exp W_{h,j&#x27;}} $$ --&gt;
&lt;p&gt;$$ softmax(W\rparen_{h,j} = \cfrac {exp W_{h,j}} {\sum_{j’=1}^k exp W_{h,j’}} $$&lt;&#x2F;p&gt;
&lt;p&gt;对比深度可分离卷积，如果不考虑 Pointwise 部分，在一个感受野下，轻权重卷积的参数量由
Depthwise 卷积的 $k\cdot c$ 降低到 $k\cdot H$。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;dong-tai-juan-ji-dynamic-conv&quot;&gt;动态卷积(Dynamic Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;动态卷积 ( Dynamic Convolution )
继续在卷积核上做文章，前面的卷积核仅仅是个可训练的权重，动态卷积把它发展为一个随输入
X 中元素变化的线性函数。
$$DynamicConv(X, i, c) = LightConv(X, f(X_i\rparen_{h,:}, i, c)$$
其中，
$$f(X_i) = \sum_{c=1}^d W_{h,j,c}^Q X_{i,c}$$ 
这里的 $W^Q \in R^{H\times k\times d}$ ，因此对于一个感受野，参数量为 $k\cdot H
\cdot d$。&lt;&#x2F;p&gt;
&lt;p&gt;观察轻权重卷积和动态卷积的构成，我认为都属于 depthwise
卷积家族，它们的不同点仅在于对卷积核上施加了不同的函数。对于轻权重卷积而已，它通过减少卷积核数量，同时兼顾卷积核中其他元素的相关性，进一步降低了参数量；对于动态卷积而言，它在轻权重卷积基础上，又多兼顾了通道间的相关性，但参数量有所增加。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ke-bian-xing-juan-ji-deformable-conv&quot;&gt;可变形卷积(Deformable Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;可变形卷积 (Deformable Convolution) 的想法是在卷积核上做文章。卷积核一般的形状为正方形或长方形，由于图像中我们感兴趣的区域往往是不规则图形，如果让卷积核的形状能根据关注区域的形状发生形变，那么是否会获得更好的性能呢？比如针对缩放的物体，就训练出能缩放的卷积核，对于旋转的物体，就训练出能旋转的卷积核，这样抽取出的特征将有更好的泛化性。&lt;&#x2F;p&gt;
&lt;p&gt;该想法的提出来自微软亚洲研究院。它的方法是在原来的卷积核前面再加一层过滤器（记为偏移过滤器），这层过滤器学习的是下一层卷积核的位置偏移量。假设输入的形状为 [H，W，C]，经过偏移过滤器后输出的形状为 [H, W, 2 x C]，这里的通道变为原来的两倍。因为我们需要在这个偏移区域( offset field  )里面取一组卷积核的偏移坐标(记为 offsets )，而一个offset肯定不能一个值就表示的，最少也要用两个值（x 方向上的偏移和 y 方向上的偏移）。取完了这些值，就可以顺利使卷积核形变，后面再按常规卷积操作即可。&lt;&#x2F;p&gt;
&lt;p&gt;该卷积网络在自动驾驶的图像语义分割任务上获得了 5 个点的提升。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;font color=&quot;red&quot;&gt;分类二：卷积结构&lt;&#x2F;font&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zu-juan-ji-grouped-conv&quot;&gt;组卷积(Grouped Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;组卷积(Grouped Convolution)曾被用于 AlexNet 中，将模型分布在两个 GPU 上以解决内存处理问题。用一个 32 通道的例子来解释一下组卷积：把 32 个输入通道平均分为 4 组，每组拥有 8 个通道，并分别对 4 组单独做卷积运算。这样的好处是参数较少可以提升计算速度，但是同时，由于每组卷积之间不存在交互，不同组的输出通道与输入通道并不相关。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;hun-he-juan-ji&quot;&gt;混合卷积&lt;&#x2F;h3&gt;
&lt;p&gt;传统的层叠式网络，基本上都是一个个卷积层的堆叠，每层只用一个尺寸的卷积核。于是有人想，如果每层用多个尺寸的卷积核，是否能提炼到更好的特征呢？&lt;&#x2F;p&gt;
&lt;p&gt;Google的研究员首次在 Inception
家族的结构中采用了这种想法，把上层的输入分别同时经过 1x1、3x3、5x5
的卷积核进行处理，得出的特征再组合起来，传到下一层。这种方法确实令模型的性能有所提高，但紧接而来的副作用是，由于卷积核的数量增多，参数量暴增。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shen-du-ke-fen-chi-juan-ji-depthwise-separable-conv&quot;&gt;深度可分离卷积(Depthwise separable Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;深度可分离卷积 (Depthwise Separable Convolution) 的流行来自谷歌 Xception 结构的流行。这种卷积的思想是把过去一个卷积核对多通道的计算变为一个卷积核仅针对一个通道，被称为 Depthwise 操作，然后再用1x1卷积将每个通道输出的特征图( Feature map) &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;进行融合，这被称为 Pointwise 操作，也即是前面提到的瓶颈卷积。
因此，深度可分离卷积实质上是 Depthwise Convolution 和 Pointwise Convolution
的组合。&lt;&#x2F;p&gt;
&lt;p&gt;深度可分离卷积可以看成是交错组卷积的特例，相较于常规卷积操作，显著降低了参数量和运算成本，从而增加了神经网络性能。假设卷积核的尺寸为 k，输入的通道和卷积核的数量均为 c ，则对于一个感受野来说，所需的参数量为 $k\cdot c + c^2$，而传统卷积的参数量为 $k\cdot c^2$。&lt;&#x2F;p&gt;
&lt;p&gt;举例说明：&lt;&#x2F;p&gt;
&lt;p&gt;假设有一个3×3大小的卷积层，其输入通道为16、输出通道为32。常规卷积的做法是，32个3×3大小的卷积核会遍历16个通道中的每个数据，从而产生16×32=512个特征图谱，进而通过叠加每个输入通道对应的特征图谱后融合得到1个特征图谱， 最后得到32个输出通道。涉及的参数有16×32×3×3=4608个。&lt;&#x2F;p&gt;
&lt;p&gt;深度可分离卷积的方式为，用16个3×3大小的卷积核分别遍历16通道的数据，得到了16个特征图谱。接着用32个1×1大小的卷积核遍历这16个特征图谱，进行相加融合。这个过程使用了16×3×3+16×32×1×1=656个参数， 远少于上面的参数个数。&lt;&#x2F;p&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;1&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;1&lt;&#x2F;sup&gt;
&lt;p&gt;特征图( Feature map)是卷积过滤器的输出结果，也叫通道(Channel) ，表示之前输入上某个特征分布的数据。&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
&lt;h3 id=&quot;jiao-cuo-zu-juan-ji-interleaved-group-conv&quot;&gt;交错组卷积(Interleaved Group Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;交错组卷积( Interleaved Group Convolution ) 的思路由微软研究院提出。前面提到，组卷积中不同组的输出与输入通道之间缺乏相关性，要增加这种相关性，只需要把其他组的输出通道叠加进来，再次进行卷积运算即可。&lt;&#x2F;p&gt;
&lt;p&gt;具体方法为，做两次组卷积，第一次还是和一般组卷积一样，然后把所有组卷积的输出交错排列后，作为第二次组卷积的输入，以达到交错互补的目的。交错组卷积增加了网络的宽度，同时增加了通道间的相关性，自然模型性能会提升。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;duo-tou-zi-zhu-yi-li-multi-head-self-attention&quot;&gt;多头自注意力(Multi-Head Self-Attention)&lt;&#x2F;h3&gt;
&lt;p&gt;没错，你没看错，如果你参悟透了多头自注意力结构，它实质仍然是卷积计算。
多头自注意力(Multi-Head Self-Attention，以下简称“MHSA“) 来自 Google Brain
的大作。2018年底，在这个基础上又诞生了 Bert 模型，当时拿下 11
项 NLP 任务最高分。&lt;&#x2F;p&gt;
&lt;p&gt;首先看看 MHSA 到底有什么用？宏观上，当机器理解一句话时，对于句中一个词，MHSA
可以告诉这个词，句中其他词跟它的亲疏程度。换句话说，MHSA
的输出结果用来衡量当前词和其他词之间的关系。多头自注意力机制的主要公式如下：&lt;&#x2F;p&gt;
&lt;p&gt;$$ Attention(Q, K, V) = softmax({QK^T\over \sqrt{d_k}})V  \tag 1$$
$$ MultiHead(Q, K, V ) = Concat(head_1, \ldots, head_h)W^O \tag 2$$&lt;&#x2F;p&gt;
&lt;p&gt;其中，&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$head_i = Attention(QW_i^Q , KW_i^K , VW_i^V )$，这里的$QW_i^Q , KW_i^K , VW_i^V$ 分别对应 (1) 式中的$Q,K,V$，不要搞混了，原版论文就这么写的；&lt;&#x2F;li&gt;
&lt;li&gt;$W_i^Q \in R^{d_{model} \times d_k} , W_i^K \in R^{d_{model} \times d_k} , W_i^V \in R^{d_{model}\times d_v}, W^O\in R^{hd_v\times d_{model}}$, 在 NLP 任务中， $d_{model}$ 表示词向量维度，$d_k,d_k,d_v$ 表示变换后维度，这些权重用来给词向量降维; $h$ 表示多头数量。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;在公式 (1) 中，Q、K、V 分别表示查询向量、键向量和值向量，这三个向量是通过词嵌入与三个权重矩阵相乘后求得。注意力机制本身就是一个打分函数，目标是告诉当前词，句中其他词与当前词的亲疏程度。这里，查询向量代表当前词，键向量代表用来比较的其他词，值向量代表其他词的表达。$QK^T$ 这步运算，你可以把 Q 看成卷积核，然后依次与其他词 K 做卷积运算，计算出向量相似度，下面除以 $\sqrt{d_k}$ ，外面再 softmax 相当于先规范化再归一化，把相似度转成概率值。下一步，把对每个词的相似度权重分别乘以该词的表达向量 V, 最后在求和，这时得出的向量对于当前词来说，它不仅包含了当前词的语义，还包含了当前词与其他词的关系，它是融入句子语境的单词表达。 &lt;&#x2F;p&gt;
&lt;p&gt;还要多说一句，为什么叫做自注意力？请注意，公式 (1) 中有三个向量
Q、K、V，它们其实都是单词的语义表达，降低参数量是我们不变的追求，那么对于一个单词而言，令$Q
=K=V$，就是自注意力名称的由来。&lt;&#x2F;p&gt;
&lt;p&gt;在公式 (2) 中，将 (1) 式重复做 h
次，再把结果拼接起来，乘以一个权重得到一个维度为 $d_{model}$
的向量。这里的重复次数就好比一个卷积核的多个通道，它算出当前词向量表达的不同方面的特征。观察动态卷积的设计，发现和多头自注意力的设计似有借鉴之处。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;can-chai-wang-luo-residual-net&quot;&gt;残差网络(Residual Net)&lt;&#x2F;h3&gt;
&lt;p&gt;当卷积的层数加深时，网络的表现会越来越差，很大程度上的原因是因为当层数加深时，梯度消散得越来越严重，以至于反向传播很难训练到浅层的网络（靠近输入层的网络）。残差网络 ( Resnet ) 思想是在原来层叠网络的基础上增加跳线连接，在跳线的末端那层同时接入上层输入和跳线过来的输入（即前面某层的输出）。&lt;&#x2F;p&gt;
&lt;p&gt;下面解释一下残差结构为什么能解决梯度消散问题。&lt;&#x2F;p&gt;
&lt;p&gt;$$ a^{l+2} = g(z^{l+2} + a^l) = g(W^{l+2} a^{l+1} + b^{l+2} + a^l)$$
其中：$a^{l}$ 表示第 $l$ 层的激活值，$g(\cdot)$ 表示激活函数 。&lt;&#x2F;p&gt;
&lt;p&gt;若 $W^{l+2} \approx 0 ,  b^{l+2} \approx 0$, 激活函数为ReLU,  则：
$$ a^{l+2} = ReLU( a^l) \approx a^l $$&lt;&#x2F;p&gt;
&lt;p&gt;从效果上说，相当于直接忽略了 $a^l$ 之后的第 $l+1$ 和第 $l+2$ 两层神经网络，实现了隔层传递。即如果残差网络能训练得到非线性关系，就会忽略跳线输入(short cut 或 skip connection), 
否则（发生梯度消失时）忽略非线性层，保留线性层，故能连接更深的网络。&lt;&#x2F;p&gt;
&lt;p&gt;这里还有一个问题，为什么不在 $a^l$ 前加个参数了，并且让这个参数参与学习。残差网络的作者 kaiming 在 papper 中有解释： 假如不是 $x$ 而是 $\lambda_i{x}$ 的话, 梯度里会有一项 $\prod_{i=l}^{L-1}\lambda_i$, 就是从输出到当前层之间经过的 shortcut 上的所有 $\lambda_i$ 相乘，假如  都大于 1 那经过多层之后就会爆炸，都小于 1 就会趋向于 0 而变得和没有 shortcut 一样了。&lt;&#x2F;p&gt;
&lt;p&gt;那如果 $a^l$ 和 $a^{l+2}$ 的维度不同，则 ${W_s}\cdot{a^l}$ 使维度与 $a^{l+2}$ 一致。$W_s$ 有两种方法来得到：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;将 $W_s$ 作为学习参数训练得到；&lt;&#x2F;li&gt;
&lt;li&gt;固定 $W_s$ 值( 类似单位矩阵 ) ，${W_s} \cdot {a^l}$ 仅使 $a^l$ 截断或补零。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;te-zheng-zhong-biao-ding-juan-ji-se-conv&quot;&gt;特征重标定卷积(SE Conv)&lt;&#x2F;h3&gt;
&lt;p&gt;图像的特征通过卷积核抽取出来，形成一个个特征通道，那么每个特征通道的作用都同等重要么？显然不是。如果有办法通过学习的方式自动获取每个特征通道的重要程度，然后依照计算出来的重要程度去提升有用的特征，抑制用处不大的特征，这就是特征重标定卷积思想的由来。该思路构建的模型结构叫做Squeeze-and-Excitation Network，简称 SENet，曾是 ImageNet 2017 竞赛的冠军模型。&lt;&#x2F;p&gt;
&lt;p&gt;SENet 结构为：首先做普通卷积，输出一个特征图，它的形状为[C，H，W]，再分两条路线，第一条直接通过( 类似残差结构的跳线 )，第二条首先进行&lt;strong&gt;压缩( Squeeze )&lt;strong&gt;操作（ 全局平均池化 Global Average Pooling ），把每个通道 2 维的特征压缩成 1 维，得到长度为 C 的特征通道向量（每个数字代表对应通道的特征）；然后进行&lt;&#x2F;strong&gt;激发( Excitation )&lt;strong&gt;操作，即对特征通道向量加两个全连接层( FC Layer )，和非线性映射( Sigmoid )，建模出特征通道间的相关性，得到的输出其实就是每个通道对应的权重，
把这些权重加权到原来的特征上（第一条路），这样就完成了特征通道的权重分配。最后是&lt;&#x2F;strong&gt;赋权操作&lt;&#x2F;strong&gt;，论文里叫做
scale 操作，把特征通道的权重先归一化，然后加权到每个通道的特征上。&lt;&#x2F;p&gt;
&lt;p&gt;具体分为三个步骤：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一步 Squeeze 操作为每个特征通道生成一个实数，该实数在某种程度上拥有全局感受野，表征着在特征通道上响应的全局分布，而且使得靠近输入的层也可以获得全局的感受野。&lt;&#x2F;p&gt;
&lt;p&gt;$$ z_c = F_{sq}(u_c) = \frac{1}{W \times H}\sum_{i=1}^{W}\sum_{j=1}^{H}u_c(i,j)$$&lt;&#x2F;p&gt;
&lt;p&gt;其中：$u_c$ 表示第c个特征通道。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;第二步 Excitation 操作类似于循环神经网络的门机制， 通过参数 w 来为每个特征通道生成权重，其中参数 w 被学习用来显式地建模特征通道间的相关性。&lt;&#x2F;p&gt;
&lt;p&gt;$$ s = F_ex(z,W) = \sigma(g(z,W)) = \sigma(W_2(\delta(W_{1}z)) $$&lt;&#x2F;p&gt;
&lt;p&gt;其中，$\sigma$ 表示 $ReLU$ 函数,  $\delta$ 表示 $sigmoid$ 函数。 &lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;第三步 赋权( Reweight or Scale )操作, 把特征通道的权重加权到每个通道的特征上。&lt;&#x2F;p&gt;
&lt;p&gt;$$\tilde{x_c} = F_{scale}(u_c, s_c) = s_c \cdot u_c$$&lt;&#x2F;p&gt;
&lt;p&gt;其中，下标$c$表示第 c 个通道，令$u$表示特征图，$s$表示所有通道的权重，则 $F_{scale}(u,s)$ 表示每个通道乘以各自的权重，它就是一个深度卷积( Depthwise Convolution )。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;bu-wan-quan-xiao-jie&quot;&gt;不完全小结&lt;&#x2F;h2&gt;
&lt;p&gt;神经网络恒古不变的追求其实就一条，减少参数并提升性能。
观察上述各种卷积的变化，发现确有规律可循，它们包括：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;调整连接方式&lt;&#x2F;strong&gt;：如在一层用不同的卷积核就是混合卷积，给层叠卷积网络增加跳线连接就是残差网络； &lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;调整卷积核&lt;&#x2F;strong&gt;：如把卷积核尺寸降低为1，就是瓶颈卷积；让卷积核位置发生偏移，就是可变形卷积；把卷积核添些洞就是空洞卷积；&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;调整通道&lt;&#x2F;strong&gt;：如把通道分组计算就是分组卷积，分组数量与通道数一致时就是深度卷积；给通道分配权重，就是特征重标定卷积；&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;调整卷积核和通道&lt;&#x2F;strong&gt;：如按通道先分组，然后对每组的卷积核套上函数做变换后，
再做卷积运算，这样的卷积有轻权重卷积和动态卷积。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;其实，对于卷积网络，还可以讨论的方面有激活函数的选择，卷积网络的宽度和深度如何平衡，池化层和dropout的考虑等等，但考虑到上述有些问题目前还没定论，有些内容写在这里，可能会冲淡主题，所以暂时略过，以后发现有必要时再增补。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;can-kao-wen-xian&quot;&gt;参考文献&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1901.10430v1.pdf&quot;&gt;F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay Less Attention With Lightweight and Dynamic Convolutions. 2019.&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1709.01507.pdf&quot;&gt;J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu. Squeeze-and-Excitation Networks. 2017.&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1707.02725.pdf&quot;&gt;T. Zhang, G. Qi, B. Xiao, and J. Wang. Interleaved Group Convolutions for Deep Neural Networks. 2017.&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762.pdf&quot;&gt;A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention Is All You Need. 2017.&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1706.03059.pdf&quot;&gt;Ł. Kaiser, A. N. Gomez, and F. Chollet. Depthwise Separable Convolutions for Neural Machine Translation. 2017.&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1703.06211.pdf&quot;&gt;J. Dai, H. Qi,Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei. Deformable Convolutional Networks. 2017.&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1512.03385.pdf&quot;&gt;K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2015.&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>概率图模型 HMM、MEMM、CRF</title>
        <published>2018-05-08T00:00:00+00:00</published>
        <updated>2018-05-08T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/crf/" type="text/html"/>
        <id>https://www.freeopen.tech/crf/</id>
        
        <content type="html">&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;35866596&#x2F;answer&#x2F;236886066&quot;&gt;转自知乎&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;此文不错，在原译文基础上轻微修订&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;yi-preface&quot;&gt;&lt;strong&gt;一、Preface&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;之前刚接触NLP时做相关的任务，也必然地涉及到了序列处理任务，然后自然要接触到概率图模型。当时在全网搜中文资料，陆续失望地发现竟然真的没有讲得清楚的博文，发现基本是把李航老师书里或CRF tutorial等资料的文字论述和公式抄来抄去的。当然，没有说别人讲的是错的，只是觉得，要是没有把东西说的让读者看得懂，那也是没意义啊。或者有些吧，就是讲了一大堆的东西，貌似也明白了啥，但还是不能让我很好的理解CRF这些模型究竟是个啥，完了还是有一头雾水散不开的感觉。试想，一堆公式扔过来，没有个感性理解的过渡，怎么可能理解的了。我甚至觉得，如果博客让人看不懂，那说明要么自己没理解透要么就是思维不清晰讲不清楚。所以默想，深水区攻坚还是要靠自己，然后去做调研做research，所以就写了个这个学习记录。&lt;&#x2F;p&gt;
&lt;p&gt;所以概率图的研究学习思考列入了我的任务清单。不过平时的时间又非常的紧，只能陆陆续续的思考着，所以时间拖得也真是长啊。&lt;&#x2F;p&gt;
&lt;p&gt;这是个学习笔记。相比其他的学习模型，概率图貌似确实是比较难以理解的。这里我基本全部用自己的理解加上自己的语言习惯表达出来，off the official form，表达尽量接地气。我会尽量将我所有理解过程中的每个关键小细节都详细描述出来，以使对零基础的初学者友好。包括理论的来龙去脉，抽象具象化，模型的构成，模型的训练过程，会注重类比的学习。&lt;&#x2F;p&gt;
&lt;p&gt;根据现有资料，我是按照概率图模型将HMM，MEMM，CRF放在这里一起对比学习。之所以把他们拿在一起，是因为他们都用于标注问题。并且之所以放在概率图框架下，是完全因为自己top-down思维模式使然。另外，概率图下还有很多的模型，这儿只学习标注模型。&lt;&#x2F;p&gt;
&lt;p&gt;正儿八经的，我对这些个概率图模型有了彻悟，是从我明白了生成式模型与判别式模型的那一刻。一直在思考从概率图模型角度讲他们的区别到底在哪。&lt;&#x2F;p&gt;
&lt;p&gt;另外，篇幅略显长，但咱们不要急躁，好好看完这篇具有良好的上下文的笔记，那肯定是能理解的，或者就多看几遍。&lt;&#x2F;p&gt;
&lt;p&gt;个人学习习惯就是，&lt;strong&gt;要尽可能地将一群没有结构的知识点融会贯通，再用一条树状结构的绳将之串起来，结构化，就是说要成体系，这样把绳子头一拎所有的东西都能拿起来&lt;&#x2F;strong&gt;。学习嘛，应该要是一个熵减的过程，卓有成效的学习应该是混乱度越来越小！这个思维方式对我影响还是蛮大的。&lt;&#x2F;p&gt;
&lt;p&gt;在正式内容之前，还是先要明确下面这一点，最好脑子里形成一个定势：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;统计机器学习所有的模型（个别instant model和优化算法以及其他的特种工程知识点除外）的工作流程都是如此：&lt;br &#x2F;&gt;
a.训练模型参数，得到模型（由参数唯一确定），&lt;br &#x2F;&gt;
b.预测给定的测试数据。&lt;br &#x2F;&gt;
拿这个流程去挨个学习模型，思路上会非常顺畅。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;除此之外，对初学者的关于机器学习的入门学习方式也顺带表达一下(empirical speaking)：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;a.完整特征工程竞赛&lt;br &#x2F;&gt;
b.野博客理论入门理解&lt;br &#x2F;&gt;
c.再回到代码深入理解模型内部&lt;br &#x2F;&gt;
d.再跨理论，查阅经典理论巨作。这时感性理性都有一定高度，会遇到很多很大的理解上的疑惑，这时3大经典可能就可以发挥到最大作用了。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;很多beginer，就比如说学CRF模型，然后一上来就摆一套复杂的公式，什么我就问，这能理解的了吗？这是正确的开启姿势吗？当然了，也要怪那些博主，直接整一大堆核心公式，实际上读者的理解门槛可能就是一个过渡性的细枝末节而已。没有上下文的教育肯定是失败的（这一点我又想吐槽国内绝大部分本科的院校教育模式）。所以说带有完整上下文信息以及过程来龙去脉交代清楚才算到位吧。&lt;&#x2F;p&gt;
&lt;p&gt;而不是一上来就死啃被人推荐的“经典资料”，这一点相信部分同学会理解。好比以前本科零基础学c++ JAVA，上来就看primr TIJ，结果浪费了时间精力一直在门外兜圈。总结方法吸取教训，应该快速上手代码，才是最高效的。经典最好是用来查阅的工具书，我目前是李航周志华和经典的那3本迭代轮询看了好多轮，经常会反复查询某些model或理论的来龙去脉；有时候要查很多相关的东西，看这些书还是难以贯通，然后发现有些人的博客写的会更容易去理解。所以另外，学习资料渠道也要充分才行。&lt;&#x2F;p&gt;
&lt;p&gt;最后提示一下，&lt;strong&gt;请务必按照标题层级结构和目录一级一级阅读，防止跟丢。&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;er-prerequisite&quot;&gt;&lt;strong&gt;二、Prerequisite&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;2-1-gai-lu-tu&quot;&gt;&lt;strong&gt;2.1 概率图&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;之前刚接触CRF时，一上来试图越过一堆繁琐的概率图相关概念，不过sad to say, 这是后面的前驱知识，后面还得反过来补这个点。所以若想整体把握，系统地拿下这一块，应该还是要越过这块门槛的。&lt;&#x2F;p&gt;
&lt;p&gt;当然了，一开始只需略略快速看一篇，后面可再返过来补查。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-1-1-gai-lan&quot;&gt;&lt;strong&gt;2.1.1 概览&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构（个人非常喜欢这种类型的图）：&lt;&#x2F;p&gt;
&lt;img src=&quot;.&#x2F;v2.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;631&quot; data-rawheight=&quot;336&quot;  width=&quot;631&quot; &gt;
&lt;p&gt;在概率图模型中，数据(样本)由公式 $G=(V,E)$ 建模表示：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$V$ 表示节点，即随机变量（放在此处的，可以是一个token或者一个label），具体地，用 $Y = (y_1, \cdots, y_n)$ 为随机变量建模，注意 $Y$ 现在是代表了一批随机变量（想象对应一条sequence，包含了很多的token）， $P(Y)$ 为这些随机变量的分布；&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$E$ 表示边，即概率依赖关系。具体咋理解，还是要在后面结合HMM或CRF的graph具体解释。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;2-1-2-you-xiang-tu-vs-wu-xiang-tu&quot;&gt;&lt;strong&gt;2.1.2 有向图 vs. 无向图&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;上图可以看到，贝叶斯网络（信念网络）都是有向的，马尔科夫网络无向。所以，贝叶斯网络适合为有单向依赖的数据建模，马尔科夫网络适合实体之间互相依赖的建模。具体地，他们的核心差异表现在如何求 $P=(Y)$ ，即怎么表示 $Y=(y_1,\cdots,y_n)$ 这个的联合概率。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 有向图&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;对于有向图模型，这么求联合概率： $P(x_1, \cdots, x_n )=\prod_{i=0}P(x_i | \pi(x_{i}))$&lt;&#x2F;p&gt;
&lt;p&gt;举个例子，对于下面的这个有向图的随机变量(注意，这个图我画的还是比较广义的)：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;453&quot; src=&quot;.&#x2F;2.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;应该这样表示他们的联合概率:&lt;&#x2F;p&gt;
&lt;p&gt;$P(x_1, \cdots, x_n )=P(x_1)·P(x_2|x_1 )·P(x_3|x_2 )·P(x_4|x_2 )·P(x_5|x_3,x_4 )$&lt;&#x2F;p&gt;
&lt;p&gt;应该很好理解吧。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. 无向图&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;对于无向图，我看资料一般就指马尔科夫网络(注意，这个图我画的也是比较广义的)。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;260&quot; src=&quot;.&#x2F;3.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;如果一个graph太大，可以用因子分解将 &lt;img src=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;equation?tex=P%3D%28Y%29&quot; alt=&quot;P=(Y)&quot; &#x2F;&gt; 写为若干个联合概率的乘积。咋分解呢，将一个图分为若干个“小团”，注意每个团必须是“最大团”（就是里面任何两个点连在了一块，具体……算了不解释，有点“最大连通子图”的感觉），则有：&lt;&#x2F;p&gt;
&lt;p&gt;$P(Y )=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c} )$ &lt;&#x2F;p&gt;
&lt;p&gt;其中, $Z(x) = \sum_{Y} \prod_{c}\psi_{c}(Y_{c} )$，公式应该不难理解吧，归一化是为了让结果算作概率。&lt;&#x2F;p&gt;
&lt;p&gt;所以像上面的无向图：&lt;&#x2F;p&gt;
&lt;p&gt;$P(Y )=\frac{1}{Z(x)} ( \psi_{1}(X_{1}, X_{3}, X_{4} ) · \psi_{2}(X_{2}, X_{3}, X_{4} ) )$&lt;&#x2F;p&gt;
&lt;p&gt;其中， $\psi_{c}(Y_{c} )$ 是一个最大团 &lt;img src=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;equation?tex=C&quot; alt=&quot;C&quot; &#x2F;&gt; 上随机变量们的联合概率，一般取指数函数的：&lt;&#x2F;p&gt;
&lt;p&gt;$\psi_{c}(Y_{c} ) = e^{-E(Y_{c})} =e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)}$
好了，管这个东西叫做&lt;code&gt;势函数&lt;&#x2F;code&gt;。注意 $e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)}$ 是否有看到CRF的影子。&lt;&#x2F;p&gt;
&lt;p&gt;那么概率无向图的联合概率分布可以在因子分解下表示为：&lt;&#x2F;p&gt;
&lt;p&gt;$P(Y )=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c} ) = \frac{1}{Z(x)} \prod_{c} e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)} = \frac{1}{Z(x)} e^{\sum_{c}\sum_{k}\lambda_{k}f_{k}(y_{i},y_{i-1},x,i)}$&lt;&#x2F;p&gt;
&lt;p&gt;注意，这里的理解还蛮重要的，注意递推过程，敲黑板，这是CRF的开端！&lt;br &#x2F;&gt;
这个由&lt;code&gt;Hammersly-Clifford law&lt;&#x2F;code&gt;保证，具体不展开。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-1-3-ma-er-ke-fu-jia-she-ma-er-ke-fu-xing&quot;&gt;&lt;strong&gt;2.1.3 马尔科夫假设&amp;amp;马尔科夫性&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;这个也属于前馈知识。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 马尔科夫假设&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;额应该是齐次马尔科夫假设，这样假设：马尔科夫链 $(x_{1},\cdots,x_{n})$ 里的 $x_{i}$ 总是只受 $x_{i-1}$ 一个人的影响。&lt;br &#x2F;&gt;
马尔科夫假设这里相当于就是个1-gram。&lt;&#x2F;p&gt;
&lt;p&gt;马尔科夫过程呢？即，在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。最简单的马尔科夫过程是一阶的，即只依赖于前一个状态。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. 马尔科夫性&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;马尔科夫性是是保证或者判断概率图是否为概率无向图的条件。&lt;&#x2F;p&gt;
&lt;p&gt;三点内容：a. 成对，b. 局部，c. 全局。&lt;&#x2F;p&gt;
&lt;p&gt;我觉得这个不用展开。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-2-pan-bie-shi-discriminative-mo-xing-vs-sheng-cheng-shi-generative-mo-xing&quot;&gt;&lt;strong&gt;2.2 判别式(discriminative)模型 vs. 生成式(generative)模型&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;在监督学习下，模型可以分为判别式模型与生成式模型。&lt;&#x2F;p&gt;
&lt;p&gt;重点来了。上面有提到，我理解了HMM、CRF模型的区别是从理解了判别式模型与生成式模型的那刻，并且瞬间对其他的模型有一个恍然大悟。我记得是一年前就开始纠结这两者的区别，但我只能说，栽在了一些烂博客上，大部分都没有自己的insightful理解，也就是一顿官话，也真是难以理解。后来在知乎上一直琢磨别人的答案，然后某日早晨终于豁然开朗，就是这种感觉。&lt;&#x2F;p&gt;
&lt;p&gt;好了，我要用自己的理解来转述两者的区别了below。&lt;&#x2F;p&gt;
&lt;p&gt;先问个问题，根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT……）与B批模型（NB、LDA……），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。&lt;&#x2F;p&gt;
&lt;p&gt;2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布 $P(X,Y)$ (注意 $X$ 包含所有的特征 $x_{i}$ ， $Y$ 包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布 $P(X,Y)$ ，再结合新样本给的 $X$ ，通过条件概率就能出来 $Y$：&lt;br &#x2F;&gt;
$P(Y|X) = \frac{P(X,Y)}{P(X)}$
好了，应该说清楚了。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;1. 判别式模型&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：&lt;strong&gt;判别模型是直接对&lt;&#x2F;strong&gt; $P(Y|X)$ &lt;strong&gt;建模&lt;&#x2F;strong&gt;，就是说，直接根据X特征来对Y建模训练。&lt;&#x2F;p&gt;
&lt;p&gt;具体地，我的训练过程是确定构件 $P(Y|X)$ 模型里面“复杂映射关系”中的参数，完了再去inference一批新的sample。&lt;&#x2F;p&gt;
&lt;p&gt;所以判别式模型的特征总结如下：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;对 $P(Y|X)$ 建模&lt;&#x2F;li&gt;
&lt;li&gt;对所有的样本只构建一个模型，确认总体判别边界&lt;&#x2F;li&gt;
&lt;li&gt;观测到输入什么特征，就预测最可能的label&lt;&#x2F;li&gt;
&lt;li&gt;另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;2. 生成式模型&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型 $P(X,Y)$ ，也就是说，&lt;strong&gt;我在训练阶段是只对&lt;&#x2F;strong&gt; $P(X,Y)$&lt;strong&gt;建模&lt;&#x2F;strong&gt;，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算 $P(Y|X)$，导出 $Y$ ,但这已经不属于建模阶段了。&lt;&#x2F;p&gt;
&lt;p&gt;结合NB过一遍生成式模型的工作流程。学习阶段，建模： $P(X,Y)=P(X|Y)P(Y)$ （当然，NB具体流程去隔壁参考）,然后 $P(Y|X) = \frac{P(X,Y)}{P(X)}$ 。&lt;br &#x2F;&gt;
另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。&lt;&#x2F;p&gt;
&lt;p&gt;所以生成式总结下有如下特点：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;对 $P(X,Y)$ 建模&lt;&#x2F;li&gt;
&lt;li&gt;这里我们主要讲分类问题，所以是要对每个label($y_{i}$) 都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）&lt;&#x2F;li&gt;
&lt;li&gt;中间生成联合分布，并可生成采样数据。&lt;&#x2F;li&gt;
&lt;li&gt;生成式模型的优点在于，所包含的信息非常齐全，我称之为“上帝信息”，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。&lt;br &#x2F;&gt;
最后identity the picture below:&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;80%&quot; src=&quot;.&#x2F;4.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-3-xu-lie-jian-mo&quot;&gt;&lt;strong&gt;2.3 序列建模&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;为了号召零门槛理解，现在解释如何为序列问题建模。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;80%&quot; src=&quot;.&#x2F;5.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;序列包括时间序列以及general sequence，但两者无异。连续的序列在分析时也会先离散化处理。常见的序列有如：时序数据、本文句子、语音数据、等等。&lt;&#x2F;p&gt;
&lt;p&gt;广义下的序列有这些特点：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;节点之间有关联依赖性&#x2F;无关联依赖性&lt;&#x2F;li&gt;
&lt;li&gt;序列的节点是随机的&#x2F;确定的&lt;&#x2F;li&gt;
&lt;li&gt;序列是线性变化&#x2F;非线性的&lt;&#x2F;li&gt;
&lt;li&gt;……&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;对不同的序列有不同的问题需求，常见的序列建模方法总结有如下：&lt;&#x2F;p&gt;
&lt;p&gt;1. 拟合，预测未来节点（或走势分析）：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a. 常规序列建模方法：AR、MA、ARMA、ARIMA&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;b. 回归拟合&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;c. Neural Networks&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;2. 判断不同序列类别，即分类问题：HMM、CRF、General Classifier（ML models、NN models）&lt;&#x2F;p&gt;
&lt;p&gt;3. 不同时序对应的状态的分析，即序列标注问题：HMM、CRF、RecurrentNNs&lt;&#x2F;p&gt;
&lt;p&gt;在本篇文字中，我们只关注在2. &amp;amp; 3.类问题下的建模过程和方法。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;san-hmm&quot;&gt;&lt;strong&gt;三、HMM&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;最早接触的是HMM。较早做过一个项目，关于声波手势识别，跟声音识别的机制一样，使用的正是HMM的一套方法。后来又用到了 &lt;em&gt;kalman filter&lt;&#x2F;em&gt;，之后做序列标注任务接触到了CRF，所以整个概率图模型还是接触的方面还蛮多。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-1-li-jie-hmm&quot;&gt;&lt;strong&gt;3.1 理解HMM&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;在2.2、2.3中提序列的建模问题时，我们只是讨论了常规的序列数据，e.g., $(X_{1},\cdots,X_{n})$ ,像2.3的图片那样。像这种序列一般用马尔科夫模型就可以胜任。实际上我们碰到的更多的使用HMM的场景是每个节点 $X_{i}$ 下还附带着另一个节点 $Y_{i}$ ，正所谓&lt;strong&gt;隐含&lt;&#x2F;strong&gt;马尔科夫模型，那么除了正常的节点，还要将&lt;strong&gt;隐含状态节点&lt;&#x2F;strong&gt;也得建模进去。正儿八经地，将 $X_{i} 、 Y_{i}$ 换成 $i_{i} 、o_{i}$ ,并且他们的名称变为状态节点、观测节点。状态节点正是我的隐状态。&lt;&#x2F;p&gt;
&lt;p&gt;HMM属于典型的生成式模型。对照2.1的讲解，应该是要从训练数据中学到数据的各种分布，那么有哪些分布呢以及是什么呢？直接正面回答的话，正是&lt;strong&gt;HMM的5要素&lt;&#x2F;strong&gt;，其中有3个就是整个数据的不同角度的概率分布：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$N$ ，隐藏状态集 $N = \lbrace q_{1}, \cdots, q_{N} \rbrace$ , 我的隐藏节点不能随意取，只能限定取包含在隐藏状态集中的符号。&lt;&#x2F;li&gt;
&lt;li&gt;$M$，观测集 $M = \lbrace v_{1}, \cdots, v_{M} \rbrace$ , 同样我的观测节点不能随意取，只能限定取包含在观测状态集中的符号。&lt;&#x2F;li&gt;
&lt;li&gt;$A$ ，状态转移概率矩阵，这个就是其中一个概率分布。他是个矩阵， $ A = [a_{ij}\rbrack_{N \times N} $ （N为隐藏状态集元素个数），其中 $a_{ij} = P(i_{t+1}|i_{t})， i_{t}$ 即第i个隐状态节点,即所谓的状态转移嘛。&lt;&#x2F;li&gt;
&lt;li&gt;$B$ ，观测概率矩阵，这个就是另一个概率分布。他是个矩阵， $B = [b_{ij}\rbrack_{N \times M}$ （$N$为隐藏状态集元素个数，$M$为观测集元素个数），其中 $b_{ij} = P(o_{t}|i_{t})， o_{t}$ 即第$i$个观测节点，$i_{t}$ 即第$i$个隐状态节点，即所谓的观测概率（发射概率）嘛。&lt;&#x2F;li&gt;
&lt;li&gt;$π$ ，指模型在初始时刻各状态(来自状态集$N$)出现的概率。通常，第一个隐状态节点 $i_{t}$的隐状态可由EM方法学得,故$π$在初始化时可随机给定。(&lt;em&gt;这里原句读不通，由freeopenn修订&lt;&#x2F;em&gt;)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;所以图看起来是这样的：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;415&quot; src=&quot;.&#x2F;6.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;看的很清楚，我的模型先去学习要确定以上5要素，之后在inference阶段的工作流程是：首先，隐状态节点 $i_{t}$ 是不能直接观测到的数据节点， $o_{t}$ 才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系， $i_{t}$ 在A的指导下生成下一个隐状态节点 $i_{t+1}$ ，并且 $i_{t}$ 在 $B$ 的指导下生成依赖于该 $i_{t}$ 的观测节点 $o_{t}$ , 并且我只能观测到序列 $(o_{1}, \cdots, o_{i})$ 。&lt;&#x2F;p&gt;
&lt;p&gt;好，举例子说明（序列标注问题，POS，标注集BES）：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;input: “学习出一个模型，然后再预测出一条指定”&lt;&#x2F;p&gt;
&lt;p&gt;expected output: 学&#x2F;B 习&#x2F;E 出&#x2F;S 一&#x2F;B 个&#x2F;E 模&#x2F;B 型&#x2F;E ，&#x2F;S 然&#x2F;B 后&#x2F;E 再&#x2F;E 预&#x2F;B 测&#x2F;E ……&lt;&#x2F;p&gt;
&lt;p&gt;其中，input里面所有的char构成的字表，形成观测集 $M$ ，因为字序列在inference阶段是我所能看见的；标注集BES构成隐藏状态集 $N$ ，这是我无法直接获取的，也是我的预测任务；至于 $A、B、π$ ，这些概率分布信息（上帝信息）都是我在学习过程中所确定的参数。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;然后一般初次接触的话会疑问：为什么要这样？……好吧，就应该是这样啊，根据具有同时带着隐藏状态节点和观测节点的类型的序列，在HMM下就是这样子建模的。&lt;&#x2F;p&gt;
&lt;p&gt;下面来点高层次的理解：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模 $P(O,I) = \sum_{t=1}^{T}P(I_{t} | I_{t-1})P(O_{t} | I_{t})$ (注意，这个公式不在模型运行的任何阶段能体现出来，只是我们都去这么来表示HMM是个生成式模型，他的联合概率 $P(O,I)$ 就是这么计算的)。&lt;&#x2F;li&gt;
&lt;li&gt;并且B中 $b_{ij} = P(o_{t}|i_{t})$ ，这意味着o对i有依赖性。&lt;&#x2F;li&gt;
&lt;li&gt;在A中， $a_{ij} = P(i_{t+1}|i_{t})$ ，也就是说只遵循了一阶马尔科夫假设，1-gram。试想，如果数据的依赖超过1-gram，那肯定HMM肯定是考虑不进去的。这一点限制了HMM的性能。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;3-2-mo-xing-yun-xing-guo-cheng&quot;&gt;&lt;strong&gt;3.2 模型运行过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;模型的运行过程（工作流程）对应了HMM的3个问题。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-2-1-xue-xi-xun-lian-guo-cheng&quot;&gt;&lt;strong&gt;3.2.1 学习训练过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;对照2.1的讲解，HMM学习训练的过程，就是找出数据的分布情况，也就是模型参数的确定。&lt;&#x2F;p&gt;
&lt;p&gt;主要学习算法按照训练数据除了观测状态序列 $(o_{1}, \cdots, o_{i})$ 是否还有隐状态序列 $(i_{1}, \cdots, i_{i})$ 分为：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;极大似然估计, with 隐状态序列&lt;&#x2F;li&gt;
&lt;li&gt;Baum-Welch(前向后向), without 隐状态序列&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;感觉不用做很多的介绍，都是很实实在在的算法，看懂了就能理解。简要提一下。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 极大似然估计&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的。所以极大似然估计法是非常常用的学习算法，我见过的很多代码里面也是这么计算的。比较简单。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;step1. 算A&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$\hat{a_{ij}} = \frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;step2. 算B&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$\hat{b_{j}}(k) = \frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;step3. 直接估计 $π$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;比如说，在代码里计算完了就是这样的：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;7.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;8.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;9.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. Baum-Welch(前向后向)&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;就是一个EM的过程，如果你对EM的工作流程有经验的话，对这个Baum-Welch一看就懂。EM的过程就是初始化一套值，然后迭代计算，根据结果再调整值，再迭代，最后收敛……好吧，这个理解是没有捷径的，去隔壁钻研EM吧。&lt;&#x2F;p&gt;
&lt;p&gt;这里只提一下核心。因为我们手里没有隐状态序列 $(i_{1}, \cdots, i_{i})$ 信息，所以我先必须给初值 $a_{ij}^{0}, b_{j}(k)^{0}, \pi^{0}$ ，初步确定模型，然后再迭代计算出 $a_{ij}^{n}, b_{j}(k)^{n}, \pi^{n}$ ,中间计算过程会用到给出的观测状态序列 $(o_{1}, \cdots, o_{i})$。另外，收敛性由EM的XXX定理保证。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-2-2-xu-lie-biao-zhu-jie-ma-guo-cheng&quot;&gt;&lt;strong&gt;3.2.2 序列标注（解码）过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;好了，学习完了HMM的分布参数，也就确定了一个HMM模型。需要注意的是，这个HMM是对我这一批全部的数据进行训练所得到的参数。&lt;&#x2F;p&gt;
&lt;p&gt;序列标注问题也就是“预测过程”，通常称为解码过程。对应了序列建模问题3.。对于序列标注问题，我们只需要学习出一个HMM模型即可，后面所有的新的sample我都用这一个HMM去apply。&lt;&#x2F;p&gt;
&lt;p&gt;我们的目的是，在学习后已知了 $P(Q,O)$ ,现在要求出 $P(Q|O)$ ，进一步&lt;&#x2F;p&gt;
&lt;p&gt;$Q_{max} = argmax_{allQ}\frac{P(Q,O)}{P(O)}$&lt;&#x2F;p&gt;
&lt;p&gt;再直白点就是，我现在要在给定的观测序列下找出一条隐状态序列，条件是这个隐状态序列的概率是最大的那个。&lt;&#x2F;p&gt;
&lt;p&gt;具体地，都是用Viterbi算法解码，是用DP思想减少重复的计算。Viterbi也是满大街的，不过要说的是，Viterbi不是HMM的专属，也不是任何模型的专属，他只是恰好被满足了被HMM用来使用的条件。谁知，现在大家都把Viterbi跟HMM捆绑在一起了, shame。&lt;&#x2F;p&gt;
&lt;p&gt;Viterbi计算有向无环图的一条最大路径，应该还好理解。如图：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;418&quot; src=&quot;.&#x2F;10.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;关键是注意，每次工作热点区只涉及到t 与 t-1,这对应了DP的无后效性的条件。如果对某些同学还是很难理解，请参考&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20136144&quot;&gt;这个答案&lt;&#x2F;a&gt;下@Kiwee的回答吧。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-2-3-xu-lie-gai-lu-guo-cheng&quot;&gt;&lt;strong&gt;3.2.3 序列概率过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;我通过HMM计算出序列的概率又有什么用？针对这个点我把这个问题详细说一下。&lt;&#x2F;p&gt;
&lt;p&gt;实际上，序列概率过程对应了序列建模问题2.，即序列分类。&lt;br &#x2F;&gt;
在3.2.2第一句话我说，在序列标注问题中，我用一批完整的数据训练出了一支HMM模型即可。好，那在序列分类问题就不是训练一个HMM模型了。我应该这么做（结合语音分类识别例子）：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;目标：识别声音是A发出的还是B发出的。&lt;br &#x2F;&gt;
HMM建模过程：&lt;br &#x2F;&gt;
1. 训练：我将所有A说的语音数据作为dataset_A,将所有B说的语音数据作为dataset_B（当然，先要分别对dataset A ,B做预处理encode为元数据节点，形成sequences）,然后分别用dataset_A、dataset_B去训练出HMM_A&#x2F;HMM_B&lt;br &#x2F;&gt;
2. inference：来了一条新的sample（sequence），我不知道是A的还是B的，没问题，分别用HMM_A&#x2F;HMM_B计算一遍序列的概率得到 $P_{A}(S)、P_{B}(S)$ ，比较两者大小，哪个概率大说明哪个更合理，更大概率作为目标类别。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;所以，本小节的理解重点在于，&lt;strong&gt;如何对一条序列计算其整体的概率&lt;&#x2F;strong&gt;。即目标是计算出 $P(O|λ)$ 。这个问题前辈们在他们的经典中说的非常好了，比如参考李航老师整理的：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;直接计算法（穷举搜索）&lt;&#x2F;li&gt;
&lt;li&gt;前向算法&lt;&#x2F;li&gt;
&lt;li&gt;后向算法&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;后面两个算法采用了DP思想，减少计算量，即每一次直接引用前一个时刻的计算结果以避免重复计算，跟Viterbi一样的技巧。&lt;&#x2F;p&gt;
&lt;p&gt;还是那句，因为这篇文档不是专门讲算法细节的，所以不详细展开这些。毕竟，所有的科普HMM、CRF的博客貌似都是在扯这些算法，妥妥的街货，就不搬运了。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;si-memm&quot;&gt;&lt;strong&gt;四、MEMM&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;MEMM，即最大熵马尔科夫模型，这个是在接触了HMM、CRF之后才知道的一个模型。说到MEMM这一节时，得转换思维了，因为现在这MEMM属于判别式模型。&lt;&#x2F;p&gt;
&lt;p&gt;不过有一点很尴尬，MEMM貌似被使用或者讲解引用的不及HMM、CRF。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-1-li-jie-memm&quot;&gt;&lt;strong&gt;4.1 理解MEMM&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;这里还是啰嗦强调一下，MEMM正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，比如说序列求概率（分类）问题，我直接考虑找出函数分类边界。这一点跟HMM的思维方式发生了很大的变化，如果不对这一点有意识，那么很难理解为什么MEMM、CRF要这么做。&lt;&#x2F;p&gt;
&lt;p&gt;HMM中，观测节点 $o_{i}$ 依赖隐藏状态节点 $i_{i}$ ,也就意味着我的观测节点只依赖当前时刻的隐藏状态。但在更多的实际场景下，观测序列是需要很多的特征来刻画的，比如说，我在做NER时，我的标注 $i_{i}$ 不仅跟当前状态 $o_{i}$ 相关，而且还跟前后标注 $o_{j}(j \neq i)$ 相关，比如字母大小写、词性等等。&lt;&#x2F;p&gt;
&lt;p&gt;为此，提出来的MEMM模型就是能够直接允许**“定义特征”**，直接学习条件概率，即 $P(i_{i}|i_{i-1},o_{i}) (i = 1,\cdots,n)$ , 总体为：&lt;&#x2F;p&gt;
&lt;p&gt;$P(I|O) = \prod_{t=1}^{n}P(i_{i}|i_{i-1},o_{i}), i = 1,\cdots,n$&lt;&#x2F;p&gt;
&lt;p&gt;并且， $P(i|i^{’},o)$ 这个概率通过最大熵分类器建模（取名MEMM的原因）:&lt;&#x2F;p&gt;
&lt;p&gt;$P(i|i^{‘},o) = \frac{1}{Z(o,i^{’})} exp(\sum_{a})\lambda_{a}f_{a}(o,i)$&lt;&#x2F;p&gt;
&lt;p&gt;重点来了，这是ME的内容，也是理解MEMM的关键： $Z(o,i^{’})$ 这部分是归一化； $f_{a}(o,i)$ 是&lt;strong&gt;特征函数&lt;&#x2F;strong&gt;，具体点，这个函数是需要去定义的; $λ$ 是特征函数的权重，这是个未知参数，需要从训练阶段学习而得。&lt;&#x2F;p&gt;
&lt;p&gt;比如我可以这么定义特征函数：&lt;&#x2F;p&gt;
&lt;p&gt;$$\begin{equation} f_{a}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定条件}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;&#x2F;p&gt;
&lt;p&gt;其中，特征函数 $f_{a}(o,i)$ 个数可任意制定， $(a = 1, \cdots, n)$&lt;&#x2F;p&gt;
&lt;p&gt;所以总体上，MEMM的建模公式这样：&lt;&#x2F;p&gt;
&lt;p&gt;$P(I|O) = \prod_{t=1}^{n}\frac{ exp(\sum_{a})\lambda_{a}f_{a}(o,i) }{Z(o,i_{i-1})} , i = 1,\cdots,n$&lt;&#x2F;p&gt;
&lt;p&gt;是的，公式这部分之所以长成这样，是由ME模型决定的。&lt;&#x2F;p&gt;
&lt;p&gt;请务必注意，理解&lt;strong&gt;判别模型&lt;&#x2F;strong&gt;和&lt;strong&gt;定义特征&lt;&#x2F;strong&gt;两部分含义，这已经涉及到CRF的雏形了。&lt;&#x2F;p&gt;
&lt;p&gt;所以说，他是判别式模型，直接对条件概率建模。 上图：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;415&quot; src=&quot;.&#x2F;11.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;MEMM需要两点注意：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;与HMM的 $o_{i}$ 依赖 $i_{i}$ 不一样，MEMM当前隐藏状态 $i_{i}$ 应该是依赖当前时刻的观测节点 $o_{i}$ 和上一时刻的隐藏节点 $i_{i-1}$&lt;&#x2F;li&gt;
&lt;li&gt;需要注意，之所以图的箭头这么画，是由MEMM的公式决定的，而公式是creator定义出来的。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;好了，走一遍完整流程。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;step1. 先预定义特征函数 $f_{a}(o,i)$ ，&lt;br &#x2F;&gt;
step2. 在给定的数据上，训练模型，确定参数，即确定了MEMM模型&lt;br &#x2F;&gt;
step3. 用确定的模型做序列标注问题或者序列求概率问题。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;4-2-mo-xing-yun-xing-guo-cheng&quot;&gt;&lt;strong&gt;4.2 模型运行过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;MEMM模型的工作流程也包括了学习训练问题、序列标注问题、序列求概率问题。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-2-1-xue-xi-xun-lian-guo-cheng&quot;&gt;&lt;strong&gt;4.2.1 学习训练过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;一套MEMM由一套参数唯一确定，同样地，我需要通过训练数据学习这些参数。MEMM模型很自然需要学习里面的特征权重λ。&lt;&#x2F;p&gt;
&lt;p&gt;不过跟HMM不用的是，因为HMM是生成式模型，参数即为各种概率分布元参数，数据量足够可以用最大似然估计。而判别式模型是用函数直接判别，学习边界，MEMM即通过特征函数来界定。但同样，MEMM也有极大似然估计方法、梯度下降、牛顿迭代发、拟牛顿下降、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。&lt;&#x2F;p&gt;
&lt;p&gt;嗯，具体详细求解过程貌似问题不大。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-2-2-xu-lie-biao-zhu-guo-cheng&quot;&gt;&lt;strong&gt;4.2.2 序列标注过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;还是跟HMM一样的，用学习好的MEMM模型，在新的sample（观测序列 $o_{1}, \cdots, o_{i}$ ）上找出一条概率最大最可能的隐状态序列 $i_{1}, \cdots, i_{i}$。&lt;&#x2F;p&gt;
&lt;p&gt;只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-2-3-xu-lie-qiu-gai-lu-guo-cheng&quot;&gt;&lt;strong&gt;4.2.3 序列求概率过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的MEMM，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。&lt;&#x2F;p&gt;
&lt;p&gt;应该可以不用展开，吧……&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-3-biao-zhu-pian-zhi&quot;&gt;&lt;strong&gt;4.3 标注偏置？&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;MEMM讨论的最多的是他的labeling bias 问题。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 现象&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;是从街货上烤过来的……&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;558&quot; src=&quot;.&#x2F;12.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;用Viterbi算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2。 解码过程细节（需要会viterbi算法这个前提）：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;P(1-&amp;gt; 1-&amp;gt; 1-&amp;gt; 1)= 0.4 x 0.45 x 0.5 = 0.09 ，&lt;br &#x2F;&gt;
P(2-&amp;gt;2-&amp;gt;2-&amp;gt;2)= 0.2 X 0.3 X 0.3 = 0.018，&lt;br &#x2F;&gt;
P(1-&amp;gt;2-&amp;gt;1-&amp;gt;2)= 0.6 X 0.2 X 0.5 = 0.06，&lt;br &#x2F;&gt;
P(1-&amp;gt;1-&amp;gt;2-&amp;gt;2)= 0.4 X 0.55 X 0.3 = 0.066&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;但是得到的最优的状态转换路径是1-&amp;gt;1-&amp;gt;1-&amp;gt;1，为什么呢？因为状态2可以转换的状态比状态1要多，从而使转移概率降低,即MEMM倾向于选择拥有更少转移的状态。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. 解释原因&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;直接看MEMM公式：&lt;&#x2F;p&gt;
&lt;p&gt;$P(I|O) = \prod_{t=1}^{n}\frac{ exp[(\sum_{a})\lambda_{a}f_{a}(o,i)] }{Z(o,i_{i-1})} , i = 1,\cdots,n$&lt;&#x2F;p&gt;
&lt;p&gt;$∑$ 求和的作用在概率中是归一化，但是这里归一化放在了指数内部，管这叫local归一化。 来了，viterbi求解过程，是用dp的状态转移公式（MEMM的没展开，请参考CRF下面的公式），因为是局部归一化，所以MEMM的viterbi的转移公式的第二部分出现了问题，导致dp无法正确的递归到全局的最优。&lt;&#x2F;p&gt;
&lt;p&gt;$\delta_{i+1} = max_{1 \le j \le m}\lbrace \delta_{i}(I) + \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i) \rbrace$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;wu-crf&quot;&gt;&lt;strong&gt;五、CRF&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;我觉得一旦有了一个清晰的工作流程，那么按部就班地，没有什么很难理解的地方，因为整体框架已经胸有成竹了，剩下了也只有添砖加瓦小修小补了。有了上面的过程基础，CRF也是类似的，只是有方法论上的细微区别。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-1-li-jie-crf&quot;&gt;&lt;strong&gt;5.1 理解CRF&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;请看第一张概率图模型构架图，CRF上面是马尔科夫随机场（马尔科夫网络），而条件随机场是在给定的随机变量 $X$ （具体，对应观测序列 $o_{1}, \cdots, o_{i}$ ）条件下，随机变量 $Y$ （具体，对应隐状态序列 $i_{1}, \cdots, i_{i}$ ）的马尔科夫随机场。&lt;br &#x2F;&gt;
广义的CRF的定义是： 满足 $P(Y_{v}|X,Y_{w},w \neq v) = P(Y_{v}|X,Y_{w},w \sim v)$ 的马尔科夫随机场叫做条件随机场（CRF）。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen注:&lt;br &#x2F;&gt;
$Y_{w},w \neq v$ 表示除$v$以外观测集中的所有节点，&lt;br &#x2F;&gt;
$Y_{w},w \sim v$ 表示观测集中$v$的邻接节点，&lt;&#x2F;p&gt;
&lt;p&gt;下面是另一种表达方式：&lt;br &#x2F;&gt;
$P(Y_v|X,Y_{V\backslash{v}}) = P(Y_v|X,Y_{n(v)}) \$ 
其中：&lt;br &#x2F;&gt;
$Y_{V\backslash{v}}$ 表示除$v$以外的$V$中所有节点，&lt;br &#x2F;&gt;
$Y_{n(v)}$ 表示结点$v$的邻接节点&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;不过一般说CRF为序列建模，就专指CRF线性链（linear chain CRF）：&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;415&quot; src=&quot;.&#x2F;13.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;在2.1.2中有提到过，概率无向图的联合概率分布可以在因子分解下表示为：&lt;&#x2F;p&gt;
&lt;p&gt;$$P(Y | X)=\frac{1}{Z(x)} \prod_{c}\psi_{c}(Y_{c}|X ) = \frac{1}{Z(x)} \prod_{c} e^{\sum_{k}\lambda_{k}f_{k}(c,y|c,x)} = \frac{1}{Z(x)} e^{\sum_{c}\sum_{k}\lambda_{k}f_{k}(y_{i},y_{i-1},x,i)}$$&lt;&#x2F;p&gt;
&lt;p&gt;而在线性链CRF示意图中，每一个（ $I_{i} \sim O_{i}$ ）对为一个最大团,即在上式中 $c = i$ 。并且线性链CRF满足 $P(I_{i}|O,I_{1},\cdots, I_{n}) = P(I_{i}|O,I_{i-1},I_{i+1})$ 。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;所以CRF的建模公式如下：&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;$$P(I | O)=\frac{1}{Z(O)} \prod_{i}\psi_{i}(I_{i}|O ) = \frac{1}{Z(O)} \prod_{i} e^{\sum_{k}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)} = \frac{1}{Z(O)} e^{\sum_{i}\sum_{k}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;&#x2F;p&gt;
&lt;p&gt;我要敲黑板了，这个公式是非常非常关键的，注意递推过程啊，我是怎么从 $∏$ 跳到 $e^{\sum}$ 的。&lt;&#x2F;p&gt;
&lt;p&gt;不过还是要多啰嗦一句，想要理解CRF，必须判别式模型的概念要深入你心。
正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，因
为我创造出来就是为了这个分边界的目的的。比如说序列求概率（分类）问
题，我直接考虑找出函数分类边界。所以才为什么会有这个公式。所以再看
到这个公式也别懵逼了，he was born for discriminating the given data
from different classes. 就这样。不过待会还会具体介绍特征函数部分的东西。&lt;&#x2F;p&gt;
&lt;p&gt;除了建模总公式，关键的CRF重点概念在MEMM中已强调过：&lt;strong&gt;判别式模型&lt;&#x2F;strong&gt;、&lt;strong&gt;特征函数&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. 特征函数&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;上面给出了CRF的建模公式：&lt;&#x2F;p&gt;
&lt;p&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;下标 &lt;em&gt;i&lt;&#x2F;em&gt; 表示我当前所在的节点（token）位置。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;下标 &lt;em&gt;k&lt;&#x2F;em&gt; 表示我这是第几个特征函数，并且每个特征函数都附属一个权重 $\lambda_{k}$ ，也就是这么回事，每个团里面，我将为 $token_{i}$ 构造M个特征，每个特征执行一定的限定作用，然后建模时我再为每个特征函数加权求和。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$Z(O)$ 是用来归一化的，为什么？想想LR以及softmax为何有归一化呢，一样的嘛，形成概率值。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;再来个重要的理解。 $P(I|O)$ 这个表示什么？具体地，表示了在给定的一条观测序列 $O=(o_{1},\cdots, o_{i})$ 条件下，我用CRF所求出来的隐状态序列 $I=(i_{1},\cdots, i_{i})$ 的概率，注意，这里的 $I$ 是一条序列，有多个元素（一组随机变量），而至于观测序列 $O=(o_{1},\cdots, o_{i})$ ，它可以是一整个训练语料的所有的观测序列；也可以是在inference阶段的一句sample，比如说对于序列标注问题，我对一条sample进行预测，可能能得到 $P_{j}(I | O)（j=1,…,J)$,  $J$条隐状态$I$，但我肯定最终选的是最优概率的那条（by viterbi）。这一点希望你能理解。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;对于CRF，可以为他定义两款特征函数：转移特征&amp;amp;状态特征。 我们将建模总公式展开：&lt;&#x2F;p&gt;
&lt;p&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}=\frac{1}{Z(O)} e^{ [ \sum_{i}^{T}\sum_{j}^{J}\lambda_{j}t_{j}(O,I_{i-1},I_{i},i) + \sum_{i}^{T}\sum_{l}^{L}\mu_{l}s_{l}(O,I_{i},i) ] }$$&lt;&#x2F;p&gt;
&lt;p&gt;其中：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$t_{j}$ 为i处的转移特征，对应权重 $\lambda_{j}$ ,每个 $token_{i}$ 都有J个特征,转移特征针对的是前后token之间的限定。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;举个例子：&lt;&#x2F;p&gt;
&lt;p&gt;$$\begin{equation} t_{k=1}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定转移条件，比如前一个token是‘I’}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$s_l$为i 处的状态特征，对应权重$μ_l$，每个$token_i$都有L个特征&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;举个例子：&lt;&#x2F;p&gt;
&lt;p&gt;$$\begin{equation} s_{l=1}(o,i) = \begin{cases} 1&amp;amp; \text{满足特定状态条件，比如当前token的POS是‘V’}，\\ 0&amp;amp; \text{other} \end{cases} \end{equation}$$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;不过一般情况下，我们不把两种特征区别的那么开，合在一起：&lt;&#x2F;p&gt;
&lt;p&gt;$$P(I | O)=\frac{1}{Z(O)} e^{\sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)}$$&lt;&#x2F;p&gt;
&lt;p&gt;满足特征条件就取值为1，否则没贡献，甚至你还可以让他打负分，充分惩罚。&lt;&#x2F;p&gt;
&lt;p&gt;再进一步理解的话，我们需要把特征函数部分抠出来：&lt;&#x2F;p&gt;
&lt;p&gt;$$Score = \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i)$$&lt;&#x2F;p&gt;
&lt;p&gt;是的，我们为 $token_{i}$ 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值……完了又扯到了log线性模型。现在稍作解释：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;log-linear models take the following form:&lt;br &#x2F;&gt;
$P(y|x;\omega) = \frac{ exp(\omega·\phi(x,y)) }{ \sum_{y^{’}\in Y }exp(\omega·\phi(x,y^{‘})) }$&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;我觉得对LR或者sotfmax熟悉的对这个应该秒懂。然后CRF完美地满足这个形式，所以又可以归入到了log-linear models之中。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-2-mo-xing-yun-xing-guo-cheng&quot;&gt;&lt;strong&gt;5.2 模型运行过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;模型的工作流程，跟MEMM是一样的：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;step1. 先预定义特征函数 $f_{a}(o,i)$ ，&lt;&#x2F;li&gt;
&lt;li&gt;step2. 在给定的数据上，训练模型，确定参数 $\lambda_{k}$&lt;&#x2F;li&gt;
&lt;li&gt;step3. 用确定的模型做&lt;code&gt;序列标注问题&lt;&#x2F;code&gt;或者&lt;code&gt;序列求概率问题&lt;&#x2F;code&gt;。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;可能还是没做到100%懂，结合例子说明：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;5-2-1-xue-xi-xun-lian-guo-cheng&quot;&gt;&lt;strong&gt;5.2.1 学习训练过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;一套CRF由一套参数λ唯一确定（先定义好各种特征函数）。&lt;&#x2F;p&gt;
&lt;p&gt;同样，CRF用极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。其实能用在log-linear models上的求参方法都可以用过来。&lt;&#x2F;p&gt;
&lt;p&gt;嗯，具体详细求解过程貌似问题不大。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-2-2-xu-lie-biao-zhu-guo-cheng&quot;&gt;&lt;strong&gt;5.2.2 序列标注过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;还是跟HMM一样的，用学习好的CRF模型，在新的sample（观测序列 $o_{1}, \cdots, o_{i}$ ）上找出一条概率最大最可能的隐状态序列 $i_{1}, \cdots, i_{i}$ 。&lt;&#x2F;p&gt;
&lt;p&gt;只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。&lt;&#x2F;p&gt;
&lt;p&gt;啰嗦一下，我们就定义i处的局部状态为 $\delta_{i}(I)$ ,表示在位置i处的隐状态的各种取值可能为 &lt;em&gt;I&lt;&#x2F;em&gt; ，然后递推位置i+1处的隐状态，写出来的DP转移公式为：&lt;&#x2F;p&gt;
&lt;p&gt;$\delta_{i+1} = max_{1 \le j \le m}\lbrace \delta_{i}(I) + \sum_{i}^{T}\sum_{k}^{M}\lambda_{k}f_{k}(O,I_{i-1},I_{i},i) \rbrace$&lt;&#x2F;p&gt;
&lt;p&gt;这里没写规范因子 $Z(O)$ 是因为不规范化不会影响取最大值后的比较。&lt;&#x2F;p&gt;
&lt;p&gt;具体还是不展开为好。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-2-3-xu-lie-qiu-gai-lu-guo-cheng&quot;&gt;&lt;strong&gt;5.2.3 序列求概率过程&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的CRF，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。只是貌似很少看到拿CRF或者MEMM来做分类的，直接用网络模型不就完了不……&lt;&#x2F;p&gt;
&lt;p&gt;应该可以不用展开，吧……&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-3-crf-fen-xi&quot;&gt;&lt;strong&gt;5.3 CRF++分析&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;本来做task用CRF++跑过baseline,后来在对CRF做调研时，非常想透析CRF++的工作原理，以identify以及verify做的各种假设猜想。当然，也看过其他的CRF实现源码。&lt;&#x2F;p&gt;
&lt;p&gt;所以干脆写到这里来，结合CRF++实例讲解过程。&lt;&#x2F;p&gt;
&lt;p&gt;有一批语料数据，并且已经tokenized好了：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nuclear&lt;br &#x2F;&gt;
theory&lt;br &#x2F;&gt;
devoted&lt;br &#x2F;&gt;
major&lt;br &#x2F;&gt;
efforts&lt;br &#x2F;&gt;
……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;并且我先确定了13个标注元素：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;B_MAT&lt;br &#x2F;&gt;
B_PRO&lt;br &#x2F;&gt;
B_TAS&lt;br &#x2F;&gt;
E_MAT&lt;br &#x2F;&gt;
E_PRO&lt;br &#x2F;&gt;
E_TAS&lt;br &#x2F;&gt;
I_MAT&lt;br &#x2F;&gt;
I_PRO&lt;br &#x2F;&gt;
I_TAS&lt;br &#x2F;&gt;
O&lt;br &#x2F;&gt;
S_MAT&lt;br &#x2F;&gt;
S_PRO&lt;br &#x2F;&gt;
S_TAS&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;1. 定义模板&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;按道理应该是定义特征函数才对吧？好的，在CRF++下，应该是先定义特征模板，然后用模板自动批量产生大量的特征函数。我之前也蛮confused的，用完CRF++还以为模板就是特征，后面就搞清楚了：每一条模板将在每一个token处生产若干个特征函数。&lt;&#x2F;p&gt;
&lt;p&gt;CRF++的模板（template）有U系列（unigram）、B系列(bigram)，不过我至今搞不清楚B系列的作用，因为U模板都可以完成2-gram的作用。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;U00:%x[-2,0]&lt;br &#x2F;&gt;
U01:%x[-1,0]&lt;br &#x2F;&gt;
U02:%x[0,0]&lt;br &#x2F;&gt;
U03:%x[1,0]&lt;br &#x2F;&gt;
U04:%x[2,0]&lt;&#x2F;p&gt;
&lt;p&gt;U05:%x[-2,0]&#x2F;%x[-1,0]&#x2F;%x[0,0]&lt;br &#x2F;&gt;
U06:%x[-1,0]&#x2F;%x[0,0]&#x2F;%x[1,0]&lt;br &#x2F;&gt;
U07:%x[0,0]&#x2F;%x[1,0]&#x2F;%x[2,0]&lt;br &#x2F;&gt;
U08:%x[-1,0]&#x2F;%x[0,0]&lt;br &#x2F;&gt;
U09:%x[0,0]&#x2F;%x[1,0]&lt;&#x2F;p&gt;
&lt;p&gt;B&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;所以，U00 - U09 我定义了10个模板。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. 产生特征函数&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;是的，会产生大量的特征。 U00 - U04的模板产生的是状态特征函数；U05 - U09的模板产生的是转移特征函数。&lt;&#x2F;p&gt;
&lt;p&gt;在CRF++中，每个特征都会try每个标注label（这里有13个），总共将生成 $N * L = i * k^{‘} * L$ 个特征函数以及对应的权重出来。N表示每一套特征函数 $N= i * k^{’}$ ，L表示标注集元素个数。&lt;&#x2F;p&gt;
&lt;p&gt;比如训练好的CRF模型的部分特征函数是这样存储的：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;22607 B&lt;br &#x2F;&gt;
790309 U00:%&lt;br &#x2F;&gt;
3453892 U00:%)&lt;br &#x2F;&gt;
2717325 U00:&amp;amp;&lt;br &#x2F;&gt;
2128269 U00:’t&lt;br &#x2F;&gt;
2826239 U00:(0.3534&lt;br &#x2F;&gt;
2525055 U00:(0.593–1.118&lt;br &#x2F;&gt;
197093 U00:(1)&lt;br &#x2F;&gt;
2079519 U00:(1)L=14w2−12w−FμνaFaμν&lt;br &#x2F;&gt;
2458547 U00:(1)δn=∫−∞En+1ρ˜(E)dE−n&lt;br &#x2F;&gt;
1766024 U00:(1.0g&lt;br &#x2F;&gt;
2679261 U00:(1.1wt%)&lt;br &#x2F;&gt;
1622517 U00:(100)&lt;br &#x2F;&gt;
727701 U00:(1000–5000A)&lt;br &#x2F;&gt;
2626520 U00:(10a)&lt;br &#x2F;&gt;
2626689 U00:(10b)&lt;br &#x2F;&gt;
……&lt;br &#x2F;&gt;
2842814 U07:layer&#x2F;thicknesses&#x2F;Using&lt;br &#x2F;&gt;
2847533 U07:layer&#x2F;thicknesses&#x2F;are&lt;br &#x2F;&gt;
2848651 U07:layer&#x2F;thicknesses&#x2F;in&lt;br &#x2F;&gt;
331539 U07:layer&#x2F;to&#x2F;the&lt;br &#x2F;&gt;
1885871 U07:layer&#x2F;was&#x2F;deposited&lt;br &#x2F;&gt;
……（数量非常庞大）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;其实也就是对应了这样些个特征函数：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;func1 = if (output = B and feature=“U02:一”) return 1 else return 0&lt;br &#x2F;&gt;
func2 = if (output = M and feature=“U02:一”) return 1 else return 0&lt;br &#x2F;&gt;
func3 = if (output = E and feature=“U02:一”) return 1 else return 0&lt;br &#x2F;&gt;
func4 = if (output = S and feature=“U02:一”) return 1 else return 0&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;比如模板U06会从语料中one by one逐句抽出这些各个特征：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;一&#x2F;个&#x2F;人&#x2F;……&lt;br &#x2F;&gt;
个&#x2F;人&#x2F;走&#x2F;……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;3. 求参&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;对上述的各个特征以及初始权重进行迭代参数学习。&lt;&#x2F;p&gt;
&lt;p&gt;在CRF++ 训练好的模型里，权重是这样的：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;0.3972716048310705&lt;br &#x2F;&gt;
0.5078838237171732&lt;br &#x2F;&gt;
0.6715316559507898&lt;br &#x2F;&gt;
-0.4198827647512405&lt;br &#x2F;&gt;
-0.4233310655891150&lt;br &#x2F;&gt;
-0.4176580083832543&lt;br &#x2F;&gt;
-0.4860489836004728&lt;br &#x2F;&gt;
-0.6156475863742051&lt;br &#x2F;&gt;
-0.6997919485753300&lt;br &#x2F;&gt;
0.8309956709647820&lt;br &#x2F;&gt;
0.3749695682658566&lt;br &#x2F;&gt;
0.2627347894057647&lt;br &#x2F;&gt;
0.0169732441379157&lt;br &#x2F;&gt;
0.3972716048310705&lt;br &#x2F;&gt;
0.5078838237171732&lt;br &#x2F;&gt;
0.6715316559507898&lt;br &#x2F;&gt;
……（数量非常庞大，与每个label的特征函数对应，我这有300W个）&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;4. 预测解码&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;结果是这样的：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nuclear B_TAS&lt;br &#x2F;&gt;
theory E_TAS&lt;br &#x2F;&gt;
devoted O&lt;br &#x2F;&gt;
major O&lt;br &#x2F;&gt;
efforts O&lt;br &#x2F;&gt;
……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;5-4-lstm-crf&quot;&gt;&lt;strong&gt;5.4 LSTM+CRF&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;LSTM+CRF这个组合其实我在知乎上答过问题，然后顺便可以整合到这里来。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1、perspectively&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;大家都知道，LSTM已经可以胜任序列标注问题了，为每个token预测一个label（LSTM后面接:分类器）；而CRF也是一样的，为每个token预测一个label。&lt;&#x2F;p&gt;
&lt;p&gt;但是，他们的预测机理是不同的。CRF是全局范围内统计归一化的条件状态转移概率矩阵，再预测出一条指定的sample的每个token的label；LSTM（RNNs，不区分here）是依靠神经网络的超强非线性拟合能力，在训练时将samples通过复杂到让你窒息的高阶高纬度异度空间的非线性变换，学习出一个模型，然后再预测出一条指定的sample的每个token的label。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2、LSTM+CRF&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;既然LSTM都OK了，为啥researchers搞一个LSTM+CRF的hybrid model?&lt;&#x2F;p&gt;
&lt;p&gt;哈哈，因为a single LSTM预测出来的标注有问题啊！举个segmentation例子(BES; char level)，plain LSTM 会搞出这样的结果：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;input&lt;&#x2F;strong&gt;: “学习出一个模型，然后再预测出一条指定”&lt;br &#x2F;&gt;
&lt;strong&gt;expected output&lt;&#x2F;strong&gt;: 学&#x2F;B 习&#x2F;E 出&#x2F;S 一&#x2F;B 个&#x2F;E 模&#x2F;B 型&#x2F;E ，&#x2F;S 然&#x2F;B 后&#x2F;E 再&#x2F;E 预&#x2F;B 测&#x2F;E ……&lt;br &#x2F;&gt;
&lt;strong&gt;real output&lt;&#x2F;strong&gt;: 学&#x2F;B 习&#x2F;E 出&#x2F;S 一&#x2F;B 个&#x2F;B 模&#x2F;B 型&#x2F;E ，&#x2F;S 然&#x2F;B 后&#x2F;B 再&#x2F;E 预&#x2F;B 测&#x2F;E ……&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;看到不，用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。当然了，CRF还能学到更多的限定特征，那越多越好啊！&lt;&#x2F;p&gt;
&lt;p&gt;好了，那就把CRF接到LSTM上面，把LSTM在time_step上把每一个hidden_state的tensor输入给CRF，让LSTM负责在CRF的特征限定下，依照新的loss function，学习出一套新的非线性变换空间。&lt;&#x2F;p&gt;
&lt;p&gt;最后，不用说，结果还真是好多了呢。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;github.com&#x2F;scofield7419&#x2F;sequence-labeling-BiLSTM-CRF&quot;&gt;LSTM+CRF codes&lt;&#x2F;a&gt;, here. Go just take it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;liu-zong-jie&quot;&gt;&lt;strong&gt;六、总结&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;1-zong-ti-dui-bi&quot;&gt;&lt;strong&gt;1. 总体对比&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;应该看到了熟悉的图了，现在看这个图的话，应该可以很清楚地get到他所表达的含义了。这张图的内容正是按照生成式&amp;amp;判别式来区分的，NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;90%&quot; src=&quot;.&#x2F;14.jpg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-hmm-vs-memm-vs-crf&quot;&gt;&lt;strong&gt;2. HMM vs. MEMM vs. CRF&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;将三者放在一块做一个总结：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;HMM -&amp;gt; MEMM： HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;MEMM -&amp;gt; CRF:&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;ul&gt;
&lt;li&gt;CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。&lt;&#x2F;li&gt;
&lt;li&gt;HMM、MEMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去（这点问题应该只有HMM）。
CRF属于无向图，没有这种依赖性，克服此问题。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;3-machine-learning-models-vs-sequential-models&quot;&gt;&lt;strong&gt;3. Machine Learning models vs. Sequential models&lt;&#x2F;strong&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;为了一次将概率图模型理解的深刻到位，我们需要再串一串，更深度与原有的知识体系融合起来。&lt;&#x2F;p&gt;
&lt;p&gt;机器学习模型，按照学习的范式或方法，以及加上自己的理解，给常见的部分的他们整理分了分类（主流上，都喜欢从训练样本的歧义型分，当然也可以从其他角度来）：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;一、监督：{

1.1 分类算法(线性和非线性)：{

    感知机

    KNN

    概率{
        朴素贝叶斯（NB）
        Logistic Regression（LR）
        最大熵MEM（与LR同属于对数线性分类模型）
    }

    支持向量机(SVM)

    决策树(ID3、CART、C4.5)

    assembly learning{
        Boosting{
            Gradient Boosting{
                GBDT
                xgboost（传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）；xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。）
            }
            AdaBoost
        }   
        Bagging{
            随机森林
        }
        Stacking
    }

    ……
}

1.2 概率图模型：{
    HMM
    MEMM（最大熵马尔科夫）
    CRF
    ……
}

1.3 回归预测：{
    线性回归
    树回归
    Ridge岭回归
    Lasso回归
    ……
}

……  
}

二、非监督：{
2.1 聚类：{
    1. 基础聚类
        K—mean
        二分k-mean
        K中值聚类
        GMM聚类
    2. 层次聚类
    3. 密度聚类
    4. 谱聚类()
}

2.2 主题模型:{
    pLSA
    LDA隐含狄利克雷分析
}

2.3 关联分析：{
    Apriori算法
    FP-growth算法
}

2.4 降维：{
    PCA算法
    SVD算法
    LDA线性判别分析
    LLE局部线性嵌入
}

2.5 异常检测：
……
}

三、半监督学习

四、迁移学习
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;（注意到，没有把神经网络体系加进来。因为NNs的范式很灵活，不太适用这套分法，largely, off this framework）&lt;&#x2F;p&gt;
&lt;p&gt;Generally speaking，机器学习模型，尤其是有监督学习，一般是为一条sample预测出一个label，作为预测结果。 但与典型常见的机器学习模型不太一样，序列模型（概率图模型）是试图为一条sample里面的每个基本元数据分别预测出一个label。这一点，往往是beginner伊始难以理解的。&lt;&#x2F;p&gt;
&lt;p&gt;具体的实现手段差异，就是：ML models通过直接预测得出label；Sequential models是给每个token预测得出label还没完，还得将他们每个token对应的labels进行组合，具体的话，用viterbi来挑选最好的那个组合。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;over&quot;&gt;&lt;strong&gt;over&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;有了这道开胃菜，接下来，读者可以完成这些事情：完善细节算法、阅读原著相关论文达到彻底理解、理解相关拓展概念、理论创新……&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;hope those hlpe!&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;欢迎留言！&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;有错误之处请多多指正，谢谢！&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;referrences&quot;&gt;&lt;strong&gt;Referrences:&lt;&#x2F;strong&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;《统计学习方法》，李航&lt;&#x2F;p&gt;
&lt;p&gt;《统计自然语言处理》，宗成庆&lt;&#x2F;p&gt;
&lt;p&gt;《 An Introduction to Conditional Random Fields for Relational Learning》， Charles Sutton， Andrew McCallum&lt;&#x2F;p&gt;
&lt;p&gt;《Log-Linear Models, MEMMs, and CRFs》，ichael Collins&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;35866596&quot;&gt;如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;www.cnblogs.com&#x2F;en-heng&#x2F;p&#x2F;6201893.html&quot;&gt;【中文分词】最大熵马尔可夫模型MEMM - Treant - 博客园&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;github.com&#x2F;timvieira&#x2F;crf&quot;&gt;timvieira&#x2F;crf&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;github.com&#x2F;shawntan&#x2F;python-crf&quot;&gt;shawntan&#x2F;python-crf&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=http%3A&#x2F;&#x2F;videolectures.net&#x2F;cikm08_elkan_llmacrf&#x2F;&quot;&gt;Log-linear Models and Conditional Random Fields&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;55755fc649b1&quot;&gt;如何轻松愉快地理解条件随机场（CRF）？&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;www.cnblogs.com&#x2F;pinard&#x2F;p&#x2F;7068574.html&quot;&gt;条件随机场CRF(三) 模型学习与维特比算法解码&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20279019&quot;&gt;crf++里的特征模板得怎么理解？&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=http%3A&#x2F;&#x2F;www.hankcs.com&#x2F;ml&#x2F;crf-code-analysis.html&quot;&gt;CRF++代码分析-码农场&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=http%3A&#x2F;&#x2F;blog.csdn.net&#x2F;aws3217150&#x2F;article&#x2F;details&#x2F;69212445&quot;&gt;CRF++源码解读 - CSDN博客&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=http%3A&#x2F;&#x2F;www.hankcs.com&#x2F;nlp&#x2F;the-crf-model-format-description.html&quot;&gt;CRF++模型格式说明-码农场&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;link.zhihu.com&#x2F;?target=https%3A&#x2F;&#x2F;www.cnblogs.com&#x2F;syx-1987&#x2F;p&#x2F;4077325.html&quot;&gt;标注偏置问题(Label Bias Problem)和HMM、MEMM、CRF模型比较&amp;lt;转&amp;gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;……&lt;&#x2F;p&gt;
&lt;p&gt;编辑于 2018-03-21&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;$Y_{w},w \neq v$ 表示除$v$以外观测集中的所有&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>TensorLayer 教程</title>
        <published>2017-09-09T00:00:00+00:00</published>
        <updated>2017-09-09T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.freeopen.tech/tensorlayer/" type="text/html"/>
        <id>https://www.freeopen.tech/tensorlayer/</id>
        
        <content type="html">&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;zsdonghao&#x2F;tensorlayer&#x2F;blob&#x2F;master&#x2F;docs&#x2F;user&#x2F;tutorial.rst&quot;&gt;原文&lt;&#x2F;a&gt; | &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;shorxp&#x2F;tensorlayer-chinese&#x2F;blob&#x2F;master&#x2F;docs&#x2F;user&#x2F;tutorial.rst&quot;&gt;原译文&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 为什么研究 TensorLayer ? &lt;br&#x2F;&gt;
在TF的编码中，常看到model被封装成class，进一步研究，发现为了简化模型的创建，有各种封装库，包括Keras, Tflearn, TFSlim, tf.contrib.learn(或称skflow) 等。
其中TFSlim和skflow属TensorFlow自带，考虑到TFSlim中有现成的模型，就修改了一个试试，发现坑不小。我需要首先把数据源转成TFRecord格式，然后用模型训练后报错（out of range), 然后我就首先检查数据源的格式是否有问题，我采用TFSlim的Data
provider来输出数据，可能我的数据维度太大，程序不报错，但一直死在那里不出结果，而同样我在使用numpy输出数据源数据时，没有任何问题，也没什么延时，瞬间我感觉到TFSlim的封装模式不仅把问题复杂化了，甚至还降低了程序的性能。Keras据说跑TF性能较慢，Tflearn感觉活跃度较低，考虑到TensorLayer是skflow上的再次封装，且不丧失灵活性，加上这篇最主要的教程写得真心不错，所以我发愿要好好研究一番。 &lt;br&#x2F;&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;为什么我总喜欢修订一些长文翻译？&lt;br&#x2F;&gt;
技术翻译文章经常在阅读时有不知所云的现象，我总结的原因主要有三：一是译者没有理解作者意图，甚至连文章都没读懂，就开始直译，导致文理不通；
二是对技术术语不统一的称谓，本来英文是一个词，翻成中文后变成几个词，导致读者以为是不同的东西；三是过度翻译，把没必要或该省略的词也翻出来，反而影响理解。
比如vanilla RNN 中的vanilla, 并不代表“香草”，而是表示一个“普通的”或“原始的”意思，有时把这个词加进译文反而觉得多余，删掉最好。还有本文中“理解机器翻译－实现细节”部分的第6个段落，encoder_input_size 这个词，其实就是个代码变量，原译文居然也把它翻译出来，没有必要。我一般也是看到一点改一点，不保证能穷尽译文的所有地方，欢迎读者批评指正。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;对于深度学习，该教程会引导您使用MNIST数据集构建不同的手写数字的分类器，
这可以说是神经网络的 “Hello World” 。
对于强化学习，我们将让计算机根据屏幕输入来学习打乒乓球。
对于自然语言处理。我们从词嵌套（word embedding）开始，然后再实现语言建模和机器翻译。
此外，TensorLayer的Tutorial包含了所有TensorFlow官方深度学习教程的模块化实现，因此你可以对照TensorFlow深度学习教程&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.tensorflow.org&#x2F;versions&#x2F;master&#x2F;tutorials&#x2F;index.html&quot;&gt;(英文&lt;&#x2F;a&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;wiki.jikexueyuan.com&#x2F;project&#x2F;tensorflow-zh&#x2F;&quot;&gt;|中文)&lt;&#x2F;a&gt;来学习 。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;若你已经对TensorFlow非常熟悉，阅读 &lt;code&gt;InputLayer&lt;&#x2F;code&gt; 和 &lt;code&gt;DenseLayer&lt;&#x2F;code&gt; 的源代码可让您很好地理解 TensorLayer 是如何工作的。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;zai-wo-men-kai-shi-zhi-qian&quot;&gt;在我们开始之前&lt;&#x2F;h2&gt;
&lt;p&gt;本教程假定您在神经网络和 TensorFlow (TensorLayer在它的基础上构建的)方面具有一定的基础。
您可以尝试从&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;deeplearning.stanford.edu&#x2F;tutorial&#x2F;&quot;&gt;Deeplearning Tutorial&lt;&#x2F;a&gt; 同时进行学习。&lt;&#x2F;p&gt;
&lt;p&gt;对于人工神经网络更系统的介绍，我们推荐 Andrej Karpathy 等人所著的 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;cs231n.github.io&#x2F;&quot;&gt;Convolutional Neural Networks for Visual Recognition&lt;&#x2F;a&gt;
和 Michael Nielsen 的 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;neuralnetworksanddeeplearning.com&#x2F;&quot;&gt;Neural Networks and Deep Learning&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;要了解TensorFlow的更多内容，请阅读 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.tensorflow.org&#x2F;versions&#x2F;r0.9&#x2F;tutorials&#x2F;index.html&quot;&gt;TensorFlow tutorial&lt;&#x2F;a&gt; 。
您不需要会它的全部，只要知道TensorFlow是如何工作的，就能够使用TensorLayer。
如果您是TensorFlow的新手，建议你阅读整个教程。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;tensorlayerhen-jian-dan&quot;&gt;TensorLayer很简单&lt;&#x2F;h2&gt;
&lt;p&gt;下面的代码是TensorLayer的一个简单例子，来自 &lt;code&gt;tutorial_mnist_simple.py&lt;&#x2F;code&gt; 。
我们提供了很多方便的函数（如： &lt;code&gt;fit()&lt;&#x2F;code&gt; ，&lt;code&gt;test()&lt;&#x2F;code&gt; ），但如果你想了解更多实现细节，或想成为机器学习领域的专家，我们鼓励
您尽可能地直接使用TensorFlow原本的方法如 &lt;code&gt;sess.run()&lt;&#x2F;code&gt; 来训练模型，请参考  &lt;code&gt;tutorial_mnist.py&lt;&#x2F;code&gt; 。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python z-code&quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;z-source z-python&quot;&gt;&lt;span class=&quot;z-meta z-statement z-import z-python&quot;&gt;&lt;span class=&quot;z-keyword z-control z-import z-python&quot;&gt;import&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorflow&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-control z-import z-as z-python&quot;&gt;as&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-statement z-import z-python&quot;&gt;&lt;span class=&quot;z-keyword z-control z-import z-python&quot;&gt;import&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-control z-import z-as z-python&quot;&gt;as&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;sess&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;InteractiveSession&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 准备数据
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;, &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;, &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;, &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;, &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_test&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;, &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_test&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-punctuation z-separator z-continuation z-line z-python&quot;&gt;\&lt;&#x2F;span&gt;
                                &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;files&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;load_mnist_dataset&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-sequence z-tuple z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;1&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;784&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 定义 placeholder
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;x&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;placeholder&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-sequence z-list z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;[&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-language z-python&quot;&gt;None&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;784&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;x&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;placeholder&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;int64&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-sequence z-list z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;[&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-language z-python&quot;&gt;None&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;y_&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 定义模型
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;InputLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;x&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;input_layer&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DropoutLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;keep&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;8&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;drop1&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DenseLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;n_units&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;
                                &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;act&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;nn&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;relu&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;relu1&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DropoutLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;keep&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;5&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;drop2&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DenseLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;n_units&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;
                                &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;act&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;nn&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;relu&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;relu2&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DropoutLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;keep&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;5&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;drop3&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DenseLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;n_units&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;
                                &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;act&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;identity&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;
                                &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;output_layer&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 定义损失函数和衡量指标
&lt;&#x2F;span&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; tl.cost.cross_entropy 在内部使用 tf.nn.sparse_softmax_cross_entropy_with_logits() 实现 softmax
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;outputs&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;cost&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;cost&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;cross_entropy&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;cost&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;correct_prediction&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;equal&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;argmax&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;1&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;reduce_mean&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;cast&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;correct_prediction&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_op&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;argmax&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;nn&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;softmax&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;1&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 定义 optimizer
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train_params&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;all_params&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train_op&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tf&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;AdamOptimizer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;learning_rate&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;0001&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;beta1&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;9&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;beta2&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;999&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;
          &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;epsilon&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;1e-08&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;use_locking&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-language z-python&quot;&gt;False&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;minimize&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;cost&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;var_list&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train_params&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 初始化 session 中的所有参数
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;initialize_global_variables&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;sess&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 列出模型信息
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;print_params&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;print_layers&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 训练模型
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;utils&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;fit&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;sess&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train_op&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;cost&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;x&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;
            &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;batch_size&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;500&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;n_epoch&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;500&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;print_freq&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;5&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;
            &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;X_val&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;y_val&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;eval_train&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-language z-python&quot;&gt;False&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 评估模型
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;utils&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;test&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;sess&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_test&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_test&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;x&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;batch_size&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-language z-python&quot;&gt;None&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;cost&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;cost&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;span class=&quot;z-comment z-line z-number-sign z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-python&quot;&gt;#&lt;&#x2F;span&gt; 把模型保存成 .npz 文件
&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tl&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;files&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;save_npz&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;network&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;all_params&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;name&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-single z-python&quot;&gt;model.npz&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;sess&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;close&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;yun-xing-mnistli-zi&quot;&gt;运行MNIST例子&lt;&#x2F;h2&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;80%&quot; src=&quot;.&#x2F;mnist.jpeg&quot;&#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;在本教程的第一部分，我们仅仅运行TensorLayer内置的MNIST例子。
MNIST数据集包含了60000个28x28像素的手写数字图片，它通常用于训练各种图片识别系统。&lt;&#x2F;p&gt;
&lt;p&gt;我们假设您已经按照 &lt;code&gt;installation&lt;&#x2F;code&gt; 安装好了TensorLayer。如果您还没有，请复制一个TensorLayer的source目录到终端中，并进入该文件夹，
然后运行 &lt;code&gt;tutorial_mnist.py&lt;&#x2F;code&gt; 这个例子脚本：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  python tutorial_mnist.py
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;如果所有设置都正确，您将得到下面的结果：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python z-code&quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;z-source z-python&quot;&gt;  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;GPU&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;MEM&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Fraction&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;300000&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Downloading&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;images&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;idx3&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;ubyte&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;gz&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Downloading&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;labels&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;idx1&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;ubyte&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;gz&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Downloading&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;t10k&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;images&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;idx3&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;ubyte&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;gz&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Downloading&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;t10k&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;labels&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;idx1&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;ubyte&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;gz&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

  &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;50000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;784&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;50000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;784&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X_test&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;784&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y_test&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-accessor z-dot z-python&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;X&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;   &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;y&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;int64&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Instantiate&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;InputLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;input_layer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;?&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;784&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Instantiate&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DropoutLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;drop1&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;keep&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;800000&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Instantiate&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DenseLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;relu1&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;, &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;relu&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Instantiate&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DropoutLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;drop2&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;keep&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;500000&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Instantiate&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DenseLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;relu2&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;, &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;relu&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Instantiate&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DropoutLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;drop3&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;keep&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;500000&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;tensorlayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Instantiate&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;DenseLayer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;output_layer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10&lt;&#x2F;span&gt;, &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;identity&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;param&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;0&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-sequence z-tuple z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;784&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt; &lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;mean&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000053&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;median&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000043&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;std&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;035558&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;param&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;1&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-sequence z-tuple z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt; &lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;mean&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;median&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;std&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;param&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;2&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-sequence z-tuple z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt; &lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;mean&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000008&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;median&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000041&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;std&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;035371&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;param&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;3&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-sequence z-tuple z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt; &lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;mean&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;median&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;std&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;param&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;4&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-sequence z-tuple z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt; &lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;mean&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000469&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;median&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000432&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;std&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;049895&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;param&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;5&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-sequence z-tuple z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt; &lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;mean&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;median&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;std&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;: &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000000&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;num&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;of&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;params&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;1276810&lt;&#x2F;span&gt;

  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;0&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Tensor&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;dropout&#x2F;mul_1:0&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-group z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-group z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;?&lt;span class=&quot;z-punctuation z-separator z-tuple z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;784&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-group z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;1&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Tensor&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;Relu:0&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-group z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-group z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;?&lt;span class=&quot;z-punctuation z-separator z-tuple z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-group z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;2&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Tensor&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;dropout_1&#x2F;mul_1:0&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-group z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-group z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;?&lt;span class=&quot;z-punctuation z-separator z-tuple z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-group z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;3&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Tensor&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;Relu_1:0&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-group z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-group z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;?&lt;span class=&quot;z-punctuation z-separator z-tuple z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-group z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;4&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Tensor&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;dropout_2&#x2F;mul_1:0&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-group z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-group z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;?&lt;span class=&quot;z-punctuation z-separator z-tuple z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;800&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-group z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;layer&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;5&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Tensor&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;add_2:0&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-group z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-group z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;?&lt;span class=&quot;z-punctuation z-separator z-tuple z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-group z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-arguments z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-variable z-parameter z-python&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;float32&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;

  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;learning_rate&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;000100&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;batch_size&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;128&lt;&#x2F;span&gt;

  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Epoch&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;1&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;of&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;500&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;took&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;342539&lt;&#x2F;span&gt;s
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;330111&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;298098&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;910700&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Epoch&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;10&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;of&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;500&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;took&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;356471&lt;&#x2F;span&gt;s
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;085225&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;097082&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;971700&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Epoch&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;20&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;of&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;500&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;took&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;352137&lt;&#x2F;span&gt;s
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;040741&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;070149&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;978600&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Epoch&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;30&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;of&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;500&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;took&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;350814&lt;&#x2F;span&gt;s
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;022995&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;060471&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;982800&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;Epoch&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;40&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;of&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;500&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;took&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;350996&lt;&#x2F;span&gt;s
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;train&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;013713&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;loss&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;055777&lt;&#x2F;span&gt;
    &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;val&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;acc&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-annotation z-variable z-python&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-float z-decimal z-python&quot;&gt;0&lt;span class=&quot;z-punctuation z-separator z-decimal z-python&quot;&gt;.&lt;&#x2F;span&gt;983700&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-constant z-language z-python&quot;&gt;...&lt;&#x2F;span&gt;

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这个例子脚本允许您从 &lt;code&gt;if__name__==&#x27;__main__&#x27;:&lt;&#x2F;code&gt; 中选择不同的模型进行尝试，包括多层神经网络（Multi-Layer Perceptron），
退出（Dropout），退出连接（DropConnect），堆栈式降噪自编码器（Stacked Denoising Autoencoder）和卷积神经网络（CNN）。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  main_test_layers(model=&amp;#39;relu&amp;#39;)
  main_test_denoise_AE(model=&amp;#39;relu&amp;#39;)
  main_test_stacked_denoise_AE(model=&amp;#39;relu&amp;#39;)
  main_test_cnn_layer()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;li-jie-mnistli-zi&quot;&gt;理解MNIST例子&lt;&#x2F;h2&gt;
&lt;p&gt;现在就让我们看看它是如何做到的！跟着下面的步骤，打开源代码。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;xu-yan&quot;&gt;序言&lt;&#x2F;h3&gt;
&lt;p&gt;您可能会首先注意到，除TensorLayer之外，我们还导入了Numpy和TensorFlow：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  import tensorflow as tf
  import tensorlayer as tl
  from tensorlayer.layers import set_keep
  import numpy as np
  import time
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这是因为TensorLayer是建立在TensorFlow上的，TensorLayer设计的初衷是为了简化工作并提供帮助而不是取代TensorFlow。
所以您会需要一起使用TensorLayer和一些常见的TensorFlow代码。&lt;&#x2F;p&gt;
&lt;p&gt;请注意，当使用降噪自编码器(Denoising Autoencoder)时，代码中的 &lt;code&gt;set_keep&lt;&#x2F;code&gt; 被当作用来访问保持概率(Keeping Probabilities)的占位符。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zai-ru-shu-ju&quot;&gt;载入数据&lt;&#x2F;h3&gt;
&lt;p&gt;下面第一部分的代码首先定义了 &lt;code&gt;load_mnist_dataset()&lt;&#x2F;code&gt; 函数。
其目的是为了下载MNIST数据集（如果还未下载），并且返回标准numpy数列通过numpy array的格式。
到这里还没有涉及TensorLayer，所以我们可以把它简单看作：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  X_train, y_train, X_val, y_val, X_test, y_test = \
                    tl.files.load_mnist_dataset(shape=(-1,784))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;X_train.shape&lt;&#x2F;code&gt; 为 &lt;code&gt;(50000,784)&lt;&#x2F;code&gt;，可以理解成共有50000张图片并且每张图片有784个像素点。
&lt;code&gt;Y_train.shape&lt;&#x2F;code&gt; 为 &lt;code&gt;(50000,)&lt;&#x2F;code&gt; ，它是一个和 &lt;code&gt;X_train&lt;&#x2F;code&gt; 长度相同的向量，用于给出每幅图的数字标签，即这些图片所包含的位于0-9之间的数字（如果画这些数字的人没有想乱画别的东西）。&lt;&#x2F;p&gt;
&lt;p&gt;另外对于卷积神经网络的例子，MNIST还可以按下面的4D版本来载入：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  X_train, y_train, X_val, y_val, X_test, y_test = \
              tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;X_train.shape&lt;&#x2F;code&gt; 为 &lt;code&gt;(50000,28,28,1)&lt;&#x2F;code&gt; ，这代表了50000张图片，每张图片使用一个通道（Channel），28行，28列。
通道为1是因为它是灰度图像，每个像素只能有一个值。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;jian-li-mo-xing&quot;&gt;建立模型&lt;&#x2F;h3&gt;
&lt;p&gt;来到这里，就轮到TensorLayer来一显身手了！TensorLayer允许您通过创建，堆叠或者合并图层(Layers)来定义任意结构的神经网络。
每一层都知道它在网络中的直接输入层, 而每层的输出同时作为该层和整个网络的一个句柄，
通常是我们要传递给其余代码的唯一东西。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;freeopen: 上段的最后一句翻译，我认为原文中的output layer说法有问题，因为output layer通常指网络的最后一层, 或者表示当前层的下一层，这里应该表示为“每层的输出”才符合上下文，也符合代码逻辑。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;正如上文提到的， &lt;code&gt;tutorial_mnist.py&lt;&#x2F;code&gt; 支持四类模型，我们通过同样的接口实现，只需简单的替换一下函数即可。
首先，我们将定义一个结构固定的多层次感知器（Multi-Layer Perceptron），所有的步骤都会详细的讲解。
然后，我们会实现一个去噪自编码器(Denosing Autoencoding)。
接着，我们要将所有去噪自编码器堆叠起来并对他们进行监督微调(Supervised Fine-tune)。
最后，我们将展示如何去创建一个卷积神经网络(Convolutional Neural Network)。&lt;&#x2F;p&gt;
&lt;p&gt;此外，如果您有兴趣，我们还提供了一个简化版的MNIST例子在 &lt;code&gt;tutorial_mnist_simple.py&lt;&#x2F;code&gt; 中，和一个
CIFAR-10数据集的卷积神经网络(CNN)的例子在 &lt;code&gt;tutorial_cifar10_tfrecord.py&lt;&#x2F;code&gt; 中, 供你参考。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;duo-ceng-shen-jing-wang-luo&quot;&gt;多层神经网络&lt;&#x2F;h3&gt;
&lt;p&gt;第一个脚本 &lt;code&gt;main_test_layers()&lt;&#x2F;code&gt; ，创建了一个具有两个隐藏层，每层800个单元的多层次感知器，并且具有10个单元的SOFTMAX输出层紧随其后。
它对输入数据采用20%的退出率(dropout)并且对隐藏层应用50%的退出率(dropout)。&lt;&#x2F;p&gt;
&lt;p&gt;为了提供数据给这个网络，TensorFlow占位符(placeholder)需要按如下定义。
在这里 &lt;code&gt;None&lt;&#x2F;code&gt; 是指在编译之后，网络将接受任意批规模(batchsize)的数据
&lt;code&gt;x&lt;&#x2F;code&gt; 是用来存放 &lt;code&gt;X_train&lt;&#x2F;code&gt; 数据的并且 &lt;code&gt;y_&lt;&#x2F;code&gt; 是用来存放 &lt;code&gt;y_train&lt;&#x2F;code&gt; 数据的。
如果你已经知道批规模，那就不需要这种灵活性了。您可以在这里给出批规模，特别是对于卷积层，这样可以运用TensorFlow一些优化功能。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    x = tf.placeholder(tf.float32, shape=[None, 784], name=&amp;#39;x&amp;#39;)
    y_ = tf.placeholder(tf.int64, shape=[None, ], name=&amp;#39;y_&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在TensorLayer中每个神经网络的基础是一个 &lt;code&gt;InputLayer&lt;&#x2F;code&gt; 实例。它代表了将要提供(feed)给网络的输入数据。
值得注意的是 &lt;code&gt;InputLayer&lt;&#x2F;code&gt; 并不依赖任何特定的数据。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    network = tl.layers.InputLayer(x, name=&amp;#39;input_layer&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在添加第一层隐藏层之前，我们要对输入数据应用20%的退出率(dropout)。
这里我们是通过一个 &lt;code&gt;DropoutLayer&lt;&#x2F;code&gt; 的实例来实现的。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    network = tl.layers.DropoutLayer(network, keep=0.8, name=&amp;#39;drop1&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;请注意构造函数的第一个参数是输入层，第二个参数是激活值的保持概率(keeping probability for the activation value)
现在我们要继续构造第一个800个单位的全连接的隐藏层。
尤其是当要堆叠一个 &lt;code&gt;DenseLayer&lt;&#x2F;code&gt; 时，要特别注意。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    network = tl.layers.DenseLayer(network, n_units=800, act = tf.nn.relu, name=&amp;#39;relu1&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;同样，构造函数的第一个参数意味着这我们正在 &lt;code&gt;network&lt;&#x2F;code&gt; 之上堆叠 &lt;code&gt;network&lt;&#x2F;code&gt; 。
&lt;code&gt;n_units&lt;&#x2F;code&gt; 简明得给出了全连接层的单位数。
&lt;code&gt;act&lt;&#x2F;code&gt; 指定了一个激活函数，这里的激活函数有一部分已经被定义在了&lt;code&gt;tensorflow.nn&lt;&#x2F;code&gt; 和  &lt;code&gt;tensorlayer.activation&lt;&#x2F;code&gt; 中。
我们在这里选择了整流器(rectifier)，我们将得到ReLUs。
我们现在来添加50%的退出率，以及另外800个单位的稠密层(dense layer)，和50%的退出率：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    network = tl.layers.DropoutLayer(network, keep=0.5, name=&amp;#39;drop2&amp;#39;)
    network = tl.layers.DenseLayer(network, n_units=800, act = tf.nn.relu, name=&amp;#39;relu2&amp;#39;)
    network = tl.layers.DropoutLayer(network, keep=0.5, name=&amp;#39;drop3&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;最后，我们加入&lt;code&gt;n_units&lt;&#x2F;code&gt;等于分类个数的全连接的输出层。注意，&lt;code&gt;cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))&lt;&#x2F;code&gt; 在内部实现 Softmax，以提高计算效率，因此最后一层的输出为 identity ，更多细节请参考 &lt;code&gt;tl.cost.cross_entropy()&lt;&#x2F;code&gt; 。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    network = tl.layers.DenseLayer(network,
                                  n_units=10,
                                  act = tl.activation.identity,
                                  name=&amp;#39;output_layer&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;如上所述，因为每一层都被链接到了它的输入层，所以我们只需要在TensorLayer中将输出层接入一个网络：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    y = network.outputs
    y_op = tf.argmax(tf.nn.softmax(y), 1)
    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在这里，&lt;code&gt;network.outputs&lt;&#x2F;code&gt; 是网络的10个特征的输出(按照一个热门的格式)。
&lt;code&gt;y_op&lt;&#x2F;code&gt; 是代表类索引的整数输出， &lt;code&gt;cost&lt;&#x2F;code&gt; 是目标和预测标签的交叉熵。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;jiang-zao-zi-bian-ma-qi&quot;&gt;降噪自编码器&lt;&#x2F;h3&gt;
&lt;p&gt;自编码器是一种无监督学习（Unsupervisered Learning）模型，可从数据中学习出更好的表达，
目前已经用于逐层贪婪的预训练（Greedy layer-wise pre-train）。
有关自编码器内容，请参考教程 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;deeplearning.stanford.edu&#x2F;tutorial&#x2F;&quot;&gt;Deeplearning Tutorial&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;脚本 &lt;code&gt;main_test_denoise_AE()&lt;&#x2F;code&gt; 实现了有50%的腐蚀率(corrosion rate)的降噪自编码器。
这个自编码器可以按如下方式定义，这里的 &lt;code&gt;DenseLayer&lt;&#x2F;code&gt; 代表了一个自编码器：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    network = tl.layers.InputLayer(x, name=&amp;#39;input_layer&amp;#39;)
    network = tl.layers.DropoutLayer(network, keep=0.5, name=&amp;#39;denoising1&amp;#39;)
    network = tl.layers.DenseLayer(network, n_units=200, act=tf.nn.sigmoid, name=&amp;#39;sigmoid1&amp;#39;)
    recon_layer1 = tl.layers.ReconLayer(network,
                                        x_recon=x,
                                        n_units=784,
                                        act=tf.nn.sigmoid,
                                        name=&amp;#39;recon_layer1&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;训练 &lt;code&gt;DenseLayer&lt;&#x2F;code&gt; ，只需要运行 &lt;code&gt;ReconLayer.Pretrain()&lt;&#x2F;code&gt; 即可。
如果要使用降噪自编码器，腐蚀层(corrosion layer)(&lt;code&gt;DropoutLayer&lt;&#x2F;code&gt;)的名字需要按后面说的指定。
如果要保存特征图像，设置 &lt;code&gt;save&lt;&#x2F;code&gt; 为 True 。
根据不同的架构和应用这里可以设置许多预训练的度量(metric)&lt;&#x2F;p&gt;
&lt;p&gt;对于 sigmoid型激活函数来说，自编码器可以用KL散度来实现。
而对于整流器(Rectifier)来说，对激活函数输出的L1正则化能使得输出变得稀疏。
所以 &lt;code&gt;ReconLayer&lt;&#x2F;code&gt; 默认只对整流激活函数(ReLU)提供KLD和交叉熵这两种损失度量，而对sigmoid型激活函数提供均方误差以及激活输出的L1范数这两种损失度量。
我们建议您修改 &lt;code&gt;ReconLayer&lt;&#x2F;code&gt; 来实现自己的预训练度量。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    recon_layer1.pretrain(sess,
                          x=x,
                          X_train=X_train,
                          X_val=X_val,
                          denoise_name=&amp;#39;denoising1&amp;#39;,
                          n_epoch=200,
                          batch_size=128,
                          print_freq=10,
                          save=True,
                          save_name=&amp;#39;w1pre_&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;此外，脚本 &lt;code&gt;main_test_stacked_denoise_AE()&lt;&#x2F;code&gt; 展示了如何将多个自编码器堆叠到一个网络，然后进行微调。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;juan-ji-shen-jing-wang-luo&quot;&gt;卷积神经网络&lt;&#x2F;h3&gt;
&lt;p&gt;最后，&lt;code&gt;main_test_cnn_layer()&lt;&#x2F;code&gt; 脚本创建了两个CNN层和最大汇流阶段(max pooling stages)，一个全连接的隐藏层和一个全连接的输出层。&lt;&#x2F;p&gt;
&lt;p&gt;首先，我们用 &lt;code&gt;Conv2dLayer&lt;&#x2F;code&gt;添加一个卷积层，它带有32个5x5的过滤器，紧接是2x2维度的最大池化。接着是64个5x5的过滤器的卷积层和同样的最大池化。之后，用｀FlattenLayer｀把4维输出转为1维向量，和50%的dropout在最后的隐层中。这里的&lt;code&gt;?&lt;&#x2F;code&gt;表示每批数量(batch_size)。&lt;&#x2F;p&gt;
&lt;p&gt;注，&lt;code&gt;tutorial_mnist.py&lt;&#x2F;code&gt; 中介绍了针对初学者的简化版的 CNN API。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;network = tl.layers.InputLayer(x, name=&amp;#39;input_layer&amp;#39;)
network = tl.layers.Conv2dLayer(network,
                        act = tf.nn.relu,
                        shape = [5, 5, 1, 32],  # 32 features for each 5x5 patch
                        strides=[1, 1, 1, 1],
                        padding=&amp;#39;SAME&amp;#39;,
                        name =&amp;#39;cnn_layer1&amp;#39;)     # output: (?, 28, 28, 32)
network = tl.layers.PoolLayer(network,
                        ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1],
                        padding=&amp;#39;SAME&amp;#39;,
                        pool = tf.nn.max_pool,
                        name =&amp;#39;pool_layer1&amp;#39;,)   # output: (?, 14, 14, 32)
network = tl.layers.Conv2dLayer(network,
                        act = tf.nn.relu,
                        shape = [5, 5, 32, 64], # 64 features for each 5x5 patch
                        strides=[1, 1, 1, 1],
                        padding=&amp;#39;SAME&amp;#39;,
                        name =&amp;#39;cnn_layer2&amp;#39;)     # output: (?, 14, 14, 64)
network = tl.layers.PoolLayer(network,
                        ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1],
                        padding=&amp;#39;SAME&amp;#39;,
                        pool = tf.nn.max_pool,
                        name =&amp;#39;pool_layer2&amp;#39;,)   # output: (?, 7, 7, 64)
network = tl.layers.FlattenLayer(network, name=&amp;#39;flatten_layer&amp;#39;)
                                                # output: (?, 3136)
network = tl.layers.DropoutLayer(network, keep=0.5, name=&amp;#39;drop1&amp;#39;)
                                                # output: (?, 3136)
network = tl.layers.DenseLayer(network, n_units=256, act = tf.nn.relu, name=&amp;#39;relu1&amp;#39;)
                                                # output: (?, 256)
network = tl.layers.DropoutLayer(network, keep=0.5, name=&amp;#39;drop2&amp;#39;)
                                                # output: (?, 256)
network = tl.layers.DenseLayer(network, n_units=10, act = tl.identity, name=&amp;#39;output_layer&amp;#39;)
                                                # output: (?, 10)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;blockquote&gt;
&lt;p&gt;对于专家们来说， &lt;code&gt;Conv2dLayer&lt;&#x2F;code&gt; 将使用 &lt;code&gt;tensorflow.nn.conv2d&lt;&#x2F;code&gt; ,TensorFlow默认的卷积方式来创建一个卷积层。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;xun-lian-mo-xing&quot;&gt;训练模型&lt;&#x2F;h3&gt;
&lt;p&gt;在 &lt;code&gt;tutorial_mnist.py&lt;&#x2F;code&gt; 脚本的其余部分，仅使用交叉熵代价函数来对MNIST数据集进行训练循环。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;shu-ju-ji-die-dai&quot;&gt;数据集迭代&lt;&#x2F;h4&gt;
&lt;p&gt;迭代函数分别对inputs 和 targets 两个numpy数组，按照小批量的数量尺度进行迭代。
更多有关迭代函数的说明，可以在 &lt;code&gt;tensorlayer.iterate&lt;&#x2F;code&gt; 中找到。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    tl.iterate.minibatches(inputs, targets, batchsize, shuffle=False)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;sun-shi-he-geng-xin-gong-shi&quot;&gt;损失和更新公式&lt;&#x2F;h4&gt;
&lt;p&gt;我们继续创建一个在训练中被最小化的损失表达式：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    y = network.outputs
    y_op = tf.argmax(tf.nn.softmax(y), 1)
    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;举 &lt;code&gt;main_test_layers()&lt;&#x2F;code&gt; 这个例子来说，更多的成本或者正则化方法可以被应用在这里。
如果要在权重矩阵中应用最大模(max-norm)方法，你可以添加下列代码：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    cost = cost + tl.cost.maxnorm_regularizer(1.0)(network.all_params[0]) +
                  tl.cost.maxnorm_regularizer(1.0)(network.all_params[2])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;根据要解决的问题，您会需要使用不同的损失函数，更多有关损失函数的说明请见： &lt;code&gt;tensorlayer.cost&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;有了模型和定义的损失函数之后，我们就可以创建用于训练网络的更新公式。
接下去，我们将使用TensorFlow的优化器如下：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    train_params = network.all_params
    train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
        epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;为了训练网络，我们需要提供数据和保持概率给 &lt;code&gt;feed_dict&lt;&#x2F;code&gt;。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    feed_dict = {x: X_train_a, y_: y_train_a}
    feed_dict.update( network.all_drop )
    sess.run(train_op, feed_dict=feed_dict)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;同时为了进行验证和测试，我们这里用了略有不同的方法。
所有的Dropout，退连(DropConnect)，腐蚀层(Corrosion Layers)都将被禁用。
&lt;code&gt;tl.utils.dict_to_one&lt;&#x2F;code&gt; 将会设置所有 &lt;code&gt;network.all_drop&lt;&#x2F;code&gt; 值为1。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    dp_dict = tl.utils.dict_to_one( network.all_drop )
    feed_dict = {x: X_test_a, y_: y_test_a}
    feed_dict.update(dp_dict)
    err, ac = sess.run([cost, acc], feed_dict=feed_dict)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;最后，作为一个额外的监测量，我们需要创建一个分类准确度的公式：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    correct_prediction = tf.equal(tf.argmax(y, 1), y_)
    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;xia-yi-bu&quot;&gt;下一步？&lt;&#x2F;h4&gt;
&lt;p&gt;在 &lt;code&gt;tutorial_cifar10_tfrecord.py&lt;&#x2F;code&gt; 中我们还有更高级的图像分类的例子。
请阅读代码及注释，用以明白如何来生成更多的训练数据以及什么是局部响应正则化。
在这之后，您可以尝试着去实现 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;doi.org&#x2F;10.3389&#x2F;fpsyg.2013.00124&quot;&gt;残差网络(Residual Network)&lt;&#x2F;a&gt;。
&lt;em&gt;小提示：您可能会用到Layer.outputs。&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;yun-xing-ping-pang-qiu-li-zi&quot;&gt;运行乒乓球例子&lt;&#x2F;h2&gt;
&lt;p&gt;在本教程的第二部分，我们将运行一个深度强化学习的例子，它在Karpathy的两篇博客 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;karpathy.github.io&#x2F;2016&#x2F;05&#x2F;31&#x2F;rl&#x2F;&quot;&gt;Deep Reinforcement Learning:Pong from Pixels&lt;&#x2F;a&gt; 有介绍。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  python tutorial_atari_pong.py
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在运行教程代码之前 你需要安装 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;gym.openai.com&#x2F;docs&quot;&gt;OpenAI gym environment&lt;&#x2F;a&gt; ,它是强化学习的一个标杆。
如果一切设置正确，您将得到一个类似以下的输出：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  [2016-07-12 09:31:59,760] Making new env: Pong-v0
    tensorlayer:Instantiate InputLayer input_layer (?, 6400)
    tensorlayer:Instantiate DenseLayer relu1: 200, relu
    tensorlayer:Instantiate DenseLayer output_layer: 3, identity
    param 0: (6400, 200) (mean: -0.000009, median: -0.000018 std: 0.017393)
    param 1: (200,) (mean: 0.000000, median: 0.000000 std: 0.000000)
    param 2: (200, 3) (mean: 0.002239, median: 0.003122 std: 0.096611)
    param 3: (3,) (mean: 0.000000, median: 0.000000 std: 0.000000)
    num of params: 1280803
    layer 0: Tensor(&amp;quot;Relu:0&amp;quot;, shape=(?, 200), dtype=float32)
    layer 1: Tensor(&amp;quot;add_1:0&amp;quot;, shape=(?, 3), dtype=float32)
  episode 0: game 0 took 0.17381s, reward: -1.000000
  episode 0: game 1 took 0.12629s, reward: 1.000000  !!!!!!!!
  episode 0: game 2 took 0.17082s, reward: -1.000000
  episode 0: game 3 took 0.08944s, reward: -1.000000
  episode 0: game 4 took 0.09446s, reward: -1.000000
  episode 0: game 5 took 0.09440s, reward: -1.000000
  episode 0: game 6 took 0.32798s, reward: -1.000000
  episode 0: game 7 took 0.74437s, reward: -1.000000
  episode 0: game 8 took 0.43013s, reward: -1.000000
  episode 0: game 9 took 0.42496s, reward: -1.000000
  episode 0: game 10 took 0.37128s, reward: -1.000000
  episode 0: game 11 took 0.08979s, reward: -1.000000
  episode 0: game 12 took 0.09138s, reward: -1.000000
  episode 0: game 13 took 0.09142s, reward: -1.000000
  episode 0: game 14 took 0.09639s, reward: -1.000000
  episode 0: game 15 took 0.09852s, reward: -1.000000
  episode 0: game 16 took 0.09984s, reward: -1.000000
  episode 0: game 17 took 0.09575s, reward: -1.000000
  episode 0: game 18 took 0.09416s, reward: -1.000000
  episode 0: game 19 took 0.08674s, reward: -1.000000
  episode 0: game 20 took 0.09628s, reward: -1.000000
  resetting env. episode reward total was -20.000000. running mean: -20.000000
  episode 1: game 0 took 0.09910s, reward: -1.000000
  episode 1: game 1 took 0.17056s, reward: -1.000000
  episode 1: game 2 took 0.09306s, reward: -1.000000
  episode 1: game 3 took 0.09556s, reward: -1.000000
  episode 1: game 4 took 0.12520s, reward: 1.000000  !!!!!!!!
  episode 1: game 5 took 0.17348s, reward: -1.000000
  episode 1: game 6 took 0.09415s, reward: -1.000000

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这个例子让电脑从屏幕输入来学习如何像人类一样打乒乓球。
在经过15000个序列的训练之后，计算机就可以赢得20%的比赛。
在20000个序列的训练之后，计算机可以赢得35%的比赛，
我们可以看到计算机学的越来越快，这是因为它有更多的胜利的数据来进行训练。
如果您用30000个序列来训练它，那么它会一直赢。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  render = False
  resume = False
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;如果您想显示游戏过程，那就设置 &lt;code&gt;render&lt;&#x2F;code&gt; 为 &lt;code&gt;True&lt;&#x2F;code&gt; 。
当您再次运行该代码，您可以设置 &lt;code&gt;resume&lt;&#x2F;code&gt; 为 &lt;code&gt;True&lt;&#x2F;code&gt;,那么代码将加载现有的模型并且会基于它进行训练。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;30%&quot; src=&quot;.&#x2F;pong_game.jpeg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;h2 id=&quot;li-jie-qiang-hua-xue-xi&quot;&gt;理解强化学习&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;ping-pang-qiu&quot;&gt;乒乓球&lt;&#x2F;h3&gt;
&lt;p&gt;要理解强化学习，我们要让电脑学习如何从原始的屏幕输入(像素输入)打乒乓球。
在我们开始之前，我们强烈建议您去浏览一个著名的博客叫做 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;karpathy.github.io&#x2F;2016&#x2F;05&#x2F;31&#x2F;rl&#x2F;&quot;&gt;Deep Reinforcement Learning:pong from Pixels&lt;&#x2F;a&gt; ,
这是使用python numpy库和OpenAI gym environment=来实现的一个深度强化学习的最简实现。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  python tutorial_atari_pong.py
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;ce-lue-wang-luo-policy-network&quot;&gt;策略网络(Policy Network)&lt;&#x2F;h3&gt;
&lt;p&gt;在深度强化学习中，Policy Network 等同于 深度神经网络。
它是我们的选手(或者说“代理人(agent)”），它的输出告诉我们应该做什么(向上移动或向下移动)：
在Karpathy的代码中，他只定义了2个动作，向上移动和向下移动，并且仅使用单个simgoid输出：
为了使我们的教程更具有普遍性，我们使用3个 softmax 输出来定义向上移动，向下移动和停止(什么都不做)3个动作。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    # observation for training
    states_batch_pl = tf.placeholder(tf.float32, shape=[None, D])

    network = tl.layers.InputLayer(states_batch_pl, name=&amp;#39;input_layer&amp;#39;)
    network = tl.layers.DenseLayer(network, n_units=H,
                                    act = tf.nn.relu, name=&amp;#39;relu1&amp;#39;)
    network = tl.layers.DenseLayer(network, n_units=3,
                            act = tl.activation.identity, name=&amp;#39;output_layer&amp;#39;)
    probs = network.outputs
    sampling_prob = tf.nn.softmax(probs)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;然后我们的代理人就一直打乒乓球。它计算不同动作的概率，
并且之后会从这个均匀的分布中选取样本(动作)。
因为动作被1,2和3代表，但是softmax输出应该从0开始，所以我们从-1计算这个标签的价值。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    prob = sess.run(
        sampling_prob,
        feed_dict={states_batch_pl: x}
    )
    # action. 1: STOP  2: UP  3: DOWN
    action = np.random.choice([1,2,3], p=prob.flatten())
    ...
    ys.append(action - 1)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;ce-lue-bi-jin-policy-gradient&quot;&gt;策略逼近(Policy Gradient)&lt;&#x2F;h3&gt;
&lt;p&gt;策略梯度下降法是一个end-to-end的算法，它直接学习从状态映射到动作的策略函数。
一个近似最优的策略可以通过最大化预期的奖励来直接学习。
策略函数的参数(例如，在乒乓球例子终使用的策略网络的参数)在预期奖励的近似值的引导下能够被训练和学习。
换句话说，我们可以通过更新它的参数来逐步调整策略函数，这样它能从给定的状态做出一系列行为来获得更高的奖励。&lt;&#x2F;p&gt;
&lt;p&gt;策略迭代的一个替代算法就是深度Q-learning(DQN)。
他是基于Q-learning,学习一个映射状态和动作到一些值的价值函数的算法(叫Q函数)。
DQN采用了一个深度神经网络来作为Q函数的逼近来代表Q函数。
训练是通过最小化时序差分(temporal-difference)误差来实现。
一个名为“再体验(experience replay)”的神经生物学的启发式机制通常和DQN一起被使用来帮助提高非线性函数的逼近的稳定性&lt;&#x2F;p&gt;
&lt;p&gt;您可以阅读以下文档，来得到对强化学习更好的理解：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;webdocs.cs.ualberta.ca&#x2F;~sutton&#x2F;book&#x2F;the-book.html&quot;&gt;Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.iclr.cc&#x2F;lib&#x2F;exe&#x2F;fetch.php?media=iclr2015:silver-iclr2015.pdf&quot;&gt;Deep Reinforcement Learning. David Silver, Google DeepMind&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www0.cs.ucl.ac.uk&#x2F;staff&#x2F;d.silver&#x2F;web&#x2F;Teaching.html&quot;&gt;UCL Course on RL&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;强化深度学习近些年来最成功的应用就是让模型去学习玩Atari的游戏。 AlphaGO同时也是使用类似的策略逼近方法来训练他们的策略网络而战胜了世界级的专业围棋选手。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~vmnih&#x2F;docs&#x2F;dqn.pdf&quot;&gt;Atari - Playing Atari with Deep Reinforcement Learning&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.nature.com&#x2F;nature&#x2F;journal&#x2F;v518&#x2F;n7540&#x2F;full&#x2F;nature14236.html&quot;&gt;Atari - Human-level control through deep reinforcement learning&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.nature.com&#x2F;nature&#x2F;journal&#x2F;v529&#x2F;n7587&#x2F;full&#x2F;nature16961.html&quot;&gt;AlphaGO - Mastering the game of Go with deep neural networks and tree search&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;shu-ju-ji-die-dai-1&quot;&gt;数据集迭代&lt;&#x2F;h4&gt;
&lt;p&gt;在强化学习中，我们把每场比赛所产生的所有决策来作为一个序列 (up,up,stop,…,down)。在乒乓球游戏中，比赛是在某一方达到21分后结束的，所以一个序列可能包含几十个决策。
然后我们可以设置一个批规模的大小，每一批包含一定数量的序列，基于这个批规模来更新我们的模型。
在本教程中，我们把每批规模设置成10个序列。使用RMSProp训练一个具有200个单元的隐藏层的2层策略网络&lt;&#x2F;p&gt;
&lt;h4 id=&quot;sun-shi-he-geng-xin-gong-shi-1&quot;&gt;损失和更新公式&lt;&#x2F;h4&gt;
&lt;p&gt;接着我们创建一个在训练中被最小化的损失公式：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;    actions_batch_pl = tf.placeholder(tf.int32, shape=[None])
    discount_rewards_batch_pl = tf.placeholder(tf.float32, shape=[None])
    loss = tl.rein.cross_entropy_reward_loss(probs, actions_batch_pl,
                                                  discount_rewards_batch_pl)
    ...
    ...
    sess.run(
        train_op,
        feed_dict={
            states_batch_pl: epx,
            actions_batch_pl: epy,
            discount_rewards_batch_pl: disR
        }
    )
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;一batch的损失和一个batch内的策略网络的所有输出，所有的我们做出的动作和相应的被打折的奖励有关
我们首先通过累加被打折的奖励和实际输出和真实动作的交叉熵计算每一个动作的损失。
最后的损失是所有动作的损失的和。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;xia-yi-bu-1&quot;&gt;下一步?&lt;&#x2F;h3&gt;
&lt;p&gt;上述教程展示了您如何去建立自己的代理人，end-to-end。
虽然它有很合理的品质，但它的默认参数不会给你最好的代理人模型。
这有一些您可以优化的内容。&lt;&#x2F;p&gt;
&lt;p&gt;首先，与传统的MLP模型不同，比起 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~vmnih&#x2F;docs&#x2F;dqn.pdf&quot;&gt;Playing Atari with Deep Reinforcement Learning&lt;&#x2F;a&gt; 更好的是我们可以使用CNNs来采集屏幕信息&lt;&#x2F;p&gt;
&lt;p&gt;另外这个模型默认参数没有调整，您可以更改学习率，衰退率，或者用不同的方式来初始化您的模型的权重。&lt;&#x2F;p&gt;
&lt;p&gt;最后，您可以尝试不同任务(游戏)的模型。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;yun-xing-word2vecli-zi&quot;&gt;运行Word2Vec例子&lt;&#x2F;h2&gt;
&lt;p&gt;在教程的这一部分，我们训练一个词嵌套矩阵，每个词可以通过矩阵中唯一的行向量来表示。
在训练结束时，意思类似的单词会有相识的词向量。
在代码的最后，我们通过把单词放到一个平面上来可视化，我们可以看到相似的单词会被聚集在一起。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  python tutorial_word2vec_basic.py
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;如果一切设置正确，您最后会得到如下的可视化图。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;100%&quot; src=&quot;.&#x2F;tsne.png&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;h2 id=&quot;li-jie-ci-qian-tao&quot;&gt;理解词嵌套&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;ci-qian-tao-qian-ru&quot;&gt;词嵌套（嵌入）&lt;&#x2F;h3&gt;
&lt;p&gt;我们强烈建议您先阅读Colah的博客 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;colah.github.io&#x2F;posts&#x2F;2014-07-NLP-RNNs-Representations&#x2F;&quot;&gt;Word Representations&lt;&#x2F;a&gt; &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;dataunion.org&#x2F;9331.html&quot;&gt;(中文翻译)&lt;&#x2F;a&gt; ，
以理解为什么我们要使用一个向量来表示一个单词。更多Word2vec的细节可以在 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1411.2738&quot;&gt;Word2vec Parameter Learning Explained&lt;&#x2F;a&gt; 中找到。&lt;&#x2F;p&gt;
&lt;p&gt;基本来说，训练一个嵌套矩阵是一个非监督学习的过程。一个单词使用唯一的ID来表示，而这个ID号就是嵌套矩阵的行号（row index），对应的行向量就是用来表示该单词的，使用向量来表示单词可以更好地表达单词的意思。比如，有4个单词的向量， &lt;code&gt;woman − man = queen - king&lt;&#x2F;code&gt; ，这个例子中可以看到，嵌套矩阵中有一个纬度是用来表示性别的。&lt;&#x2F;p&gt;
&lt;p&gt;定义一个Word2vec词嵌套矩阵如下。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  # train_inputs is a row vector, a input is an integer id of single word.
  # train_labels is a column vector, a label is an integer id of single word.
  # valid_dataset is a column vector, a valid set is an integer id of single word.
  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

  # Look up embeddings for inputs.
  emb_net = tl.layers.Word2vecEmbeddingInputlayer(
          inputs = train_inputs,
          train_labels = train_labels,
          vocabulary_size = vocabulary_size,
          embedding_size = embedding_size,
          num_sampled = num_sampled,
          nce_loss_args = {},
          E_init = tf.random_uniform_initializer(minval=-1.0, maxval=1.0),
          E_init_args = {},
          nce_W_init = tf.truncated_normal_initializer(
                            stddev=float(1.0&#x2F;np.sqrt(embedding_size))),
          nce_W_init_args = {},
          nce_b_init = tf.constant_initializer(value=0.0),
          nce_b_init_args = {},
          name =&amp;#39;word2vec_layer&amp;#39;,
      )
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;shu-ju-die-dai-he-sun-shi-han-shu&quot;&gt;数据迭代和损失函数&lt;&#x2F;h4&gt;
&lt;p&gt;Word2vec使用负采样（Negative sampling）和Skip-gram模型进行训练。
噪音对比估计损失（NCE）会帮助减少损失函数的计算量，加快训练速度。
Skip-Gram 将文本（context）和目标（target）反转，尝试从目标单词预测目标文本单词。
我们使用 &lt;code&gt;tl.nlp.generate_skip_gram_batch&lt;&#x2F;code&gt; 函数来生成训练数据，如下：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  # NCE损失函数由 Word2vecEmbeddingInputlayer 提供
  cost = emb_net.nce_cost
  train_params = emb_net.all_params

  train_op = tf.train.AdagradOptimizer(learning_rate, initial_accumulator_value=0.1,
            use_locking=False).minimize(cost, var_list=train_params)

  data_index = 0
  while (step &amp;lt; num_steps):
    batch_inputs, batch_labels, data_index = tl.nlp.generate_skip_gram_batch(
                  data=data, batch_size=batch_size, num_skips=num_skips,
                  skip_window=skip_window, data_index=data_index)
    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}
    _, loss_val = sess.run([train_op, cost], feed_dict=feed_dict)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;jia-zai-yi-xun-lian-hao-de-de-ci-qian-tao-ju-zhen&quot;&gt;加载已训练好的的词嵌套矩阵&lt;&#x2F;h4&gt;
&lt;p&gt;在训练嵌套矩阵的最后，我们保存矩阵及其词汇表、单词转ID字典、ID转单词字典。
然后，当下次做实际应用时，可以想下面的代码中那样加载这个已经训练好的矩阵和字典，
参考 &lt;code&gt;tutorial_generate_text.py&lt;&#x2F;code&gt; 。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  vocabulary_size = 50000
  embedding_size = 128
  model_file_name = &amp;quot;model_word2vec_50k_128&amp;quot;
  batch_size = None

  print(&amp;quot;Load existing embedding matrix and dictionaries&amp;quot;)
  all_var = tl.files.load_npy_to_any(name=model_file_name+&amp;#39;.npy&amp;#39;)
  data = all_var[&amp;#39;data&amp;#39;]; count = all_var[&amp;#39;count&amp;#39;]
  dictionary = all_var[&amp;#39;dictionary&amp;#39;]
  reverse_dictionary = all_var[&amp;#39;reverse_dictionary&amp;#39;]

  tl.nlp.save_vocab(count, name=&amp;#39;vocab_&amp;#39;+model_file_name+&amp;#39;.txt&amp;#39;)

  del all_var, data, count

  load_params = tl.files.load_npz(name=model_file_name+&amp;#39;.npz&amp;#39;)

  x = tf.placeholder(tf.int32, shape=[batch_size])
  y_ = tf.placeholder(tf.int32, shape=[batch_size, 1])

  emb_net = tl.layers.EmbeddingInputlayer(
                  inputs = x,
                  vocabulary_size = vocabulary_size,
                  embedding_size = embedding_size,
                  name =&amp;#39;embedding_layer&amp;#39;)

  tl.layers.initialize_global_variables(sess)

  tl.files.assign_params(sess, [load_params[0]], emb_net)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;yun-xing-ptbli-zi&quot;&gt;运行PTB例子&lt;&#x2F;h2&gt;
&lt;p&gt;Penn TreeBank（PTB）数据集被用在很多语言建模（Language Modeling）的论文中，包括“Empirical Evaluation and Combination of Advanced Language Modeling Techniques“和
“Recurrent Neural Network Regularization”。该数据集的训练集有929k个单词，验证集有73K个单词，测试集有82k个单词。
在它的词汇表刚好有10k个单词。&lt;&#x2F;p&gt;
&lt;p&gt;PTB例子是为了展示如何用递归神经网络（Recurrent Neural Network）来进行语言建模的。&lt;&#x2F;p&gt;
&lt;p&gt;给一句话 “I am from Imperial College London”, 这个模型可以从中学习出如何从“from Imperial College”来预测出“Imperial College London”。也就是说，它根据之前输入的单词序列来预测出下一步输出的单词序列，在刚才的例子中 &lt;code&gt;num_steps (序列长度，sequence length)&lt;&#x2F;code&gt; 为 3。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  python tutorial_ptb_lstm.py
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;该脚本提供三种设置(小，中，大)，越大的模型有越好的建模性能，您可以修改下面的代码片段来选择不同的模型设置。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  flags.DEFINE_string(
      &amp;quot;model&amp;quot;, &amp;quot;small&amp;quot;,
      &amp;quot;A type of model. Possible options are: small, medium, large.&amp;quot;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;如果您选择小设置，您将会看到：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  Epoch: 1 Learning rate: 1.000
  0.004 perplexity: 5220.213 speed: 7635 wps
  0.104 perplexity: 828.871 speed: 8469 wps
  0.204 perplexity: 614.071 speed: 8839 wps
  0.304 perplexity: 495.485 speed: 8889 wps
  0.404 perplexity: 427.381 speed: 8940 wps
  0.504 perplexity: 383.063 speed: 8920 wps
  0.604 perplexity: 345.135 speed: 8920 wps
  0.703 perplexity: 319.263 speed: 8949 wps
  0.803 perplexity: 298.774 speed: 8975 wps
  0.903 perplexity: 279.817 speed: 8986 wps
  Epoch: 1 Train Perplexity: 265.558
  Epoch: 1 Valid Perplexity: 178.436
  ...
  Epoch: 13 Learning rate: 0.004
  0.004 perplexity: 56.122 speed: 8594 wps
  0.104 perplexity: 40.793 speed: 9186 wps
  0.204 perplexity: 44.527 speed: 9117 wps
  0.304 perplexity: 42.668 speed: 9214 wps
  0.404 perplexity: 41.943 speed: 9269 wps
  0.504 perplexity: 41.286 speed: 9271 wps
  0.604 perplexity: 39.989 speed: 9244 wps
  0.703 perplexity: 39.403 speed: 9236 wps
  0.803 perplexity: 38.742 speed: 9229 wps
  0.903 perplexity: 37.430 speed: 9240 wps
  Epoch: 13 Train Perplexity: 36.643
  Epoch: 13 Valid Perplexity: 121.475
  Test Perplexity: 116.716
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;PTB例子证明了递归神经网络能够实现语言建模，但是这个例子并没有做什么实际的事情。
在做具体应用之前，您应该浏览这个例子的代码和下一章 “理解 LSTM” 来学好递归神经网络的基础。
之后，您将学习如何用递归神经网络来生成文本，如何实现语言翻译和问题应答系统。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;li-jie-lstm&quot;&gt;理解LSTM&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;di-gui-shen-jing-wang-luo-recurrent-neural-network&quot;&gt;递归神经网络 (Recurrent Neural Network)&lt;&#x2F;h3&gt;
&lt;p&gt;我们认为Andrey Karpathy的博客 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;karpathy.github.io&#x2F;2015&#x2F;05&#x2F;21&#x2F;rnn-effectiveness&#x2F;&quot;&gt;Understand Recurrent Neural Network&lt;&#x2F;a&gt; 是了解递归神经网络最好的材料。
读完这个博客后，Colah的博客 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;colah.github.io&#x2F;posts&#x2F;2015-08-Understanding-LSTMs&#x2F;&quot;&gt;Understand LSTM Network&lt;&#x2F;a&gt; 能帮助你了解LSTM。
我们在这里不介绍更多关于递归神经网络的内容，所以在你继续下面的内容之前，请先阅读我们建议阅读的博客。&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;100%&quot; src=&quot;.&#x2F;karpathy_rnn.jpeg&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;图片由Andrey Karpathy提供&lt;&#x2F;p&gt;
&lt;h3 id=&quot;tong-bu-shu-ru-yu-shu-chu-xu-lie-synced-sequence-input-and-output&quot;&gt;同步输入与输出序列 (Synced sequence input and output)&lt;&#x2F;h3&gt;
&lt;p&gt;PTB例子中的模型是一个典型的同步输入与输出，Karpathy 把它描述为
“(5) 同步序列输入与输出(例如视频分类中我们希望对每一帧进行标记)。“&lt;&#x2F;p&gt;
&lt;p&gt;模型的构建如下，第一层是词嵌套层（嵌入），把每一个单词转换成对应的词向量，在该例子中没有使用预先训练好的
嵌套矩阵。第二，堆叠两层LSTM，使用Dropout来实现规则化，防止overfitting。
最后，使用全连接层输出一序列的softmax输出。&lt;&#x2F;p&gt;
&lt;p&gt;第一层LSTM的输出形状是 [batch_size, num_steps, hidden_size]，这是为了让下一层LSTM可以堆叠在其上面。
第二层LSTM的输出形状是 [batch_size&lt;em&gt;num_steps, hidden_size]，这是为了让输出层（全连接层 Dense）可以堆叠在其上面。
然后计算每个样本的softmax输出，样本总数为 n_examples = batch_size&lt;&#x2F;em&gt;num_steps。&lt;&#x2F;p&gt;
&lt;p&gt;若想要更进一步理解该PTB教程，您也可以阅读 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.tensorflow.org&#x2F;versions&#x2F;r0.9&#x2F;tutorials&#x2F;recurrent&#x2F;index.html#recurrent-neural-networks&quot;&gt;TensorFlow 官方的PTB教程&lt;&#x2F;a&gt; ，中文翻译请见极客学院。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  network = tl.layers.EmbeddingInputlayer(
              inputs = x,
              vocabulary_size = vocab_size,
              embedding_size = hidden_size,
              E_init = tf.random_uniform_initializer(-init_scale, init_scale),
              name =&amp;#39;embedding_layer&amp;#39;)
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name=&amp;#39;drop1&amp;#39;)
  network = tl.layers.RNNLayer(network,
              cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
              cell_init_args={&amp;#39;forget_bias&amp;#39;: 0.0},
              n_hidden=hidden_size,
              initializer=tf.random_uniform_initializer(-init_scale, init_scale),
              n_steps=num_steps,
              return_last=False,
              name=&amp;#39;basic_lstm_layer1&amp;#39;)
  lstm1 = network
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name=&amp;#39;drop2&amp;#39;)
  network = tl.layers.RNNLayer(network,
              cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
              cell_init_args={&amp;#39;forget_bias&amp;#39;: 0.0},
              n_hidden=hidden_size,
              initializer=tf.random_uniform_initializer(-init_scale, init_scale),
              n_steps=num_steps,
              return_last=False,
              return_seq_2d=True,
              name=&amp;#39;basic_lstm_layer2&amp;#39;)
  lstm2 = network
  if is_training:
      network = tl.layers.DropoutLayer(network, keep=keep_prob, name=&amp;#39;drop3&amp;#39;)
  network = tl.layers.DenseLayer(network,
              n_units=vocab_size,
              W_init=tf.random_uniform_initializer(-init_scale, init_scale),
              b_init=tf.random_uniform_initializer(-init_scale, init_scale),
              act = tl.activation.identity, name=&amp;#39;output_layer&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;shu-ju-die-dai&quot;&gt;数据迭代&lt;&#x2F;h4&gt;
&lt;p&gt;batch_size 数值可以被视为并行计算的数量。
如下面的例子所示，第一个 batch 使用 0 到 9 来学习序列信息。
第二个 batch 使用 10 到 19 来学习序列。
所以它忽略了 9 到 10 之间的信息。
只当我们 bath_size 设为 1，它才使用 0 到 20 之间所有的序列信息来学习。&lt;&#x2F;p&gt;
&lt;p&gt;这里的 batch_size 的意思与 MNIST 例子略有不同。
在 MNIST 例子，batch_size 是每次迭代中我们使用的样本数量，
而在 PTB 的例子中，batch_size 是为加快训练速度的并行进程数。&lt;&#x2F;p&gt;
&lt;p&gt;虽然当 batch_size &amp;gt; 1 时有些信息将会被忽略，
但是如果你的数据是足够长的（一个语料库通常有几十亿个字），被忽略的信息不会影响最终的结果。&lt;&#x2F;p&gt;
&lt;p&gt;在PTB教程中，我们设置了 batch_size = 20，所以，我们将整个数据集拆分成 20 段（segment）。
在每一轮（epoch）的开始时，我们有 20 个初始化的 LSTM 状态（State），然后分别对 20 段数据进行迭代学习。&lt;&#x2F;p&gt;
&lt;p&gt;训练数据迭代的例子如下：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  train_data = [i for i in range(20)]
  for batch in tl.iterate.ptb_iterator(train_data, batch_size=2, num_steps=3):
      x, y = batch
      print(x, &amp;#39;\n&amp;#39;,y)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  ... [[ 0  1  2] &amp;lt;---x                       1st subset&#x2F; iteration
  ...  [10 11 12]]
  ... [[ 1  2  3] &amp;lt;---y
  ...  [11 12 13]]
  ...
  ... [[ 3  4  5]  &amp;lt;--- 1st batch input       2nd subset&#x2F; iteration
  ...  [13 14 15]] &amp;lt;--- 2nd batch input
  ... [[ 4  5  6]  &amp;lt;--- 1st batch target
  ...  [14 15 16]] &amp;lt;--- 2nd batch target
  ...
  ... [[ 6  7  8]                             3rd subset&#x2F; iteration
  ...  [16 17 18]]
  ... [[ 7  8  9]
  ...  [17 18 19]]

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;&amp;gt; 这个例子可以当作词嵌套矩阵的预训练。
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;sun-shi-he-geng-xin-gong-shi-2&quot;&gt;损失和更新公式&lt;&#x2F;h4&gt;
&lt;p&gt;损失函数是一系列输出cross entropy的均值。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  # 更多细节请见 tensorlayer.cost.cross_entropy_seq()
  def loss_fn(outputs, targets, batch_size, num_steps):
      # Returns the cost function of Cross-entropy of two sequences, implement
      # softmax internally.
      # outputs : 2D tensor [batch_size*num_steps, n_units of output layer]
      # targets : 2D tensor [batch_size, num_steps], need to be reshaped.
      # n_examples = batch_size * num_steps
      # so
      # cost is the averaged cost of each mini-batch (concurrent process).
      loss = tf.nn.seq2seq.sequence_loss_by_example(
          [outputs],
          [tf.reshape(targets, [-1])],
          [tf.ones([batch_size * num_steps])])
      cost = tf.reduce_sum(loss) &#x2F; batch_size
      return cost

  # Cost for Training
  cost = loss_fn(network.outputs, targets, batch_size, num_steps)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在训练时，该例子在若干个epoch之后（由 &lt;code&gt;max_epoch&lt;&#x2F;code&gt; 定义），才开始按比例下降学习率（learning rate），新学习率是前一个epoch的学习率乘以一个下降率（由 &lt;code&gt;lr_decay&lt;&#x2F;code&gt; 定义）。
此外，截断反向传播（truncated backpropagation）截断了&lt;&#x2F;p&gt;
&lt;p&gt;为使学习过程易于处理，通常的做法是将反向传播的梯度在（按时间）展开的步骤上照一个固定长度( &lt;code&gt;num_steps&lt;&#x2F;code&gt; )截断。 通过在一次迭代中的每个时刻上提供长度为 &lt;code&gt;num_steps&lt;&#x2F;code&gt; 的输入和每次迭代完成之后反向传导，这会很容易实现。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  # 截断反响传播 Truncated Backpropagation for training
  with tf.variable_scope(&amp;#39;learning_rate&amp;#39;):
      lr = tf.Variable(0.0, trainable=False)
  tvars = tf.trainable_variables()
  grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                    max_grad_norm)
  optimizer = tf.train.GradientDescentOptimizer(lr)
  train_op = optimizer.apply_gradients(zip(grads, tvars))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;如果当前epoch值大于 &lt;code&gt;max_epoch&lt;&#x2F;code&gt; ，则把当前学习率乘以 &lt;code&gt;lr_decay&lt;&#x2F;code&gt; 来降低学习率。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  new_lr_decay = lr_decay ** max(i - max_epoch, 0.0)
  sess.run(tf.assign(lr, learning_rate * new_lr_decay))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在每一个epoch的开始之前，LSTM的状态要被重置为零状态；在每一个迭代之后，LSTM状态都会被改变，所以要把最新的LSTM状态
作为下一个迭代的初始化状态。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  # 在每一个epoch之前，把所有LSTM状态设为零状态
  state1 = tl.layers.initialize_rnn_state(lstm1.initial_state)
  state2 = tl.layers.initialize_rnn_state(lstm2.initial_state)
  for step, (x, y) in enumerate(tl.iterate.ptb_iterator(train_data,
                                              batch_size, num_steps)):
      feed_dict = {input_data: x, targets: y,
                  lstm1.initial_state: state1,
                  lstm2.initial_state: state2,
                  }
      # 启用dropout
      feed_dict.update( network.all_drop )
      # 把新的状态作为下一个迭代的初始状态
      _cost, state1, state2, _ = sess.run([cost,
                                      lstm1.final_state,
                                      lstm2.final_state,
                                      train_op],
                                      feed_dict=feed_dict
                                      )
      costs += _cost; iters += num_steps
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;yu-ce&quot;&gt;预测&lt;&#x2F;h4&gt;
&lt;p&gt;在训练完模型之后，当我们预测下一个输出时，我们不需要考虑序列长度了，因此 &lt;code&gt;batch_size&lt;&#x2F;code&gt; 和 &lt;code&gt;num_steps&lt;&#x2F;code&gt; 都设为 1 。
然后，我们可以一步一步地输出下一个单词，而不是通过一序列的单词来输出一序列的单词。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  input_data_test = tf.placeholder(tf.int32, [1, 1])
  targets_test = tf.placeholder(tf.int32, [1, 1])
  ...
  network_test, lstm1_test, lstm2_test = inference(input_data_test,
                        is_training=False, num_steps=1, reuse=True)
  ...
  cost_test = loss_fn(network_test.outputs, targets_test, 1, 1)
  ...
  print(&amp;quot;Evaluation&amp;quot;)
  # 测试
  # go through the test set step by step, it will take a while.
  start_time = time.time()
  costs = 0.0; iters = 0
  # 与训练时一样，设置所有LSTM状态为零状态
  state1 = tl.layers.initialize_rnn_state(lstm1_test.initial_state)
  state2 = tl.layers.initialize_rnn_state(lstm2_test.initial_state)
  for step, (x, y) in enumerate(tl.iterate.ptb_iterator(test_data,
                                          batch_size=1, num_steps=1)):
      feed_dict = {input_data_test: x, targets_test: y,
                  lstm1_test.initial_state: state1,
                  lstm2_test.initial_state: state2,
                  }
      _cost, state1, state2 = sess.run([cost_test,
                                      lstm1_test.final_state,
                                      lstm2_test.final_state],
                                      feed_dict=feed_dict
                                      )
      costs += _cost; iters += 1
  test_perplexity = np.exp(costs &#x2F; iters)
  print(&amp;quot;Test Perplexity: %.3f took %.2fs&amp;quot; % (test_perplexity, time.time() - start_time))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;xia-yi-bu-2&quot;&gt;下一步？&lt;&#x2F;h3&gt;
&lt;p&gt;您已经明白了同步序列输入和序列输出（Synced sequence input and output）。
现在让我们思考下序列输入单一输出的情况（Sequence input and one output），
LSTM 也可以学会通过给定一序列输入如 “我来自北京，我会说..“ 来输出
一个单词 “中文”。&lt;&#x2F;p&gt;
&lt;p&gt;请仔细阅读并理解 &lt;code&gt;tutorial_generate_text.py&lt;&#x2F;code&gt; 的代码，它讲了如何加载一个已经训练好的词嵌套矩阵，
以及如何给定机器一个文档，让它来学习文字自动生成。&lt;&#x2F;p&gt;
&lt;p&gt;Karpathy的博客：
“(3) Sequence input (e.g. sentiment analysis where a given sentence is
classified as expressing positive or negative sentiment). “&lt;&#x2F;p&gt;
&lt;h2 id=&quot;yun-xing-ji-qi-fan-yi-li-zi&quot;&gt;运行机器翻译例子&lt;&#x2F;h2&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  python tutorial_translate.py
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;该脚本将训练一个神经网络来把英文翻译成法文。
如果一切正常，您将看到：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;下载WMT英文-法文翻译数据库，包括训练集和测试集。&lt;&#x2F;li&gt;
&lt;li&gt;通过训练集创建英文和法文的词汇表。&lt;&#x2F;li&gt;
&lt;li&gt;把训练集和测试集的单词转换成数字ID表示。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  Prepare raw data
  Load or Download WMT English-to-French translation &amp;gt; wmt
  Training data : wmt&#x2F;giga-fren.release2
  Testing data : wmt&#x2F;newstest2013

  Create vocabularies
  Vocabulary of French : wmt&#x2F;vocab40000.fr
  Vocabulary of English : wmt&#x2F;vocab40000.en
  Creating vocabulary wmt&#x2F;vocab40000.fr from data wmt&#x2F;giga-fren.release2.fr
    processing line 100000
    processing line 200000
    processing line 300000
    processing line 400000
    processing line 500000
    processing line 600000
    processing line 700000
    processing line 800000
    processing line 900000
    processing line 1000000
    processing line 1100000
    processing line 1200000
    ...
    processing line 22500000
  Creating vocabulary wmt&#x2F;vocab40000.en from data wmt&#x2F;giga-fren.release2.en
    processing line 100000
    ...
    processing line 22500000

  ...

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;首先，我们从WMT’15网站上下载英语-法语翻译数据。训练数据和测试数据如下。
训练数据用于训练模型，测试数据用于评估该模型。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  wmt&#x2F;training-giga-fren.tar  &amp;lt;-- 英文－法文训练集 (2.6GB)
                                  giga-fren.release2.* 从该文件解压出来
  wmt&#x2F;dev-v2.tgz              &amp;lt;-- 多种语言的测试集 (21.4MB)
                                  newstest2013.* 从该文件解压出来

  wmt&#x2F;giga-fren.release2.fr   &amp;lt;-- 法文训练集 (4.57GB)
  wmt&#x2F;giga-fren.release2.en   &amp;lt;-- 英文训练集 (3.79GB)

  wmt&#x2F;newstest2013.fr         &amp;lt;-- 法文测试集 (393KB)
  wmt&#x2F;newstest2013.en         &amp;lt;-- 英文测试集 (333KB)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;所有 &lt;code&gt;giga-fren.release2.*&lt;&#x2F;code&gt; 是训练数据， &lt;code&gt;giga-fren.release2.fr&lt;&#x2F;code&gt; 内容如下：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  Il a transformé notre vie | Il a transformé la société | Son fonctionnement | La technologie, moteur du changement Accueil | Concepts | Enseignants | Recherche | Aperçu | Collaborateurs | Web HHCC | Ressources | Commentaires Musée virtuel du Canada
  Plan du site
  Rétroaction
  Crédits
  English
  Qu’est-ce que la lumière?
  La découverte du spectre de la lumière blanche Des codes dans la lumière Le spectre électromagnétique Les spectres d’émission Les spectres d’absorption Les années-lumière La pollution lumineuse
  Le ciel des premiers habitants La vision contemporaine de l&amp;#39;Univers L’astronomie pour tous
  Bande dessinée
  Liens
  Glossaire
  Observatoires
  ...
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;giga-fren.release2.en&lt;&#x2F;code&gt; 内容如下，我们可以看到单词或者句子用 &lt;code&gt;|&lt;&#x2F;code&gt; 或 &lt;code&gt;\n&lt;&#x2F;code&gt; 来分隔。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  Changing Lives | Changing Society | How It Works | Technology Drives Change Home | Concepts | Teachers | Search | Overview | Credits | HHCC Web | Reference | Feedback Virtual Museum of Canada Home Page
  Site map
  Feedback
  Credits
  Français
  What is light ?
  The white light spectrum Codes in the light The electromagnetic spectrum Emission spectra Absorption spectra Light-years Light pollution
  The sky of the first inhabitants A contemporary vison of the Universe Astronomy for everyone
  Cartoon
  Links
  Glossary
  Observatories
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;测试数据 &lt;code&gt;newstest2013.en&lt;&#x2F;code&gt; 和 &lt;code&gt;newstest2013.fr&lt;&#x2F;code&gt; 如下所示：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  newstest2013.en :
  A Republican strategy to counter the re-election of Obama
  Republican leaders justified their policy by the need to combat electoral fraud.
  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.

  newstest2013.fr :
  Une stratégie républicaine pour contrer la réélection d&amp;#39;Obama
  Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
  Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;下载完数据之后，开始创建词汇表文件。
从训练数据 &lt;code&gt;giga-fren.release2.fr&lt;&#x2F;code&gt; 和 &lt;code&gt;giga-fren.release2.en&lt;&#x2F;code&gt;创建 &lt;code&gt;vocab40000.fr&lt;&#x2F;code&gt; 和 &lt;code&gt;vocab40000.en&lt;&#x2F;code&gt; 这个过程需要较长一段时间，数字 &lt;code&gt;40000&lt;&#x2F;code&gt; 代表了词汇库的大小。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;vocab40000.fr&lt;&#x2F;code&gt; (381KB) 按下列所示地按每行一个单词的方式存储（one-item-per-line）。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  _PAD
  _GO
  _EOS
  _UNK
  de
  ,
  .
  &amp;#39;
  la
  et
  des
  les
  à
  le
  du
  l
  en
  )
  d
  0
  (
  00
  pour
  dans
  un
  que
  une
  sur
  au
  0000
  a
  par

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;vocab40000.en&lt;&#x2F;code&gt; (344KB) 也是如此。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  _PAD
  _GO
  _EOS
  _UNK
  the
  .
  ,
  of
  and
  to
  in
  a
  )
  (
  0
  for
  00
  that
  is
  on
  The
  0000
  be
  by
  with
  or
  :
  as
  &amp;quot;
  000
  are
  ;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;接着我们开始创建英文和法文的数字化（ID）训练集和测试集。这也要较长一段时间。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  Tokenize data
  Tokenizing data in wmt&#x2F;giga-fren.release2.fr  &amp;lt;-- Training data of French
    tokenizing line 100000
    tokenizing line 200000
    tokenizing line 300000
    tokenizing line 400000
    ...
    tokenizing line 22500000
  Tokenizing data in wmt&#x2F;giga-fren.release2.en  &amp;lt;-- Training data of English
    tokenizing line 100000
    tokenizing line 200000
    tokenizing line 300000
    tokenizing line 400000
    ...
    tokenizing line 22500000
  Tokenizing data in wmt&#x2F;newstest2013.fr        &amp;lt;-- Testing data of French
  Tokenizing data in wmt&#x2F;newstest2013.en        &amp;lt;-- Testing data of English
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;最后，我们所有的文件如下所示：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  wmt&#x2F;training-giga-fren.tar  &amp;lt;-- 英文－法文训练集 (2.6GB)
                                  giga-fren.release2.* 从该文件解压出来
  wmt&#x2F;dev-v2.tgz              &amp;lt;-- 多种语言的测试集 (21.4MB)
                                  newstest2013.* 从该文件解压出来

  wmt&#x2F;giga-fren.release2.fr   &amp;lt;-- 法文训练集 (4.57GB)
  wmt&#x2F;giga-fren.release2.en   &amp;lt;-- 英文训练集 (3.79GB)

  wmt&#x2F;newstest2013.fr         &amp;lt;-- 法文测试集 (393KB)
  wmt&#x2F;newstest2013.en         &amp;lt;-- 英文测试集 (333KB)

  wmt&#x2F;vocab40000.fr           &amp;lt;-- 法文词汇表 (381KB)
  wmt&#x2F;vocab40000.en           &amp;lt;-- 英文词汇表 (344KB)

  wmt&#x2F;giga-fren.release2.ids40000.fr   &amp;lt;-- 数字化法文训练集 (2.81GB)
  wmt&#x2F;giga-fren.release2.ids40000.en   &amp;lt;-- 数字化英文训练集 (2.38GB)

  wmt&#x2F;newstest2013.ids40000.fr         &amp;lt;-- 数字化法文训练集 (268KB)
  wmt&#x2F;newstest2013.ids40000.en         &amp;lt;-- 数字化英文测试集 (232KB)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;现在，把数字化的数据读入buckets中，并计算不同buckets中数据样本的个数。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  Read development (test) data into buckets
  dev data: (5, 10) [[13388, 4, 949], [23113, 8, 910, 2]]
  en word_ids: [13388, 4, 949]
  en context: [b&amp;#39;Preventing&amp;#39;, b&amp;#39;the&amp;#39;, b&amp;#39;disease&amp;#39;]
  fr word_ids: [23113, 8, 910, 2]
  fr context: [b&amp;#39;Pr\xc3\xa9venir&amp;#39;, b&amp;#39;la&amp;#39;, b&amp;#39;maladie&amp;#39;, b&amp;#39;_EOS&amp;#39;]

  Read training data into buckets (limit: 0)
    reading data line 100000
    reading data line 200000
    reading data line 300000
    reading data line 400000
    reading data line 500000
    reading data line 600000
    reading data line 700000
    reading data line 800000
    ...
    reading data line 22400000
    reading data line 22500000
  train_bucket_sizes: [239121, 1344322, 5239557, 10445326]
  train_total_size: 17268326.0
  train_buckets_scale: [0.013847375825543252, 0.09169638099257565, 0.3951164693091849, 1.0]
  train data: (5, 10) [[1368, 3344], [1089, 14, 261, 2]]
  en word_ids: [1368, 3344]
  en context: [b&amp;#39;Site&amp;#39;, b&amp;#39;map&amp;#39;]
  fr word_ids: [1089, 14, 261, 2]
  fr context: [b&amp;#39;Plan&amp;#39;, b&amp;#39;du&amp;#39;, b&amp;#39;site&amp;#39;, b&amp;#39;_EOS&amp;#39;]

  the num of training data in each buckets: [239121, 1344322, 5239557, 10445326]
  the num of training data: 17268326
  train_buckets_scale: [0.013847375825543252, 0.09169638099257565, 0.3951164693091849, 1.0]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;最后开始训练模型，当 &lt;code&gt;steps_per_checkpoint = 10&lt;&#x2F;code&gt; 时，您将看到：&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;steps_per_checkpoint = 10&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  Create Embedding Attention Seq2seq Model

  global step 10 learning rate 0.5000 step-time 22.26 perplexity 12761.50
    eval: bucket 0 perplexity 5887.75
    eval: bucket 1 perplexity 3891.96
    eval: bucket 2 perplexity 3748.77
    eval: bucket 3 perplexity 4940.10
  global step 20 learning rate 0.5000 step-time 20.38 perplexity 28761.36
    eval: bucket 0 perplexity 10137.01
    eval: bucket 1 perplexity 12809.90
    eval: bucket 2 perplexity 15758.65
    eval: bucket 3 perplexity 26760.93
  global step 30 learning rate 0.5000 step-time 20.64 perplexity 6372.95
    eval: bucket 0 perplexity 1789.80
    eval: bucket 1 perplexity 1690.00
    eval: bucket 2 perplexity 2190.18
    eval: bucket 3 perplexity 3808.12
  global step 40 learning rate 0.5000 step-time 16.10 perplexity 3418.93
    eval: bucket 0 perplexity 4778.76
    eval: bucket 1 perplexity 3698.90
    eval: bucket 2 perplexity 3902.37
    eval: bucket 3 perplexity 22612.44
  global step 50 learning rate 0.5000 step-time 14.84 perplexity 1811.02
    eval: bucket 0 perplexity 644.72
    eval: bucket 1 perplexity 759.16
    eval: bucket 2 perplexity 984.18
    eval: bucket 3 perplexity 1585.68
  global step 60 learning rate 0.5000 step-time 19.76 perplexity 1580.55
    eval: bucket 0 perplexity 1724.84
    eval: bucket 1 perplexity 2292.24
    eval: bucket 2 perplexity 2698.52
    eval: bucket 3 perplexity 3189.30
  global step 70 learning rate 0.5000 step-time 17.16 perplexity 1250.57
    eval: bucket 0 perplexity 298.55
    eval: bucket 1 perplexity 502.04
    eval: bucket 2 perplexity 645.44
    eval: bucket 3 perplexity 604.29
  global step 80 learning rate 0.5000 step-time 18.50 perplexity 793.90
    eval: bucket 0 perplexity 2056.23
    eval: bucket 1 perplexity 1344.26
    eval: bucket 2 perplexity 767.82
    eval: bucket 3 perplexity 649.38
  global step 90 learning rate 0.5000 step-time 12.61 perplexity 541.57
    eval: bucket 0 perplexity 180.86
    eval: bucket 1 perplexity 350.99
    eval: bucket 2 perplexity 326.85
    eval: bucket 3 perplexity 383.22
  global step 100 learning rate 0.5000 step-time 18.42 perplexity 471.12
    eval: bucket 0 perplexity 216.63
    eval: bucket 1 perplexity 348.96
    eval: bucket 2 perplexity 318.20
    eval: bucket 3 perplexity 389.92
  global step 110 learning rate 0.5000 step-time 18.39 perplexity 474.89
    eval: bucket 0 perplexity 8049.85
    eval: bucket 1 perplexity 1677.24
    eval: bucket 2 perplexity 936.98
    eval: bucket 3 perplexity 657.46
  global step 120 learning rate 0.5000 step-time 18.81 perplexity 832.11
    eval: bucket 0 perplexity 189.22
    eval: bucket 1 perplexity 360.69
    eval: bucket 2 perplexity 410.57
    eval: bucket 3 perplexity 456.40
  global step 130 learning rate 0.5000 step-time 20.34 perplexity 452.27
    eval: bucket 0 perplexity 196.93
    eval: bucket 1 perplexity 655.18
    eval: bucket 2 perplexity 860.44
    eval: bucket 3 perplexity 1062.36
  global step 140 learning rate 0.5000 step-time 21.05 perplexity 847.11
    eval: bucket 0 perplexity 391.88
    eval: bucket 1 perplexity 339.09
    eval: bucket 2 perplexity 320.08
    eval: bucket 3 perplexity 376.44
  global step 150 learning rate 0.4950 step-time 15.53 perplexity 590.03
    eval: bucket 0 perplexity 269.16
    eval: bucket 1 perplexity 286.51
    eval: bucket 2 perplexity 391.78
    eval: bucket 3 perplexity 485.23
  global step 160 learning rate 0.4950 step-time 19.36 perplexity 400.80
    eval: bucket 0 perplexity 137.00
    eval: bucket 1 perplexity 198.85
    eval: bucket 2 perplexity 276.58
    eval: bucket 3 perplexity 357.78
  global step 170 learning rate 0.4950 step-time 17.50 perplexity 541.79
    eval: bucket 0 perplexity 1051.29
    eval: bucket 1 perplexity 626.64
    eval: bucket 2 perplexity 496.32
    eval: bucket 3 perplexity 458.85
  global step 180 learning rate 0.4950 step-time 16.69 perplexity 400.65
    eval: bucket 0 perplexity 178.12
    eval: bucket 1 perplexity 299.86
    eval: bucket 2 perplexity 294.84
    eval: bucket 3 perplexity 296.46
  global step 190 learning rate 0.4950 step-time 19.93 perplexity 886.73
    eval: bucket 0 perplexity 860.60
    eval: bucket 1 perplexity 910.16
    eval: bucket 2 perplexity 909.24
    eval: bucket 3 perplexity 786.04
  global step 200 learning rate 0.4901 step-time 18.75 perplexity 449.64
    eval: bucket 0 perplexity 152.13
    eval: bucket 1 perplexity 234.41
    eval: bucket 2 perplexity 249.66
    eval: bucket 3 perplexity 285.95
  ...
  global step 980 learning rate 0.4215 step-time 18.31 perplexity 208.74
    eval: bucket 0 perplexity 78.45
    eval: bucket 1 perplexity 108.40
    eval: bucket 2 perplexity 137.83
    eval: bucket 3 perplexity 173.53
  global step 990 learning rate 0.4173 step-time 17.31 perplexity 175.05
    eval: bucket 0 perplexity 78.37
    eval: bucket 1 perplexity 119.72
    eval: bucket 2 perplexity 169.11
    eval: bucket 3 perplexity 202.89
  global step 1000 learning rate 0.4173 step-time 15.85 perplexity 174.33
    eval: bucket 0 perplexity 76.52
    eval: bucket 1 perplexity 125.97
    eval: bucket 2 perplexity 150.13
    eval: bucket 3 perplexity 181.07
  ...
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;经过350000轮训练模型之后，您可以将代码中的 &lt;code&gt;main_train()&lt;&#x2F;code&gt; 换为 &lt;code&gt;main_decode()&lt;&#x2F;code&gt; 来使用训练好的翻译器，
您输入一个英文句子，程序将输出一个对应的法文句子。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  Reading model parameters from wmt&#x2F;translate.ckpt-350000
  &amp;gt;  Who is the president of the United States?
  Qui est le président des États-Unis ?
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;li-jie-ji-qi-fan-yi&quot;&gt;理解机器翻译&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;seq2seq&quot;&gt;Seq2seq&lt;&#x2F;h3&gt;
&lt;p&gt;序列到序列模型（Seq2seq）通常被用来转换一种语言到另一种语言。
但实际上它能用来做很多您可能无法想象的事情，比如我们可以将一个长的句子翻译成意思一样但短且简单的句子，
再比如，从莎士比亚的语言翻译成现代英语。若用上卷积神经网络(CNN)的话，我们能将视频翻译成句子，则自动看一段视频给出该视频的文字描述（Video captioning）。&lt;&#x2F;p&gt;
&lt;p&gt;如果你只是想用 Seq2seq，你只需要考虑训练集的格式，比如如何切分单词、如何数字化单词等等。
所以，在本教程中，我们将讨论很多如何整理训练集。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ji-chu&quot;&gt;基础&lt;&#x2F;h4&gt;
&lt;p&gt;序列到序列模型是一种多对多（Many to many）的模型，但与PTB教程中的同步序列输入与输出(Synced sequence input and output）不一样，Seq2seq是在输入了整个序列之后，才开始输出新的序列（非同步）。
该教程用了下列两种最新的方法来提高准确度：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;把输入序列倒转输入（Reversing the inputs）&lt;&#x2F;li&gt;
&lt;li&gt;注意机制（Attention mechanism）&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;为了要加快训练速度，我们使用了：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;softmax 抽样（Sampled softmax）&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Karpathy的博客是这样描述Seq2seq的：“(4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French).”&lt;&#x2F;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;80%&quot; src=&quot;.&#x2F;basic_seq2seq.png&quot; &#x2F;&gt;
&lt;br&gt;
&lt;&#x2F;p&gt;
&lt;p&gt;如上图所示，编码器输入（encoder input），解码器输入（decoder input）以及输出目标（targets）如下：&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;   encoder_input =  A    B    C
   decoder_input =  &amp;lt;go&amp;gt; W    X    Y    Z
   targets       =  W    X    Y    Z    &amp;lt;eos&amp;gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Note：在代码实现中，targets的长度比decoder_input的长度小一，更多实现细节将在下文说明。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h4 id=&quot;wen-xian&quot;&gt;文献&lt;&#x2F;h4&gt;
&lt;p&gt;该英语-法语的机器翻译例子使用了多层递归神经网络以及注意机制。
该模型和如下论文中一样：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1412.7449&quot;&gt;Grammar as a Foreign Language&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;该例子采用了 softmax 抽样（sampled softmax）来解决当词汇表很大时计算量大的问题。
在该例子中，&lt;code&gt;target_vocab_size=4000&lt;&#x2F;code&gt; ，若词汇量小于 &lt;code&gt;512&lt;&#x2F;code&gt; 时用普通的softmax cross entropy即可。
Softmax 抽样在这篇论文的第三小节中描述:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1412.2007&quot;&gt;On Using Very Large Target Vocabulary for Neural Machine Translation&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;如下文章讲述了把输入序列倒转（Reversing the inputs）和多层神递归神经网络用在Seq2seq的翻译应用非常成功：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.3215&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;如下文章讲述了注意机制（Attention Mechanism）让解码器可以更直接地得到每一个输入的信息：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;如下文章讲述了另一种Seq2seq模型，则使用双向编码器（Bi-directional encoder）：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;shi-xian-xi-jie&quot;&gt;实现细节&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;bucketing-and-padding&quot;&gt;Bucketing and Padding&lt;&#x2F;h4&gt;
&lt;p&gt;Bucketing 是一种能有效处理不同句子长度的方法，为什么使用Bucketing，在 &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;42057513&quot;&gt;知乎&lt;&#x2F;a&gt;上已经有很好的回答了。&lt;&#x2F;p&gt;
&lt;p&gt;当将英文翻译成法文的时，我们有不同长度的英文句子输入（长度为 &lt;code&gt;L1 &lt;&#x2F;code&gt; ），以及不同长度的法文句子输出，（长度为 &lt;code&gt;L2&lt;&#x2F;code&gt; ）。
我们原则上要建立每一种长度的可能性，则有很多个 &lt;code&gt;(L1, L2+1)&lt;&#x2F;code&gt; ，其中 &lt;code&gt;L2&lt;&#x2F;code&gt; 加一是因为有 GO 标志符。&lt;&#x2F;p&gt;
&lt;p&gt;为了减少 bucket 的数量以及为句子找到最合适的 bucket，若 bucket 大于句子的长度，我们则使用 PAD 标志符填充之。&lt;&#x2F;p&gt;
&lt;p&gt;为了提高效率，我们只使用几个 bucket，然后使用 padding 来让句子匹配到最相近的 bucket 中。
在该例子中，我们使用如下 4 个 buckets。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;如果输入的是一个有 &lt;code&gt;3&lt;&#x2F;code&gt; 个单词的英文句子，对应的法文输出有 &lt;code&gt;6&lt;&#x2F;code&gt; 个单词，
那么改数据将被放在第一个 bucket 中并且把 encoder inputs 和 decoder inputs 通过 padding 来让其长度变成 &lt;code&gt;5&lt;&#x2F;code&gt; 和 &lt;code&gt;10&lt;&#x2F;code&gt; 。
如果我们有 &lt;code&gt;8&lt;&#x2F;code&gt; 个单词的英文句子，及 &lt;code&gt;18&lt;&#x2F;code&gt; 个单词的法文句子，它们会被放到 &lt;code&gt;(20, 25)&lt;&#x2F;code&gt; 的 bucket 中。&lt;&#x2F;p&gt;
&lt;p&gt;换句话说，bucket &lt;code&gt;(I,O)&lt;&#x2F;code&gt; 是 (encoder_input_size，decoder_inputs_size) 。&lt;&#x2F;p&gt;
&lt;p&gt;给出一对数字化训练样本 &lt;code&gt;[[&amp;quot;I&amp;quot;, &amp;quot;go&amp;quot;, &amp;quot;.&amp;quot;], [&amp;quot;Je&amp;quot;, &amp;quot;vais&amp;quot;, &amp;quot;.&amp;quot;]]&lt;&#x2F;code&gt; ，我们把它转换为 &lt;code&gt;(5,10)&lt;&#x2F;code&gt; 。
编码器输入（encoder inputs）的训练数据为  &lt;code&gt;[PAD PAD &amp;quot;.&amp;quot; &amp;quot;go&amp;quot; &amp;quot;I&amp;quot;]&lt;&#x2F;code&gt; ，而解码器的输入（decoder inputs）为 &lt;code&gt;[GO &amp;quot;Je&amp;quot; &amp;quot;vais&amp;quot; &amp;quot;.&amp;quot; EOS PAD PAD PAD PAD PAD]&lt;&#x2F;code&gt; 。
而输出目标（targets）是解码器输入（decoder inputs）平移一位。 &lt;code&gt;target_weights&lt;&#x2F;code&gt; 是输出目标（targets）的掩码。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  bucket = (I, O) = (5, 10)
  encoder_inputs = [PAD PAD &amp;quot;.&amp;quot; &amp;quot;go&amp;quot; &amp;quot;I&amp;quot;]                       &amp;lt;-- 5  x batch_size
  decoder_inputs = [GO &amp;quot;Je&amp;quot; &amp;quot;vais&amp;quot; &amp;quot;.&amp;quot; EOS PAD PAD PAD PAD PAD] &amp;lt;-- 10 x batch_size
  target_weights = [1   1     1     1   0 0 0 0 0 0 0]          &amp;lt;-- 10 x batch_size
  targets        = [&amp;quot;Je&amp;quot; &amp;quot;vais&amp;quot; &amp;quot;.&amp;quot; EOS PAD PAD PAD PAD PAD]    &amp;lt;-- 9  x batch_size
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;在该代码中，一个句子是由一个列向量表示，假设 &lt;code&gt;batch_size = 3&lt;&#x2F;code&gt; ， &lt;code&gt;bucket = (5, 10)&lt;&#x2F;code&gt; ，训练集如下所示。&lt;&#x2F;p&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;  encoder_inputs    decoder_inputs    target_weights    targets
  0    0    0       1    1    1       1    1    1       87   71   16748
  0    0    0       87   71   16748   1    1    1       2    3    14195
  0    0    0       2    3    14195   0    1    1       0    2    2
  0    0    3233    0    2    2       0    0    0       0    0    0
  3    698  4061    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0       0    0    0
                    0    0    0       0    0    0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;其中 0 : _PAD    1 : _GO     2 : _EOS      3 : _UNK&lt;&#x2F;p&gt;
&lt;p&gt;在训练过程中，解码器输入是目标，而在预测过程中，下一个解码器的输入是最后一个解码器的输出。&lt;&#x2F;p&gt;
&lt;p&gt;在训练过程中，编码器输入（decoder inputs）就是目标输出（targets）；
当使用模型时，下一个编码器输入（decoder inputs）是上一个解码器输出（ decoder output）。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;te-shu-biao-zhi-fu-biao-dian-fu-hao-yu-a-la-bo-shu-zi&quot;&gt;特殊标志符、标点符号与阿拉伯数字&lt;&#x2F;h4&gt;
&lt;p&gt;该例子中的特殊标志符是：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; class=&quot;language-python z-code&quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;z-source z-python&quot;&gt;  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_PAD&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-storage z-type z-string z-python&quot;&gt;b&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;_PAD&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_GO&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-storage z-type z-string z-python&quot;&gt;b&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;_GO&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_EOS&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-storage z-type z-string z-python&quot;&gt;b&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;_EOS&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_UNK&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-storage z-type z-string z-python&quot;&gt;b&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-string z-python&quot;&gt;&lt;span class=&quot;z-string z-quoted z-double z-python&quot;&gt;_UNK&lt;span class=&quot;z-punctuation z-definition z-string z-end z-python&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;PAD_ID&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;0&lt;&#x2F;span&gt;      &lt;span class=&quot;z-keyword z-operator z-comparison z-python&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt;&lt;span class=&quot;z-keyword z-operator z-arithmetic z-python&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-function-call z-python&quot;&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-function z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;index&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-begin z-python&quot;&gt;(&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;row&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;number&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-arguments z-end z-python&quot;&gt;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-logical z-python&quot;&gt;in&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-meta z-generic-name z-python&quot;&gt;vocabulary&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;GO_ID&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;1&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;EOS_ID&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;2&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;UNK_ID&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-python&quot;&gt;3&lt;&#x2F;span&gt;
  &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_START_VOCAB&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-keyword z-operator z-assignment z-python&quot;&gt;=&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-sequence z-list z-python&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-begin z-python&quot;&gt;[&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_PAD&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_GO&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_EOS&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-python&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-qualified-name z-python&quot;&gt;&lt;span class=&quot;z-variable z-other z-constant z-python&quot;&gt;_UNK&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-sequence z-end z-python&quot;&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre class=&quot;z-code&quot;&gt;&lt;code&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;          ID号    意义
  _PAD    0       Padding, empty word
  _GO     1       decoder_inputs 的第一个元素
  _EOS    2       targets 的结束符
  _UNK    3       不明单词（Unknown word），没有在词汇表出现的单词被标记为3
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;对于阿拉伯数字，建立词汇表时与数字化数据集时的 &lt;code&gt;normalize_digits&lt;&#x2F;code&gt; 必须是一致的，若
&lt;code&gt;normalize_digits=True&lt;&#x2F;code&gt; 所有阿拉伯数字都将被 &lt;code&gt;0&lt;&#x2F;code&gt; 代替。比如 &lt;code&gt;123&lt;&#x2F;code&gt; 被 &lt;code&gt;000&lt;&#x2F;code&gt; 代替，&lt;code&gt;9&lt;&#x2F;code&gt; 被 &lt;code&gt;0&lt;&#x2F;code&gt;代替
，&lt;code&gt;1990-05&lt;&#x2F;code&gt; 被 &lt;code&gt;0000-00` 代替，最后 &lt;&#x2F;code&gt;000&lt;code&gt;，&lt;&#x2F;code&gt;0&lt;code&gt;，&lt;&#x2F;code&gt;0000-00&lt;code&gt;等将在词汇库中(看&lt;&#x2F;code&gt;vocab40000.en`` )。&lt;&#x2F;p&gt;
&lt;p&gt;反之，如果 &lt;code&gt;normalize_digits=False&lt;&#x2F;code&gt; ，不同的阿拉伯数字将会放入词汇表中，那么词汇表就变得十分大了。
本例子中寻找阿拉伯数字使用的正则表达式是 &lt;code&gt;_DIGIT_RE = re.compile(br&amp;quot;\d&amp;quot;)&lt;&#x2F;code&gt; 。(详见 &lt;code&gt;tl.nlp.create_vocabulary()&lt;&#x2F;code&gt; 和 ``tl.nlp.data_to_token_ids()` )&lt;&#x2F;p&gt;
&lt;p&gt;对于分离句子成独立单词，本例子使用正则表达式 &lt;code&gt;_WORD_SPLIT = re.compile(b&amp;quot;([.,!?\&amp;quot;&#x27;:;)(])&amp;quot;)&lt;&#x2F;code&gt; ，
这意味着使用这几个标点符号 &lt;code&gt;[ . , ! ? &amp;quot; &#x27; : ; ) ( ]&lt;&#x2F;code&gt; 以及空格来分割句子，详情请看 &lt;code&gt;tl.nlp.basic_tokenizer()&lt;&#x2F;code&gt; 。这个分割方法是 &lt;code&gt;tl.nlp.create_vocabulary()&lt;&#x2F;code&gt; 和  &lt;code&gt;tl.nlp.data_to_token_ids()&lt;&#x2F;code&gt; 的默认方法。&lt;&#x2F;p&gt;
&lt;p&gt;所有的标点符号，比如 &lt;code&gt;. , ) (&lt;&#x2F;code&gt; 在英文和法文数据库中都会被全部保留下来。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;softmax-chou-yang-sampled-softmax&quot;&gt;Softmax 抽样 (Sampled softmax)&lt;&#x2F;h4&gt;
&lt;p&gt;softmax抽样是一种词汇表很大（Softmax 输出很多）的时候用来降低损失（cost）计算量的方法。
与从所有输出中计算 cross-entropy 相比，这个方法只从 &lt;code&gt;num_samples&lt;&#x2F;code&gt; 个输出中计算 cross-entropy。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;sun-shi-he-geng-xin-han-shu&quot;&gt;损失和更新函数&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;code&gt;EmbeddingAttentionSeq2seqWrapper&lt;&#x2F;code&gt; 内部实现了 SGD optimizer。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;xia-yi-bu-3&quot;&gt;下一步？&lt;&#x2F;h3&gt;
&lt;p&gt;您可以尝试其他应用。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;fan-yi-dui-zhao&quot;&gt;翻译对照&lt;&#x2F;h2&gt;
&lt;p&gt;Stacked Denosing Autoencoder 堆栈式降噪自编码器&lt;&#x2F;p&gt;
&lt;p&gt;Word Embedding               词嵌套、词嵌入&lt;&#x2F;p&gt;
&lt;p&gt;Iteration                    迭代&lt;&#x2F;p&gt;
&lt;p&gt;Natural Language Processing  自然语言处理&lt;&#x2F;p&gt;
&lt;p&gt;Sparse                       稀疏的&lt;&#x2F;p&gt;
&lt;p&gt;Cost function                损失函数&lt;&#x2F;p&gt;
&lt;p&gt;Regularization               规则化、正则化&lt;&#x2F;p&gt;
&lt;p&gt;Tokenization                 数字化&lt;&#x2F;p&gt;
&lt;p&gt;Truncated backpropagation    截断反向传播&lt;&#x2F;p&gt;
&lt;h2 id=&quot;geng-duo-xin-xi&quot;&gt;更多信息&lt;&#x2F;h2&gt;
&lt;p&gt;TensorLayer 还能做什么？请继续阅读本文档。&lt;&#x2F;p&gt;
&lt;p&gt;最后，API 参考列表和说明如下：&lt;&#x2F;p&gt;
&lt;p&gt;layers (&lt;code&gt;tensorlayer.layers&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;activation (&lt;code&gt;tensorlayer.activation&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;natural language processing (&lt;code&gt;tensorlayer.nlp&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;reinforcement learning (&lt;code&gt;tensorlayer.rein&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;cost expressions and regularizers (&lt;code&gt;tensorlayer.cost&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;load and save files (&lt;code&gt;tensorlayer.files&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;operating system (&lt;code&gt;tensorlayer.ops&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;helper functions (&lt;code&gt;tensorlayer.utils&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;visualization (&lt;code&gt;tensorlayer.visualize&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;iteration functions (&lt;code&gt;tensorlayer.iterate&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
&lt;p&gt;preprocessing functions (&lt;code&gt;tensorlayer.prepro&lt;&#x2F;code&gt;),&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
